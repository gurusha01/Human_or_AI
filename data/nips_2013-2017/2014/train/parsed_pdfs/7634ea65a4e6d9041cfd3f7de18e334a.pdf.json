{
  "name" : "7634ea65a4e6d9041cfd3f7de18e334a.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bayesian Inference for Structured Spike and Slab Priors",
    "authors" : [ "Michael Riis Andersen", "Ole Winther" ],
    "emails" : [ "lkh}@dtu.dk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Consider a linear inverse problem of the form:\ny = Ax+ e, (1)\nwhere A ∈ RN×D is the measurement matrix, y ∈ RN is the measurement vector, x ∈ RD is the desired solution and e ∈ RN is a vector of corruptive noise. The field of sparse signal recovery deals with the task of reconstructing the sparse solution x from (A,y) in the ill-posed regime where N < D. In many applications it is beneficial to encourage a structured sparsity pattern rather than independent sparsity. In this paper we consider a model for exploiting a priori information on the sparsity pattern, which has applications in many different fields, e.g., structured sparse PCA [1], background subtraction [2] and neuroimaging [3].\nIn the framework of probabilistic modelling sparsity can be enforced using so-called sparsity promoting priors, which conventionally has the following form\np(x ∣∣λ) = D∏\ni=1\np(xi ∣∣λ), (2)\nwhere p(xi ∣∣λ) is the marginal prior on xi and λ is a fixed hyperparameter controlling the degree of sparsity. Examples of such sparsity promoting priors include the Laplace prior (LASSO [4]), and the Bernoulli-Gaussian prior (the spike and slab model [5]). The main advantage of this formulation is that the inference schemes become relatively simple due to the fact that the prior factorizes over the variables xi. However, this fact also implies that the models cannot encode any prior knowledge of the structure of the sparsity pattern.\nOne approach to model a richer sparsity structure is the so-called group sparsity approach, where the set of variables x has been partitioned into groups beforehand. This\napproach has been extensively developed for the `1 minimization community, i.e. group LASSO, sparse group LASSO [6] and graph LASSO [7]. Let G be a partition of the set of variables into G groups. A Bayesian equivalent of group sparsity is the group spike and slab model [8], which takes the form\np(x ∣∣z) = G∏\ng=1\n[ (1− zg) δ (xg) + zgN ( xg ∣∣0, τIg] , p(z∣∣λ) = G∏\ng=1\nBernoulli ( zg ∣∣λg) , (3)\nwhere z ∈ [0, 1]G are binary support variables indicating whether the variables in different groups are active or not. Other relevant work includes [9] and [10]. Another more flexible approach is to use a Markov random field (MRF) as prior for the binary variables [2].\nRelated to the MRF-formulation, we propose a novel model called the Structured Spike and Slab model. This model allows us to encode a priori information of the sparsity pattern into the model using generic covariance functions rather than through clique potentials as for the MRF-formulation [2]. Furthermore, we provide a Bayesian inference scheme based on expectation propagation for the proposed model."
    }, {
      "heading" : "2 The structured spike and slab prior",
      "text" : "We propose a hierarchical prior of the following form:\np(x ∣∣γ) = D∏\ni=1\np(xi ∣∣g(γi)), p(γ) = N (γ∣∣µ0,Σ0) , (4)\nwhere g : R → R is a suitable injective transformation. That is, we impose a Gaussian process [11] as a prior on the parameters γi. Using this parametrization, prior knowledge of the structure of the sparsity pattern can be encoded using µ0 and Σ0. The mean value µ0 controls the prior belief of the support and the covariance matrix determines the prior correlation of the support. In the remainder of this paper we restrict p(xi|g(γi)) to be a spike and slab model, i.e.\np(xi ∣∣zi) = (1− zi)δ(xi) + ziN (xi∣∣0, τ0) , zi ∼ Ber (g(γi)) . (5)\nThis formulation clearly fits into eq. (4) when zi is marginalized out. Furthermore, we will assume that g is the standard Normal CDF, i.e. g(x) = φ(x). Using this formulation, the marginal prior probability of the i’th weight being active is given by:\np(zi = 1) = ∫ p(zi = 1 ∣∣γi)p(γi)dγi = ∫ φ(γi)N (γi∣∣µi,Σii)dγi = φ( µi√ 1 + Σii ) . (6)\nThis implies that the probability of zi = 1 is 0.5 when µi = 0 as expected. In contrast to the `1-based methods and the MRF-priors, the Gaussian process formulation makes it easy to generate samples from the model. Figures 1(a), 1(b) each show three realizations of the support from the prior using a squared exponential kernel of the form: Σij = 50 exp(− (i− j)2 /2s2) and µi is fixed such that the expected level of sparsity is 10%. It is seen that when the scale, s, is small, the support consists of scattered spikes. As the scale increases, the support of the signals becomes more contiguous and clustered, where the sizes of the clusters increase with the scale.\nTo gain insight into the relationship between γ and z, we consider the two dimensional system with µi = 0 and the following covariance structure\nΣ0 = κ [ 1 ρ ρ 1 ] , κ > 0. (7)\nThe correlation between z1 and z2 is then computed as a function of ρ and κ by sampling. The resulting curves in Figure 1(c) show that the desired correlation is an increasing function of ρ as expected. However, the figure also reveals that for ρ = 1, i.e. 100% correlation between the γ parameters, does not imply 100% correlation of the support variables z. This\nis due to the fact that there are two levels of uncertainty in the prior distribution of the support. That is, first we sample γ, and then we sample the support z conditioned on γ.\nThe proposed prior formulation extends easily to the multiple measurement vector (MMV) formulation [12, 13, 14], in which multiple linear inverse problems are solved simultaneously. The most straightforward way is to assume all problem instances share the same support variable, commonly known as joint sparsity [14]\np ( X ∣∣z) = T∏\nt=1 D∏ i=1 [ (1− zi)δ(xti) + ziN ( xti ∣∣0, τ)] , (8)\np(zi ∣∣γi) = Ber (zi∣∣φ(γi)) , (9) p(γ) = N ( γ ∣∣µ0,Σ0) , (10)\nwhere X = [ x1 . . . xT ] ∈ RD×T . The model can also be extended to problems, where the sparsity pattern changes in time\np ( X ∣∣z) = T∏\nt=1 D∏ i=1 [ (1− zti)δ(xti) + ztiN ( xti ∣∣0, τ)] , (11)\np(zti ∣∣γti ) = Ber (zti ∣∣φ(γti )) , (12)\np(γ1, ...,γT ) = N ( γ1 ∣∣µ0,Σ0) T∏\nt=2\nN ( γt ∣∣(1− α)µ0 + αγt−1, βΣ0) , (13)\nwhere the parameters 0 ≤ α ≤ 1 and β ≥ 0 controls the temporal dynamics of the support."
    }, {
      "heading" : "3 Bayesian inference using expectation propagation",
      "text" : "In this section we combine the structured spike and slab prior as given in eq. (5) with an isotropic Gaussian noise model and derive an inference algorithm based on expectation propagation. The likelihood function is p(y ∣∣x) = N (y∣∣Ax, σ20I) and the joint posterior distribution of interest thus becomes\np(x, z,γ ∣∣y) = 1 Z p(y ∣∣x)p(x∣∣z)p(z∣∣γ)p(γ) (14)\n= 1 Z N ( y ∣∣Ax, σ20I)︸ ︷︷ ︸ f1 D∏ i=1 [ (1− zi)δ(xi) + ziN ( xi ∣∣0, τ0)]︸ ︷︷ ︸\nf2\nD∏ i=1 Ber ( zi ∣∣φ (γi))︸ ︷︷ ︸\nf3\nN ( γ ∣∣µ0,Σ0)︸ ︷︷ ︸ f4 ,\nwhere Z is the normalization constant independent of x, z and γ. Unfortunately, the true posterior is intractable and therefore we have to settle for an approximation. In particular, we apply the framework of expectation propagation (EP) [15, 16], which is an iterative deterministic framework for approximating probability distributions using distributions from the exponential family. The algorithm proposed here can be seen as an extension of the work in [8].\nAs shown in eq. (14), the true posterior is a composition of 4 factors, i.e. fa for a = 1, .., 4. The terms f2 and f3 are further decomposed into D conditionally independent factors\nf2(x, z) = D∏ i=1 f2,i(xi, zi) = D∏ i=1 [ (1− zi)δ(xi) + ziN ( xi ∣∣0, τ0)] , (15)\nf3(z,γ) = D∏ i=1 f3,i(zi, γi) = D∏ i=1 Ber ( zi ∣∣φ (γi)) (16)\nThe idea is then to approximate each term in the true posterior density, i.e. fa, by simpler terms, i.e. f̃a for a = 1, .., 4. The resulting approximation Q (x, z,γ) then becomes\nQ (x, z,γ) = 1\nZEP 4∏ a=1 f̃a (x, z,γ) . (17)\nThe terms f̃1 and f̃4 can be computed exact. In fact, f̃4 is simply equal to the prior over γ and f̃1 is a multivariate Gaussian distribution with mean m̃1 and covariance matrix Ṽ1 determined by Ṽ −1 1 m̃1 = 1 σ2A Ty and Ṽ −11 = 1 σ2A TA. Therefore, we only have to approximate the factors f̃2 and f̃3 using EP. Note that the exact term f1 is a distribution of y conditioned on x, whereas the approximate term f̃1 is a function of x that depends on y through m̃1 and Ṽ1 etc. In order to take full advantage of the structure of the true posterior distribution, we will further assume that the terms f̃2 and f̃3 also are decomposed into D independent factors.\nThe EP scheme provides great flexibility in the choice of the approximating factors. This choice is a trade-off between analytical tractability and sufficient flexibility for capturing the important characteristics of the true density. Due to the product over the binary support variables {zi} for i = 1, .., D, the true density is highly multimodal. Finally, f2 couples the variables x and z, while f3 couples the variables z and γ. Based on these observations, we choose f̃2 and f̃3 to have the following forms\nf̃2 (x, z) ∝ D∏ i=1 N ( xi ∣∣m̃2,i, ṽ2,i) D∏ i=1 Ber ( zi ∣∣φ (γ̃2,i)) = N (x∣∣m̃2, Ṽ2) D∏ i=1 Ber ( zi ∣∣φ (γ̃2,i)) ,\nf̃3 (z,γ) ∝ D∏ i=1 Ber ( zi ∣∣φ (γ̃3,i)) D∏ i=1 N ( γi ∣∣µ̃3,i, σ̃3,i) = N (γ∣∣µ̃3, Σ̃3) D∏ i=1 Ber ( zi ∣∣φ (γ̃2,i)) ,\nwhere m̃2 = [m̃2,1, .., m̃2,D] T\n, Ṽ2 = diag (ṽ2,1, ..., ṽ2,D) and analogously for µ̃3 and Σ̃3. These choices lead to a joint variational approximation Q(x, z,γ) of the form\nQ (x, z,γ) = N ( x ∣∣m̃, Ṽ ) D∏\ni=1\nBer ( zi ∣∣g (γ̃i))N (γ∣∣µ̃, Σ̃) , (18)\nwhere the joint parameters are given by Ṽ = ( Ṽ −11 + Ṽ −1 2 )−1 , m̃ = Ṽ ( Ṽ −11 m̃1 + Ṽ −1 2 m̃2 ) (19)\nΣ̃ = ( Σ̃−13 + Σ̃ −1 4 )−1 , µ̃ = Σ̃ ( Σ̃−13 µ̃3 + Σ̃ −1 4 µ̃4 ) (20)\nγ̃j = φ −1\n[( (1− φ(γ̃2,j)) (1− φ(γ̃3,j))\nφ(γ̃2,j)φ(γ̃3,j) + 1\n)−1] , ∀j ∈ {1, .., D} . (21)\nwhere φ−1(x) is the probit function. The function in eq. (21) amounts to computing the product of two Bernoulli densities parametrized using φ (·)."
    }, {
      "heading" : "3.1 The EP algorithm",
      "text" : "Consider the update of the term f̃a,i for a given a and a given i, where f̃a = ∏ i f̃a,i. This update is performed by first removing the contribution of f̃a,i from the joint approximation by forming the so-called cavity distribution\nQ\\a,i ∝ Q f̃a,i\n(22)\nfollowed by the minimization of the Kullbach-Leibler [17] divergence between fa,iQ \\a,i and Qa,new w.r.t. Qa,new. For distributions within the exponential family, minimizing this form of KL divergence amounts to matching moments between fa,iQ \\2,i and Qa,new [15]. Finally,\nthe new update of f̃a,i is given by\nf̃a,i ∝ Qa,new\nQ\\a,i . (23)\nAfter all the individual approximation terms f̃a,i for a = 1, 2 and i = 1, .., D have been updated, the joint approximation is updated using eq. (19)-(21). To minimize the compu-\ntational load, we use parallel updates of f̃2,i [8] followed by parallel updates of f̃3,i rather\nthan the conventional sequential update scheme. Furthermore, due to the fact that f̃2 and f̃3 factorizes, we only need the marginals of the cavity distributions Q\n\\a,i and the marginals of the updated joint distributions Qa,new for a = 2, 3.\nComputing the cavity distributions and matching the moments are tedious, but straightforward. The moments of fa,iQ\n\\2,i require evaluation of the zeroth, first and second order moment of the distributions of the form φ(γi)N ( γi ∣∣µi,Σii). Derivation of analytical expressions for these moments can be found in [11]. See the supplementary material for more details. The proposed algorithm is summarized in figure 2. Note, that the EP framework also provides an approximation of the marginal likelihood [11], which can be useful for learning the hyperparameters of the model. Furthermore, the proposed inference scheme can easily be extended to the MMV formulation eq. (8)-(10) by introducing a f̃ t2,i for each time step t = 1, .., T ."
    }, {
      "heading" : "3.2 Computational details",
      "text" : "Most linear inverse problems of practical interest are high dimensional, i.e. D is large. It is therefore of interest to simplify the computational complexity of the algorithm as much as possible. The dominating operations in this algorithm are the inversions of the two D ×D covariance matrices in eq. (19) and eq. (20), and therefore the algorithm scales as O ( D3 ) .\nBut Ṽ1 has low rank and Ṽ2 is diagonal, and therefore we can apply the Woodbury matrix identity [18] to eq. (19) to get\nṼ = Ṽ2 − Ṽ2AT ( σ2oI +AṼ2A T )−1 AṼ2. (24)\nFor N < D, this scales as O ( ND2 ) , where N is the number of observations. Unfortunately,\nwe cannot apply the same identity to the inversion in eq. (20) since Σ̃4 has full rank and is non-diagonal in general. The eigenvalue spectrum of many prior covariance structures of interest, i.e. simple neighbourhoods etc., decay relatively fast. Therefore, we can approximate Σ0 with a low rank approximation Σ0 ≈ PΛP T , where Λ ∈ RR×R is a diagonal matrix of the R largest eigenvalues and P ∈ RD×R is the corresponding eigenvectors. Using the R-rank approximation, we can now invoke the Woodbury matrix identity again to get:\nΣ̃ = Σ̃3 + Σ̃3P ( Λ + P T Σ̃3P )−1 P T Σ̃3. (25)\nSimilarly, for R < D, this scales as O ( RD2 ) . Another better approach that preserves the\ntotal variance would be to use probabilistic PCA [19] to approximate Σ0. A third alternative is to consider other structures for Σ0, which facilitate fast matrix inversions such as block structures and Toeplitz structures. Numerical issues can arise in EP implementations and in order to avoid this, we use the same precautions as described in [8]."
    }, {
      "heading" : "4 Numerical experiments",
      "text" : "This section describes a series of numerical experiments that have been designed and conducted in order to investigate the properties of the proposed algorithm."
    }, {
      "heading" : "4.1 Experiment 1",
      "text" : "The first experiment compares the proposed method to the LARS algorithm [20] and to the BG-AMP method [21], which is an approximate message passing-based method for the spike and slab model. We also compare the method to an ”oracle least squares estimator” that knows the true support of the solutions. We generate 100 problem instances from y = Ax0 + e, where the solutions vectors have been sampled from the proposed prior using the kernel Σi,j = 50 exp(−||i− j||22/(2 · 102)), but constrained to have a fixed sparsity level of the K/D = 0.25. That is, each solution x0 has the same number of non-zero entries, but different sparsity patterns. We vary the degree of undersampling from N/D = 0.05 to N/D = 0.95. The elements of A ∈ RN×250 are i.i.d Gaussian and the columns of A have been scaled to unit `2-norm. The SNR is fixed at 20dB. We apply the four methods to each of the 100 problems, and for each solution we compute the Normalized Mean Square Error (NMSE) between the true signal x0 and the estimated signal x̂ as well as the F -measure:\nNMSE = ||x0 − x̂||2 ||x0||2\nF = 2 precision · recall precision + recall , (26)\nwhere precision and recall are computed using a MAP estimate of the support. For the structured spike and slab method, we consider three different covariance structures: Σij = κ · δ(i − j), Σij = κ exp(−||i − j||2/s) and Σij = κ exp(−||i − j||22/(2s2)) with parameters κ = 50 and s = 10. In each case, we use a R = 50 rank approximation of Σ. The average results are shown in figures 3(a)-(f). Figure (a) shows an example of one of the sampled vectors x0 and figure (b) shows the three covariance functions.\nFrom figure 3(c)-(d), it is seen that the two EP methods with neighbour correlation are able to improve the phase transition point. That is, in order to obtain a reconstruction\nof the signal such that F ≈ 0.8, EP with diagonal covariance and BG-AMP need an undersamplingratio of N/D ≈ 0.55, while the EP methods with neighbour correlation only need N/D ≈ 0.35 to achieve F ≈ 0.8. For this specific problem, this means that utilizing the neighbourhood structure allows us to reconstruct the signal with 50 fewer observations. Note that, the reconstruction using the exponential covariance function does also improve the result even if the true underlying covariance structure corresponds to a squared exponential function. Furthermore, we see similar performance of BG-AMP and EP with a diagonal covariance matrix. This is expected for problems where Aij is drawn iid as assumed in BG-AMP. However, the price of the improved phase transition is clear from figure 3(e). The proposed algorithm has significantly higher computational complexity than BG-AMP and LARS. Figure 4(a) shows the posterior mean of z for the signal shown in figure 3(a). Here it is seen that the two models with neighbour correlation provide a better approximation to the posterior activation probabilities. Figure 4(b) shows the posterior mean of γ for the model with the squared exponential kernel along with ± one standard deviation."
    }, {
      "heading" : "4.2 Experiment 2",
      "text" : "In this experiment we consider an application of the MMV formulation as given in eq. (8)- (10), namely EEG source localization with synthetic sources [22]. Here we are interested in localizing the active sources within a specific region of interest on the cortical surface (grey area on figure 5(a)). To do this, we now generate a problem instance of Y = AEEGX0 + E using the procedure as described in experiment 1, where AEEG ∈ R128×800 is now a submatrix of a real EEG forward matrix corresponding to the grey area on the figure. The condition number ofAEEG is ≈ 8·1015. The true sourcesX0 ∈ R800×20 are sampled from the structured spike and slab prior in eq. (8) using a squared exponential kernel with parameters A = 50, s = 10 and T = 20. The number of active sources is 46, i.e. x has 46 non-zero rows. SNR is fixed to 20dB. The true sources are shown in figure 5(a). We now use the EP algorithm to recover the sources using the true prior, i.e. squared exponential kernel and\nthe results are shown in figure 5(b). We see that the algorithm detects most of the sources correctly, even the small blob on the right hand side. However, it also introduces a small number of false positives in the neighbourhood of the true active sources. The resulting F -measure is Fsq = 0.78. Figure 5(c) shows the result of reconstructing the sources using a diagonal covariance matrix, where Fdiag = 0.34. Here the BG-AMP algorithm is expected to perform poorly due to the heavy violation of the assumption of Aij being Gaussian iid."
    }, {
      "heading" : "4.3 Experiment 3",
      "text" : "We have also recreated the Shepp-Logan Phantom experiment from [2] with D = 104 unknowns, K = 1723 non-zero weights, N = 2K observations and SNR = 10dB (see supplementary material for more details). The EP method yields Fsq = 0.994 and NMSEsq = 0.336 for this experiment, whereas BG-AMP yields F = 0.624 and NMSE = 0.717. For reference, the oracle estimator yields NMSE = 0.326."
    }, {
      "heading" : "5 Conclusion and outlook",
      "text" : "We introduced the structured spike and slab model, which allows incorporation of a priori knowledge of the sparsity pattern. We developed an expectation propagation-based algorithm for Bayesian inference under the proposed model. Future work includes developing a scheme for learning the structure of the sparsity pattern and extending the algorithm to the multiple measurement vector formulation with slowly changing support."
    } ],
    "references" : [ {
      "title" : "Structured sparse principal component analysis",
      "author" : [ "R. Jenatton", "G. Obozinski", "F. Bach" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Sparse signal recovery using markov random fields",
      "author" : [ "V. Cevher", "M.F. Duarte", "C. Hegde", "R.G. Baraniuk" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Structured sparsity models for brain decoding from fMRI data",
      "author" : [ "M. Pontil", "L. Baldassarre", "J. Mouro-Miranda" ],
      "venue" : "Proceedings - 2012 2nd International Workshop on Pattern Recognition in NeuroImaging,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Regression shrinkage and selection via the lasso. Journal of the royal statistical society series b-methodological",
      "author" : [ "R. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1996
    }, {
      "title" : "Bayesian variable selection in linear-regression",
      "author" : [ "T.J. Mitchell", "J. Beauchamp" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1988
    }, {
      "title" : "A sparse-group lasso",
      "author" : [ "N. Simon", "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Journal Of Computational And Graphical Statistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Group lasso with overlap and graph lasso",
      "author" : [ "G. Obozinski", "J.P. Vert", "L. Jacob" ],
      "venue" : "ACM International Conference Proceeding Series,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Generalized spike-andslab priors for bayesian group feature selection using expectation propagation",
      "author" : [ "D. Hernandez-Lobato", "J. Hernandez-Lobato", "P. Dupont" ],
      "venue" : "Journal Of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Bayesian compressive sensing for cluster structured sparse signals",
      "author" : [ "L. Yu", "H. Sun", "J.P. Barbot", "G. Zheng" ],
      "venue" : "Signal Processing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Bayesian source localization with the multivariate laplace prior",
      "author" : [ "M. Van Gerven", "B. Cseke", "R. Oostenveld", "T. Heskes" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Gaussian processes for machine learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Sparse solutions to linear inverse problems with multiple measurement vectors",
      "author" : [ "S.F. Cotter", "B.D. Rao", "K. Engan", "K. Kreutz-delgado" ],
      "venue" : "IEEE Trans. Signal Processing,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "An empirical bayesian strategy for solving the, simultaneous sparse approximation problem",
      "author" : [ "D.P. Wipf", "B.D. Rao" ],
      "venue" : "IEEE Transactions On Signal Processing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Dynamic compressive sensing of time-varying signals via approximate message passing",
      "author" : [ "J. Ziniel", "P. Schniter" ],
      "venue" : "IEEE Transactions On Signal Processing,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Expectation propagation for approximate bayesian inference",
      "author" : [ "T. Minka" ],
      "venue" : "In Proceedings of the Seventeenth Conference Annual Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2001
    }, {
      "title" : "Gaussian processes for classification: Mean-field algorithms",
      "author" : [ "M. Opper", "O. Winther" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "Pattern recognition and machine",
      "author" : [ "C.M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2006
    }, {
      "title" : "The matrix cookbook",
      "author" : [ "K.B. Petersen", "M.S. Pedersen" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Probabilistic principal component analysis",
      "author" : [ "M. E Tipping", "C.M. Bishop" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1999
    }, {
      "title" : "Least angle regression",
      "author" : [ "B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Expectation-maximization gaussian-mixture approximate message passing",
      "author" : [ "P. Schniter", "J. Vila" ],
      "venue" : "46th Annual Conference on Information Sciences and Systems,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Electromagnetic brain mapping",
      "author" : [ "S. Baillet", "J.C. Mosher", "R.M. Leahy" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", structured sparse PCA [1], background subtraction [2] and neuroimaging [3].",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 1,
      "context" : ", structured sparse PCA [1], background subtraction [2] and neuroimaging [3].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : ", structured sparse PCA [1], background subtraction [2] and neuroimaging [3].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "Examples of such sparsity promoting priors include the Laplace prior (LASSO [4]), and the Bernoulli-Gaussian prior (the spike and slab model [5]).",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "Examples of such sparsity promoting priors include the Laplace prior (LASSO [4]), and the Bernoulli-Gaussian prior (the spike and slab model [5]).",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "group LASSO, sparse group LASSO [6] and graph LASSO [7].",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 6,
      "context" : "group LASSO, sparse group LASSO [6] and graph LASSO [7].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "A Bayesian equivalent of group sparsity is the group spike and slab model [8], which takes the form p(x ∣∣z) = G ∏",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : "Other relevant work includes [9] and [10].",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "Other relevant work includes [9] and [10].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "Another more flexible approach is to use a Markov random field (MRF) as prior for the binary variables [2].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "This model allows us to encode a priori information of the sparsity pattern into the model using generic covariance functions rather than through clique potentials as for the MRF-formulation [2].",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 10,
      "context" : "That is, we impose a Gaussian process [11] as a prior on the parameters γi.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 11,
      "context" : "The proposed prior formulation extends easily to the multiple measurement vector (MMV) formulation [12, 13, 14], in which multiple linear inverse problems are solved simultaneously.",
      "startOffset" : 99,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "The proposed prior formulation extends easily to the multiple measurement vector (MMV) formulation [12, 13, 14], in which multiple linear inverse problems are solved simultaneously.",
      "startOffset" : 99,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "The proposed prior formulation extends easily to the multiple measurement vector (MMV) formulation [12, 13, 14], in which multiple linear inverse problems are solved simultaneously.",
      "startOffset" : 99,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "The most straightforward way is to assume all problem instances share the same support variable, commonly known as joint sparsity [14]",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "In particular, we apply the framework of expectation propagation (EP) [15, 16], which is an iterative deterministic framework for approximating probability distributions using distributions from the exponential family.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "In particular, we apply the framework of expectation propagation (EP) [15, 16], which is an iterative deterministic framework for approximating probability distributions using distributions from the exponential family.",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "The algorithm proposed here can be seen as an extension of the work in [8].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "followed by the minimization of the Kullbach-Leibler [17] divergence between fa,iQ \\a,i and Q w.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "For distributions within the exponential family, minimizing this form of KL divergence amounts to matching moments between fa,iQ \\2,i and Q [15].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : "To minimize the computational load, we use parallel updates of f̃2,i [8] followed by parallel updates of f̃3,i rather than the conventional sequential update scheme.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : "Derivation of analytical expressions for these moments can be found in [11].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "Note, that the EP framework also provides an approximation of the marginal likelihood [11], which can be useful for learning the hyperparameters of the model.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "But Ṽ1 has low rank and Ṽ2 is diagonal, and therefore we can apply the Woodbury matrix identity [18] to eq.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : "Another better approach that preserves the total variance would be to use probabilistic PCA [19] to approximate Σ0.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "Numerical issues can arise in EP implementations and in order to avoid this, we use the same precautions as described in [8].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "1 Experiment 1 The first experiment compares the proposed method to the LARS algorithm [20] and to the BG-AMP method [21], which is an approximate message passing-based method for the spike and slab model.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 20,
      "context" : "1 Experiment 1 The first experiment compares the proposed method to the LARS algorithm [20] and to the BG-AMP method [21], which is an approximate message passing-based method for the spike and slab model.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "(8)(10), namely EEG source localization with synthetic sources [22].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "3 Experiment 3 We have also recreated the Shepp-Logan Phantom experiment from [2] with D = 10(4) unknowns, K = 1723 non-zero weights, N = 2K observations and SNR = 10dB (see supplementary material for more details).",
      "startOffset" : 78,
      "endOffset" : 81
    } ],
    "year" : 2014,
    "abstractText" : "Sparse signal recovery addresses the problem of solving underdetermined linear inverse problems subject to a sparsity constraint. We propose a novel prior formulation, the structured spike and slab prior, which allows to incorporate a priori knowledge of the sparsity pattern by imposing a spatial Gaussian process on the spike and slab probabilities. Thus, prior information on the structure of the sparsity pattern can be encoded using generic covariance functions. Furthermore, we provide a Bayesian inference scheme for the proposed model based on the expectation propagation framework. Using numerical experiments on synthetic data, we demonstrate the benefits of the model.",
    "creator" : null
  }
}