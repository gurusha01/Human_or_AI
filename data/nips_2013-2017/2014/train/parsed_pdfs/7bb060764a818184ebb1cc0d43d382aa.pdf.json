{
  "name" : "7bb060764a818184ebb1cc0d43d382aa.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-Class Deep Boosting",
    "authors" : [ "Vitaly Kuznetsov", "Mehryar Mohri" ],
    "emails" : [ "vitaly@cims.nyu.edu", "mohri@cims.nyu.edu", "usyed@google.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Devising ensembles of base predictors is a standard approach in machine learning which often helps improve performance in practice. Ensemble methods include the family of boosting meta-algorithms among which the most notable and widely used one is AdaBoost [Freund and Schapire, 1997], also known as forward stagewise additive modeling [Friedman et al., 1998]. AdaBoost and its other variants learn convex combinations of predictors. They seek to greedily minimize a convex surrogate function upper bounding the misclassification loss by augmenting, at each iteration, the current ensemble, with a new suitably weighted predictor.\nOne key advantage of AdaBoost is that, since it is based on a stagewise procedure, it can learn an effective ensemble of base predictors chosen from a very large and potentially infinite family, provided that an efficient algorithm is available for selecting a good predictor at each stage. Furthermore, AdaBoost and its L1-regularized counterpart [Rätsch et al., 2001a] benefit from favorable learning guarantees, in particular theoretical margin bounds [Schapire et al., 1997, Koltchinskii and Panchenko, 2002]. However, those bounds depend not just on the margin and the sample size, but also on the complexity of the base hypothesis set, which suggests a risk of overfitting when using too complex base hypothesis sets. And indeed, overfitting has been reported in practice for AdaBoost in the past [Grove and Schuurmans, 1998, Schapire, 1999, Dietterich, 2000, Rätsch et al., 2001b].\nCortes, Mohri, and Syed [2014] introduced a new ensemble algorithm, DeepBoost, which they proved to benefit from finer learning guarantees, including favorable ones even when using as base classifier set relatively rich families, for example a family of very deep decision trees, or other similarly complex families. In DeepBoost, the decisions in each iteration of which classifier to add to the ensemble and which weight to assign to that classifier, depend on the (data-dependent) complexity\nof the sub-family to which the classifier belongs – one interpretation of DeepBoost is that it applies the principle of structural risk minimization to each iteration of boosting. Cortes, Mohri, and Syed [2014] further showed that empirically DeepBoost achieves a better performance than AdaBoost, Logistic Regression, and their L1-regularized variants. The main contribution of this paper is an extension of these theoretical, algorithmic, and empirical results to the multi-class setting.\nTwo distinct approaches have been considered in the past for the definition and the design of boosting algorithms in the multi-class setting. One approach consists of combining base classifiers mapping each example x to an output label y. This includes the SAMME algorithm [Zhu et al., 2009] as well as the algorithm of Mukherjee and Schapire [2013], which is shown to be, in a certain sense, optimal for this approach. An alternative approach, often more flexible and more widely used in applications, consists of combining base classifiers mapping each pair (x, y) formed by an example x and a label y to a real-valued score. This is the approach adopted in this paper, which is also the one used for the design of AdaBoost.MR [Schapire and Singer, 1999] and other variants of that algorithm.\nIn Section 2, we prove a novel generalization bound for multi-class classification ensembles that depends only on the Rademacher complexity of the hypothesis classes to which the classifiers in the ensemble belong. Our result generalizes the main result of Cortes et al. [2014] to the multi-class setting, and also represents an improvement on the multi-class generalization bound due to Koltchinskii and Panchenko [2002], even if we disregard our finer analysis related to Rademacher complexity. In Section 3, we present several multi-class surrogate losses that are motivated by our generalization bound, and discuss and compare their functional and consistency properties. In particular, we prove that our surrogate losses are realizable H-consistent, a hypothesis-set-specific notion of consistency that was recently introduced by Long and Servedio [2013]. Our results generalize those of Long and Servedio [2013] and admit simpler proofs. We also present a family of multi-class DeepBoost learning algorithms based on each of these surrogate losses, and prove general convergence guarantee for them. In Section 4, we report the results of experiments demonstrating that multi-class DeepBoost outperforms AdaBoost.MR and multinomial (additive) logistic regression, as well as their L1-norm regularized variants, on several datasets."
    }, {
      "heading" : "2 Multi-class data-dependent learning guarantee for convex ensembles",
      "text" : "In this section, we present a data-dependent learning bound in the multi-class setting for convex ensembles based on multiple base hypothesis sets. Let X denote the input space. We denote by Y = {1, . . . , c} a set of c ≥ 2 classes. The label associated by a hypothesis f : X × Y → R to x ∈ X is given by argmaxy∈Y f(x, y). The margin ρf (x, y) of the function f for a labeled example (x, y) ∈ X × Y is defined by\nρf (x, y) = f(x, y)−max y′ 6=y\nf(x, y′). (1)\nThus, f misclassifies (x, y) iff ρf (x, y) ≤ 0. We consider p families H1, . . . ,Hp of functions mapping from X × Y to [0, 1] and the ensemble family F = conv( ⋃p k=1 Hk), that is the family of\nfunctions f of the form f = ∑T\nt=1 αtht, where α = (α1, . . . , αT ) is in the simplex ∆ and where, for each t ∈ [1, T ], ht is in Hkt for some kt ∈ [1, p]. We assume that training and test points are drawn i.i.d. according to some distribution D over X × Y and denote by S = ((x1, y1), . . . , (xm, ym)) a training sample of size m drawn according to Dm. For any ρ > 0, the generalization error R(f), its ρ-margin error Rρ(f) and its empirical margin error are defined as follows:\nR(f) = E (x,y)∼D [1ρf (x,y)≤0], Rρ(f) = E (x,y)∼D [1ρf (x,y)≤ρ], and R̂S,ρ(f) = E (x,y)∼S [1ρf (x,y)≤ρ], (2) where the notation (x, y) ∼ S indicates that (x, y) is drawn according to the empirical distribution defined by S. For any family of hypotheses G mapping X × Y to R, we define Π1(G) by\nΠ1(G) = {x 7→ h(x, y) : y ∈ Y, h ∈ G}. (3) The following theorem gives a margin-based Rademacher complexity bound for learning with ensembles of base classifiers with multiple hypothesis sets. As with other Rademacher complexity learning guarantees, our bound is data-dependent, which is an important and favorable characteristic of our results.\nTheorem 1. Assume p > 1 and let H1, . . . ,Hp be p families of functions mapping from X × Y to [0, 1]. Fix ρ > 0. Then, for any δ > 0, with probability at least 1− δ over the choice of a sample S of size m drawn i.i.d. according to D, the following inequality holds for all f = ∑T t=1 αtht ∈ F:\nR(f) ≤ R̂S,ρ(f)+ 8c ρ T∑ t=1 αtRm(Π1(Hkt))+ 2 cρ √ log p m + √⌈ 4 ρ2 log ( c2ρ2m 4 log p )⌉ log p m + log 2δ 2m ,\nThus, R(f) ≤ R̂S,ρ(f) + 8cρ ∑T t=1 αtRm(Hkt) + O (√ log p ρ2m log [ ρ2c2m 4 log p ]) .\nThe full proof of theorem 3 is given in Appendix B. Even for p = 1, that is for the special case of a single hypothesis set, our analysis improves upon the multi-class margin bound of Koltchinskii and Panchenko [2002] since our bound admits only a linear dependency on the number of classes c instead of a quadratic one. However, the main remarkable benefit of this learning bound is that its complexity term admits an explicit dependency on the mixture coefficients αt. It is a weighted average of Rademacher complexities with mixture weights αt, t ∈ [1, T ]. Thus, the second term of the bound suggests that, while some hypothesis sets Hk used for learning could have a large Rademacher complexity, this may not negatively affect generalization if the corresponding total mixture weight (sum of αts corresponding to that hypothesis set) is relatively small. Using such potentially complex families could help achieve a better margin on the training sample.\nThe theorem cannot be proven via the standard Rademacher complexity analysis of Koltchinskii and Panchenko [2002] since the complexity term of the bound would then be Rm(conv( ⋃p k=1 Hk)) =\nRm( ⋃p\nk=1 Hk) which does not admit an explicit dependency on the mixture weights and is lower bounded by ∑T t=1 αtRm(Hkt). Thus, the theorem provides a finer learning bound than the one obtained via a standard Rademacher complexity analysis."
    }, {
      "heading" : "3 Algorithms",
      "text" : "In this section, we will use the learning guarantees just described to derive several new ensemble algorithms for multi-class classification."
    }, {
      "heading" : "3.1 Optimization problem",
      "text" : "Let H1, . . . ,Hp be p disjoint families of functions taking values in [0, 1] with increasing Rademacher complexities Rm(Hk), k ∈ [1, p]. For any hypothesis h ∈ ∪pk=1Hk, we denote by d(h) the index of the hypothesis set it belongs to, that is h ∈ Hd(h). The bound of Theorem 3 holds uniformly for all ρ > 0 and functions f ∈ conv( ⋃p k=1 Hk). Since the last term of the bound does not depend on α, it suggests selecting α that would minimize:\nG(α) = 1 m m∑ i=1 1ρf (xi,yi)≤ρ + 8c ρ T∑ t=1 αtrt,\nwhere rt = Rm(Hd(ht)) and α ∈ ∆.1 Since for any ρ > 0, f and f/ρ admit the same generalization error, we can instead search for α ≥ 0 with ∑T t=1 αt ≤ 1/ρ, which leads to\nmin α≥0 1 m m∑ i=1 1ρf (xi,yi)≤1 + 8c T∑ t=1 αtrt s.t. T∑ t=1 αt ≤ 1 ρ . (4)\nThe first term of the objective is not a convex function of α and its minimization is known to be computationally hard. Thus, we will consider instead a convex upper bound. Let u 7→ Φ(−u) be a non-increasing convex function upper-bounding u 7→ 1u≤0 over R. Φ may be selected to be\n1 The condition PT t=1 αt = 1 of Theorem 3 can be relaxed to PT\nt=1 αt ≤ 1. To see this, use for example a null hypothesis (ht = 0 for some t).\nfor example the exponential function as in AdaBoost [Freund and Schapire, 1997] or the logistic function. Using such an upper bound, we obtain the following convex optimization problem:\nmin α≥0 1 m m∑ i=1 Φ ( 1− ρf (xi, yi) ) + λ T∑ t=1 αtrt s.t. T∑ t=1 αt ≤ 1 ρ , (5)\nwhere we introduced a parameter λ ≥ 0 controlling the balance between the magnitude of the values taken by function Φ and the second term.2 Introducing a Lagrange variable β ≥ 0 associated to the constraint in (5), the problem can be equivalently written as\nmin α≥0 1 m m∑ i=1 Φ ( 1− min y 6=yi [ T∑ t=1 αtht(xi, yi)− αtht(xi, y) ]) + T∑ t=1 (λrt + β)αt.\nHere, β is a parameter that can be freely selected by the algorithm since any choice of its value is equivalent to a choice of ρ in (5). Since Φ is a non-decreasing function, the problem can be equivalently written as\nmin α≥0 1 m m∑ i=1 max y 6=yi Φ ( 1− [ T∑ t=1 αtht(xi, yi)− αtht(xi, y) ]) + T∑ t=1 (λrt + β)αt.\nLet {h1, . . . , hN} be the set of distinct base functions, and let Fmax be the objective function based on that expression:\nFmax(α) = 1 m m∑ i=1 max y 6=yi Φ ( 1− N∑ j=1 αjhj(xi, yi, y) ) + N∑ j=1 Λjαj , (6)\nwith α = (α1, . . . , αN ) ∈ RN , hj(xi, yi, y) = hj(xi, yi)−hj(xi, y), and Λj = λrj +β for all j ∈ [1, N ]. Then, our optimization problem can be rewritten as minα≥0 Fmax(α). This defines a convex optimization problem since the domain {α ≥ 0} is a convex set and since Fmax is convex: each term of the sum in its definition is convex as a pointwise maximum of convex functions (composition of the convex function Φ with an affine function) and the second term is a linear function of α. In general, Fmax is not differentiable even when Φ is, but, since it is convex, it admits a sub-differential at every point. Additionally, along each direction, Fmax admits left and right derivatives both nonincreasing and a differential everywhere except for a set that is at most countable."
    }, {
      "heading" : "3.2 Alternative objective functions",
      "text" : "We now consider the following three natural upper bounds on Fmax which admit useful properties that we will discuss later, the third one valid when Φ can be written as the composition of two function Φ1 and Φ2 with Φ1 a non-increasing function:\nFsum(α) = 1 m m∑ i=1 ∑ y 6=yi Φ ( 1− N∑ j=1 αjhj(xi, yi, y) ) + N∑ j=1 Λjαj (7)\nFmaxsum(α) = 1 m m∑ i=1 Φ ( 1− N∑ j=1 αjρhj (xi, yi) ) + N∑ j=1 Λjαj (8)\nFcompsum(α) = 1 m m∑ i=1 Φ1 ( ∑\ny 6=yi\nΦ2 ( 1− N∑ j=1 αjhj(xi, yi, y) )) + N∑ j=1 Λjαj . (9)\nFsum is obtained from Fmax simply by replacing in the definition of Fmax the max operator by a sum. Clearly, function Fsum is convex and inherits the differentiability properties of Φ. A drawback of Fsum is that for problems with very large c as in structured prediction, the computation of the sum\n2Note that this is a standard practice in the field of optimization. The optimization problem in (4) is equivalent to a vector optimization problem, where ( Pm i=1 1ρf (xi,yi)≤1, PT t=1 αtrt) is minimized over α. The latter problem can be scalarized leading to the introduction of a parameter λ in (5).\nmay require resorting to approximations. Fmaxsum is obtained from Fmax by noticing that, by the sub-additivity of the max operator, the following inequality holds:\nmax y 6=yi N∑ j=1 −αjhj(xi, yi, y) ≤ N∑ j=1 max y 6=yi −αjhj(xi, yi, y) = N∑ j=1 αjρhj (xi, yi).\nAs with Fsum, function Fmaxsum is convex and admits the same differentiability properties as Φ. Unlike Fsum, Fmaxsum does not require computing a sum over the classes. Furthermore, note that the expressions ρhj (xi, yi), i ∈ [1,m], can be pre-computed prior to the application of any optimization algorithm. Finally, for Φ = Φ1 ◦Φ2 with Φ1 non-increasing, the max operator can be replaced by a sum before applying φ1, as follows:\nmax y 6=yi\nΦ ( 1− f(xi, yi, y) ) = Φ1 ( max y 6=yi Φ2 ( 1− f(xi, yi, y) )) ≤ Φ1 ( ∑ y 6=yi Φ2 ( 1− f(xi, yi, y) )) ,\nwhere f(xi, yi, y) = ∑N j=1 αjhj(xi, yi, y). This leads to the definition of Fcompsum.\nIn Appendix C, we discuss the consistency properties of the loss functions just introduced. In particular, we prove that the loss functions associated to Fmax and Fsum are realizable H-consistent (see Long and Servedio [2013]) in the common cases where the exponential or logistic losses are used and that, similarly, in the common case where Φ1(u) = log(1 + u) and Φ2(u) = exp(u + 1), the loss function associated to Fcompsum is H-consistent.\nFurthermore, in Appendix D, we show that, under some mild assumptions, the objective functions we just discussed are essentially within a constant factor of each other. Moreover, in the case of binary classification all of these objectives coincide."
    }, {
      "heading" : "3.3 Multi-class DeepBoost algorithms",
      "text" : "In this section, we discuss in detail a family of multi-class DeepBoost algorithms, which are derived by application of coordinate descent to the objective functions discussed in the previous paragraphs. We will assume that Φ is differentiable over R and that Φ′(u) 6= 0 for all u. This condition is not necessary, in particular, our presentation can be extended to non-differentiable functions such as the hinge loss, but it simplifies the presentation. In the case of the objective function Fmaxsum, we will assume that both Φ1 and Φ2, where Φ = Φ1 ◦Φ2, are differentiable. Under these assumptions, Fsum, Fmaxsum, and Fcompsum are differentiable. Fmax is not differentiable due to the presence of the max operators in its definition, but it admits a sub-differential at every point.\nFor convenience, let αt = (αt,1, . . . , αt,N )> denote the vector obtained after t ≥ 1 iterations and let α0 = 0. Let ek denote the kth unit vector in RN , k ∈ [1, N ]. For a differentiable objective F , we denote by F ′(α, ej) the directional derivative of F along the direction ej at α. Our coordinate descent algorithm consists of first determining the direction of maximal descent, that is k = argmaxj∈[1,N ] |F ′(αt−1, ej)|, next of determining the best step η along that direction that preserves non-negativity of α, η = argminαt−1+ηek≥0 F (αt−1 + ηek), and updating αt−1 to αt = αt−1 + ηek. We will refer to this method as projected coordinate descent. The following theorem provides a convergence guarantee for our algorithms in that case. Theorem 2. Assume that Φ is twice differentiable and that Φ′′(u) > 0 for all u ∈ R. Then, the projected coordinate descent algorithm applied to F converges to the solution α∗ of the optimization maxα≥0 F (α) for F = Fsum, F = Fmaxsum, or F = Fcompsum. If additionally Φ is strongly convex over the path of the iterates αt, then there exists τ > 0 and γ > 0 such that for all t > τ ,\nF (αt+1)− F (α∗) ≤ (1− 1γ )(F (αt)− F (α ∗)). (10)\nThe proof is given in Appendix I and is based on the results of Luo and Tseng [1992]. The theorem can in fact be extended to the case where instead of the best direction, the derivative for the direction selected at each round is within a constant threshold of the best [Luo and Tseng, 1992]. The conditions of Theorem 2 hold for many cases in practice, in particular in the case of the exponential loss (Φ = exp) or the logistic loss (Φ(−x) = log2(1 + e−x)). In particular, linear convergence is guaranteed in those cases since both the exponential and logistic losses are strongly convex over a compact set containing the converging sequence of αts.\nWe will refer to the algorithm defined by projected coordinate descent applied to Fsum by MDeepBoostSum, to Fmaxsum by MDeepBoostMaxSum, to Fcompsum by MDeepBoostCompSum, and to Fmax by MDeepBoostMax. In the following, we briefly describe MDeepBoostSum, including its pseudocode. We give a detailed description of all of these algorithms in the supplementary material: MDeepBoostSum (Appendix E), MDeepBoostMaxSum (Appendix F), MDeepBoostCompSum (Appendix G), MDeepBoostMax (Appendix H).\nDefine ft−1 = ∑N j=1 αt−1,jhj . Then, Fsum(αt−1) can be rewritten as follows:\nFsum(αt−1) = 1 m m∑ i=1 ∑ y 6=yi Φ ( 1− ft−1(xi, yi, y) ) + N∑ j=1 Λjαt−1,j .\nFor any t ∈ [1, T ], we denote by Dt the distribution over [1,m]× [1, c] defined for all i ∈ [1,m] and y ∈ Y − {yi} by\nDt(i, y) = Φ′\n( 1− ft−1(xi, yi, y) ) St , (11)\nwhere St is a normalization factor, St = ∑m\ni=1 ∑ y 6=yi Φ\n′(1 − ft−1(xi, yi, y)). For any j ∈ [1, N ] and s ∈ [1, T ], we also define the weighted error s,j as follows:\n[\n]]\nThe algorithms presented above have several connections with other boosting algorithms, particularly in the absence of regularization. We discuss these connections in detail in Appendix K."
    }, {
      "heading" : "4 Experiments",
      "text" : "The algorithms presented in the previous sections can be used with a variety of different base classifier sets. For our experiments, we used multi-class binary decision trees. A multi-class binary decision tree in dimension d can be defined by a pair (t,h), where t is a binary tree with a variablethreshold question at each internal node, e.g., Xj ≤ θ, j ∈ [1, d], and h = (hl)l∈Leaves(t) a vector of distributions over the leaves Leaves(t) of t. At any leaf l ∈ Leaves(t), hl(y) ∈ [0, 1] for all y ∈ Y and ∑ y∈Y hl(y) = 1. For convenience, we will denote by t(x) the leaf l ∈ Leaves(t) associated to x by t. Thus, the score associated by (t,h) to a pair (x, y) ∈ X × Y is hl(y) where l = t(x). Let Tn denote the family of all multi-class decision trees with n internal nodes in dimension d. In Appendix J, we derive the following upper bound on the Rademacher complexity of Tn:\nR(Π1(Tn)) ≤ √\n(4n + 2) log2(d + 2) log(m + 1) m . (13)\nAll of the experiments in this section use Tn as the family of base hypothesis sets (parametrized by n). Since Tn is a very large hypothesis set when n is large, for the sake of computational efficiency we make a few approximations. First, although our MDeepBoost algorithms were derived in terms of Rademacher complexity, we use the upper bound in Eq. (13) in place of the Rademacher complexity (thus, in Algorithm 1 we let Λn = λBn + β, where Bn is the bound given in Eq. (13)). Secondly, instead of exhaustively searching for the best decision tree in Tn for each possible size n, we use the following greedy procedure: Given the best decision tree of size n (starting with n = 1), we find the best decision tree of size n+1 that can be obtained by splitting one leaf, and continue this procedure until some maximum depth K. Decision trees are commonly learned in this manner, and so in this context our Rademacher-complexity-based bounds can be viewed as a novel stopping criterion for decision tree learning. Let H∗K be the set of trees found by the greedy algorithm just described. In each iteration t of MDeepBoost, we select the best tree in the set H∗K ∪ {h1, . . . , ht−1}, where h1, . . . , ht−1 are the trees selected in previous iterations.\nWhile we described many objective functions that can be used as the basis of a multi-class deep boosting algorithm, the experiments in this section focus on algorithms derived from Fsum. We also refer the reader to Table 3 in Appendix A for results of experiments with Fcompsum objective functions. The Fsum and Fcompsum objectives combine several advantages that suggest they will perform well empirically. Fsum is consistent and both Fsum and Fcompsum are (by Theorem 4) H-consistent. Also, unlike Fmax both of these objectives are differentiable, and therefore the convergence guarantee in Theorem 2 applies. Our preliminary findings also indicate that algorithms based on Fsum and Fcompsum objectives perform better than those derived from Fmax and Fmaxsum. All of our objective functions require a choice for Φ, the loss function. Since Cortes et al. [2014] reported comparable results for exponential and logistic loss for the binary version of DeepBoost, we let Φ be the exponential loss in all of our experiments with MDeepBoostSum. For MDeepBoostCompSum we select Φ1(u) = log2(1 + u) and Φ2(−u) = exp(−u). In our experiments, we used 8 UCI data sets: abalone, handwritten, letters, pageblocks, pendigits, satimage, statlog and yeast – see more details on these datasets in Table 4, Appendix L. In Appendix K, we explain that when λ = β = 0 then MDeepBoostSum is equivalent to AdaBoost.MR. Also, if we set λ = 0 and β 6= 0 then the resulting algorithm is an L1-norm regularized variant of AdaBoost.MR. We compared MDeepBoostSum to these two algorithms, with the results also reported in Table 1 and Table 2 in Appendix A. Likewise, we compared MDeepBoostCompSum with multinomial (additive) logistic regression, LogReg, and its L1-regularized version LogReg-L1, which, as discussed in Appendix K, are equivalent to MDeepBoostCompSum when λ = β = 0 and λ = 0, β ≥ 0 respectively. Finally, we remark that it can be argued that the parameter optimization procedure (described below) significantly extends AdaBoost.MR since it effectively implements structural risk minimization: for each tree depth, the empirical error is minimized and we choose the depth to achieve the best generalization error.\nAll of these algorithms use maximum tree depth K as a parameter. L1-norm regularized versions admit two parameters: K and β ≥ 0. Deep boosting algorithms have a third parameter, λ ≥ 0. To set these parameters, we used the following parameter optimization procedure: we randomly partitioned each dataset into 4 folds and, for each tuple (λ, β, K) in the set of possible parameters (described below), we ran MDeepBoostSum, with a different assignment of folds to the training\nset, validation set and test set for each run. Specifically, for each run i ∈ {0, 1, 2, 3}, fold i was used for testing, fold i + 1 (mod 4) was used for validation, and the remaining folds were used for training. For each run, we selected the parameters that had the lowest error on the validation set and then measured the error of those parameters on the test set. The average test error and the standard deviation of the test error over all 4 runs is reported in Table 1. Note that an alternative procedure to compare algorithms that is adopted in a number of previous studies of boosting [Li, 2009a,b, Sun et al., 2012] is to simply record the average test error of the best parameter tuples over all runs. While it is of course possible to overestimate the performance of a learning algorithm by optimizing hyperparameters on the test set, this concern is less valid when the size of the test set is large relative to the “complexity” of the hyperparameter space. We report results for this alternative procedure in\nTable 2 and Table 3, Appendix A.\nFor each dataset, the set of possible values for λ and β was initialized to {10−5, 10−6, . . . , 10−10}, and to {1, 2, 3, 4, 5} for the maximum tree depth K. However, if we found an optimal parameter value to be at the end point of these ranges, we extended the interval in that direction (by an order of magnitude for λ and β, and by 1 for the maximum tree depth K) and re-ran the experiments. We have also experimented with 200 and 500 iterations but we have observed that the errors do not change significantly and the ranking of the algorithms remains the same.\nThe results of our experiments show that, for each dataset, deep boosting algorithms outperform the other algorithms evaluated in our experiments. Let us point out that, even though not all of our results are statistically significant, MDeepBoostSum outperforms AdaBoost.MR and AdaBoost.MRL1 (and, hence, effectively structural risk minimization) on each dataset. More importantly, for each dataset MDeepBoostSum outperforms other algorithms on most of the individual runs. Moreover, results for some datasets presented here (namely pendigits) appear to be state-of-the-art. We also refer our reader to experimental results summarized in Table 2 and Table 3 in Appendix A. These results provide further evidence in favor of DeepBoost algorithms. The consistent performance improvement by MDeepBoostSum over AdaBoost.MR or its L1-norm regularized variant shows the benefit of the new complexity-based regularization we introduced."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We presented new data-dependent learning guarantees for convex ensembles in the multi-class setting where the base classifier set is composed of increasingly complex sub-families, including very deep or complex ones. These learning bounds generalize to the multi-class setting the guarantees presented by Cortes et al. [2014] in the binary case. We also introduced and discussed several new multi-class ensemble algorithms benefiting from these guarantees and proved positive results for the H-consistency and convergence of several of them. Finally, we reported the results of several experiments with DeepBoost algorithms, and compared their performance with that of AdaBoost.MR and additive multinomial Logistic Regression and their L1-regularized variants."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Andres Muñoz Medina and Scott Yang for discussions and help with the experiments. This work was partly funded by the NSF award IIS-1117591 and supported by a NSERC PGS grant."
    } ],
    "references" : [ {
      "title" : "Boosting with the L2 loss",
      "author" : [ "P. Bühlmann", "B. Yu" ],
      "venue" : "J. of the Amer. Stat. Assoc.,",
      "citeRegEx" : "Bühlmann and Yu.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bühlmann and Yu.",
      "year" : 2003
    }, {
      "title" : "Logistic regression, Adaboost and Bregman distances",
      "author" : [ "M. Collins", "R.E. Schapire", "Y. Singer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Collins et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Collins et al\\.",
      "year" : 2002
    }, {
      "title" : "Deep boosting",
      "author" : [ "C. Cortes", "M. Mohri", "U. Syed" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Cortes et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cortes et al\\.",
      "year" : 2014
    }, {
      "title" : "An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization",
      "author" : [ "T.G. Dietterich" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Dietterich.,? \\Q2000\\E",
      "shortCiteRegEx" : "Dietterich.",
      "year" : 2000
    }, {
      "title" : "Boosting with structural sparsity",
      "author" : [ "J.C. Duchi", "Y. Singer" ],
      "venue" : "In ICML, page",
      "citeRegEx" : "Duchi and Singer.,? \\Q2009\\E",
      "shortCiteRegEx" : "Duchi and Singer.",
      "year" : 2009
    }, {
      "title" : "Potential boosters? In NIPS, pages 258–264",
      "author" : [ "N. Duffy", "D.P. Helmbold" ],
      "venue" : null,
      "citeRegEx" : "Duffy and Helmbold.,? \\Q1999\\E",
      "shortCiteRegEx" : "Duffy and Helmbold.",
      "year" : 1999
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "Journal of Computer System Sciences,",
      "citeRegEx" : "Freund and Schapire.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1997
    }, {
      "title" : "Greedy function approximation: A gradient boosting machine",
      "author" : [ "J.H. Friedman" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Friedman.,? \\Q2000\\E",
      "shortCiteRegEx" : "Friedman.",
      "year" : 2000
    }, {
      "title" : "Additive logistic regression: a statistical view of boosting",
      "author" : [ "J.H. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Friedman et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 1998
    }, {
      "title" : "Boosting in the limit: Maximizing the margin of learned ensembles",
      "author" : [ "A.J. Grove", "D. Schuurmans" ],
      "venue" : "In AAAI/IAAI,",
      "citeRegEx" : "Grove and Schuurmans.,? \\Q1998\\E",
      "shortCiteRegEx" : "Grove and Schuurmans.",
      "year" : 1998
    }, {
      "title" : "Boosting as entropy projection",
      "author" : [ "J. Kivinen", "M.K. Warmuth" ],
      "venue" : "In COLT, pages 134–144,",
      "citeRegEx" : "Kivinen and Warmuth.,? \\Q1999\\E",
      "shortCiteRegEx" : "Kivinen and Warmuth.",
      "year" : 1999
    }, {
      "title" : "Empirical margin distributions and bounding the generalization error of combined classifiers",
      "author" : [ "V. Koltchinskii", "D. Panchenko" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Koltchinskii and Panchenko.,? \\Q2002\\E",
      "shortCiteRegEx" : "Koltchinskii and Panchenko.",
      "year" : 2002
    }, {
      "title" : "Probability in Banach Spaces: Isoperimetry and Processes",
      "author" : [ "M. Ledoux", "M. Talagrand" ],
      "venue" : null,
      "citeRegEx" : "Ledoux and Talagrand.,? \\Q1991\\E",
      "shortCiteRegEx" : "Ledoux and Talagrand.",
      "year" : 1991
    }, {
      "title" : "ABC-boost: adaptive base class boost for multi-class classification",
      "author" : [ "P. Li" ],
      "venue" : "In ICML, page",
      "citeRegEx" : "Li.,? \\Q2009\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2009
    }, {
      "title" : "ABC-logitboost for multi-class classification",
      "author" : [ "P. Li" ],
      "venue" : "Technical report, Rutgers University,",
      "citeRegEx" : "Li.,? \\Q2009\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2009
    }, {
      "title" : "On the convergence of coordinate descent method for convex differentiable minimization",
      "author" : [ "Z.-Q. Luo", "P. Tseng" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "Luo and Tseng.,? \\Q1992\\E",
      "shortCiteRegEx" : "Luo and Tseng.",
      "year" : 1992
    }, {
      "title" : "Boosting algorithms as gradient descent",
      "author" : [ "L. Mason", "J. Baxter", "P.L. Bartlett", "M.R. Frean" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Mason et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Mason et al\\.",
      "year" : 1999
    }, {
      "title" : "Foundations of Machine Learning",
      "author" : [ "M. Mohri", "A. Rostamizadeh", "A. Talwalkar" ],
      "venue" : null,
      "citeRegEx" : "Mohri et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mohri et al\\.",
      "year" : 2012
    }, {
      "title" : "A theory of multiclass boosting",
      "author" : [ "I. Mukherjee", "R.E. Schapire" ],
      "venue" : "JMLR, 14(1):437–497,",
      "citeRegEx" : "Mukherjee and Schapire.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mukherjee and Schapire.",
      "year" : 2013
    }, {
      "title" : "Maximizing the margin with boosting",
      "author" : [ "G. Rätsch", "M.K. Warmuth" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Rätsch and Warmuth.,? \\Q2002\\E",
      "shortCiteRegEx" : "Rätsch and Warmuth.",
      "year" : 2002
    }, {
      "title" : "Efficient margin maximizing with boosting",
      "author" : [ "G. Rätsch", "M.K. Warmuth" ],
      "venue" : "JMLR, 6:2131–2152,",
      "citeRegEx" : "Rätsch and Warmuth.,? \\Q2005\\E",
      "shortCiteRegEx" : "Rätsch and Warmuth.",
      "year" : 2005
    }, {
      "title" : "On the convergence of leveraging",
      "author" : [ "G. Rätsch", "S. Mika", "M.K. Warmuth" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Rätsch et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Rätsch et al\\.",
      "year" : 2001
    }, {
      "title" : "Soft margins for AdaBoost",
      "author" : [ "G. Rätsch", "T. Onoda", "K.-R. Müller" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Rätsch et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Rätsch et al\\.",
      "year" : 2001
    }, {
      "title" : "Theoretical views of boosting and applications",
      "author" : [ "R.E. Schapire" ],
      "venue" : "In Proceedings of ALT 1999,",
      "citeRegEx" : "Schapire.,? \\Q1999\\E",
      "shortCiteRegEx" : "Schapire.",
      "year" : 1999
    }, {
      "title" : "Boosting: Foundations and Algorithms",
      "author" : [ "R.E. Schapire", "Y. Freund" ],
      "venue" : null,
      "citeRegEx" : "Schapire and Freund.,? \\Q2012\\E",
      "shortCiteRegEx" : "Schapire and Freund.",
      "year" : 2012
    }, {
      "title" : "Improved boosting algorithms using confidence-rated predictions",
      "author" : [ "R.E. Schapire", "Y. Singer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Schapire and Singer.,? \\Q1999\\E",
      "shortCiteRegEx" : "Schapire and Singer.",
      "year" : 1999
    }, {
      "title" : "Boosting the margin: A new explanation for the effectiveness of voting methods",
      "author" : [ "R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S. Lee" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Schapire et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Schapire et al\\.",
      "year" : 1997
    }, {
      "title" : "Aoso-logitboost: Adaptive one-vs-one logitboost for multi-class problem",
      "author" : [ "P. Sun", "M.D. Reid", "J. Zhou" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Sun et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2012
    }, {
      "title" : "On the consistency of multiclass classification methods",
      "author" : [ "A. Tewari", "P.L. Bartlett" ],
      "venue" : "JMLR, 8:1007–1025,",
      "citeRegEx" : "Tewari and Bartlett.,? \\Q2007\\E",
      "shortCiteRegEx" : "Tewari and Bartlett.",
      "year" : 2007
    }, {
      "title" : "Totally corrective boosting algorithms that maximize the margin",
      "author" : [ "M.K. Warmuth", "J. Liao", "G. Rätsch" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Warmuth et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Warmuth et al\\.",
      "year" : 2006
    }, {
      "title" : "Statistical analysis of some multi-category large margin classification methods",
      "author" : [ "T. Zhang" ],
      "venue" : "JMLR, 5:1225–1251,",
      "citeRegEx" : "Zhang.,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2004
    }, {
      "title" : "Statistical behavior and consistency of classification methods based on convex risk minimization",
      "author" : [ "T. Zhang" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Zhang.,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhang.",
      "year" : 2004
    }, {
      "title" : "Multi-class adaboost",
      "author" : [ "J. Zhu", "H. Zou", "S. Rosset", "T. Hastie" ],
      "venue" : "Statistics and Its Interface,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2009
    }, {
      "title" : "New multicategory boosting algorithms based on multicategory fisher-consistent losses",
      "author" : [ "H. Zou", "J. Zhu", "T. Hastie" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Zou et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Ensemble methods include the family of boosting meta-algorithms among which the most notable and widely used one is AdaBoost [Freund and Schapire, 1997], also known as forward stagewise additive modeling [Friedman et al.",
      "startOffset" : 125,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "Ensemble methods include the family of boosting meta-algorithms among which the most notable and widely used one is AdaBoost [Freund and Schapire, 1997], also known as forward stagewise additive modeling [Friedman et al., 1998].",
      "startOffset" : 204,
      "endOffset" : 227
    }, {
      "referenceID" : 32,
      "context" : "This includes the SAMME algorithm [Zhu et al., 2009] as well as the algorithm of Mukherjee and Schapire [2013], which is shown to be, in a certain sense, optimal for this approach.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : "MR [Schapire and Singer, 1999] and other variants of that algorithm.",
      "startOffset" : 3,
      "endOffset" : 30
    }, {
      "referenceID" : 16,
      "context" : ", 2009] as well as the algorithm of Mukherjee and Schapire [2013], which is shown to be, in a certain sense, optimal for this approach.",
      "startOffset" : 36,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "Our result generalizes the main result of Cortes et al. [2014] to the multi-class setting, and also represents an improvement on the multi-class generalization bound due to Koltchinskii and Panchenko [2002], even if we disregard our finer analysis related to Rademacher complexity.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "Our result generalizes the main result of Cortes et al. [2014] to the multi-class setting, and also represents an improvement on the multi-class generalization bound due to Koltchinskii and Panchenko [2002], even if we disregard our finer analysis related to Rademacher complexity.",
      "startOffset" : 42,
      "endOffset" : 207
    }, {
      "referenceID" : 2,
      "context" : "Our result generalizes the main result of Cortes et al. [2014] to the multi-class setting, and also represents an improvement on the multi-class generalization bound due to Koltchinskii and Panchenko [2002], even if we disregard our finer analysis related to Rademacher complexity. In Section 3, we present several multi-class surrogate losses that are motivated by our generalization bound, and discuss and compare their functional and consistency properties. In particular, we prove that our surrogate losses are realizable H-consistent, a hypothesis-set-specific notion of consistency that was recently introduced by Long and Servedio [2013]. Our results generalize those of Long and Servedio [2013] and admit simpler proofs.",
      "startOffset" : 42,
      "endOffset" : 645
    }, {
      "referenceID" : 2,
      "context" : "Our result generalizes the main result of Cortes et al. [2014] to the multi-class setting, and also represents an improvement on the multi-class generalization bound due to Koltchinskii and Panchenko [2002], even if we disregard our finer analysis related to Rademacher complexity. In Section 3, we present several multi-class surrogate losses that are motivated by our generalization bound, and discuss and compare their functional and consistency properties. In particular, we prove that our surrogate losses are realizable H-consistent, a hypothesis-set-specific notion of consistency that was recently introduced by Long and Servedio [2013]. Our results generalize those of Long and Servedio [2013] and admit simpler proofs.",
      "startOffset" : 42,
      "endOffset" : 703
    }, {
      "referenceID" : 11,
      "context" : "Even for p = 1, that is for the special case of a single hypothesis set, our analysis improves upon the multi-class margin bound of Koltchinskii and Panchenko [2002] since our bound admits only a linear dependency on the number of classes c instead of a quadratic one.",
      "startOffset" : 132,
      "endOffset" : 166
    }, {
      "referenceID" : 11,
      "context" : "Even for p = 1, that is for the special case of a single hypothesis set, our analysis improves upon the multi-class margin bound of Koltchinskii and Panchenko [2002] since our bound admits only a linear dependency on the number of classes c instead of a quadratic one. However, the main remarkable benefit of this learning bound is that its complexity term admits an explicit dependency on the mixture coefficients αt. It is a weighted average of Rademacher complexities with mixture weights αt, t ∈ [1, T ]. Thus, the second term of the bound suggests that, while some hypothesis sets Hk used for learning could have a large Rademacher complexity, this may not negatively affect generalization if the corresponding total mixture weight (sum of αts corresponding to that hypothesis set) is relatively small. Using such potentially complex families could help achieve a better margin on the training sample. The theorem cannot be proven via the standard Rademacher complexity analysis of Koltchinskii and Panchenko [2002] since the complexity term of the bound would then be Rm(conv( ⋃p k=1 Hk)) = Rm( ⋃p k=1 Hk) which does not admit an explicit dependency on the mixture weights and is lower bounded by ∑T t=1 αtRm(Hkt).",
      "startOffset" : 132,
      "endOffset" : 1021
    }, {
      "referenceID" : 6,
      "context" : "for example the exponential function as in AdaBoost [Freund and Schapire, 1997] or the logistic function.",
      "startOffset" : 52,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : "The theorem can in fact be extended to the case where instead of the best direction, the derivative for the direction selected at each round is within a constant threshold of the best [Luo and Tseng, 1992].",
      "startOffset" : 184,
      "endOffset" : 205
    }, {
      "referenceID" : 15,
      "context" : "The proof is given in Appendix I and is based on the results of Luo and Tseng [1992]. The theorem can in fact be extended to the case where instead of the best direction, the derivative for the direction selected at each round is within a constant threshold of the best [Luo and Tseng, 1992].",
      "startOffset" : 64,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "Since Cortes et al. [2014] reported comparable results for exponential and logistic loss for the binary version of DeepBoost, we let Φ be the exponential loss in all of our experiments with MDeepBoostSum.",
      "startOffset" : 6,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "These learning bounds generalize to the multi-class setting the guarantees presented by Cortes et al. [2014] in the binary case.",
      "startOffset" : 88,
      "endOffset" : 109
    } ],
    "year" : 2014,
    "abstractText" : "We present new ensemble learning algorithms for multi-class classification. Our algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees. We give new data-dependent learning bounds for convex ensembles in the multiclass classification setting expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. These bounds are finer than existing ones both thanks to an improved dependency on the number of classes and, more crucially, by virtue of a more favorable complexity term expressed as an average of the Rademacher complexities based on the ensemble’s mixture weights. We introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees, prove positive results for the H-consistency of several of them, and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L1regularized counterparts.",
    "creator" : "pdftk 1.44 - www.pdftk.com"
  }
}