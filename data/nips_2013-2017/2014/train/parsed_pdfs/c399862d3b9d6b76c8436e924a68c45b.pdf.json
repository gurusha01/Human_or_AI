{
  "name" : "c399862d3b9d6b76c8436e924a68c45b.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bandit Convex Optimization: Towards Tight Bounds",
    "authors" : [ "Elad Hazan", "Kfir Y. Levy" ],
    "emails" : [ "ehazan@ie.technion.ac.il", "kfiryl@tx.technion.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The power of Online Convex Optimization (OCO) framework is in its ability to generalize many problems from the realm of online and statistical learning, and supply universal tools to solving them. Extensive investigation throughout the last decade has yield efficient algorithms with worst case guarantees. This has lead many practitioners to embrace the OCO framework in modeling and solving real world problems.\nOne of the greatest challenges in OCO is finding tight bounds to the problem of Bandit Convex Optimization (BCO). In this “bandit” setting the learner observes the loss function only at the point that she has chosen. Hence, the learner has to balance between exploiting the information she has gathered and between exploring the new data. The seminal work of [5] elegantly resolves this “exploration-exploitation” dilemma by devising a combined explore-exploit gradient descent algorithm. They obtain a bound of O(T 3/4) on the expected regret for the general case of an adversary playing bounded and Lipschitz-continuous convex losses.\nIn this paper we investigate the BCO setting assuming that the adversary is limited to inflicting strongly-convex and smooth losses and the player may choose points from a constrained decision set. In this setting we devise an efficient algorithm that achieves a regret of Õ( √ T ). This rate is the best possible up to logarithmic factors as implied by a recent work of [11], cleverly obtaining a lower bound of Ω( √ T ) for the same setting.\nDuring our analysis, we develop a full-information algorithm that takes advantage of the strongconvexity of loss functions and uses a self-concordant barrier as a regularization term. This algorithm enables us to perform “shrinking exploration” which is a key ingredient in our BCO algorithm. Conversely, all previous works on BCO use a time invariant exploration scheme.\nThis paper is organized as follows. In Section 2 we introduce our setting and review necessary preliminaries regarding self-concordant barriers. In Section 3 we discuss schemes to perform single-\npoint gradient estimations, then we define first-order online methods and analyze the performance of such methods receiving noisy gradient estimates. Our main result is described and analyzed in Section 4; Section 5 concludes."
    }, {
      "heading" : "1.1 Prior work",
      "text" : "For BCO with general convex loss functions, almost simultaneously to [5], a bound of O(T 3/4) was also obtained by [7] for the setting of Lipschitz-continuous convex losses. Conversely, the best known lower bound for this problem is Ω( √ T ) proved for the easier full-information setting.\nIn case the adversary is limited to using linear losses, it can be shown that the player does not “pay” for exploration; this property was used by [4] to devise the Geometric Hedge algorithm that achieves an optimal regret rate of Õ( √ T ). Later [1], inspired by interior point methods, devised the first efficient algorithm that attains the same nearly-optimal regret rate for this setup of bandit linear optimization.\nFor some special classes of nonlinear convex losses, there are several works that lean on ideas from [5] to achieve improved upper bounds for BCO. In the case of convex and smooth losses [9] attained an upper bound of Õ(T 2/3). The same regret rate of Õ(T 2/3) was achieved by [2] in the case of strongly-convex losses. For the special case of unconstrained BCO with strongly-convex and smooth losses, [2] obtained a regret of Õ( √ T ). A recent paper by Shamir [11], significantly advanced our understanding of BCO by devising a lower bound of Ω( √ T ) for the setting of stronglyconvex and smooth BCO. The latter implies the tightness of our bound.\nA comprehensive survey by Bubeck and Cesa-Bianchi [3], provides a review of the bandit optimization literature in both stochastic and online setting."
    }, {
      "heading" : "2 Setting and Background",
      "text" : "Notation: During this paper we denote by || · || the `2 norm when referring to vectors, and use the same notation for the spectral norm when referring to matrices. We denote by Bn and Sn the n-dimensional euclidean unit ball and unit sphere, and by v ∼ Bn and u ∼ Sn random variables chosen uniformly from these sets. The symbol I is used for the identity matrix (its dimension will be clear from the context). For a positive definite matrix A 0 we denote by A1/2 the matrix B such thatB>B = A, and byA−1/2 the inverse ofB. Finally, we denote [N ] := {1, . . . , N}."
    }, {
      "heading" : "2.1 Bandit Convex Optimization",
      "text" : "We consider a repeated game of T rounds between a player and an adversary, at each round t ∈ [T ]\n1. player chooses a point xt ∈ K. 2. adversary independently chooses a loss function ft ∈ F . 3. player suffers a loss ft(xt) and receives a feedback Ft.\nIn the OCO (Online Convex Optimization) framework we assume that the decision set K is convex and that all functions in F are convex. Our paper focuses on adversaries limited to choosing functions from the set Fσ,β ; the set off all σ-strongly-convex and β-smooth functions. We also limit ourselves to oblivious adversaries where the loss sequence {ft}Tt=1 is predetermined and is therefore independent of the player’s choices. Mind that in this case the best point in hindsight is also independent of the player’s choices. We also assume that the loss functions are defined over the entire space Rn and are strongly-convex and smooth there; yet the player may only choose points from a constrained set K. Let us define the regret of A, and its regret with respect to a comparator w ∈ K:\nRegretAT = T∑ t=1 ft(xt)− min w∗∈K T∑ t=1 ft(w ∗), RegretAT (w) = T∑ t=1 ft(xt)− T∑ t=1 ft(w)\nA player aims at minimizing his regret, and we are interested in players that ensure an o(T ) regret for any loss sequence that the adversary may choose.\nThe player learns through the feedback Ft received in response to his actions. In the full informations setting, he receives the loss function ft itself as a feedback, usually by means of a gradient oracle - i.e. the decision maker has access to the gradient of the loss function at any point in the decision set. Conversely, in the BCO setting the given feedback is ft(xt), i.e., the loss function only at the point that he has chosen; and the player aims at minimizing his expected regret, E [ RegretAT ] ."
    }, {
      "heading" : "2.2 Strong Convexity and Smoothness",
      "text" : "As mentioned in the last subsection we consider an adversary limited to choosing loss functions from the set Fσ,β , the set of σ-strongly convex and β-smooth functions, here we define these properties. Definition 1. (Strong Convexity) We say that a function f : Rn → R is σ-strongly convex over the set K if for all x, y ∈ K it holds that,\nf(y) ≥ f(x) +∇f(x)>(y − x) + σ 2 ||x− y||2 (1)\nDefinition 2. (Smoothness) We say that a convex function f : Rn → R is β-smooth over the set K if the following holds:\nf(y) ≤ f(x) +∇f(x)>(y − x) + β 2 ||x− y||2, ∀x, y ∈ K (2)"
    }, {
      "heading" : "2.3 Self Concordant Barriers",
      "text" : "Interior point methods are polynomial time algorithms to solving constrained convex optimization programs. The main tool in these methods is a barrier function that encodes the constrained set and enables the use of a fast unconstrained optimization machinery. More on this subject can be found in [8].\nLet K ∈ Rn be a convex set with a non empty interior int(K) Definition 3. A functionR : int(K)→ R is called ν-self-concordant if:\n1. R is three times continuously differentiable and convex, and approaches infinity along any sequence of points approaching the boundary of K.\n2. For every h ∈ Rn and x ∈ int(K) the following holds:\n|∇3R(x)[h, h, h]| ≤ 2(∇2R(x)[h, h])3/2 and |∇R(x)[h]| ≤ ν1/2(∇2R(x)[h, h])1/2\nhere, ∇3R(x)[h, h, h] := ∂3∂t1∂t2∂t3R(x+ t1h+ t2h+ t3h) ∣∣∣ t1=t2=t3=0 . Our algorithm requires a ν-self-concordant barrier over K, and its regret depends on √ν. It is well known that any convex set in Rn admits a ν = O(n) such barrier (ν might be much smaller), and that most interesting convex sets admit a self-concordant barrier that is efficiently represented.\nThe Hessian of a self-concordant barrier induces a local norm at every x ∈ int(K), we denote this norm by || · ||x and its dual by || · ||∗x and define ∀h ∈ Rn:\n||h||x = √ h>∇2R(x)h, ||h||∗x = √ h>(∇2R(x))−1h\nwe assume that∇2R(x) always has a full rank. The following fact is a key ingredient in the sampling scheme of BCO algorithms [1, 9]. Let R is be self-concordant barrier and x ∈ int(K) then the Dikin Ellipsoide,\nW1(x) := {y ∈ Rn : ||y − x||x ≤ 1} (3) i.e. the || · ||x-unit ball centered around x, is completely contained in K. Our regret analysis requires a bound on R(y) − R(x); hence, we will find the following lemma useful: Lemma 4. LetR be a ν-self-concordant function over K, then:\nR(y)−R(x) ≤ ν log 1 1− πx(y) , ∀x, y ∈ int(K)\nwhere πx(y) = inf{t ≥ 0 : x+ t−1(y − x) ∈ K}, ∀x, y ∈ int(K) Note that πx(y) is called the Minkowsky function and it is always in [0, 1]. Moreover, as y approaches the boundary of K then πx(y)→ 1."
    }, {
      "heading" : "3 Single Point Gradient Estimation and Noisy First-Order Methods",
      "text" : ""
    }, {
      "heading" : "3.1 Single Point Gradient Estimation",
      "text" : "A main component of BCO algorithms is a randomized sampling scheme for constructing gradient estimates. Here, we survey the previous schemes as well as the more general scheme that we use.\nSpherical estimators: Flaxman et al. [5] introduced a method that produces single point gradient estimates through spherical sampling. These estimates are then inserted into a full-information procedure that chooses the next decision point for the player. Interestingly, these gradient estimates are unbiased predictions for the gradients of a smoothed version function which we next define.\nLet δ > 0 and v ∼ Bn, the smoothed version of a function f : Rn → R is defined as follows:\nf̂(x) = E[f(x+ δv)] (4)\nThe next lemma of [5] ties between the gradients of f̂ and an estimate based on samples of f : Lemma 5. Let u ∼ Sn, and consider the smoothed version f̂ defined in Equation (4), then the following applies:\n∇f̂(x) = E[n δ f(x+ δu)u] (5)\nTherefore, nδ f(x+ δu)u is an unbiased estimator for the gradients of the smoothed version.\nEllipsoidal estimators: Abernethy et al. [1] introduced the idea of sampling from an ellipsoid (specifically the Dikin ellipsoid) rather than a sphere in the context of BCO. They restricted the sampling to the eigenpoles of the ellipsoid (Fig. 1a). A more general method of sampling continuously from an ellipsoid was introduced in [9] (Fig. 1b). We shall see later that our algorithm uses a “shrinking-sampling” scheme (Fig. 1c), which is crucial in achieving the Õ( √ T ) regret bound.\nThe following lemma of [9] shows that we can sample f non uniformly over all directions and create an unbiased gradient estimate of a respective smoothed version: Corollary 6. Let f : Rn → R be a continuous function, let A ∈ Rn×n be invertible, and v ∼ Bn, u ∼ Sn. Define the smoothed version of f with respect to A:\nf̂(x) = E[f(x+Av)] (6)\nThen the following holds:\n∇f̂(x) = E[nf(x+Au)A−1u] (7)\nNote that if A 0 then {Au : u ∈ Sn} is an ellipsoid’s boundary. Our next lemma shows that the smoothed version preserves the strong-convexity of f , and that we can measure the distance between f̂ and f using the spectral norm of A2: Lemma 7. Consider a function f : Rn → R, and a positive definite matrix A ∈ Rn×n. Let f̂ be the smoothed version of f with respect to A as defined in Equation (6). Then the following holds:\n• If f is σ-strongly convex then so is f̂ . • If f is convex and β-smooth, and λmax be the largest eigenvalue of A then:\n0 ≤ f̂(x)− f(x) ≤ β 2 ||A2||2 = β 2 λ2max (8)\nRemark: Lemma 7 also holds if we define the smoothed version of f as f̂(x) = Eu∼Sn [f(x+Au)] i.e. an average of the original function values over the unit sphere rather than the unit ball as defined in Equation (6). Proof is similar to the one of Lemma 7."
    }, {
      "heading" : "3.2 Noisy First-Order Methods",
      "text" : "Our algorithm utilizes a full-information online algorithm, but instead of providing this method with exact gradient values we insert noisy estimates of the gradients. In what follows we define first-order online algorithms, and present a lemma that analyses the regret of such algorithm receiving noisy gradients.\nDefinition 8. (First-Order Online Algorithm) Let A be an OCO algorithm receiving an arbitrary sequence of differential convex loss functions f1, . . . , fT , and providing points x1 ← A and xt ← A(f1, . . . , ft−1). Given thatA requires all loss functions to belong to some set F0. ThenA is called first-order online algorithm if the following holds:\n• Adding a linear function to a member of F0 remains in F0; i.e., for every f ∈ F0 and a ∈ Rn then also f + a>x ∈ F0 • The algorithm’s choices depend only on its gradient values taken in the past choices of A,\ni.e. : A(f1, . . . , ft−1) = A(∇f1(x1), . . . ,∇ft−1(xt−1)), ∀t ∈ [T ]\nThe following is a generalization of Lemma 3.1 from [5]: Lemma 9. Let w be a fixed point inK. LetA be a first-order online algorithm receiving a sequence of differential convex loss functions f1, . . . , fT : K → R (ft+1 possibly depending on z1, . . . zt). Where z1 . . . zT are defined as follows: z1 ← A, zt ← A(g1, . . . , gt−1) where gt’s are vector valued random variables such that:\nE[gt ∣∣z1, f1, . . . , zt, ft] = ∇ft(zt)\nThen if A ensures a regret bound of the form: RegretAT ≤ BA(∇f1(x1), . . . ,∇fT (xT )) in the full information case then, in the case of noisy gradients it ensures the following bound:\nE[ T∑ t=1 ft(zt)]− T∑ t=1 ft(w) ≤ E[BA(g1, . . . , gT )]"
    }, {
      "heading" : "4 Main Result and Analysis",
      "text" : "Following is the main theorem of this paper: Theorem 10. Let K be a convex set with diameter DK and R be a ν-self-concordant barrier over K. Then in the BCO setting where the adversary is limited to choosing β-smooth and σ-stronglyconvex functions and |ft(x)| ≤ L, ∀x ∈ K, then the expected regret of Algorithm 1 with η =√\n(ν+2β/σ) log T 2n2L2T is upper bounded as\nE[RegretT ] ≤ 4nL √( ν + 2β\nσ\n) T log T + 2L+\nβD2K 2\n= O (√ βν\nσ T log T ) whenever T/ log T ≥ 2 (ν + 2β/σ).\nAlgorithm 1 BCO Algorithm for Str.-convex & Smooth losses Input: η > 0, σ > 0, ν-self-concordant barrierR Choose x1 = arg minx∈KR(x) for t = 1, 2 . . . T do\nDefine Bt = ( ∇2R(xt) + ησtI )−1/2 Draw u ∼ Sn Play yt = xt +Btu Observe ft(xt +Btu) and define gt = nft(xt +Btu)B−1t u Update xt+1 = arg minx∈K ∑t τ=1 { g>τ x+ σ 2 ||x− xτ ||2 } + η−1R(x)\nend for\nAlgorithm 1 shrinks the exploration magnitude with time (Fig. 1c); this is enabled thanks to the strong-convexity of the losses. It also updates according to a full-information first-order algorithm\ndenoted FTARL-σ, which is defined below. This algorithm is a variant of the FTRL methodology as defined in [6, 10].\nAlgorithm 2 FTARL-σ Input: η > 0, ν-self concordant barrierR Choose x1 = arg minx∈KR(x) for t = 1, 2 . . . T do\nReceive ∇ht(xt) Output xt+1 = arg minx∈K ∑t τ=1 { ∇hτ (xτ )>x+ σ2 ||x− xτ ||2 } + η−1R(x)\nend for\nNext we give a proof sketch of Theorem 10\nProof sketch of Therorem 10. Let us decompose the expected regret of Algorithm 1 with respect to w ∈ K:\nE [RegretT (w)] := ∑T t=1 E [ft(yt)− ft(w)]\n= ∑T t=1 E [ft(yt)− ft(xt)] (9)\n+ ∑T t=1 E [ ft(xt)− f̂t(xt) ] (10)\n−∑Tt=1 E [ft(w)− f̂t(w)] (11) + ∑T t=1 E [ f̂t(xt)− f̂t(w) ] (12)\nwhere expectation is taken with respect to the player’s choices, and f̂t is defined as\nf̂t(x) = E[ft(x+Btv)], ∀x ∈ K\nhere v ∼ Bn and the smoothing matrix Bt is defined in Algorithm 1. The sampling scheme used by Algorithm 1 yields an unbiased gradient estimate gt of the smoothed version f̂t, which is then inserted to FTARL-σ (Algorithm 2). We can therefore interpret Algorithm 1 as performing noisy first-order method (FTARL-σ) over the smoothed versions. The xt’s in Algorithm 1 are the outputs of FTARL-σ, thus the term in Equation (12) is associated with “exploitation”. The other terms in Equations (9)-(11) measure the cost of sampling away from xt, and the distance between the smoothed version and the original function, hence these term are associated with “exploration”. In what follows we analyze these terms separately and show that Algorithm 1 achieves Õ( √ T ) regret.\nThe Exploration Terms: The next hold by the remark that follows Lemma 7 and by the lemma itself:\nE[ft(yt)− ft(xt)] = E [ Eu[ft(xt +Btu)]− ft(xt) ∣∣xt]] ≤ 0.5βE [||B2t ||2] ≤ β/2ησt (13) −E[ft(w)− f̂t(w)] = E [ E[f̂t(w)− ft(w)\n∣∣xt]] ≤ 0.5βE [||B2t ||2] ≤ β/2ησt (14) E[ft(xt)− f̂t(xt)] = E [ E[ft(xt)− f̂t(xt)\n∣∣xt]] ≤ 0 (15) where ||B2t ||2 ≤ 1/ησt follows by the definition of Bt and by the fact that ∇2R(xt) is positive definite.\nThe Exploitation Term: The next Lemma bounds the regret of FTARL-σ in the full-information setting:\nLemma 11. Let R be a self-concordant barrier over a convex set K, and η > 0. Consider an online player receiving σ-strongly-convex loss functions h1, . . . , hT and choosing points according to FTARL-σ (Algorithm 2), and η||∇ht(xt)||∗t ≤ 1/2, ∀t ∈ [T ]. Then the player’s regret is upper bounded as follows:\nT∑ t=1 ht(xt)− T∑ t=1 ht(w) ≤ 2η T∑ t=1 (||∇ht(xt)||∗t )2 + η−1R(w), ∀z ∈ K\nhere (||a||∗t )2 = aT (∇2R(xt) + ησtI)−1a\nNote that Algorithm 1 uses the estimates gt as inputs into FTARL-σ. Using Corollary 6 we can show that the gt’s are unbiased estimates for the gradients of the smoothed versions f̂t’s. Using the regret bound of the above lemma, and the unbiasedness of the gt’s, Lemma 9 ensures us:\nT∑ t=1 E [ f̂t(xt)− f̂t(w) ] ≤ 2η T∑ t=1 E[(||gt||∗t )2] + η−1R(w) (16)\nBy the definitions of gt and Bt, and recalling |ft(x)| ≤ L, ∀x ∈ K, we can bound: E[(||gt||∗t )2 ∣∣xt] = E [n2 (ft(xt +Btu))2 u>B−1t (∇2R(xt) + ησtI)−1B−1t u∣∣xt] ≤ (nL)2\nConcluding: Plugging the latter into Equation (16) and combining Equations (9)-(16) we get: E[RegretT (w)] ≤ 2η(nL)2T + η−1 ( R(w) + 2βσ−1 log T ) (17)\nRecall that x1 = arg minx∈KR(x) and assume w.l.o.g. that R(x1) = 0 (we can always add R a constant). Thus, for a point w ∈ K such that πx1(w) ≤ 1 − T−1 Lemma 4 ensures us that R(w) ≤ ν log T . Combining the latter with Equation (17) and the choice of η in Theorem 10 assures an expected regret bounded by 4nL √ (ν + 2βσ−1)T log T . Forw ∈ K such that πx1(w) > 1−T−1 we can always find w′ ∈ K such that ||w − w′|| ≤ O(T−1) and πx1(w′) ≤ 1 − T−1, using the Lipschitzness of the ft’s, Theorem 10 holds. Correctness: Note that Algorithm 1 chooses points from the set {xt +( ∇2R(xt) + ησtI )−1/2 u, u ∈ Sn} which is inside the Dikin ellipsoid and therefore belongs to K (the Dikin Eliipsoid is always in K)."
    }, {
      "heading" : "5 Summary and open questions",
      "text" : "We have presented an efficient algorithm that attains near optimal regret for the setting of BCO with strongly-convex and smooth losses, advancing our understanding of optimal regret rates for bandit learning.\nPerhaps the most important question in bandit learning remains the resolution of the attainable regret bounds for smooth but non-strongly-convex, or vice versa, and generally convex cost functions (see Table 1). Ideally, this should be accompanied by an efficient algorithm, although understanding the optimal rates up to polylogarithmic factors would be a significant advancement by itself."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The research leading to these results has received funding from the European Union’s Seventh Framework Programme (FP7/2007-2013) under grant agreement n◦ 336078 – ERCSUBLRN."
    } ],
    "references" : [ {
      "title" : "Competing in the dark: An efficient algorithm for bandit linear optimization",
      "author" : [ "Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin" ],
      "venue" : "In COLT,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Optimal algorithms for online convex optimization with multi-point bandit feedback",
      "author" : [ "Alekh Agarwal", "Ofer Dekel", "Lin Xiao" ],
      "venue" : "In COLT,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolo Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "The price of bandit information for online optimization",
      "author" : [ "Varsha Dani", "Thomas P. Hayes", "Sham Kakade" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Online convex optimization in the bandit setting: gradient descent without a gradient",
      "author" : [ "Abraham Flaxman", "Adam Tauman Kalai", "H. Brendan McMahan" ],
      "venue" : "In SODA,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "A survey: The convex optimization approach to regret minimization",
      "author" : [ "Elad Hazan" ],
      "venue" : "Optimization for Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Nearly tight bounds for the continuum-armed bandit problem",
      "author" : [ "Robert D Kleinberg" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2004
    }, {
      "title" : "Interior point polynomial time methods in convex programming",
      "author" : [ "Arkadii Nemirovskii" ],
      "venue" : "Lecture Notes,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "Improved regret guarantees for online smooth convex optimization with bandit feedback",
      "author" : [ "Ankan Saha", "Ambuj Tewari" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "Shai Shalev-Shwartz" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "On the complexity of bandit and derivative-free stochastic convex optimization",
      "author" : [ "Ohad Shamir" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "The seminal work of [5] elegantly resolves this “exploration-exploitation” dilemma by devising a combined explore-exploit gradient descent algorithm.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 10,
      "context" : "This rate is the best possible up to logarithmic factors as implied by a recent work of [11], cleverly obtaining a lower bound of Ω( √ T ) for the same setting.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : "1 Prior work For BCO with general convex loss functions, almost simultaneously to [5], a bound of O(T ) was also obtained by [7] for the setting of Lipschitz-continuous convex losses.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "1 Prior work For BCO with general convex loss functions, almost simultaneously to [5], a bound of O(T ) was also obtained by [7] for the setting of Lipschitz-continuous convex losses.",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "In case the adversary is limited to using linear losses, it can be shown that the player does not “pay” for exploration; this property was used by [4] to devise the Geometric Hedge algorithm that achieves an optimal regret rate of Õ( √ T ).",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 0,
      "context" : "Later [1], inspired by interior point methods, devised the first efficient algorithm that attains the same nearly-optimal regret rate for this setup of bandit linear optimization.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 4,
      "context" : "For some special classes of nonlinear convex losses, there are several works that lean on ideas from [5] to achieve improved upper bounds for BCO.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 8,
      "context" : "In the case of convex and smooth losses [9] attained an upper bound of Õ(T ).",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "The same regret rate of Õ(T ) was achieved by [2] in the case of strongly-convex losses.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "For the special case of unconstrained BCO with strongly-convex and smooth losses, [2] obtained a regret of Õ( √ T ).",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "A recent paper by Shamir [11], significantly advanced our understanding of BCO by devising a lower bound of Ω( √ T ) for the setting of stronglyconvex and smooth BCO.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "A comprehensive survey by Bubeck and Cesa-Bianchi [3], provides a review of the bandit optimization literature in both stochastic and online setting.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 7,
      "context" : "More on this subject can be found in [8].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "The following fact is a key ingredient in the sampling scheme of BCO algorithms [1, 9].",
      "startOffset" : 80,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "The following fact is a key ingredient in the sampling scheme of BCO algorithms [1, 9].",
      "startOffset" : 80,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "[5] introduced a method that produces single point gradient estimates through spherical sampling.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "The next lemma of [5] ties between the gradients of f̂ and an estimate based on samples of f : Lemma 5.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "[1] introduced the idea of sampling from an ellipsoid (specifically the Dikin ellipsoid) rather than a sphere in the context of BCO.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "A more general method of sampling continuously from an ellipsoid was introduced in [9] (Fig.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "The following lemma of [9] shows that we can sample f non uniformly over all directions and create an unbiased gradient estimate of a respective smoothed version: Corollary 6.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "This algorithm is a variant of the FTRL methodology as defined in [6, 10].",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "This algorithm is a variant of the FTRL methodology as defined in [6, 10].",
      "startOffset" : 66,
      "endOffset" : 73
    } ],
    "year" : 2014,
    "abstractText" : "Bandit Convex Optimization (BCO) is a fundamental framework for decision making under uncertainty, which generalizes many problems from the realm of online and statistical learning. While the special case of linear cost functions is well understood, a gap on the attainable regret for BCO with nonlinear losses remains an important open question. In this paper we take a step towards understanding the best attainable regret bounds for BCO: we give an efficient and near-optimal regret algorithm for BCO with strongly-convex and smooth loss functions. In contrast to previous works on BCO that use time invariant exploration schemes, our method employs an exploration scheme that shrinks with time.",
    "creator" : null
  }
}