{
  "name" : "58e4d44e550d0f7ee0a23d6b02d9b0db.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Low Rank Approximation Lower Bounds in Row-Update Streams",
    "authors" : [ "David P. Woodruff" ],
    "emails" : [ "dpwoodru@us.ibm.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In the last decade many algorithms for numerical linear algebra problems have been proposed, often providing substantial gains over more traditional algorithms based on the singular value decomposition (SVD). Much of this work was influenced by the seminal work of Frieze, Kannan, and Vempala [8]. These include algorithms for matrix product, low rank approximation, regression, and many other problems. These algorithms are typically approximate and succeed with high probability. Moreover, they also generally only require one or a small number of passes over the data.\nWhen the algorithm only makes a single pass over the data and uses a small amount of memory, it is typically referred to as a streaming algorithm. The memory restriction is especially important for large-scale data sets, e.g., matrices whose elements arrive online and/or are too large to fit in main memory. These elements may be in the form of an entry or entire row seen at a time; we refer to the former as the entry-update model and the latter as the row-update model. The rowupdate model often makes sense when the rows correspond to individual entities. Typically one is interested in designing robust streaming algorithms which do not need to assume a particular order of the arriving elements for their correctness. Indeed, if data is collected online, such an assumption may be unrealistic.\nMuthukrishnan asked the question of determining the memory required of data stream algorithms for numerical linear algebra problems, including best rank-k approximation, matrix product, eigenvalues, determinants, and inverses [18]. This question was posed again by Sarlós [21]. A number of exciting streaming algorithms now exist for matrix problems. Sarlós [21] gave 2-pass algorithms for matrix product, low rank approximation, and regression, which were sharpened by Clarkson and Woodruff [5], who also proved lower bounds in the entry-update model for a number of these problems. See also work by Andoni and Nguyen for estimating eigenvalues in a stream [2], and work in [1, 4, 6] which implicitly provides algorithms for approximate matrix product.\nIn this work we focus on the low rank approximation problem. In this problem we are given an n × d matrix A and would like to compute a matrix B of rank at most k for which ‖A − B‖F ≤\n(1+ )‖A−Ak‖F . Here, for a matrix A, ‖A‖F denotes its Frobenius norm √∑n\ni=1 ∑d j=1A 2 i,j and\nAk is the best rank-k approximation to A in this norm given by the SVD.\nClarkson and Woodruff [5] show in the entry-update model, one can compute a factorization B = L ·U ·R with L ∈ Rn×k, U ∈ Rk×k, andR ∈ Rk×d, with a streaming algorithm usingO(k −2(n+ d/ 2) log(nd)) bits of space. They also show a lower bound of Ω(k −1(n + d) log(nd)) bits of space. One limitation of these bounds is that they hold only when the algorithm is required to output a factorization L · U · R. In many cases n d, and using memory that grows linearly with n (as the above lower bounds show is unavoidable) is prohibitive. As observed in previous work [9, 16], in downstream applications we are often only interested in an approximation to the top k principal components, i.e., the matrix R above, and so the lower bounds of Clarkson and Woodruff can be too restrictive. For example, in PCA the goal is to compute the most important directions in the row space of A.\nBy reanalyzing an algorithm of Liberty [16], Ghashami and Phillips [9] were able to overcome this restriction in the row-update model, showing that Liberty’s algorithm is a streaming algorithm which finds a k×dmatrixR for which ‖A−AR†R‖F ≤ (1+ )‖A−Ak‖F using onlyO(dk/ ) words of space. Here R† is the Moore-Penrose pseudoinverse of R and R†R denotes the projection onto the row space of R. Importantly, this space bound no longer depends on n. Moreover, their algorithm is deterministic and achieves relative error. We note that Liberty’s algorithm itself is similar in spirit to earlier work on incremental PCA [3, 10, 11, 15, 19], but that work missed the idea of using a Misra-Gries heavy hitters subroutine [17] which is used to bound the additive error (which was then improved to relative error by Ghashami and Phillips). It also seems possible to obtain a streaming algorithm using O(dk(log n)/ ) words of space, using the coreset approach in an earlier paper by Feldman et al. [7].\nThis work is motivated by the following questions: Is the O(dk/ ) space bound tight or can one achieve an even smaller amount of space? What if one also allows randomization?\nIn this work we answer the above questions. Our main theorem is the following.\nTheorem 1. Any, possibly randomized, streaming algorithm in the row-update model which outputs a k×d matrix R and guarantees that ‖A−AR†R‖2F ≤ (1 + )‖A−Ak‖2F with probability at least 2/3, must use Ω(kd/ ) bits of space.\nUp to a factor of the word size (which is typicallyO(log(nd)) bits), our main theorem shows that the algorithm of Liberty is optimal. It also shows that allowing for randomization and a small probability of error does not significantly help in reducing the memory required. We note that a simple argument gives an Ω(kd) bit lower bound, see Lemma 2 below, which intuitively follows from the fact that if A itself is rank-k, then R needs to have the same rowspace of A, and specifying a random kdimensional subspace of Rd requires Ω(kd) bits. Hence, the main interest here is improving upon this lower bound to Ω(kd/ ) bits of space. This extra 1/ factor is significant for small values of , e.g., if one wants approximations as close to machine precision as possible with a given amount of memory.\nThe only other lower bounds for streaming algorithms for low rank approximation that we know of are due to Clarkson and Woodruff [5]. As in their work, we use the Index problem in communication complexity to establish our bounds, which is a communication game between two players Alice and Bob, holding a string x ∈ {0, 1}r and an index i ∈ [r] =: {1, 2, . . . , r}, respectively. In this game Alice sends a single message to Bob who should output xi with constant probability. It is known (see, e.g., [13]) that this problem requires Alice’s message to be Ω(r) bits long. If Alg is a streaming algorithm for low rank approximation, and Alice can create a matrix Ax while Bob can create a matrix Bi (depending on their respective inputs x and i), then if from the output of Alg on the concatenated matrix [Ax;Bi] Bob can output xi with constant probability, then the memory required of Alg is Ω(r) bits, since Alice’s message is the state of Alg after running it on Ax.\nThe main technical challenges are thus in showing how to choose Ax and Bi, as well as showing how the output of Alg on [Ax;Bi] can be used to solve Index. This is where our work departs significantly from that of Clarkson and Woodruff [5]. Indeed, a major challenge is that in Theorem 1, we only require the output to be the matrix R, whereas in Clarkson and Woodruff’s work from the output one can reconstruct AR†R. This causes technical complications, since there is much less information in the output of the algorithm to use to solve the communication game.\nThe intuition behind the proof of Theorem 1 is that given a 2 × d matrix A = [1, x; 1, 0d], where x is a random unit vector, then if P = R†R is a sufficiently good projection matrix for the low rank approximation problem on A, then the second row of AP actually reveals a lot of information about x. This may be counterintuitive at first, since one may think that [1, 0d; 1, 0d] is a perfectly good low rank approximation. However, it turns out that [1, x/2; 1, x/2] is a much better low rank approximation in Frobenius norm, and even this is not optimal. Therefore, Bob, who has [1, 0d] together with the output P , can compute the second row of AP , which necessarily reveals a lot of information about x (e.g., if AP ≈ [1, x/2; 1, x/2], its second row would reveal a lot of information about x), and therefore one could hope to embed an instance of the Index problem into x. Most of the technical work is about reducing the general problem to this 2× d primitive problem."
    }, {
      "heading" : "2 Main Theorem",
      "text" : "This section is devoted to proving Theorem 1. We start with a simple lemma showing an Ω(kd) lower bound, which we will refer to. The proof of this lemma is in the full version.\nLemma 2. Any streaming algorithm which, for every input A, with constant probability (over its internal randomness) succeeds in outputting a matrix R for which ‖A− AR†R‖F ≤ (1 + )‖A− Ak‖F must use Ω(kd) bits of space.\nReturning to the proof of Theorem 1, let c > 0 be a small constant to be determined. We consider the following two player problem between Alice and Bob: Alice has a ck/ × d matrix A which can be written as a block matrix [I,R], where I is the ck/ × ck/ identity matrix, and R is a ck/ × (d− ck/ ) matrix in which the entries are in {−1/(d− ck/ )1/2,+1/(d− ck/ )1/2}. Here [I,R] means we append the columns of I to the left of the columns of R. Bob is given a set of k standard unit vectors ei1 , . . . , eik , for distinct i1, . . . , ik ∈ [ck/ ] = {1, 2, . . . , ck/ }. Here we need c/ > 1, but we can assume is less than a sufficiently small constant, as otherwise we would just need to prove an Ω(kd) lower bound, which is established by Lemma 2.\nLet B be the matrix [A; ei1 , . . . , eik ] obtained by stacking A on top of the vectors ei1 , . . . , eik . The goal is for Bob to output a rank-k projection matrix P ∈ Rd×d for which ‖B − BP‖F ≤ (1 + )‖B −Bk‖F . Denote this problem by f . We will show the randomized 1-way communication complexity of this problem R1−way1/4 (f), in which Alice sends a single message to Bob and Bob fails with probability at most 1/4, is Ω(kd/ ) bits. More precisely, let µ be the following product distribution on Alice and Bob’s inputs: the entries of R are chosen independently and uniformly at random in {−1/(d− ck/ )1/2,+1/(d − ck/ )1/2}, while {i1, . . . , ik} is a uniformly random set among all sets of k distinct indices in [ck/ ]. We will show that D1−wayµ,1/4 (f) = Ω(kd/ ), where D 1−way µ,1/4 (f) denotes the minimum communication cost over all deterministic 1-way (from Alice to Bob) protocols which fail with probability at most 1/4 when the inputs are distributed according to µ. By Yao’s minimax principle (see, e.g., [14]), R1−way1/4 (f) ≥ D 1−way µ,1/4 (f).\nWe use the following two-player problem Index in order to lower bound D1−wayµ,1/4 (f). In this problem Alice is given a string x ∈ {0, 1}r, while Bob is given an index i ∈ [r]. Alice sends a single message to Bob, who needs to output xi with probability at least 2/3. Again by Yao’s minimax principle, we have that R1−way1/3 (Index) ≥ D 1−way ν,1/3 (Index), where ν is the distribution for which x and i are chosen independently and uniformly at random from their respective domains. The following is well-known.\nFact 3. [13] D1−wayν,1/3 (Index) = Ω(r).\nTheorem 4. For c a small enough positive constant, and d ≥ k/ , we haveD1−wayµ,1/4 (f) = Ω(dk/ ).\nProof. We will reduce from the Index problem with r = (ck/ )(d−ck/ ). Alice, given her string x to Index, creates the ck/ ×dmatrixA = [I,R] as follows. The matrix I is the ck/ ×ck/ identity matrix, while the matrixR is a ck/ ×(d−ck/ ) matrix with entries in {−1/(d−ck/ )1/2,+1/(d− ck/ )1/2}. For an arbitrary bijection between the coordinates of x and the entries of R, Alice sets a\n1 1\n1 1\n1 1\n1\n0 0 0 0 0000\n0 0 0 0 0000\n0 0 0 0 0 000 000 0\n0\nB2\nB1\nS\nT\nck/\"\nk\nck/\" d ck/\"\nRS\nRT\nR\nR\nAlice\nBob\ngiven entry in R to−1/(d− ck/ )1/2 if the corresponding coordinate of x is 0, otherwise Alice sets the given entry inR to +1/(d−ck/ )1/2. In the Index problem, Bob is given an index, which under the bijection between coordinates of x and entries of R, corresponds to being given a row index i and an entry j in the i-th row of R that he needs to recover. He sets i` = i for a random ` ∈ [k], and chooses k − 1 distinct and random indices ij ∈ [ck/ ] \\ {i`}, for j ∈ [k] \\ {`}. Observe that if (x, i) ∼ ν, then (R, i1, . . . , ik) ∼ µ. Suppose there is a protocol in which Alice sends a single message to Bob who solves f with probability at least 3/4 under µ. We show that this can be used to solve Index with probability at least 2/3 under ν. The theorem will follow by Fact 3. Consider the matrix B which is the matrix A stacked on top of the rows ei1 , . . . , eik , in that order, so that B has ck/ + k rows.\nWe proceed to lower bound ‖B − BP‖2F in a certain way, which will allow our reduction to Index to be carried out. We need the following fact: Fact 5. ((2.4) of [20]) Let A be an m × n matrix with i.i.d. entries which are each +1/√n with probability 1/2 and −1/√n with probability 1/2, and suppose m/n < 1. Then for all t > 0,\nPr[‖A‖2 > 1 + t+ √ m/n] ≤ αe−α′nt3/2 .\nwhere α, α′ > 0 are absolute constants. Here ‖A‖2 is the operator norm supx ‖Ax‖/‖x‖ of A.\nWe apply Fact 5 to the matrix R, which implies,\nPr[‖R‖2 > 1 + √ c+ √ (ck/ )/(d− (ck/ ))] ≤ αe−α′(d−(ck/ ))c3/4 ,\nand using that d ≥ k/ and c > 0 is a sufficiently small constant, this implies Pr[‖R‖2 > 1 + 3 √ c] ≤ e−βd, (1)\nwhere β > 0 is an absolute constant (depending on c). Note that for c > 0 sufficiently small, (1 + 3 √ c)2 ≤ 1 + 7√c. Let E be the event that ‖R‖22 ≤ 1 + 7 √ c, which we condition on.\nWe partition the rows of B into B1 and B2, where B1 contains those rows whose projection onto the first ck/ coordinates equals ei for some i /∈ {i1, . . . , ik}. Note that B1 is (ck/ − k) × d and B2 is 2k× d. Here, B2 is 2k× d since it includes the rows in A indexed by i1, . . . , ik, together with the rows ei1 , . . . , eik . Let us also partition the rows of R into RT and RS , so that the union of the rows in RT and in RS is equal to R, where the rows of RT are the rows of R in B1, and the rows of RS are the non-zero rows of R in B2 (note that k of the rows are non-zero and k are zero in B2 restricted to the columns in R).\nLemma 6. For any unit vector u, write u = uR+uS+uT , where S = {i1, . . . , ik}, T = [ck/ ]\\S, and R = [d] \\ [ck/ ], and where uA for a set A is 0 on indices j /∈ A. Then, conditioned on E occurring, ‖Bu‖2 ≤ (1 + 7√c)(2− ‖uT ‖2 − ‖uR‖2 + 2‖uS + uT ‖‖uR‖).\nProof. Let C be the matrix consisting of the top ck/ rows of B, so that C has the form [I,R], where I is a ck/ × ck/ identity matrix. By construction of B, ‖Bu‖2 = ‖uS‖2 + ‖Cu‖2. Now, Cu = uS + uT +RuR, and so\n‖Cu‖22 = ‖uS + uT ‖2 + ‖RuR‖2 + 2(us + uT )TRuR ≤ ‖uS + uT ‖2 + (1 + 7 √ c)‖uR‖2 + 2‖uS + uT ‖‖RuR‖\n≤ (1 + 7√c)(‖uS‖2 + ‖uT ‖2 + ‖uR‖2) + (1 + 3 √ c)2‖uS + uT ‖‖uR‖\n≤ (1 + 7√c)(1 + 2‖uS + uT ‖‖uR‖), and so\n‖Bu‖2 ≤ (1 + 7√c)(1 + ‖uS‖2 + 2‖uS + uT ‖‖uR‖) = (1 + 7 √ c)(2− ‖uR‖2 − ‖uT ‖2 + 2‖uS + UT ‖‖uR‖).\nWe will also make use of the following simple but tedious fact, shown in the full version. Fact 7. For x ∈ [0, 1], the function f(x) = 2x √\n1− x2 − x2 is maximized when x =√ 1/2− √ 5/10. We define ζ to be the value of f(x) at its maximum, where ζ = 2/ √ 5 + √\n5/10− 1/2 ≈ .618. Corollary 8. Conditioned on E occurring, ‖B‖22 ≤ (1 + 7 √ c)(2 + ζ).\nProof. By Lemma 6, for any unit vector u, ‖Bu‖2 ≤ (1 + 7√c)(2− ‖uT ‖2 − ‖uR‖2 + 2‖uS + uT ‖‖uR‖). Suppose we replace the vector uS + uT with an arbitrary vector supported on coordinates in S with the same norm as uS+uT . Then the right hand side of this expression cannot increase, which means it is maximized when ‖uT ‖ = 0, for which it equals (1 + 7 √ c)(2−‖uR‖2 + 2 √ 1− ‖uR‖2‖uR‖),\nand setting ‖uR‖ to equal the x in Fact 7, we see that this expression is at most (1+7 √ c)(2+ζ).\nWrite the projection matrix P output by the streaming algorithm as UUT , where U is d × k with orthonormal columns ui (so R†R = P in the notation of Section 1). Applying Lemma 6 and Fact 7 to each of the columns ui, we show in the full version:\n‖BP‖2F ≤ (1 + 7 √ c)((2 + ζ)k − k∑ i=1 ‖uiT ‖2). (2)\nUsing the matrix Pythagorean theorem, we thus have,\n‖B −BP‖2F = ‖B‖2F − ‖BP‖2F\n≥ 2ck/ + k − (1 + 7√c)((2 + ζ)k − k∑ i=1 ‖uiT ‖2) using ‖B‖2F = 2ck/ + k\n≥ 2ck/ + k − (1 + 7√c)(2 + ζ)k + (1 + 7√c) k∑ i=1 ‖uiT ‖2. (3)\nWe now argue that ‖B−BP‖2F cannot be too large if Alice and Bob succeed in solving f . First, we need to upper bound ‖B−Bk‖2F . To do so, we create a matrix B̃k of rank-k and bound ‖B− B̃k‖2F . Matrix B̃k will be 0 on the rows in B1. We can group the rows of B2 into k pairs so that each pair has the form ei + vi, ei, where i ∈ [ck/ ] and vi is a unit vector supported on [d] \\ [ck/ ]. We let Yi be the optimal (in Frobenius norm) rank-1 approximation to the matrix [ei + vi; ei]. By direct computation 1 the maximum squared singular value of this matrix is 2 + ζ. Our matrix B̃k then consists of a single Yi for each pair in B2. Observe that B̃k has rank at most k and\n‖B −Bk‖2F ≤ ‖B − B̃k‖2F ≤ 2ck/ + k − (2 + ζ)k, 1For an online SVD calculator, see http://www.bluebit.gr/matrix-calculator/\nTherefore, if Bob succeeds in solving f on input B, then,\n‖B −BP‖2F ≤ (1 + )(2ck/ + k − (2 + ζ)k) ≤ 2ck/ + k − (2 + ζ)k + 2ck. (4) Comparing (3) and (4), we arrive at, conditioned on E :\nk∑ i=1 ‖uiT ‖2 ≤ 1 1 + 7 √ c · (7√c(2 + ζ)k + 2ck) ≤ c1k, (5)\nwhere c1 > 0 is a constant that can be made arbitrarily small by making c > 0 an arbitrarily small.\nSince P is a projector, ‖BP‖F = ‖BU‖F . Write U = Û+ Ū , where the vectors in Û are supported on T , and the vectors in Ū are supported on [d] \\ T . We have,\n‖BÛ‖2F ≤ ‖B‖22c1k ≤ (1 + 7 √ c)(2 + ζ)c1k ≤ c2k,\nwhere the first inequality uses ‖BÛ‖F ≤ ‖B‖2‖Û‖F and (5), the second inequality uses that event E occurs, and the third inequality holds for a constant c2 > 0 that can be made arbitrarily small by making the constant c > 0 arbitrarily small.\nCombining with (4) and using the triangle inequality,\n‖BŪ‖F ≥ ‖BP‖F − ‖BÛ‖F using the triangle inequality ≥ ‖BP‖F − √ c2k using our bound on ‖BÛ‖2F\n= √ ‖B‖2F − ‖B −BP‖2F − √ c2k by the matrix Pythagorean theorem\n≥ √ (2 + ζ)k − 2ck − √ c2k by (4)\n≥ √\n(2 + ζ)k − c3k, (6) where c3 > 0 is a constant that can be made arbitrarily small for c > 0 an arbitrarily small constant (note that c2 > 0 also becomes arbitrarily small as c > 0 becomes arbitrarily small). Hence, ‖BŪ‖2F ≥ (2 + ζ)k − c3k, and together with Corollary 8, that implies ‖Ū‖2F ≥ k − c4k for a constant c4 that can be made arbitrarily small by making c > 0 arbitrarily small.\nOur next goal is to show that ‖B2Ū‖2F is almost as large as ‖BŪ‖2F . Consider any column ū of Ū , and write it as ūS + ūR. Hence,\n‖Bū‖2 = ‖RT ūR‖2 + ‖B2ū‖2 using B1ū = RT ūR ≤ ‖RT ūR‖2 + ‖ūS +RS ūR‖2 + ‖ūS‖2 by definition of the components = ‖RūR‖2 + 2‖ūS‖2 + 2ūTSRS ūR using the Pythagorean theorem ≤ 1 + 7√c+ ‖ūS‖2 + 2‖ūS‖‖RS ūR‖\nusing ‖RūR‖2 ≤ (1 + 7 √ c)‖ūR‖2 and ‖ūR‖2 + ‖ūS‖2 ≤ 1\n(also using Cauchy-Schwarz to bound the other term).\nSuppose ‖RS ūR‖ = τ‖ūR‖ for a value 0 ≤ τ ≤ 1 + 7 √ c. Then\n‖Bū‖2 ≤ 1 + 7√c+ ‖ūS‖2 + 2τ‖ūS‖ √\n1− ‖ūS‖2. We thus have,\n‖Bū‖2 ≤ 1 + 7√c+ (1− τ)‖ūS‖2 + τ(‖ūS‖2 + 2‖ūS‖ √\n1− ‖ūS‖2) ≤ 1 + 7√c+ (1− τ) + τ(1 + ζ) by Fact 7 ≤ 2 + τζ + 7√c, (7)\nand hence, letting τ1, . . . , τk denote the corresponding values of τ for the k columns of Ū , we have\n‖BŪ‖2F ≤ (2 + 7 √ c)k + ζ k∑ i=1 τi. (8)\nComparing the square of (6) with (8), we have k∑ i=1 τi ≥ k − c5k, (9)\nwhere c5 > 0 is a constant that can be made arbitrarily small by making c > 0 an arbitrarily small constant. Now, ‖Ū‖2F ≥ k − c4k as shown above, while since ‖RsūR‖ = τi‖ūR‖ if ūR is the i-th column of Ū , by (9) we have\n‖RSŪR‖2F ≥ (1− c6)k (10) for a constant c6 that can be made arbitrarily small by making c > 0 an arbitarily small constant. Now ‖RŪR‖2F ≤ (1 + 7 √ c)k since event E occurs, and ‖RŪR‖2F = ‖RT ŪR‖2F + ‖RSŪR‖2F since the rows of R are the concatenation of rows of RS and RT , so combining with (10), we arrive at\n‖RT ŪR‖2F ≤ c7k, (11) for a constant c7 > 0 that can be made arbitrarily small by making c > 0 arbitrarily small.\nCombining the square of (6) with (11), we thus have\n‖B2Ū‖2F = ‖BŪ‖2F − ‖B1Ū‖2F = ‖BŪ‖2F − ‖RT ŪR‖2F ≥ (2 + ζ)k − c3k − c7k ≥ (2 + ζ)k − c8k, (12)\nwhere the constant c8 > 0 can be made arbitrarily small by making c > 0 arbitrarily small.\nBy the triangle inequality,\n‖B2U‖F ≥ ‖B2Ū‖F − ‖B2Û‖F ≥ ((2 + ζ)k − c8k)1/2 − (c2k)1/2. (13) Hence,\n‖B2 −B2P‖F = √ ‖B2‖2F − ‖B2U‖2F Matrix Pythagorean, ‖B2U‖F = ‖B2P‖F\n≤ √ ‖B2‖2F − (‖B2Ū‖F − ‖B2Û‖F )2 Triangle Inequality\n≤ √\n3k − (((2 + ζ)k − c8k)1/2 − (c2k)1/2)2 Using (13),‖B2‖2F = 3k,(14) (15)\nor equivalently, ‖B2 − B2P‖2F ≤ 3k − ((2 + ζ)k − c8k) − (c2k) + 2k(((2 + ζ) − c8)c2)1/2 ≤ (1− ζ)k+ c8k+ 2k(((2 + ζ)− c8)c2)1/2 ≤ (1− ζ)k+ c9k for a constant c9 > 0 that can be made arbitrarily small by making the constant c > 0 small enough. This intuitively says that P provides a good low rank approximation for the matrix B2. Notice that by (14),\n‖B2P‖2F = ‖B2‖2F − ‖B2 −B2P‖2F ≥ 3k − (1− ζ)k − c9k ≥ (2 + ζ)k − c9k. (16) Now B2 is a 2k × d matrix and we can partition its rows into k pairs of rows of the form Z` = (ei` +Ri` , ei`), for ` = 1, . . . , k. Here we abuse notation and think ofRi` as a d-dimensional vector, its first ck/ coordinates set to 0. Each such pair of rows is a rank-2 matrix, which we abuse notation and call ZT` . By direct computation\n2 ZT` has squared maximum singular value 2 + ζ. We would like to argue that the projection of P onto the row span of most Z` has length very close to 1. To this end, for each Z` consider the orthonormal basis V T` of right singular vectors for its row space (which is span(ei` , Ri` )). We let v T `,1, v T `,2 be these two right singular vectors with corresponding singular values σ1 and σ2 (which will be the same for all `, see below). We are interested in the quantity ∆ = ∑k `=1 ‖V T` P‖2F which intuitively measures how much of P gets projected onto the row spaces of the ZT` . The following lemma and corollary are shown in the full version.\nLemma 9. Conditioned on event E , ∆ ∈ [k− c10k, k+ c10k], where c10 > 0 is a constant that can be made arbitrarily small by making c > 0 arbitrarily small.\nThe following corollary is shown in the full version. Corollary 10. Conditioned on event E , for a 1−√c9 + 2c10 fraction of ` ∈ [k], ‖V T` P‖2F ≤ 1+c11, and for a 99/100 fraction of ` ∈ [k], we have ‖V T` P‖2F ≥ 1− c11, where c11 > 0 is a constant that can be made arbitrarily small by making the constant c > 0 arbitrarily small.\n2We again used the calculator at http://www.bluebit.gr/matrix-calculator/\nRecall that Bob holds i = i` for a random ` ∈ [k]. It follows (conditioned on E) by a union bound that with probability at least 49/50, ‖V T` P‖2F ∈ [1 − c11, 1 + c11], which we call the event F and condition on. We also condition on event G that ‖ZT` P‖2F ≥ (2+ζ)−c12, for a constant c12 > 0 that can be made arbitrarily small by making c > 0 an arbitrarily small constant. Combining the first part of Corollary 10 together with (16), event G holds with probability at least 99.5/100, provided c > 0 is a sufficiently small constant. By a union bound it follows that E , F , and G occur simultaneously with probability at least 49/51.\nAs ‖ZT` P‖2F = σ21‖vT`,1P‖2 + σ22‖vT`,2P‖2, with σ21 = 2 + ζ and σ21 = 1 − ζ, events E ,F , and G imply that ‖vT`,1P‖2 ≥ 1 − c13, where c13 > 0 is a constant that can be made arbitrarily small by making the constant c > 0 arbitrarily small. Observe that ‖vT`,1P‖2 = 〈v`,1, z〉2, where z is a unit vector in the direction of the projection of v`,1 onto P .\nBy the Pythagorean theorem, ‖v`,1 − 〈v`,1, z〉z‖2 = 1− 〈v`,1, z〉2, and so\n‖v`,1 − 〈v`,1, z〉z‖2 ≤ c14, (17) for a constant c14 > 0 that can be made arbitrarily small by making c > 0 arbitrarily small.\nWe thus have ZT` P = σ1〈v`,1, z〉u`,1zT + σ2〈v`,2, w〉u`,2wT , where w is a unit vector in the direction of the projection of of v`,2 onto P , and u`,1, u`,2 are the left singular vectors of ZT` . Since F occurs, we have that |〈v`,2, w〉| ≤ c11, where c11 > 0 is a constant that can be made arbitrarily small by making the constant c > 0 arbitrarily small. It follows now by (17) that\n‖ZT` P − σ1u`,1vt`,1‖2F ≤ c15, (18) where c15 > 0 is a constant that can be made arbitrarily small by making the constant c > 0 arbitrarily small.\nBy direct calculation3 , u`,1 = −.851ei` − .526Ri` and v`,1 = −.851ei` − .526Ri` . It follows that ‖ZT` P − (2 + ζ)[.724ei` + .448Ri` ; .448ei` + .277Ri` ]‖2F ≤ c15. Since ei` is the second row of ZT` , it follows that ‖eTi`P − (2 + ζ)(.448ei` + .277Ri`)‖2 ≤ c15. Observe that Bob has ei` and P , and can therefore compute e T i` P . Moreover, as c15 > 0 can be made arbitrarily small by making the constant c > 0 arbitrarily small, it follows that a 1− c16 fraction of the signs of coordinates of eTi`P , restricted to coordinates in [d] \\ [ck/ ], must agree with those of (2 + ζ).277Ri` , which in turn agree with those of Ri` . Here c16 > 0 is a constant that can be made arbitrarily small by making the constant c > 0 arbitrarily small. Hence, in particular, the sign of the j-th coordinate of Ri` , which Bob needs to output, agrees with that of the j-th coordinate of e T i` P with probability at least 1− c16. Call this eventH. By a union bound over the occurrence of events E ,F , G, and H, and the streaming algorithm succeeding (which occurs with probability 3/4), it follows that Bob succeeds in solving Index with probability at least 49/51− 1/4− c16 > 2/3, as required. This completes the proof."
    }, {
      "heading" : "3 Conclusion",
      "text" : "We have shown an Ω(dk/ ) bit lower bound for streaming algorithms in the row-update model for outputting a k × d matrix R with ‖A − AR†R‖F ≤ (1 + )‖A − Ak‖F , thus showing that the algorithm of [9] is optimal up to the word size. The next natural goal would be to obtain multi-pass lower bounds, which seem quite challenging. Such lower bound techniques may also be useful for showing the optimality of a constant-round O(sdk/ ) + (sk/ )O(1) communication protocol in [12] for low-rank approximation in the distributed communication model.\nAcknowledgments. I would like to thank Edo Liberty and Jeff Phillips for many useful discusions and detailed comments on this work (thanks to Jeff for the figure!). I would also like to thank the XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA8750-12-C0323 for supporting this work.\n3Using the online calculator in earlier footnotes."
    } ],
    "references" : [ {
      "title" : "Tracking join and self-join sizes in limited storage",
      "author" : [ "N. Alon", "P.B. Gibbons", "Y. Matias", "M. Szegedy" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Eigenvalues of a matrix in the streaming model",
      "author" : [ "A. Andoni", "H.L. Nguyen" ],
      "venue" : "In SODA,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Incremental singular value decomposition of uncertain data with missing values",
      "author" : [ "M. Brand" ],
      "venue" : "In ECCV",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2002
    }, {
      "title" : "Finding frequent items in data streams",
      "author" : [ "M. Charikar", "K. Chen", "M. Farach-Colton" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2004
    }, {
      "title" : "Numerical linear algebra in the streaming model",
      "author" : [ "K.L. Clarkson", "D.P. Woodruff" ],
      "venue" : "In STOC,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "An improved data stream summary: the count-min sketch and its applications",
      "author" : [ "G. Cormode", "S. Muthukrishnan" ],
      "venue" : "J. Algorithms,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering",
      "author" : [ "D. Feldman", "M. Schmidt", "C. Sohler" ],
      "venue" : "In SODA,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Fast monte-carlo algorithms for finding low-rank approximations",
      "author" : [ "A.M. Frieze", "R. Kannan", "S. Vempala" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "Relative errors for deterministic low-rank matrix approximations",
      "author" : [ "M. Ghashami", "J.M. Phillips" ],
      "venue" : "In SODA,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Matrix computations (3",
      "author" : [ "G.H. Golub", "C.F. van Loan" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1996
    }, {
      "title" : "Incremental eigenanalysis for classification",
      "author" : [ "P.M. Hall", "A.D. Marshall", "R.R. Martin" ],
      "venue" : "In BMVC, pages",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "Nimble algorithms for cloud computing",
      "author" : [ "R. Kannan", "S. Vempala", "D.P. Woodruff" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "On randomized one-round communication complexity",
      "author" : [ "I. Kremer", "N. Nisan", "D. Ron" ],
      "venue" : "Computational Complexity,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1999
    }, {
      "title" : "Communication complexity",
      "author" : [ "E. Kushilevitz", "N. Nisan" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1997
    }, {
      "title" : "Efficient sequential karhunen-loeve basis extraction",
      "author" : [ "A. Levy", "M. Lindenbaum" ],
      "venue" : "In ICCV, page 739,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2001
    }, {
      "title" : "Simple and deterministic matrix sketching",
      "author" : [ "E. Liberty" ],
      "venue" : "In KDD, pages 581–588,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Finding repeated elements",
      "author" : [ "J. Misra", "D. Gries" ],
      "venue" : "Sci. Comput. Program.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1982
    }, {
      "title" : "Data streams: Algorithms and applications",
      "author" : [ "S. Muthukrishnan" ],
      "venue" : "Foundations and Trends in Theoretical Computer Science,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2005
    }, {
      "title" : "Incremental learning for robust visual tracking",
      "author" : [ "D.A. Ross", "J. Lim", "R.-S. Lin", "M.-H. Yang" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2008
    }, {
      "title" : "Non-asymptotic theory of random matrices: extreme singular values",
      "author" : [ "M. Rudelson", "R. Vershynin" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Improved approximation algorithms for large matrices via random projections",
      "author" : [ "T. Sarlós" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Much of this work was influenced by the seminal work of Frieze, Kannan, and Vempala [8].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 17,
      "context" : "Muthukrishnan asked the question of determining the memory required of data stream algorithms for numerical linear algebra problems, including best rank-k approximation, matrix product, eigenvalues, determinants, and inverses [18].",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 20,
      "context" : "This question was posed again by Sarlós [21].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "Sarlós [21] gave 2-pass algorithms for matrix product, low rank approximation, and regression, which were sharpened by Clarkson and Woodruff [5], who also proved lower bounds in the entry-update model for a number of these problems.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 4,
      "context" : "Sarlós [21] gave 2-pass algorithms for matrix product, low rank approximation, and regression, which were sharpened by Clarkson and Woodruff [5], who also proved lower bounds in the entry-update model for a number of these problems.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "See also work by Andoni and Nguyen for estimating eigenvalues in a stream [2], and work in [1, 4, 6] which implicitly provides algorithms for approximate matrix product.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "See also work by Andoni and Nguyen for estimating eigenvalues in a stream [2], and work in [1, 4, 6] which implicitly provides algorithms for approximate matrix product.",
      "startOffset" : 91,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "See also work by Andoni and Nguyen for estimating eigenvalues in a stream [2], and work in [1, 4, 6] which implicitly provides algorithms for approximate matrix product.",
      "startOffset" : 91,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "See also work by Andoni and Nguyen for estimating eigenvalues in a stream [2], and work in [1, 4, 6] which implicitly provides algorithms for approximate matrix product.",
      "startOffset" : 91,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "Clarkson and Woodruff [5] show in the entry-update model, one can compute a factorization B = L ·U ·R with L ∈ Rn×k, U ∈ Rk×k, andR ∈ Rk×d, with a streaming algorithm usingO(k −2(n+ d/ (2)) log(nd)) bits of space.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "As observed in previous work [9, 16], in downstream applications we are often only interested in an approximation to the top k principal components, i.",
      "startOffset" : 29,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "As observed in previous work [9, 16], in downstream applications we are often only interested in an approximation to the top k principal components, i.",
      "startOffset" : 29,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "By reanalyzing an algorithm of Liberty [16], Ghashami and Phillips [9] were able to overcome this restriction in the row-update model, showing that Liberty’s algorithm is a streaming algorithm which finds a k×dmatrixR for which ‖A−ARR‖F ≤ (1+ )‖A−Ak‖F using onlyO(dk/ ) words of space.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "By reanalyzing an algorithm of Liberty [16], Ghashami and Phillips [9] were able to overcome this restriction in the row-update model, showing that Liberty’s algorithm is a streaming algorithm which finds a k×dmatrixR for which ‖A−ARR‖F ≤ (1+ )‖A−Ak‖F using onlyO(dk/ ) words of space.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 2,
      "context" : "We note that Liberty’s algorithm itself is similar in spirit to earlier work on incremental PCA [3, 10, 11, 15, 19], but that work missed the idea of using a Misra-Gries heavy hitters subroutine [17] which is used to bound the additive error (which was then improved to relative error by Ghashami and Phillips).",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "We note that Liberty’s algorithm itself is similar in spirit to earlier work on incremental PCA [3, 10, 11, 15, 19], but that work missed the idea of using a Misra-Gries heavy hitters subroutine [17] which is used to bound the additive error (which was then improved to relative error by Ghashami and Phillips).",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "We note that Liberty’s algorithm itself is similar in spirit to earlier work on incremental PCA [3, 10, 11, 15, 19], but that work missed the idea of using a Misra-Gries heavy hitters subroutine [17] which is used to bound the additive error (which was then improved to relative error by Ghashami and Phillips).",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "We note that Liberty’s algorithm itself is similar in spirit to earlier work on incremental PCA [3, 10, 11, 15, 19], but that work missed the idea of using a Misra-Gries heavy hitters subroutine [17] which is used to bound the additive error (which was then improved to relative error by Ghashami and Phillips).",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 18,
      "context" : "We note that Liberty’s algorithm itself is similar in spirit to earlier work on incremental PCA [3, 10, 11, 15, 19], but that work missed the idea of using a Misra-Gries heavy hitters subroutine [17] which is used to bound the additive error (which was then improved to relative error by Ghashami and Phillips).",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 16,
      "context" : "We note that Liberty’s algorithm itself is similar in spirit to earlier work on incremental PCA [3, 10, 11, 15, 19], but that work missed the idea of using a Misra-Gries heavy hitters subroutine [17] which is used to bound the additive error (which was then improved to relative error by Ghashami and Phillips).",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : "The only other lower bounds for streaming algorithms for low rank approximation that we know of are due to Clarkson and Woodruff [5].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : ", [13]) that this problem requires Alice’s message to be Ω(r) bits long.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "This is where our work departs significantly from that of Clarkson and Woodruff [5].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : ", [14]), R 1/4 (f) ≥ D 1−way μ,1/4 (f).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 19,
      "context" : "4) of [20]) Let A be an m × n matrix with i.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 8,
      "context" : "We have shown an Ω(dk/ ) bit lower bound for streaming algorithms in the row-update model for outputting a k × d matrix R with ‖A − ARR‖F ≤ (1 + )‖A − Ak‖F , thus showing that the algorithm of [9] is optimal up to the word size.",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 11,
      "context" : "Such lower bound techniques may also be useful for showing the optimality of a constant-round O(sdk/ ) + (sk/ ) communication protocol in [12] for low-rank approximation in the distributed communication model.",
      "startOffset" : 138,
      "endOffset" : 142
    } ],
    "year" : 2014,
    "abstractText" : "We study low-rank approximation in the streaming model in which the rows of an n × d matrix A are presented one at a time in an arbitrary order. At the end of the stream, the streaming algorithm should output a k × d matrix R so that ‖A−ARR‖F ≤ (1 + )‖A−Ak‖F , where Ak is the best rank-k approximation to A. A deterministic streaming algorithm of Liberty (KDD, 2013), with an improved analysis of Ghashami and Phillips (SODA, 2014), provides such a streaming algorithm using O(dk/ ) words of space. A natural question is if smaller space is possible. We give an almost matching lower bound of Ω(dk/ ) bits of space, even for randomized algorithms which succeed only with constant probability. Our lower bound matches the upper bound of Ghashami and Phillips up to the word size, improving on a simple Ω(dk) space lower bound.",
    "creator" : null
  }
}