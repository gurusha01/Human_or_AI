{
  "name" : "2ac2406e835bd49c70469acae337d292.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Spectral Learning of Mixture of Hidden Markov Models",
    "authors" : [ "Y. Cem Sübakan", "Johannes Traa", "Paris Smaragdis" ],
    "emails" : [ "paris}@illinois.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Method of Moments (MoM) based algorithms [1, 2, 3] for learning latent variable models have recently become popular in the machine learning community. They provide uniqueness guarantees in parameter estimation and are a computationally lighter alternative compared to more traditional maximum likelihood approaches. The main reason behind the computational advantage is that once the moment expressions are acquired, the rest of the learning work amounts to factorizing a moment matrix whose size is independent of the number of data items. However, it is unclear how to use these algorithms for more complicated models such as Mixture of Hidden Markov Models (MHMM).\nMHMM [4] is a useful model for clustering sequences, and has various applications [5, 6, 7]. The E-step of the Expectation Maximization (EM) algorithm for an MHMM requires running forwardbackward message passing along the latent state chain for each sequence in the dataset in every EM iteration. For this reason, if the number of sequences in the dataset is large, EM can be computationally prohibitive.\nIn this paper, we propose a learning algorithm based on the method of moments for MHMM. We use the fact that an MHMM can be expressed as an HMM with block diagonal transition matrix. Having made that observation, we use an existing MoM algorithm to learn the parameters up to a permutation ambiguity. However, this doesn’t recover the parameters of the individual HMMs. We exploit the spectral properties of the global transition matrix to estimate a de-permutation mapping that enables us to recover the parameters of the individual HMMs. We also specify a method that can recover the number of HMMs under several spectral conditions."
    }, {
      "heading" : "2 Model Definitions",
      "text" : ""
    }, {
      "heading" : "2.1 Hidden Markov Model",
      "text" : "In a Hidden Markov Model (HMM), an observed sequence x = x1:T = {x1, . . . , xt, . . . , xT } with xt ∈ RL is generated conditioned on a latent Markov chain r = r1:T = {r1, . . . , rt, . . . , rT }, with\nrt ∈ {1, . . .M}. The HMM is parameterized by an emission matrixO ∈ RL×M , a transition matrix A ∈ RM×M and an initial state distribution ν ∈ RM . Given the model parameters θ = (O,A, ν), the likelihood of an observation sequence x1:T is defined as follows:\np(x1:T |θ) = ∑ r1:T p(x1:T , r1:T |θ) = ∑ r1:T T∏ t=1 p(xt|rt, O) p(rt|rt−1, A)\n=1>MA diag(p(xT | :, O)) · · ·A diag(p(x1| :, O)) ν = 1>M ( T∏ t=1 Adiag(O(xt)) ) ν, (1)\nwhere 1M ∈ RM is a column vector of ones, we have switched from index notation to matrix notation in the second line such that summations are embedded in matrix multiplications, and we use the MATLAB colon notation to pick a row/column of a matrix. Note that O(xt) := p(xt| :, O). The model parameters are defined as follows:\n• ν(u) = p(r1 = u|r0) = p(r1 = u) initial latent state distribution • A(u, v) = p(rt = u|rt−1 = v), t ≥ 2 latent state transition matrix • O(:, u) = E[xt|rt = u] emission matrix\nThe choice of the observation model p(xt|rt) determines what the columns of O correspond to:\n• Gaussian: p(xt|rt = u) = N (xt;µu, σ2) ⇒ O(:, u) = E[xt|rt = u] = µu. • Poisson: p(xt|rt = u) = PO(xt;λu) ⇒ O(:, u) = E[xt|rt = u] = λu. • Multinomial: p(xt|rt = u) = Mult(xt; pu, S) ⇒ O(:, u) = E[xt|rt = u] = pu.\nThe first model is a multivariate, isotropic Gaussian with mean µu ∈ RL and covariance σ2I ∈ RL×L. The second distribution is Poisson with intensity parameter λu ∈ RL. This choice is particularly useful for counts data. The last density is a multinomial distribution with parameter pu ∈ RL and number of draws S."
    }, {
      "heading" : "2.2 Mixture of HMMs",
      "text" : "The Mixture of HMMs (MHMM) is a useful model for clustering sequences where each sequence is modeled by one of K HMMs. It is parameterized by K emission matrices Ok ∈ RL×M , K transition matrices1 Ak ∈ RM×M , and K initial state distributions νk ∈ RM as well as a cluster prior probability distribution π ∈ RK . Given the model parameters θ1:K = (O1:K , A1:K , ν1:K , π), the likelihood of an observation sequence xn = {x1,n, x2,n, . . . , xTn,n} is computed as a convex combination of the likelihood of K HMMs:\np(xn|θ1:K) = K∑ k=1 p(hn = k)p(xn|hn = k, θk) = K∑ k=1 πk ∑\nr1:Tn,n\np(xn, rn|hn = k, θk)\n= K∑ k=1 πk ∑\nr1:Tn,n\nTn∏ t=1 p(xt,n|rt,n, hn = k,Ok)p(rt,n|rt−1,n, hn = k,Ak)\n= K∑ k=1 πk { 1>J ( Tn∏ t=1 Akdiag(Ok(xt,n)) ) νk } , (2)\nwhere hn ∈ {1, 2, . . . ,K} is the latent cluster indicator, rn = {r1,n, r2,n, . . . , rTn,n} is the latent state sequence for the observed sequence xn, andOk(xt,n) is a shorthand for p(xt,n| :, hn = k,Ok). Note that if a sequence is assigned to the kth cluster (hn = k), the corresponding HMM parameters θk = (Ak, Ok, νk) are used to generate it.\n1Without loss of generality, the number of hidden states for each HMM is taken to be M to keep the notation uncluttered."
    }, {
      "heading" : "3 Spectral Learning for MHMMs",
      "text" : "Traditionally, the parameters of an MHMM are learned with the Expectation-Maximization (EM) algorithm. One drawback of EM is that it requires a good initialization. Another issue is its computational requirements. In every iteration, one has to perform forward-backward message passing for every sequence, resulting in a computationally expensive process, especially when dealing with large datasets.\nThe proposed MoM approach avoids the issues associated with EM by leveraging the information in various moments computed from the data. Given these moments, which can be computed efficiently, the computation time of the learning algorithm is independent of the amount of data (number of sequences and their lengths).\nOur approach is mainly based on the observation that an MHMM can be seen as a single HMM with a block-diagonal transition matrix. We will first establish this proposition and discuss its implications. Then, we will describe the proposed learning algorithm."
    }, {
      "heading" : "3.1 MHMM as an HMM with a special structure",
      "text" : "Lemma 1:\nAn MHMM with local parameters θ1:K = (O1:K , A1:K , ν1:K , π) is an HMM with global parameters θ̄ = (Ō, Ā, ν̄), where:\nŌ = [O1 O2 . . . OK ] , Ā =  A1 0 . . . 0 0 A2 . . . 0\n. . . 0 0 . . . AK\n , ν̄ =  π1ν1 π2ν2\n... πKνK  . (3) Proof: Consider the MHMM likelihood for a sequence xn:\np(xn|θ1:K) = K∑ k=1 πk { 1>M ( Tn∏ t=1 Ak diag(Ok(xt)) ) νk } (4)\n=1>MK  Tn∏ t=1  A1 0 . . . 0 0 A2 . . . 0\n. . . 0 0 . . . AK\n diag([O1 O2 . . . OK ] (xt))   π1ν1 π2ν2\n... πKνK  =1>MK ( Tn∏ t=1 Ā diag(Ō(xt)) ) ν̄,\nwhere [O1 O2 . . . OK ] (xt) := Ō(xt). We conclude that the MHMM and an HMM with parameters θ̄ describe equivalent probabilistic models.\nWe see that the state space of an MHMM consists of K disconnected regimes. For each sequence sampled from the MHMM, the first latent state r1 determines what region the entire latent state sequence lies in."
    }, {
      "heading" : "3.2 Learning an MHMM by learning an HMM",
      "text" : "In the previous section, we showed the equivalence between the MHMM and an HMM with a blockdiagonal transition matrix. Therefore, it should be possible to use an HMM learning algorithm such as spectral learning for HMMs [1, 2] to find the parameters of an MHMM. However, the true global parameters θ̄ are recovered inexactly due to noise : θ̄ → θ̄ and state indexing ambiguity via a permutation mapping P: θ̄ → θ̄P . Consequently, the parameters θ̄P = (ŌP , ĀP , ν̄P ) obtained from the learning algorithm are in the following form:\nŌP = Ō P >, ĀP = PĀ P >, ν̄P = P ν̄ , (5)\nwhere P is the permutation matrix corresponding to the permutation mapping P . The presence of the permutation is a fundamental nuisance for MHMM learning since it causes parameter mixing between the individual HMMs. The global parameters are permuted such that it becomes impossible to identify individual cluster parameters. A brute force search to findP requires (MK)! trials, which is infeasible for anything but very small MK. Nevertheless, it is possible to efficiently find a depermutation mapping P̃ using the spectral properties of the global transition matrix Ā. Our ultimate goal in this section is to undo the effect of P by estimating a P̃ that makes ĀP block diagonal despite the presence of the estimation noise ."
    }, {
      "heading" : "3.2.1 Spectral properties of the global transition matrix",
      "text" : "Lemma 2:\nAssuming that each of the local transition matrices A1:K has only one eigenvalue which is 1, the global transition matrix Ā has K eigenvalues which are 1.\nProof:\nĀ = V1Λ1V −1 1 . . . 0 0 . . . 0\n0 0 VKΛKV −1 K\n = V1 . . . 00 . . . 0\n0 0 VK\n Λ1 . . . 00 . . . 0\n0 0 ΛK\n V1 . . . 00 . . . 0\n0 0 VK\n −1\n︸ ︷︷ ︸ V̄ Λ̄V̄ −1\n,\nwhere VkΛkV −1k is the eigenvalue decomposition of Ak with Vk as eigenvectors, and Λk as a diagonal matrix with eigenvalues on the diagonal. The eigenvalues of A1:K appear unaltered in the eigenvalue decomposition of Ā, and consequently Ā has K eigenvalues which are 1.\nCorollary 1: lim e→∞ Āe = [ v̄11 > M . . . v̄k1 > M . . . v̄K1 > M ] , (6)\nwhere v̄k = [0> . . . v>k . . . 0 >]> and vk is the stationary distribution of Ak, ∀k ∈ {1, . . . ,K}.\nProof: lim e→∞\n(VkΛkV −1 k ) e = lim e→∞ VkΛ e kV −1 k = Vk  1 0 . . . 0 0 0 . . . 0\n. . . 0 0 . . . 0 V −1k = vk1>M . The third step follows because there is only one eigenvalue with magnitude 1. Since multiplying Ā by itself amounts to multiplying the corresponding diagonal blocks, we have the structure in (6).\nNote that equation (6) points out that the matrix lime→∞ Āe consists of K blocks of size M ×M where the k’th block is vk1>M . A straightforward algorithm can now be developed for making ĀP block diagonal. Since the eigenvalue decomposition is invariant under permutation, Ā and ĀP have the same eigenvalues and eigenvectors. As e → ∞, K clusters of columns appear in (ĀP)e. Thus, ĀP can be made block-diagonal by clustering the columns of (ĀP)∞. This idea is illustrated in the middle row of Figure 1. Note that, in an actual implementation, one would use a low-rank reconstruction by zeroing-out the eigenvalues that are not equal to 1 in Λ̄ to form (ĀP)r := V̄ P(Λ̄P)r(V̄ P)−1 = (ĀP)∞, where (Λ̄P)r ∈ RMK×MK is a diagonal matrix with only K non-zero entries, corresponding to the eigenvalues which are 1.\nThis algorithm corresponds to the noiseless case ĀP . In practice, the output of the learning algorithm is ĀP and the clear structure in Equation (6) no longer holds in (Ā P )\ne, as e → ∞, as illustrated in the bottom row of Figure 1. We can see that the three-cluster structure no longer holds for large e. Instead, the columns of the transition matrix converge to a global stationary distribution."
    }, {
      "heading" : "3.2.2 Estimating the permutation in the presence of noise",
      "text" : "In the general case with noise , we lose the spectral property that the global transition matrix has K eigenvalues which are 1. Consequently, the algorithm described in Section 3.2.1 cannot be\napplied directly to make ĀP block diagonal. In practice, the estimated transition matrix has only one eigenvalue with unit magnitude and lime→∞(ĀP )\ne converges to a global stationary distribution. However, if the noise is sufficiently small, a depermutation mapping P̃ and the number of HMM clusters K can be successfully estimated. We now specify the spectral conditions for this.\nDefinition 1: We denote λGk := αkλ1,k for k ∈ {1, . . . ,K} as the global, noisy eigenvalues with |λGk | ≥ |λ G k+1|, ∀k ∈ {1, . . . ,K − 1}, where λ1,k is the original eigenvalue of the kth cluster with magnitude 1 and αk is the noise that acts on that eigenvalue (note that α1 = 1). We denote λLj,k := βj,kλj,k for j ∈ {2, . . . ,M} and k ∈ {1, . . . ,K} as the local, noisy eigenvalues with |λLj,k| ≥ |λLj+1,k|, ∀k ∈ {1, . . . ,K} and ∀j ∈ {1, . . . ,M −1}, where λj,k is the original eigenvalue with the jth largest magnitude in the kth cluster, and βj,k is the noise that acts on that eigenvalue.\nDefinition 2: The low-rank eigendecomposition of the estimated transition matrix ĀP is defined as Ar := V Λ\nrV −1, where V is a matrix with eigenvectors in the columns and Λr is a diagonal matrix with eigenvalues λG1:K in the first K entries.\nConjecture 1:\nIf |λGK | > max k∈{1,...,K} |λL2,k|, then Ar can be formed using the eigen-decomposition of ĀP . Then, with high probability, ‖Ar − Ar‖F ≤ O(1/ √ TN), where TN is the total number of observed vectors.\nJustification: ‖Ar −Ar‖F = ‖Ar −A+A−Ar‖F ≤‖Ar −A‖F + ‖A−Ar‖F\n=‖A−Ar‖F + ‖A−A +Ar̄ ‖F ≤‖A−Ar‖F + ‖Ar̄ ‖F + ‖A−A ‖F ≤2KM +O(1/ √ TN) = O(1/ √ TN), w.h.p.,\nwhere A is used for ĀP to reduce the notation clutter (and similarly Ar for (ĀP)r and so on), we used the triangle inequality for the first and second inequalities and Ar̄ = V Λ\nr̄V −1, where Λr̄ is a diagonal matrix of eigenvalues with the first K diagonal entries equal to zero (complement of Λr). For the last inequality, we used the fact that A ∈ RMK×MK has entries in the interval [0, 1] and we used the sample complexity result from [1]. The bound specified in [1] is for a mixture model, but since the two models are similar and the estimation procedure is almost identical, we are reusing it. We believe that further analysis of the spectral learning algorithm is out of the scope of this paper, so we leave this proposition as a conjecture.\nConjecture 1 asserts that, if we have enough data we should obtain an estimate Ar close to A r in the squared error sense. Furthermore, if the following mixing rate condition is satisfied, we will be able to identify the number of clusters K from the data.\nDefinition 3: Let λ̃k denote the kth largest eigenvalue (in decreasing order) of the estimated transition matrix ĀP . We define the quantity,\nLλ̃K′ := ∞∑ e=1 ([ ∑K′ l=1 |λ̃l|e∑MK l′=1 |λ̃l′ |e > 1− γ ] − [∑K′−1 l=1 |λ̃l|e∑MK l′=1 |λ̃l′ |e > 1− γ ]) , (7)\nas the spectral longevity of λ̃K′ . The square brackets [.] denote an indicator function which outputs 1 if the argument is true and 0 otherwise, and γ is a small number such as machine epsilon.\nLemma 3: If |λGK | > max k∈{1,...,K} |λL2,k| and arg maxK′ |λ̃K′ |\n2\n|λ̃K′+1||λ̃K′−1| = K, for K ′ ∈\n{2, 3, . . . ,MK − 1}, then arg maxK′ Lλ̃K′ = K.\nProof: The first condition ensures that the top K eigenvalues are global eigenvalues. The second condition is about the convergence rates of the two ratios in equation (7). The first indicator function has the following summation inside:∑K′\nl=1 |λ̃l|e∑MK l′=1 |λ̃l′ |e =\n∑K′−1 l=1 |λ̃l|e + |λ̃K′ |e∑K′−1\nl′=1 |λ̃l′ |e + |λ̃K′ |e + |λ̃K′+1|e + ∑MK l′=K′+2 |λ̃l′ |e .\nThe rate at which this term goes to 1 is determined by the spectral gap |λK′ |/|λK′+1|. The smaller this ratio is, the faster the term (it is non-decreasing w.r.t. e) converges to 1. For the second indicator function inside Lλ̃K′ , we can do the same analysis and see that the convergence rate is again determined by the gap |λK′−1|/|λK′ |. The ratio of the two spectral gaps determines the spectral longevity. Hence, for the K ′ with largest ratio |λ̃K′ | 2\n|λ̃K′+1||λ̃K′−1| , we have arg maxK′ Lλ̃K′ = K.\nLemma 3 tells us the following. If the estimated transition matrix ĀP is not too noisy, we can determine the number of clusters by choosing the value of K ′ such that it maximizes Lλ̃K′ . This corresponds to exponentiating the sorted eigenvalues in a finite range, and recording the number of non-negligible eigenvalues. This is depicted in Figure 2."
    }, {
      "heading" : "3.3 Proposed Algorithm",
      "text" : "In previous sections, we have shown that the permutation caused by the MoM estimation procedure can be undone, and we have proposed a way to estimate the number of clusters K. We summarize the whole procedure in Algorithm 1."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Effect of noise on depermutation algorithm",
      "text" : "We have tested the algorithm’s performance with respect to amount of data. We used the parameters K = 3, M = 4, L = 20, and we have 2 sequences with length T for each cluster. We used a Gaussian observation model with unit observation variance and the columns of the emission matrices O1:K were drawn from zero mean spherical Gaussian with variance 2. Results for 10 uniformly\nAlgorithm 1 Spectral Learning for Mixture of Hidden Markov Models Inputs: x1:N : Sequences, MK : total number of states of global HMM. Output: θ̂ = ( Ô1:K̂ , Â1:K̂ ) : MHMM parameters\nMethod of Moments Parameter Estimation (ŌP , Ā P ) = HMM MethodofMoments (x1:N ,MK) Depermutation Find eigenvalues of ĀP Exponentiate eigenvalues for each discrete value e in a sufficiently large range. Identify K̂ as the eigenvalue with largest longevity. Compute rank-K̂ reconstruction Ar via eigendecomposition. Cluster the columns ofAr with K̂ clusters to find a depermutation mapping P̃ via cluster labels. Depermute ŌP and Ā P according to P̃ .\nForm θ̂ by choosing corresponding blocks from depermuted ŌP and Ā P .\nReturn θ̂.\nspaced sequence lengths from 10 to 1000 are shown in Figure 3. On the top row, we plot the total error (from centroid to point) obtained after fitting k-means with true number of HMM clusters. We can see that the correct number of clusters K = 3 as well as the block-diagonal structure of the transition matrix is correctly recovered even in the case where T = 20."
    }, {
      "heading" : "4.2 Amount of data vs accuracy and speed",
      "text" : "We have compared clustering accuracies of EM and our approach on data sampled from a Gaussian emission MHMM. Means of each state of each cluster is drawn from a zero mean unit variance Gaussian, and observation covariance is spherical with variance 2. We set L = 20, K = 5, M = 3. We used uniform mixing proportions and uniform initial state distribution. We evaluated the clustering accuracies for 10 uniformly spaced sequence lengths (every sequence has the same length) between 20 and 200, and 10 uniformly spaced number of sequences between 1 and 100 for each cluster. The results are shown in Figure 4. Although EM seems to provide higher accuracy on\nregions where we have less data, spectral algorithm is much faster. Note that, in spectral algorithm we include the time spent in moment computation. We used four restarts for EM, and take the result with highest likelihood, and used an automatic stopping criterion."
    }, {
      "heading" : "4.3 Real data experiment",
      "text" : "We ran an experiment on the handwritten character trajectory dataset from the UCI machine learning repository [8]. We formed pairs of characters and compared the clustering results for three algorithms: the proposed spectral learning approach, EM initialized at random, and EM initialized with MoM algorithm as explored in [9]. We take the maximum accuracy of EM over 5 random initializations in the third row. We set the algorithm parameters to K = 2 and M = 4. There are 140 sequences of average length 100 per class. In the original data, L = 3, but to apply MoM learning, we require that MK < L. To achieve this, we transformed the data vectors with a cubic polynomial feature transformation such that L = 10 (this is the same transformation that corresponds to a polynomial kernel). The results from these trials are shown in Table 1. We can see that although spectral learning doesn’t always surpass randomly initialized EM on its own, it does serve as a very good initialization scheme."
    }, {
      "heading" : "5 Conclusions and future work",
      "text" : "We have developed a method of moments based algorithm for learning mixture of HMMs. Our experimental results show that our approach is computationally much cheaper than EM, while being comparable in accuracy. Our real data experiment also show that our approach can be used as a good initialization scheme for EM. As future work, it would be interesting to apply the proposed approach on other hierarchical latent variable models.\nAcknowledgements: We would like to thank Taylan Cemgil, David Forsyth and John Hershey for valuable discussions. This material is based upon work supported by the National Science Foundation under Grant No. 1319708."
    } ],
    "references" : [ {
      "title" : "A method of moments for mixture models and hidden markov models",
      "author" : [ "A. Anandkumar", "D. Hsu", "S.M. Kakade" ],
      "venue" : "In COLT,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "A spectral algorithm for learning hidden markov models a spectral algorithm for learning hidden markov models",
      "author" : [ "Daniel Hsu", "Sham M. Kakade", "Tong Zhang" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Clustering sequences with hidden markov models",
      "author" : [ "P. Smyth" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Music analysis using hidden markov mixture models",
      "author" : [ "Yuting Qi", "J.W. Paisley", "L. Carin" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "Discovering clusters in motion time-series data",
      "author" : [ "A. Jonathan", "S. Sclaroff", "G. Kollios", "V. Pavlovic" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "Clustering time series with hidden markov models and dynamic time warping",
      "author" : [ "Tim Oates", "Laura Firoiu", "Paul R. Cohen" ],
      "venue" : "Proceedings of the IJCAI-99 Workshop on Neural, Symbolic and Reinforcement Learning Methods for Sequence Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1999
    }, {
      "title" : "Spectral experts for estimating mixtures of linear regressions",
      "author" : [ "Arun Chaganty", "Percy Liang" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction Method of Moments (MoM) based algorithms [1, 2, 3] for learning latent variable models have recently become popular in the machine learning community.",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "1 Introduction Method of Moments (MoM) based algorithms [1, 2, 3] for learning latent variable models have recently become popular in the machine learning community.",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "1 Introduction Method of Moments (MoM) based algorithms [1, 2, 3] for learning latent variable models have recently become popular in the machine learning community.",
      "startOffset" : 56,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "MHMM [4] is a useful model for clustering sequences, and has various applications [5, 6, 7].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 4,
      "context" : "MHMM [4] is a useful model for clustering sequences, and has various applications [5, 6, 7].",
      "startOffset" : 82,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "MHMM [4] is a useful model for clustering sequences, and has various applications [5, 6, 7].",
      "startOffset" : 82,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "MHMM [4] is a useful model for clustering sequences, and has various applications [5, 6, 7].",
      "startOffset" : 82,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "Therefore, it should be possible to use an HMM learning algorithm such as spectral learning for HMMs [1, 2] to find the parameters of an MHMM.",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "Therefore, it should be possible to use an HMM learning algorithm such as spectral learning for HMMs [1, 2] to find the parameters of an MHMM.",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "For the last inequality, we used the fact that A ∈ RMK×MK has entries in the interval [0, 1] and we used the sample complexity result from [1].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "The bound specified in [1] is for a mixture model, but since the two models are similar and the estimation procedure is almost identical, we are reusing it.",
      "startOffset" : 23,
      "endOffset" : 26
    } ],
    "year" : 2014,
    "abstractText" : "In this paper, we propose a learning approach for the Mixture of Hidden Markov Models (MHMM) based on the Method of Moments (MoM). Computational advantages of MoM make MHMM learning amenable for large data sets. It is not possible to directly learn an MHMM with existing learning approaches, mainly due to a permutation ambiguity in the estimation process. We show that it is possible to resolve this ambiguity using the spectral properties of a global transition matrix even in the presence of estimation noise. We demonstrate the validity of our approach on synthetic and real data.",
    "creator" : null
  }
}