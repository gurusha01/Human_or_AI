{
  "name" : "459a4ddcb586f24efd9395aa7662bc7c.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sparse Random Features Algorithm as Coordinate Descent in Hilbert Space",
    "authors" : [ "Ian E.H. Yen", "Ting-Wei Lin", "Shou-De Lin", "Pradeep Ravikumar", "Inderjit S. Dhillon" ],
    "emails" : [ "ianyen@cs.utexas.edu,", "pradeepr@cs.utexas.edu,", "inderjit@cs.utexas.edu,", "b97083@csie.ntu.edu.tw", "sdlin@csie.ntu.edu.tw" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Kernel methods have become standard for building non-linear models from simple feature representations, and have proven successful in problems ranging across classification, regression, structured prediction and feature extraction [16, 20]. A caveat however is that they are not scalable as the number of training samples increases. In particular, the size of the models produced by kernel methods scale linearly with the number of training samples, even for sparse kernel methods like support vector machines [17]. This makes the corresponding training and prediction computationally prohibitive for large-scale problems.\nA line of research has thus been devoted to kernel approximation methods that aim to preserve predictive performance, while maintaining computational tractability. Among these, Random Features has attracted considerable recent interest due to its simplicity and efficiency [2, 3, 4, 5, 10, 6]. Since first proposed in [2], and extended by several works [3, 4, 5, 10], the Random Features approach is a sampling based approximation to the kernel function, where by drawing D features from the distribution induced from the kernel function, one can guarantee uniform convergence of approximation error to the order of O(1/ √ D). On the flip side, such a rate of convergence suggests that in order to achieve high precision, one might need a large number of random features, which might lead to model sizes even larger than that of the vanilla kernel method.\nOne approach to remedy this problem would be to employ feature selection techniques to prevent the model size from growing linearly with D. A simple way to do so would be by adding ℓ1regularization to the objective function, so that one can simultaneously increase the number of random features D, while selecting a compact subset of them with non-zero weight. However, the resulting algorithm cannot be justified by existing analyses of Random Features, since the Representer theorem does not hold for the ℓ1-regularized problem [15, 16]. In other words, since the prediction\ncannot be expressed as a linear combination of kernel evaluations, a small error in approximating the kernel function cannot correspondingly guarantee a small prediction error.\nIn this paper, we propose a new interpretation of Random Features that justifies its usage with ℓ1-regularization — yielding the Sparse Random Features algorithm. In particular, we show that the Sparse Random Feature algorithm can be seen as Randomized Coordinate Descent (RCD) in the Hilbert Space induced from the kernel, and by taking D steps of coordinate descent, one can achieve a solution comparable to exact kernel methods within O(1/D) precision in terms of the objective function. Note that the surprising facet of this analysis is that in the finite-dimensional case, the iteration complexity of RCD increases with number of dimensions [18], which would trivially yield a bound going to infinity for our infinite-dimensional problem. In our experiments, the Sparse Random Features algorithm obtains a sparse solution that requires less memory and prediction time, while maintaining comparable performance on regression and classification tasks with various kernels. Note that our technique is complementary to that proposed in [10], which aims to reduce the cost of evaluating and storing basis functions, while our goal is to reduce the number of basis functions in a model.\nAnother interesting aspect of our algorithm is that our infinite-dimensional ℓ1-regularized objective is also considered in the literature of Boosting [7, 8], which can be interpreted as greedy coordinate descent in the infinite-dimensional space. As an approximate solver for the ℓ1-regularized problem, we compare our randomized approach to the boosting approach in theory and also in experiments. As we show, for basis functions that do not allow exact greedy search, a randomized approach enjoys better guarantees."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "We are interested in estimating a prediction function f : X→Y from training data set D = {(xn, yn)}Nn=1, (xn, yn) ∈ X × Y by solving an optimization problem over some Reproducing Kernel Hilbert Space (RKHS)H:\nf∗ = argmin f∈H\nλ 2 ∥f∥2H + 1 N N∑ n=1 L(f(xn), yn), (1)\nwhere L(z, y) is a convex loss function with Lipschitz-continuous derivative satisfying |L′(z1, y)− L′(z2, y)| ≤ β|z1 − z2|, which includes several standard loss functions such as the square-loss L(z, y) = 12 (z − y)\n2, square-hinge loss L(z, y) = max(1 − zy, 0)2 and logistic loss L(z, y) = log(1 + exp(−yz))."
    }, {
      "heading" : "2.1 Kernel and Feature Map",
      "text" : "There are two ways in practice to specify the space H. One is via specifying a positive-definite kernel k(x,y) that encodes similarity between instances, and where H can be expressed as the completion of the space spanned by {k(x, ·)}x∈X , that is,\nH = { f(·) =\nK∑ i=1 αik(xi, ·) | αi ∈ R,xi ∈ X\n} .\nThe other way is to find an explicit feature map {ϕ̄h(x)}h∈H , where each h ∈ H defines a basis function ϕ̄h(x) : X → R. The RKHSH can then be defined as\nH = { f(·) = ∫ h∈H w(h)ϕ̄h(·)dh = ⟨w, ϕ̄(·)⟩H | ∥f∥2H <∞ } , (2)\nwhere w(h) is a weight distribution over the basis {ϕh(x)}h∈H. By Mercer’s theorem [1], every positive-definite kernel k(x,y) has a decomposition s.t.\nk(x,y) = ∫ h∈H p(h)ϕh(x)ϕh(y)dh = ⟨ϕ̄(x), ϕ̄(y)⟩H, (3)\nwhere p(h) ≥ 0 and ϕ̄h(.) = √ p(h)ϕh(.), denoted as ϕ̄ = √ p ◦ ϕ. However, the decomposition is not unique. One can derive multiple decompositions from the same kernel k(x,y) based on\ndifferent sets of basis functions {ϕh(x)}h∈H . For example, in [2], the Laplacian kernel k(x,y) = exp(−γ∥x − y∥1) can be decomposed through both the Fourier basis and the Random Binning basis, while in [7], the Laplacian kernel can be obtained from the integrating of an infinite number of decision trees.\nOn the other hand, multiple kernels can be derived from the same set of basis functions via different distribution p(h). For example, in [2, 3], a general decomposition method using Fourier basis functions { ϕω(x) = cos(ω Tx) } ω∈Rd was proposed to find feature map for any shift-invariant kernel of the form k(x − y), where the feature maps (3) of different kernels k(∆) differ only in the distribution p(ω) obtained from the Fourier transform of k(∆). Similarly, [5] proposed decomposition based on polynomial basis for any dot-product kernel of the form k(⟨x,y⟩)."
    }, {
      "heading" : "2.2 Random Features as Monte-Carlo Approximation",
      "text" : "The standard kernel method, often referred to as the “kernel trick,” solves problem (1) through the Representer Theorem [15, 16], which states that the optimal decision function f∗ ∈ H lies in the span of training samples HD = { f(·) = ∑N n=1 αnk(xn, ·) | αn ∈ R, (xn, yn) ∈ D } , which reduces the infinite-dimensional problem (1) to a finite-dimensional problem with N variables {αn}Nn=1. However, it is known that even for loss functions with dual-sparsity (e.g. hinge-loss), the number of non-zero αn increases linearly with data size [17].\nRandom Features has been proposed as a kernel approximation method [2, 3, 10, 5], where a MonteCarlo approximation\nk(xi,xj) = Ep(h)[ϕh(xi)ϕh(xj)] ≈ 1\nD D∑ k=1 ϕhk(xi)ϕhk(xj) = z(xi) Tz(xj) (4)\nis used to approximate (3), so that the solution to (1) can be obtained by\nwRF = argmin w∈RD\nλ 2 ∥w∥2 + 1 N N∑ n=1 L(wTz(xn), yn). (5)\nThe corresponding approximation error ∣∣wTRFz(x)− f∗(x)∣∣ = ∣∣∣∣∣ N∑ n=1 αRFn z(xn) Tz(x)− N∑ n=1 α∗nk(xn,x) ∣∣∣∣∣ , (6) as proved in [2,Appendix B], can be bounded by ϵ given D = Ω(1/ϵ2) number of random features, which is a direct consequence of the uniform convergence of the sampling approximation (4). Unfortunately, the rate of convergence suggests that to achieve small approximation error ϵ, one needs significant amount of random features, and since the model size of (5) grows linearly with D, such an algorithm might not obtain a sparser model than kernel method. On the other hand, the ℓ1-regularized Random-Feature algorithm we are proposing aims to minimize loss with a selected subset of random feature that neither grows linearly with D nor with N . However, (6) does not hold for ℓ1-regularization, and thus one cannot transfer guarantee from kernel approximation (4) to the learned decision function."
    }, {
      "heading" : "3 Sparse Random Feature as Coordinate Descent",
      "text" : "In this section, we present the Sparse Random Features algorithm and analyze its convergence by interpreting it as a fully-corrective randomized coordinate descent in a Hilbert space. Given a feature map of orthogonal basic functions {ϕ̄h(x) = √ p(h)ϕh(x)}h∈H , the optimization program (1) can be written as the infinite-dimensional optimization problem\nmin w∈H\nλ 2 ∥w∥22 + 1 N N∑ n=1 L(⟨w, ϕ̄(xn)⟩H, yn). (7)\nInstead of directly minimizing (7), the Sparse Random Features algorithm optimizes the related ℓ1-regularized problem defined as\nmin w̄∈H\nF (w̄) = λ∥w̄∥1 + 1\nN N∑ n=1 L(⟨w̄,ϕ(xn)⟩H, yn), (8)\nwhere ϕ̄(x) = √ p ◦ ϕ(x) is replaced by ϕ(x) and ∥w̄∥1 is defined as the ℓ1-norm in function space ∥w̄∥1 = ∫ h∈H |w̄(h)|dh. The whole procedure is depicted in Algorithm 1. At each iteration, we draw R coordinates h1, h2, ..., hR from distribution p(h), add them into a working set At, and minimize (8) w.r.t. the working set At as\nmin w̄(h),h∈At λ ∑ h∈At |w̄(h)|+ 1 N N∑ n=1 L( ∑ h∈At w̄(h)ϕh(xn), yn). (9)\nAt the end of each iteration, the algorithm removes features with zero weight to maintain a compact working set.\nAlgorithm 1 Sparse Random-Feature Algorithm Initialize w̄0 = 0, working set A(0) = {}, and t = 0. repeat\n1. Sample h1, h2, ..., hR i.i.d. from distribution p(h). 2. Add h1, h2, ..., hR to the set A(t). 3. Obtain w̄t+1 by solving (9). 4. A(t+1) = A(t) \\ { h | w̄t+1(h) = 0 } .\n5. t← t+ 1. until t = T"
    }, {
      "heading" : "3.1 Convergence Analysis",
      "text" : "In this section, we analyze the convergence behavior of Algorithm 1. The analysis comprises of two parts. First, we estimate the number of iterations Algorithm 1 takes to produce a solution wt that is at most ϵ away from some arbitrary reference solution wref on the ℓ1-regularized program (8). Then, by taking wref as the optimal solution w∗ of (7), we obtain an approximation guarantee for wt with respect to w∗. The proofs for most lemmas and corollaries will be in the appendix. Lemma 1. Suppose loss function L(z, y) has β-Lipschitz-continuous derivative and |ϕh(x)| ≤ B,∀h ∈ H,∀x ∈ X . The loss term Loss(w̄;ϕ) = 1N ∑N n=1 L(⟨w̄,ϕ(xn)⟩, yn) in (8) has\nLoss(w̄ + ηδh;ϕ)− Loss(w̄;ϕ) ≤ ghη + γ\n2 η2,\nwhere δh = δ(∥x − h∥) is a Dirac function centered at h, and gh = ∇w̄Loss(w̄;ϕ)(h) is the Frechet derivative of the loss term evaluated at h, and γ = βB2.\nThe above lemma states smoothness of the loss term, which is essential to guarantee descent amount obtained by taking a coordinate descent step. In particular, we aim to express the expected progress made by Algorithm 1 as the proximal-gradient magnitude of F̄ (w) = F ( √ p ◦w) defined as\nF̄ (w) = λ∥√p ◦w∥1 + 1\nN N∑ n=1 L(⟨w, ϕ̄(xn)⟩, yn). (10)\n. Let g = ∇w̄Loss(w̄,ϕ), ḡ = ∇wLoss(w, ϕ̄) be the gradients of loss terms in (8), (10) respectively, and let ρ ∈ ∂ (λ∥w̄∥1). We have following relations between (8) and (10):\nρ̄ := √ p ◦ ρ ∈ ∂ (λ∥√p ◦w∥1), ḡ = √ p ◦ g, (11)\nby simple applications of the chain rule. We then analyze the progress made by each iteration of Algorithm 1. Recalling that we used R to denote the number of samples drawn in step 1 of our algorithm, we will first assume R = 1, and then show that same result holds also for R > 1.\nTheorem 1 (Descent Amount). The expected descent of the iterates of Algorithm 1 satisfies\nE[F (w̄t+1)]− F (w̄t) ≤ −γ∥η̄ t∥2\n2 , (12)\nwhere η̄ is the proximal gradient of (10), that is,\nη̄ = argmin η\nλ∥√p ◦ (wt + η)∥1 − λ∥ √ p ◦wt∥1 + ⟨ḡ,η⟩+ γ\n2 ∥η∥2 (13)\nand ḡ = ∇wLoss(wt, ϕ̄) is the derivative of loss term w.r.t. w.\nProof. Let gh = ∇w̄Loss(w̄t,ϕ)(h). By Corollary 1, we have\nF (w̄t + ηδh)− F (w̄t) ≤ λ|w̄t(h) + η| − λ|w̄t(h)|+ ghη + γ\n2 η2. (14)\nMinimizing RHS w.r.t. η, the minimizer ηh should satisfy\ngh + ρh + γηh = 0 (15)\nfor some sub-gradient ρh ∈ ∂ (λ|w̄t(h) + ηh|). Then by definition of sub-gradient and (15) we have\nλ|w̄t(h) + η| − λ|w̄t(h)|+ ghη + γ\n2 η2 ≤ ρhηh + ghηh +\nγ 2 η2h (16)\n= −γη2h + γ\n2 η2h = −\nγ 2 η2h. (17)\nNote the equality in (16) holds if w̄t(h) = 0 or the optimal ηh = 0, which is true for Algorithm 1. Since w̄t+1 minimizes (9) over a block At containing h, we have F (w̄t+1) ≤ F (w̄t + ηhδh). Combining (14) and (16), taking expectation over h on both sides, and then we have\nE[F (w̄t+1)]− F (w̄t) ≤ −γ 2 E[η2h] = ∥ √ p ◦ η∥2 = ∥η̄∥2\nThen it remains to verify that η̄ = √ p ◦ η is the proximal gradient (13) of F̄ (wt), which is true since η̄ satisfies the optimality condition of (13)\nḡ + ρ̄+ γη̄ = √ p ◦ (g + ρ+ γη) = 0,\nwhere first equality is from (11) and the second is from (15).\nTheorem 2 (Convergence Rate). Given any reference solution wref , the sequence {wt}∞t=1 satisfies\nE[F̄ (wt)] ≤ F̄ (wref ) + 2γ∥w ref∥2\nk , (18)\nwhere k = max{t− c, 0} and c = 2(F̄ (0)−F̄ (w ref ))\nγ∥wref∥2 is a constant.\nProof. First, the equality actually holds in inequality (16), since for h /∈ A(t−1), we have wt(h) = 0, which implies λ|wt(h) + η| − λ|wt(h)| = ρη, ρ ∈ ∂(λ|wt(h) + η|), and for h ∈ At−1 we have η̄h = 0, which gives 0 to both LHS and RHS. Therefore, we have\n− γ 2 ∥η̄∥2 = min η λ∥√p ◦ (wt + η)∥1 − λ∥ √ p ◦wt∥1 + ḡTη + γ 2 ∥η∥2. (19)\nNote the minimization in (19) is separable for different coordinates. For h ∈ A(t−1), the weight wt(h) is already optimal in the beginning of iteration t, so we have ρ̄h + ḡh = 0 for some ρ̄h ∈ ∂(| √ p(h)w(h)|). Therefore, ηh = 0, h ∈ A(t−1) is optimal both to (| √ p(h)(w(h) + ηh)|+ ḡhηh) and to γ2 η 2 h. Set ηh = 0 for the latter, we have\n−γ 2 ∥η̄∥2 = min η\n{ λ∥√p ◦ (wt + η)∥1 − λ∥ √ p ◦wt∥1 + ⟨ḡ,η⟩+ γ\n2 ∫ h/∈A(t−1) η2hdh } ≤ min\nη\n{ F̄ (wt + η)− F̄ (wt) + γ\n2 ∫ h/∈A(t−1) η2hdh }\nfrom convexity of F̄ (w). Consider solution of the form η = α(wref −wt), we have\n−γ 2 ∥η̄∥2 ≤ min α∈[0,1]\n{ F̄ ( wt + α(wref −wt) ) − F̄ (wt) + γα 2\n2 ∫ h/∈A(t−1) (wref (h)− wt(h))2dh }\n≤ min α∈[0,1]\n{ F̄ (wt) + α ( F̄ (wref )− F̄ (wt) ) − F̄ (wt) + γα 2\n2 ∫ h/∈A(t−1) wref (h)2dh } ≤ min\nα∈[0,1]\n{ −α ( F̄ (wt)− F̄ (wref ) ) + γα2\n2 ∥wref∥2\n} ,\nwhere the second inequality results from wt(h) = 0, h /∈ A(t−1). Minimizing last expression w.r.t. α, we have α∗ = min\n( F̄ (wt)−F̄ (wref ) γ∥wref∥2 , 1 ) and\n−γ 2 ∥η̄∥2 ≤\n{ − ( F̄ (wt)− F̄ (wref ) )2 /(2γ∥wref∥2) , if F̄ (wt)− F̄ (wref ) < γ∥wref∥2\n−γ2 ∥w ref∥2 , o.w.\n.\n(20)\nNote, since the function value {F̄ (wt)}∞t=1 is non-increasing, only iterations in the beginning fall in second case of (20), and the number of such iterations is at most c = ⌈ 2(F̄ (0)−F̄ (w\nref )) γ∥wref∥2 ⌉. For t > c,\nwe have\nE[F̄ (wt+1)]− F̄ (wt) ≤ −γ∥η̄ t∥22 2 ≤ − (F̄ (w t)− F̄ (wref ))2 2γ∥wref∥2 . (21)\nThe recursion then leads to the result.\nNote the above bound does not yield useful result if ∥wref∥2 → ∞. Fortunately, the optimal solution of our target problem (7) has finite ∥w∗∥2 as long as in (7) λ > 0, so it always give a useful bound when plugged into (18), as following corollary shows.\nCorollary 1 (Approximation Guarantee). The output of Algorithm 1 satisfies E [ λ∥w̄(D)∥1 + Loss(w̄(D);ϕ) ] ≤ { λ∥w∗∥2 + Loss(w∗; ϕ̄) } +\n2γ∥w∗∥22 D′\n(22)\nwith D′ = max{D − c, 0}, where w∗ is the optimal solution of problem (7), c is a constant defined in Theorem 2.\nThen the following two corollaries extend the guarantee (22) to any R ≥ 1, and a bound holds with high probability. The latter is a direct result of [18,Theorem 1] applied to the recursion (21). Corollary 2. The bound (22) holds for any R ≥ 1 in Algorithm 1, where if there are T iterations then D = TR. Corollary 3. For D ≥ 2γ∥w\n∗∥2 ϵ (1 + log 1 ρ ) + 2− 4 c + c , the output of Algorithm 1 has λ∥w̄(D)∥1 + Loss(w̄(D);ϕ) ≤ { λ∥w∗∥2 + Loss(w∗; ϕ̄) } + ϵ (23)\nwith probability 1− ρ, where c is as defined in Theorem 2 and w∗ is the optimal solution of (7)."
    }, {
      "heading" : "3.2 Relation to the Kernel Method",
      "text" : "Our result (23) states that, for D large enough, the Sparse Random Features algorithm achieves either a comparable loss to that of the vanilla kernel method, or a model complexity (measured in ℓ1-norm) less than that of kernel method (measured in ℓ2-norm). Furthermore, since w∗ is not the optimal solution of the ℓ1-regularized program (8), it is possible for the LHS of (23) to be much smaller than the RHS. On the other hand, since any w∗ of finite ℓ2-norm can be the reference solution wref , the λ used in solving the ℓ1-regularized problem (8) can be different from the λ used in the kernel method. The tightest bound is achieved by minimizing the RHS of (23), which is equivalent to minimizing (7) with some unknown λ̃(λ) due to the difference of ∥w∥1 and ∥w∥22. In practice, we can follow a regularization path to find small enough λ that yields comparable predictive performance while maintains model as compact as possible. Note, when using different sampling distribution p(h) from the decomposition (3), our analysis provides different bounds (23) for the Randomized Coordinate Descent in Hilbert Space. This is in contrast to the analysis in the finite-dimensional case, where RCD with different sampling distribution converges to the same solution [18]."
    }, {
      "heading" : "3.3 Relation to the Boosting Method",
      "text" : "Boosting is a well-known approach to minimize infinite-dimensional problems with ℓ1regularization [8, 9], and which in this setting, performs greedy coordinate descent on (8). For each iteration t, the algorithm finds the coordinate h(t) yielding steepest descent in the loss term\nh(t) = argmin h∈H\n1\nN N∑ n=1 L′nϕh(xn) (24)\nto add into a working set At and minimize (8) w.r.t. At. When the greedy step (24) can be solved exactly, Boosting has fast convergence to the optimal solution of (8) [13, 14]. On the contrary, randomized coordinate descent can only converge to a sub-optimal solution in finite time when there are infinite number of dimensions. However, in practice, only a very limited class of basis functions allow the greedy step in (24) to be performed exactly. For most basis functions (weak learners) such as perceptrons and decision trees, the greedy step (24) can only be solved approximately. In such cases, Boosting might have no convergence guarantee, while the randomized approach is still guaranteed to find a comparable solution to that of the kernel method. In our experiments, we found that the randomized coordinate descent performs considerably better than approximate Boosting with the perceptron basis functions (weak learners), where as adopted in the Boosting literature [19, 8], a convex surrogate loss is used to solve (24) approximately."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we compare Sparse Random Features (Sparse-RF) to the existing Random Features algorithm (RF) and the kernel method (Kernel) on regression and classification problems with kernels set to Gaussian RBF, Laplacian RBF [2], and Perceptron kernel [7] 1. For Gaussian and Laplacian RBF kernel, we use Fourier basis function with corresponding distribution p(h) derived in [2]; for Perceptron kernel, we use perceptron basis function with distribution p(h) being uniform over unit-sphere as shown in [7]. For regression, we solve kernel ridge regression (1) and RF regression (6) in closed-form as in [10] using Eigen, a standard C++ library of numerical linear algebra. For Sparse-RF, we solve the LASSO sub-problem (9) by standard RCD algorithm. In classification, we use LIBSVM2as solver of kernel method, and use Newton-CG method and Coordinate Descent method in LIBLINEAR [12] to solve the RF approximation (6) and Sparse-RF sub-problem (9) respectively. We set λN = Nλ = 1 for the kernel and RF methods, and for Sparse-RF, we choose λN ∈ {1, 10, 100, 1000} that gives RMSE (accuracy) closest to the RF method to compare sparsity and efficiency. The results are in Tables 1 and 2, where the cost of kernel method grows at least quadratically in the number of training samples. For YearPred, we use D = 5000 to maintain tractability of the RF method. Note for Covtype dataset, the ℓ2-norm ∥w∗∥2 from kernel machine is significantly larger than that of others, so according to (22), a larger number of random features D are required to obtain similar performance, as shown in Figure 1.\nIn Figure 1, we compare Sparse-RF (randomized coordinate descent) to Boosting (greedy coordinate descent) and the bound (23) obtained from SVM with Perceptron kernel and basis function (weak learner). The figure shows that Sparse-RF always converges to a solution comparable to that of the kernel method, while Boosting with approximate greedy steps (using convex surrogate loss) converges to a higher objective value, due to bias from the approximation."
    }, {
      "heading" : "Acknowledgement",
      "text" : "S.-D.Lin acknowledges the support of Telecommunication Lab., Chunghwa Telecom Co., Ltd via TL-1038201, AOARD via No. FA2386-13-1-4045, Ministry of Science and Technology, National Taiwan University and Intel Co. via MOST102-2911-I-002-001, NTU103R7501, 102-2923-E-002-007-MY2, 102-2221-E-002170, 103-2221-E-002-104-MY2. P.R. acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, IIS-1447574, and DMS-1264033. This research was also supported by NSF grants CCF-1320746 and CCF-1117055.\n2Data set for classification can be downloaded from LIBSVM data set web page, and data set for regression can be found at UCI Machine Learning Repository and Ali Rahimi’s page for the paper [2].\n2We follow the FAQ page of LIBSVM to replace hinge-loss by square-hinge-loss for comparison."
    } ],
    "references" : [ {
      "title" : "Functions of positive and negative type and their connection with the theory of integral equations",
      "author" : [ "J. Mercer" ],
      "venue" : "Royal Society",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1909
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "NIPS 20,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2007
    }, {
      "title" : "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "NIPS 21,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Efficient additive kernels via explicit feature maps",
      "author" : [ "A. Vedaldi", "A. Zisserman" ],
      "venue" : "In CVPR",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Random feature maps for dot product kernels",
      "author" : [ "P. Kar", "H. Karnick" ],
      "venue" : "In Proceedings of AIS- TATS’12,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Nystrom method vs. random Fourier features: A theoretical and empirical comparison",
      "author" : [ "T. Yang", "Y.-F. Li", "M. Mahdavi", "R. Jin", "Z.-H. Zhou" ],
      "venue" : "In Adv. NIPS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Support Vector Machinery for Infinite Ensemble Learnings",
      "author" : [ "Husan-Tien Lin", "Ling Li" ],
      "venue" : "JMLR",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "Boosting as a Regularized Path to a Maximum Margin Classifier",
      "author" : [ "Saharon Rosset", "Ji Zhu", "Trevor Hastie" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "l1-regularization in infinite dimensional feature spaces",
      "author" : [ "Saharon Rosset", "Grzegorz Swirszcz", "Nathan Srebro", "Ji Zhu" ],
      "venue" : "In Learning Theory: 20th Annual Conference on Learning Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "Fastfood - approximating kernel expansions in loglinear time",
      "author" : [ "Q. Le", "T. Sarlos", "A.J. Smola" ],
      "venue" : "In The 30th International Conference on Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "On the convergence of leveraging",
      "author" : [ "Gunnar Ratsch", "Sebastian Mika", "Manfred K. Warmuth" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2001
    }, {
      "title" : "The Fast Convergence of Boosting",
      "author" : [ "Matus Telgarsky" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "A correspondence between Bayesian estimation on stochastic processes and smoothing by splines",
      "author" : [ "G.S. Kimeldorf", "G. Wahba" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1970
    }, {
      "title" : "Learning with Kernels",
      "author" : [ "Scholkopf", "Bernhard", "A.J. Smola" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2002
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function, School of Mathematics, University of Edinburgh",
      "author" : [ "P. Ricktarik", "M. Takac" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "An online boosting algorithm with theoretical justifications",
      "author" : [ "Chen", "S.-T", "Lin", "H.-T", "Lu", "C.-J" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Reproducing kernel Banach spaces with the l1 norm",
      "author" : [ "G. Song et.al" ],
      "venue" : "Journal of Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Kernel methods have become standard for building non-linear models from simple feature representations, and have proven successful in problems ranging across classification, regression, structured prediction and feature extraction [16, 20].",
      "startOffset" : 231,
      "endOffset" : 239
    }, {
      "referenceID" : 1,
      "context" : "Among these, Random Features has attracted considerable recent interest due to its simplicity and efficiency [2, 3, 4, 5, 10, 6].",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "Among these, Random Features has attracted considerable recent interest due to its simplicity and efficiency [2, 3, 4, 5, 10, 6].",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "Among these, Random Features has attracted considerable recent interest due to its simplicity and efficiency [2, 3, 4, 5, 10, 6].",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "Among these, Random Features has attracted considerable recent interest due to its simplicity and efficiency [2, 3, 4, 5, 10, 6].",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "Among these, Random Features has attracted considerable recent interest due to its simplicity and efficiency [2, 3, 4, 5, 10, 6].",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 5,
      "context" : "Among these, Random Features has attracted considerable recent interest due to its simplicity and efficiency [2, 3, 4, 5, 10, 6].",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "Since first proposed in [2], and extended by several works [3, 4, 5, 10], the Random Features approach is a sampling based approximation to the kernel function, where by drawing D features from the distribution induced from the kernel function, one can guarantee uniform convergence of approximation error to the order of O(1/ √ D).",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "Since first proposed in [2], and extended by several works [3, 4, 5, 10], the Random Features approach is a sampling based approximation to the kernel function, where by drawing D features from the distribution induced from the kernel function, one can guarantee uniform convergence of approximation error to the order of O(1/ √ D).",
      "startOffset" : 59,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Since first proposed in [2], and extended by several works [3, 4, 5, 10], the Random Features approach is a sampling based approximation to the kernel function, where by drawing D features from the distribution induced from the kernel function, one can guarantee uniform convergence of approximation error to the order of O(1/ √ D).",
      "startOffset" : 59,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "Since first proposed in [2], and extended by several works [3, 4, 5, 10], the Random Features approach is a sampling based approximation to the kernel function, where by drawing D features from the distribution induced from the kernel function, one can guarantee uniform convergence of approximation error to the order of O(1/ √ D).",
      "startOffset" : 59,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "Since first proposed in [2], and extended by several works [3, 4, 5, 10], the Random Features approach is a sampling based approximation to the kernel function, where by drawing D features from the distribution induced from the kernel function, one can guarantee uniform convergence of approximation error to the order of O(1/ √ D).",
      "startOffset" : 59,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "However, the resulting algorithm cannot be justified by existing analyses of Random Features, since the Representer theorem does not hold for the l1-regularized problem [15, 16].",
      "startOffset" : 169,
      "endOffset" : 177
    }, {
      "referenceID" : 15,
      "context" : "However, the resulting algorithm cannot be justified by existing analyses of Random Features, since the Representer theorem does not hold for the l1-regularized problem [15, 16].",
      "startOffset" : 169,
      "endOffset" : 177
    }, {
      "referenceID" : 16,
      "context" : "Note that the surprising facet of this analysis is that in the finite-dimensional case, the iteration complexity of RCD increases with number of dimensions [18], which would trivially yield a bound going to infinity for our infinite-dimensional problem.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 9,
      "context" : "Note that our technique is complementary to that proposed in [10], which aims to reduce the cost of evaluating and storing basis functions, while our goal is to reduce the number of basis functions in a model.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "Another interesting aspect of our algorithm is that our infinite-dimensional l1-regularized objective is also considered in the literature of Boosting [7, 8], which can be interpreted as greedy coordinate descent in the infinite-dimensional space.",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 7,
      "context" : "Another interesting aspect of our algorithm is that our infinite-dimensional l1-regularized objective is also considered in the literature of Boosting [7, 8], which can be interpreted as greedy coordinate descent in the infinite-dimensional space.",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 0,
      "context" : "By Mercer’s theorem [1], every positive-definite kernel k(x,y) has a decomposition s.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "For example, in [2], the Laplacian kernel k(x,y) = exp(−γ∥x − y∥1) can be decomposed through both the Fourier basis and the Random Binning basis, while in [7], the Laplacian kernel can be obtained from the integrating of an infinite number of decision trees.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : "For example, in [2], the Laplacian kernel k(x,y) = exp(−γ∥x − y∥1) can be decomposed through both the Fourier basis and the Random Binning basis, while in [7], the Laplacian kernel can be obtained from the integrating of an infinite number of decision trees.",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 1,
      "context" : "For example, in [2, 3], a general decomposition method using Fourier basis functions { φω(x) = cos(ω x) } ω∈Rd was proposed to find feature map for any shift-invariant kernel of the form k(x − y), where the feature maps (3) of different kernels k(∆) differ only in the distribution p(ω) obtained from the Fourier transform of k(∆).",
      "startOffset" : 16,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "For example, in [2, 3], a general decomposition method using Fourier basis functions { φω(x) = cos(ω x) } ω∈Rd was proposed to find feature map for any shift-invariant kernel of the form k(x − y), where the feature maps (3) of different kernels k(∆) differ only in the distribution p(ω) obtained from the Fourier transform of k(∆).",
      "startOffset" : 16,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "Similarly, [5] proposed decomposition based on polynomial basis for any dot-product kernel of the form k(⟨x,y⟩).",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 14,
      "context" : "The standard kernel method, often referred to as the “kernel trick,” solves problem (1) through the Representer Theorem [15, 16], which states that the optimal decision function f∗ ∈ H lies in the span of training samples HD = { f(·) = ∑N n=1 αnk(xn, ·) | αn ∈ R, (xn, yn) ∈ D } , which reduces the infinite-dimensional problem (1) to a finite-dimensional problem with N variables {αn}n=1.",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "The standard kernel method, often referred to as the “kernel trick,” solves problem (1) through the Representer Theorem [15, 16], which states that the optimal decision function f∗ ∈ H lies in the span of training samples HD = { f(·) = ∑N n=1 αnk(xn, ·) | αn ∈ R, (xn, yn) ∈ D } , which reduces the infinite-dimensional problem (1) to a finite-dimensional problem with N variables {αn}n=1.",
      "startOffset" : 120,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "Random Features has been proposed as a kernel approximation method [2, 3, 10, 5], where a MonteCarlo approximation",
      "startOffset" : 67,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "Random Features has been proposed as a kernel approximation method [2, 3, 10, 5], where a MonteCarlo approximation",
      "startOffset" : 67,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "Random Features has been proposed as a kernel approximation method [2, 3, 10, 5], where a MonteCarlo approximation",
      "startOffset" : 67,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Random Features has been proposed as a kernel approximation method [2, 3, 10, 5], where a MonteCarlo approximation",
      "startOffset" : 67,
      "endOffset" : 80
    }, {
      "referenceID" : 16,
      "context" : "This is in contrast to the analysis in the finite-dimensional case, where RCD with different sampling distribution converges to the same solution [18].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "3 Relation to the Boosting Method Boosting is a well-known approach to minimize infinite-dimensional problems with l1regularization [8, 9], and which in this setting, performs greedy coordinate descent on (8).",
      "startOffset" : 132,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "3 Relation to the Boosting Method Boosting is a well-known approach to minimize infinite-dimensional problems with l1regularization [8, 9], and which in this setting, performs greedy coordinate descent on (8).",
      "startOffset" : 132,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : "When the greedy step (24) can be solved exactly, Boosting has fast convergence to the optimal solution of (8) [13, 14].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "When the greedy step (24) can be solved exactly, Boosting has fast convergence to the optimal solution of (8) [13, 14].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "In our experiments, we found that the randomized coordinate descent performs considerably better than approximate Boosting with the perceptron basis functions (weak learners), where as adopted in the Boosting literature [19, 8], a convex surrogate loss is used to solve (24) approximately.",
      "startOffset" : 220,
      "endOffset" : 227
    }, {
      "referenceID" : 7,
      "context" : "In our experiments, we found that the randomized coordinate descent performs considerably better than approximate Boosting with the perceptron basis functions (weak learners), where as adopted in the Boosting literature [19, 8], a convex surrogate loss is used to solve (24) approximately.",
      "startOffset" : 220,
      "endOffset" : 227
    }, {
      "referenceID" : 1,
      "context" : "In this section, we compare Sparse Random Features (Sparse-RF) to the existing Random Features algorithm (RF) and the kernel method (Kernel) on regression and classification problems with kernels set to Gaussian RBF, Laplacian RBF [2], and Perceptron kernel [7] 1.",
      "startOffset" : 231,
      "endOffset" : 234
    }, {
      "referenceID" : 6,
      "context" : "In this section, we compare Sparse Random Features (Sparse-RF) to the existing Random Features algorithm (RF) and the kernel method (Kernel) on regression and classification problems with kernels set to Gaussian RBF, Laplacian RBF [2], and Perceptron kernel [7] 1.",
      "startOffset" : 258,
      "endOffset" : 261
    }, {
      "referenceID" : 1,
      "context" : "For Gaussian and Laplacian RBF kernel, we use Fourier basis function with corresponding distribution p(h) derived in [2]; for Perceptron kernel, we use perceptron basis function with distribution p(h) being uniform over unit-sphere as shown in [7].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "For Gaussian and Laplacian RBF kernel, we use Fourier basis function with corresponding distribution p(h) derived in [2]; for Perceptron kernel, we use perceptron basis function with distribution p(h) being uniform over unit-sphere as shown in [7].",
      "startOffset" : 244,
      "endOffset" : 247
    }, {
      "referenceID" : 9,
      "context" : "For regression, we solve kernel ridge regression (1) and RF regression (6) in closed-form as in [10] using Eigen, a standard C++ library of numerical linear algebra.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 11,
      "context" : "In classification, we use LIBSVM2as solver of kernel method, and use Newton-CG method and Coordinate Descent method in LIBLINEAR [12] to solve the RF approximation (6) and Sparse-RF sub-problem (9) respectively.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 1,
      "context" : "Data set for classification can be downloaded from LIBSVM data set web page, and data set for regression can be found at UCI Machine Learning Repository and Ali Rahimi’s page for the paper [2].",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 6,
      "context" : "The dashed line shows the l2-norm plus loss achieved by kernel method (RHS of (22)) and the corresponding error rate using perceptron kernel [7].",
      "startOffset" : 141,
      "endOffset" : 144
    } ],
    "year" : 2014,
    "abstractText" : "In this paper, we propose a Sparse Random Features algorithm, which learns a sparse non-linear predictor by minimizing an l1-regularized objective function over the Hilbert Space induced from a kernel function. By interpreting the algorithm as Randomized Coordinate Descent in an infinite-dimensional space, we show the proposed approach converges to a solution within ε-precision of that using an exact kernel method, by drawing O(1/ε) random features, in contrast to the O(1/ε) convergence achieved by current Monte-Carlo analyses of Random Features. In our experiments, the Sparse Random Feature algorithm obtains a sparse solution that requires less memory and prediction time, while maintaining comparable performance on regression and classification tasks. Moreover, as an approximate solver for the infinite-dimensional l1-regularized problem, the randomized approach also enjoys better convergence guarantees than a Boosting approach in the setting where the greedy Boosting step cannot be performed exactly.",
    "creator" : null
  }
}