{
  "name" : "6a9aeddfc689c1d0e3b9ccc3ab651bc5.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Fast and Robust Least Squares Estimation in Corrupted Linear Models",
    "authors" : [ "Brian McWilliams", "Gabriel Krummenacher", "Mario Lucic", "Joachim M. Buhmann" ],
    "emails" : [ "mcbrian@inf.ethz.ch", "gabriel.krummenacher@inf.ethz.ch", "lucic@inf.ethz.ch", "jbuhmann@inf.ethz.ch" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "To improve scalability of the widely used ordinary least squares algorithm, a number of randomized approximation algorithms have recently been proposed. These methods, based on subsampling the dataset, reduce the computational time from O np2\nto o(np2)1 [14]. Most of these algorithms are concerned with the classical fixed design setting or the case where the data is assumed to be sampled i.i.d. typically from a sub-Gaussian distribution [7]. This is known to be an unrealistic modelling assumption since real-world data are rarely well-behaved in the sense of the underlying distributions.\nWe relax this limiting assumption by considering the setting where with some probability, the observed covariates are corrupted with additive noise. This scenario corresponds to a generalised version of the classical problem of “errors-in-variables” in regression analysis which has recently been considered in the context of sparse estimation [12]. This corrupted observation model poses a more realistic model of real data which may be subject to many different sources of measurement noise or heterogeneity in the dataset.\nA key consideration for sampling is to ensure that the points used for estimation are typical of the full dataset. Typicality requires the sampling distribution to be robust against outliers and corrupted points. In the i.i.d. sub-Gaussian setting, outliers are rare and can often easily be identified by examining the statistical leverage scores of the datapoints.\nCrucially, in the corrupted observation setting described in §2, the concept of an outlying point concerns the relationship between the observed predictors and the response. Now, leverage alone cannot detect the presence of corruptions. Consequently, without using additional knowledge about\n⇤Authors contributed equally. 1Informally: f(n) = o(g(n)) means f(n) grows more slowly than g(n).\nthe corrupted points, the OLS estimator (and its subsampled approximations) are biased. This also rules out stochastic gradient descent (SGD) – which is often used for large scale regression – since convex cost functions and regularizers which are typically used for noisy data are not robust with respect to measurement corruptions.\nThis setting motivates our use of influence – the effective impact of an individual datapoint exerts on the overall estimate – in order to detect and therefore avoid sampling corrupted points. We propose an algorithm which is robust to corrupted observations and exhibits reduced bias compared with other subsampling estimators.\nOutline and Contributions. In §2 we introduce our corrupted observation model before reviewing the basic concepts of statistical leverage and influence in §3. In §4 we briefly review two subsampling approaches to approximating least squares based on structured random projections and leverage weighted importance sampling. Based on these ideas we present influence weighted subsampling (IWS-LS), a novel randomized least squares algorithm based on subsampling points with small influence in §5.\nIn §6 we analyse IWS-LS in the general setting where the observed predictors can be corrupted with additive sub-Gaussian noise. Comparing the IWS-LS estimate with that of OLS and other randomized least squares approaches we show a reduction in both bias and variance. It is important to note that the simultaneous reduction in bias and variance is relative to OLS and randomized approximations which are only unbiased in the non-corrupted setting. Our results rely on novel finite sample characteristics of leverage and influence which we defer to §SI.3. Additionally, in §SI.4 we prove an estimation error bound for IWS-LS in the standard sub-Gaussian model.\nComputing influence exactly is not practical in large-scale applications and so we propose two randomized approximation algorithms based on the randomized leverage approximation of [8]. Both of these algorithms run in o(np2) time which improve scalability in large problems. Finally, in §7 we present extensive experimental evaluation which compares the performance of our algorithms against several randomized least squares methods on a variety of simulated and real datasets."
    }, {
      "heading" : "2 Statistical model",
      "text" : "In this work we consider a variant of the standard linear model\ny = X + ✏, (1)\nwhere ✏ 2 Rn is a noise term independent of X 2 Rn⇥p. However, rather than directly observing X we instead observe Z where Z = X+ UW. (2) U = diag(u\n1 , . . . , un) and ui is a Bernoulli random variable with probability ⇡ of being 1. W 2 Rn⇥p is a matrix of measurement corruptions. The rows of Z therefore are corrupted with probability ⇡ and not corrupted with probability (1 ⇡). Definition 1 (Sub-gaussian matrix). A zero-mean matrix X is called sub-Gaussian with parameter ( 1\nn 2 x, 1 n⌃x) if (a) Each row x > i 2 Rp is sampled independently and has E[xix>i ] = 1n⌃x. (b) For\nany unit vector v 2 Rp, v>xi is a sub-Gaussian random variable with parameter at most 1pp x.\nWe consider the specific instance of the linear corrupted observation model in Eqs. (1), (2) where\n• X,W 2 Rn⇥p are sub-Gaussian with parameters ( 1n 2 x, 1 n⌃x) and ( 1 n 2 w, 1 n⌃w) respectively, • ✏ 2 Rn is sub-Gaussian with parameters ( 1n 2 ✏ , 1 n 2 ✏ In),\nand all are independent of each other.\nThe key challenge is that even when ⇡ and the magnitude of the corruptions, w are relatively small, the standard linear regression estimate is biased and can perform poorly (see §6). Sampling methods which are not sensitive to corruptions in the observations can perform even worse if they somehow subsample a proportion rn > ⇡n of corrupted points. Furthermore, the corruptions may not be large enough to be detected via leverage based techniques alone.\nThe model described in this section generalises the “errors-in-variables” model from classical least squares modelling. Recently, similar models have been studied in the high dimensional (p n)\nsetting in [4–6, 12] in the context of robust sparse estimation. The “low-dimensional” (n > p) setting is investigated in [4], but the “big data” setting (n p) has not been considered so far.2\nIn the high-dimensional problem, knowledge of the corruption covariance, ⌃w [12], or the data covariance ⌃x [5], is required to obtain a consistent estimate. This assumption may be unrealistic in many settings. We aim to reduce the bias in our estimates without requiring knowledge of the true covariance of the data or the corruptions, and instead sub-sample only non-corrupted points."
    }, {
      "heading" : "3 Diagnostics for linear regression",
      "text" : "In practice, the sub-Gaussian linear model assumption is often violated either by heterogeneous noise or by a corruption model as in §2. In such scenarios, fitting a least squares model to the full dataset is unwise since the outlying or corrupted points can have a large adverse effect on the model fit. Regression diagnostics have been developed in the statistics literature to detect such points (see e.g. [2] for a comprehensive overview). Recently, [14] proposed subsampling points for least squares based on their leverage scores. Other recent works suggest related influence measures that identify subspace [16] and multi-view [15] clusters in high dimensional data."
    }, {
      "heading" : "3.1 Statistical leverage",
      "text" : "For the standard linear model in Eq. (1), the well known least squares solution is\nb = argmin ky X k2 = X>X 1 X>y. (3)\nThe projection matrix I L with L := X(X>X) 1X> specifies the subspace in which the residual lies. The diagonal elements of the “hat matrix” L, li := Lii, i = 1, . . . , n are the statistical leverage scores of the ith sample. Leverage scores quantify to what extent a particular sample is an outlier with respect to the distribution of X.\nAn equivalent definition from [14] which will be useful later concerns any matrix U 2 Rn⇥p which spans the column space of X (for example, the matrix whose columns are the left singular vectors of X). The statistical leverage scores of the rows of X are the squared row norms of U, i.e. li = kUik2.\nAlthough the use of leverage can be motivated from the least squares solution in Eq. (3), the leverage scores do not take into account the relationship between the predictor variables and the response variable y. Therefore, low-leverage points may have a weak predictive relationship with the response and vice-versa. In other words, it is possible for such points to be outliers with respect to the conditional distribution P (y|X) but not the marginal distribution on X."
    }, {
      "heading" : "3.2 Influence",
      "text" : "A concept that captures the predictive relationship between covariates and response is influence. Influential points are those that might not be outliers in the geometric sense, but instead adversely affect the estimated coefficients.\nOne way to assess the influence of a point is to compute the change in the learned model when the point is removed from the estimation step. [2]. We can compute a leave-one-out least squares estimator by straightforward application of the Sherman-Morrison-Woodbury formula (see Prop. 3 in §SI.3):\nb i = X>X x>i xi 1 X>y x>i yi = b ⌃ 1x>i ei 1 li\nwhere ei = yi xib OLS. Defining the influence3, di as the change in expected mean squared error we have\ndi = ⇣ b b i ⌘> X>X ⇣ b b i ⌘ = e2i li\n(1 li) 2\n.\n2Unlike [5, 12] and others we do not consider sparsity in our solution since n p. 3The expression we use is also called Cook’s distance [2].\nPoints with large values of di are those which, if added to the model, have the largest adverse effect on the resulting estimate. Since influence only depends on the OLS residual error and the leverage scores, it can be seen that the influence of every point can be computed at the cost of a least squares fit. In the next section we will see how to approximate both quantities using random projections."
    }, {
      "heading" : "4 Fast randomized least squares algorithms",
      "text" : "We briefly review two randomized approaches to least squares approximation: the importance weighted subsampling approach of [9] and the dimensionality reduction approach [14]. The former proposes an importance sampling probability distribution according to which, a small number of rows of X and y are drawn and used to compute the regression coefficients. If the sampling probabilities are proportional to the statistical leverages, the resulting estimator is close to the optimal estimator [9]. We refer to this as LEV-LS.\nThe dimensionality reduction approach can be viewed as a random projection step followed by a uniform subsampling. The class of Johnson-Lindenstrauss projections – e.g. the SRHT – has been shown to approximately uniformize leverage scores in the projected space. Uniformly subsampling the rows of the projected matrix proves to be equivalent to leverage weighted sampling on the original dataset [14]. We refer to this as SRHT-LS. It is analysed in the statistical setting by [7] who also propose ULURU, a two step fitting procedure which aims to correct for the subsampling bias and consequently converges to the OLS estimate at a rate independent of the number of subsamples [7].\nSubsampled Randomized Hadamard Transform (SRHT) The SHRT consists of a preconditioning step after which nsubs rows of the new matrix are subsampled uniformly at random in the following way q\nn n subs SHD · X = ⇧X with the definitions [3]: • S is a subsampling matrix. • D is a diagonal matrix whose entries are drawn independently from { 1, 1}. • H 2 Rn⇥n is a normalized Walsh-Hadamard matrix4 which is defined recursively as\nHn =\n\nHn/2 Hn/2 Hn/2 Hn/2 , H 2 =\n\n+1 +1\n+1 1\n.\nWe set H = 1p n Hn so it has orthonormal columns.\nAs a result, the rows of the transformed matrix ⇧X have approximately uniform leverage scores. (see [17] for detailed analysis of the SRHT). Due to the recursive nature of H, the cost of applying the SRHT is O (pn log nsubs) operations, where nsubs is the number of rows sampled from X [1].\nThe SRHT-LS algorithm solves b SRHT = argmin k⇧y ⇧X k2 which for an appropriate subsampling ratio, r = ⌦(p 2\n⇢2 ) results in a residual error, ˜e which satisfies\nk ˜ek  (1 + ⇢)kek (4)\nwhere e = y Xb OLS is the vector of OLS residual errors [14].\nRandomized leverage computation Recently, a method based on random projections has been proposed to approximate the leverage scores based on first reducing the dimensionality of the data using the SRHT followed by computing the leverage scores using this low-dimensional approximation [8–10, 13].\nThe leverage approximation algorithm of [8] uses a SRHT, ⇧ 1 2 Rr1⇥n to first compute the approximate SVD of X,\n⇧ 1 X = U ⇧X⌃⇧XV> ⇧X . Followed by a second SHRT ⇧2 2 Rp⇥r2 to compute an approximate orthogonal basis for X\nR 1 = V ⇧X⌃ 1 ⇧X 2 Rp⇥p, ˜U = XR 1⇧2 2 Rn⇥r2 . (5)\n4For the Hadamard transform, n must be a power of two but other transforms exist (e.g. DCT, DFT) for which similar theoretical guarantees hold and there is no restriction on n.\nThe approximate leverage scores are now the squared row norms of ˜U, ˜li = k ˜Uik2.\nFrom [14] we derive the following result relating to randomized approximation of the leverage\n˜li  (1 + ⇢l)li , (6)\nwhere the approximation error, ⇢l depends on the choice of projection dimensions r1 and r2.\nThe leverage weighted least squares (LEV-LS) algorithm samples rows of X and y with probability proportional to li (or ˜li in the approximate case) and performs least squares on this subsample. The residual error resulting from the leverage weighted least squares is bounded by Eq. (4) implying that LEV-LS and SRHT-LS are equivalent [14]. It is important to note that under the corrupted observation model these approximations will be biased."
    }, {
      "heading" : "5 Influence weighted subsampling",
      "text" : "In the corrupted observation model, OLS and therefore the random approximations to OLS described in §4 obtain poor predictions. To remedy this, we propose influence weighted subsampling (IWS-LS) which is described in Algorithm 1. IWS-LS subsamples points according to the distribution, Pi = c/di where c is a normalizing constant so that Pn i=1 Pi = 1. OLS is then estimated on the subsampled points. The sampling procedure ensures that points with high influence are selected infrequently and so the resulting estimate is less biased than the full OLS solution. Several approaches similar in spirit have previously been proposed based on identifying and down-weighting the effect of highly influential observations [19].\nObviously, IWS-LS is impractical in the scenarios we consider since it requires the OLS residuals and full leverage scores. However, we use this as a baseline and to simplify the analysis. In the next section, we propose an approximate influence weighted subsampling algorithm which combines the approximate leverage computation of [8] and the randomized least squares approach of [14].\nAlgorithm 1 Influence weighted subsampling (IWS-LS). Input: Data: Z, y\n1: Solve b OLS = argmin ky Z k 2 2: for i = 1 . . . n do 3: ei = yi zib OLS 4: li = z>i (Z\n>Z) 1zi 5: di = e2i li/(1 li) 2 6: end for 7: Sample rows (Z̃, ỹ) of (Z, y) proportional to 1d\ni\n8: Solve b IWS = argmin k˜y ˜Z k 2\nOutput: b IWS\nAlgorithm 2 Residual weighted subsampling (aRWS-LS) Input: Data: Z, y\n1: Solve b SRHT = argmin k⇧ · (y Z )k2 2: Estimate residuals: ẽ = y Zb SRHT 3: Sample rows (˜Z, ˜y) of (Z, y) proportional to\n1 ẽ2 i\n4: Solve b RWS = argmin k˜y ˜Z k 2\nOutput: b RWS\nRandomized approximation algorithms. Using the ideas from §4 and §4 we obtain the following randomized approximation to the influence scores\n˜di = ẽ2i ˜li\n(1 ˜li)2 , (7)\nwhere ẽi is the ith residual error computed using the SRHT-LS estimator. Since the approximation errors of ẽi and ˜li are bounded (inequalities (4) and (6)), this suggests that our randomized approximation to influence is close to the true influence.\nBasic approximation. The first approximation algorithm is identical to Algorithm 1 except that leverage and residuals are replaced by their randomized approximations as in Eq. (7). We refer to this algorithm as Approximate influence weighted subsampling (aIWS-LS). Full details are given in Algorithm 3 in §SI.2.\nResidual Weighted Sampling. Leverage scores are typically uniform [7, 13] for sub-Gaussian data. Even in the corrupted setting, the difference in leverage scores between corrupted and noncorrupted points is small (see §6). Therefore, the main contribution to the influence for each point will originate from the residual error, e2i . Consequently, we propose sampling with probability inversely proportional to the approximate residual, 1\nẽ2 i . The resulting algorithm Residual Weighted Subsampling (aRWS-LS) is detailed in Algorithm 2. Although aRWS-LS is not guaranteed to be a good approximation to IWS-LS, empirical results suggests that it works well in practise and is faster to compute than aIWS-LS.\nComputational complexity. Clearly, the computational complexity of IWS-LS is O np2 . The computation complexity of aIWS-LS is O\nnp log nsubs + npr2 + nsubsp2 , where the first term is the cost of SRHT-LS, the second term is the cost of approximate leverage computation and the last term solves OLS on the subsampled dataset. Here, r\n2 is the dimension of the random projection detailed in Eq. (5). The cost of aRWS-LS is O\nnp log nsubs + np+ nsubsp2 where the first term is the cost of SRHT-LS, the second term is the cost of computing the residuals e, and the last term solves OLS on the subsampled dataset. This computation can be reduced to O\nnp log nsubs + nsubsp2 . Therefore the cost of both aIWS-LS and aRWS-LS is o(np2)."
    }, {
      "heading" : "6 Estimation error",
      "text" : "In this section we will prove an upper bound on the estimation error of IWS-LS in the corrupted model. First, we show that the OLS error consists of two additional variance terms that depend on the size and proportion of the corruptions and an additional bias term. We then show that IWS-LS can significantly reduce the relative variance and bias in this setting, so that it no longer depends on the magnitude of the corruptions but only on their proportion. We compare these results to recent results from [4, 12] suggesting that consistent estimation requires knowledge about ⌃w. More recently, [5] show that incomplete knowledge about this quantity results in a biased estimator where the bias is proportional to the uncertainty about ⌃w. We see that the form of our bound matches these results.\nInequalities are said to hold with high probability (w.h.p.) if the probability of failure is not more than C\n1 exp( C 2 log p) where C 1 , C 2 are positive constants that do not depend on the scaling quantities n, p, w. The symbol . means that we ignore constants that do not depend on these scaling quantities. Proofs are provided in the supplement. Unless otherwise stated, k·k denotes the `\n2 norm for vectors and the spectral norm for matrices.\nCorrupted observation model. As a baseline, we first investigate the behaviour of the OLS estimator in the corrupted model.\nTheorem 1 (A bound on kb OLS k). If n & 2 x 2 w min(⌃x) p log p then w.h.p.\nk b OLS k .\n✏ x + ⇡ ✏ w + ⇡ 2w + w x k k\nr\np log p\nn + ⇡ 2w\np\npk k\n!\n·\n1\n(8)\nwhere 0 <  min (⌃x) + ⇡ min(⌃w). Remark 1 (No corruptions case). Notice for a fixed w, taking lim⇡!0 or for a fixed ⇡ taking lim\nw !0 (i.e. there are no corruptions) the above error reduces to the least squares result (see for example [4]). Remark 2 (Variance and Bias). The first three terms in (8) scale with p\n1/n so as n ! 1, these terms tend towards 0. The last term does not depend on p\n1/n and so for some non-zero ⇡ the least squares estimate will incur some bias depending on the fraction and magnitude of corruptions.\nWe are now ready to state our theorem characterising the mean squared error of the influence weighted subsampling estimator.\nTheorem 2 (Influence sampling in the corrupted model). For n & 2 x 2 w\nmin(⌃⇥x) p log p we have\nk b IWS k .\n✓\n✏ x + ⇡ ✏\n( w + 1) + ⇡k k\n◆\nr\np log p\nnsubs + ⇡\np\npk k\n!\n. 1\nwhere 0 <  min (⌃ ⇥x) and ⌃⇥x is the covariance of the influence weighted subsampled data.\nRemark 3. Theorem 2 states that the influence weighted subsampling estimator removes the proportional dependance of the error on w so the additional variance terms scale as O(⇡/ w · p p/nsubs) and O(⇡ p p/nsubs). The relative contribution of the bias term is ⇡ p\npk k compared with ⇡ 2w p pk k for the OLS or non-influence-based subsampling methods.\nComparison with fully corrupted setting. We note that the bound in Theorem 1 is similar to the bound in [5] for an estimator where all data points are corrupted (i.e. ⇡ = 1) and where incomplete knowledge of the covariance matrix of the corruptions, ⌃w is used. The additional bias in the estimator is proportional to the uncertainty in the estimate of ⌃w – in Theorem 1 this corresponds to 2w. Unbiased estimation is possible if ⌃w is known. See the Supplementary Information for further discussion, where the relevant results from [5] are provided in Section SI.6.1 as Lemma 16."
    }, {
      "heading" : "7 Experimental results",
      "text" : "We compare IWS-LS against the methods SRHT-LS [14], ULURU [7]. These competing methods represent current state-of-the-art in fast randomized least squares. Since SRHT-LS is equivalent to LEV-LS [9] the comparison will highlight the difference between importance sampling according to the two difference types of regression diagnostic in the corrupted model. Similar to IWS-LS, ULURU is also a two-step procedure where the first is equivalent to SRHT-LS. The second reduces bias by subtracting the result of regressing onto the residual. The experiments with the corrupted data model will demonstrate the difference in robustness of IWS-LS and ULURU to corruptions in the observations. Note that we do not compare with SGD. Although SGD has excellent properties for large-scale linear regression, we are not aware of a convex loss function which is robust to the corruption model we propose.\nWe assess the empirical performance of our method compared with standard and state-of-the-art randomized approaches to linear regression in several difference scenarios. We evaluate these methods on the basis of the estimation error: the `\n2 norm of the difference between the true weights and the learned weights, kb k. We present additional results for root mean squared prediction error (RMSE) on the test set in §SI.7.\nFor all the experiments on simulated data sets we use ntrain = 100, 000, ntest = 1000, p = 500. For datasets of this size, computing exact leverage is impractical and so we report on results for IWS-LS in §SI.7. For aIWS-LS and aRWS-LS we used the same number of sub-samples to approximate the leverage scores and residuals as for solving the regression. For aIWS-LS we set r 2 = p/2 (see Eq. (5)). The results are averaged over 100 runs.\nCorrupted data. We investigate the corrupted data noise model described in Eqs. (1)-(2). We show three scenarios where ⇡ = {0.05, 0.1, 0.3}. X and W were sampled from independent, zeromean Gaussians with standard deviation x = 1 and w = 0.4 respectively. The true regression coefficients, were sampled from a standard Gaussian. We added i.i.d. zero-mean Gaussian noise with standard deviation e = 0.1.\nFigure 1 shows the difference in distribution of influence and leverage between non-corrupted points (top) and corrupted points (bottom) for a dataset with 30% corrupted points. The distribution of leverage is very similar between the corrupted and non-corrupted points, as quantified by the `\n1\ndifference. This suggests that leverage alone cannot be used to identify corrupted points.\nOn the other hand, although there are some corrupted points with small influence, they typically have a much larger influence than non-corrupted points. We give a theoretical explanation of this phenomenon in §SI.3 (remarks 4 and 5).\nFigure 2(a) and (b) shows the estimation error and the mean squared prediction error for different subsample sizes. In this setting, computing IWS-LS is impractical (due to the exact leverage computation) so we omit the results but we notice that aIWS-LS and aRWS-LS quickly improve over the full least squares solution and the other randomized approximations in all simulation settings. In all cases, influence based methods also achieve lower-variance estimates.\nFor 30% corruptions for a small number of samples ULURU outperforms the other subsampling methods. However, as the number of samples increases, influence based methods start to outperform OLS. Here, ULURU converges quickly to the OLS solution but is not able to overcome the bias introduced by the corrupted datapoints. Results for 10% corruptions are shown in Figs. 5 and 6 and we provide results on smaller corrupted datasets (to show the performance of IWS-LS) as well as non-corrupted data simulated according to [13] in §SI.7.\nAirline delay dataset The dataset consists of details of all commercial flights in the USA over 20 years. Dataset along with visualisations available from http://stat-computing.org/dataexpo/2009/. Selecting the first ntrain = 13, 000 US Airways flights from January 2000 (corresponding to approximately 1.5 weeks) our goal is to predict the delay time of the next ntest = 5, 000 US Airways flights. The features in this dataset consist of a binary vector representing origin-destination pairs and a real value representing distance (p = 170).\nThe dataset might be expected to violate the usual i.i.d. sub-Gaussian design assumption of standard linear regression since the length of delays are often very different depending on the day. For example, delays may be longer due to public holidays or on weekends. Of course, such regular events could be accounted for in the modelling step, but some unpredictable outliers such as weather delay may also occur. Results are presented in Figure 2(c), the RMSE is the error in predicted delay time in minutes. Since the dataset is smaller, we can run IWS-LS to observe the accuracy of aIWS-LS and aRWS-LS in comparison. For more than 3000 samples, these algorithm outperform OLS and quickly approach IWS-LS. The result suggests that the corrupted observation model is a good model for this dataset. Furthermore, ULURU is unable to achieve the full accuracy of the OLS solution."
    }, {
      "heading" : "8 Conclusions",
      "text" : "We have demonstrated theoretically and empirically under the generalised corrupted observation model that influence weighted subsampling is able to significantly reduce both the bias and variance compared with the OLS estimator and other randomized approximations which do not take influence into account. Importantly our fast approximation, aRWS-LS performs similarly to IWS-LS. We find ULURU quickly converges to the OLS estimate, although it is not able to overcome the bias induced by the corrupted datapoints despite its two-step procedure. The performance of IWS-LS relative to OLS in the airline delay problem suggests that the corrupted observation model is a more realistic modelling scenario than the standard sub-Gaussian design model for some tasks. Software is available at http://people.inf.ethz.ch/kgabriel/software.html.\nAcknowledgements. We thank David Balduzzi, Cheng Soon Ong and the anonymous reviewers for invaluable discussions, suggestions and comments."
    } ],
    "references" : [ {
      "title" : "Fast dimension reduction using rademacher series on dual bch codes",
      "author" : [ "Nir Ailon", "Edo Liberty" ],
      "venue" : "In 19th Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Regression Diagnostics. Identifying Influential Data and Sources of Collinearity",
      "author" : [ "David A Belsley", "Edwin Kuh", "Roy E Welsch" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1981
    }, {
      "title" : "Improved matrix algorithms via the Subsampled Randomized Hadamard Transform",
      "author" : [ "Christos Boutsidis", "Alex Gittens" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Orthogonal Matching Pursuit with Noisy and Missing Data: Low and High Dimensional Results",
      "author" : [ "Yudong Chen", "Constantine Caramanis" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Noisy and Missing Data Regression: Distribution- Oblivious Support Recovery",
      "author" : [ "Yudong Chen", "Constantine Caramanis" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Robust Sparse Regression under Adversarial Corruption",
      "author" : [ "Yudong Chen", "Constantine Caramanis", "Shie Mannor" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "New Subsampling Algorithms for Fast Least Squares Regression",
      "author" : [ "P Dhillon", "Y Lu", "D P Foster", "L Ungar" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Fast approximation of matrix coherence and statistical leverage",
      "author" : [ "Petros Drineas", "Malik Magdon-Ismail", "Michael W Mahoney", "David P Woodruff" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Sampling algorithms for l2 regression and applications",
      "author" : [ "Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan" ],
      "venue" : "In Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithm,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Faster least squares approximation",
      "author" : [ "Petros Drineas", "Michael W Mahoney", "S Muthukrishnan", "Tamás Sarlós" ],
      "venue" : "Numerische Mathematik,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "A tail inequality for quadratic forms of subgaussian random vectors",
      "author" : [ "Daniel Hsu", "Sham Kakade", "Tong Zhang" ],
      "venue" : "Electron. Commun. Probab.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity",
      "author" : [ "Po-Ling Loh", "Martin J Wainwright" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "A Statistical Perspective on Algorithmic Leveraging",
      "author" : [ "Ping Ma", "Michael W Mahoney", "Bin Yu" ],
      "venue" : "In proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Randomized algorithms for matrices and data",
      "author" : [ "Michael W Mahoney" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Multi-view predictive partitioning in high dimensions",
      "author" : [ "Brian McWilliams", "Giovanni Montana" ],
      "venue" : "Statistical Analysis and Data Mining,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Subspace clustering of high-dimensional data: a predictive approach",
      "author" : [ "Brian McWilliams", "Giovanni Montana" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Improved analysis of the subsampled randomized Hadamard transform",
      "author" : [ "Joel A Tropp" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "Roman Vershynin" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Regression sensitivity analysis and bounded-influence estimation. In Evaluation of econometric models, pages 153–167",
      "author" : [ "Roy E Welsch" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1980
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "These methods, based on subsampling the dataset, reduce the computational time from O np2 to o(np2)1 [14].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "typically from a sub-Gaussian distribution [7].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "This scenario corresponds to a generalised version of the classical problem of “errors-in-variables” in regression analysis which has recently been considered in the context of sparse estimation [12].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 7,
      "context" : "Computing influence exactly is not practical in large-scale applications and so we propose two randomized approximation algorithms based on the randomized leverage approximation of [8].",
      "startOffset" : 181,
      "endOffset" : 184
    }, {
      "referenceID" : 3,
      "context" : "The “low-dimensional” (n > p) setting is investigated in [4], but the “big data” setting (n p) has not been considered so far.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "2 In the high-dimensional problem, knowledge of the corruption covariance, ⌃w [12], or the data covariance ⌃x [5], is required to obtain a consistent estimate.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "2 In the high-dimensional problem, knowledge of the corruption covariance, ⌃w [12], or the data covariance ⌃x [5], is required to obtain a consistent estimate.",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : "Recently, [14] proposed subsampling points for least squares based on their leverage scores.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "Other recent works suggest related influence measures that identify subspace [16] and multi-view [15] clusters in high dimensional data.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "Other recent works suggest related influence measures that identify subspace [16] and multi-view [15] clusters in high dimensional data.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "An equivalent definition from [14] which will be useful later concerns any matrix U 2 Rn⇥p which spans the column space of X (for example, the matrix whose columns are the left singular vectors of X).",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "Unlike [5, 12] and others we do not consider sparsity in our solution since n p.",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 11,
      "context" : "Unlike [5, 12] and others we do not consider sparsity in our solution since n p.",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "(3)The expression we use is also called Cook’s distance [2].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "We briefly review two randomized approaches to least squares approximation: the importance weighted subsampling approach of [9] and the dimensionality reduction approach [14].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 13,
      "context" : "We briefly review two randomized approaches to least squares approximation: the importance weighted subsampling approach of [9] and the dimensionality reduction approach [14].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 8,
      "context" : "If the sampling probabilities are proportional to the statistical leverages, the resulting estimator is close to the optimal estimator [9].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "Uniformly subsampling the rows of the projected matrix proves to be equivalent to leverage weighted sampling on the original dataset [14].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "It is analysed in the statistical setting by [7] who also propose ULURU, a two step fitting procedure which aims to correct for the subsampling bias and consequently converges to the OLS estimate at a rate independent of the number of subsamples [7].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "It is analysed in the statistical setting by [7] who also propose ULURU, a two step fitting procedure which aims to correct for the subsampling bias and consequently converges to the OLS estimate at a rate independent of the number of subsamples [7].",
      "startOffset" : 246,
      "endOffset" : 249
    }, {
      "referenceID" : 2,
      "context" : "subs SHD · X = ⇧X with the definitions [3]: • S is a subsampling matrix.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : "(see [17] for detailed analysis of the SRHT).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "Due to the recursive nature of H, the cost of applying the SRHT is O (pn log nsubs) operations, where nsubs is the number of rows sampled from X [1].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "k ̃ ek  (1 + ⇢)kek (4) where e = y Xb OLS is the vector of OLS residual errors [14].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "The leverage approximation algorithm of [8] uses a SRHT, ⇧",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "From [14] we derive the following result relating to randomized approximation of the leverage",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 13,
      "context" : "(4) implying that LEV-LS and SRHT-LS are equivalent [14].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 18,
      "context" : "Several approaches similar in spirit have previously been proposed based on identifying and down-weighting the effect of highly influential observations [19].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 7,
      "context" : "In the next section, we propose an approximate influence weighted subsampling algorithm which combines the approximate leverage computation of [8] and the randomized least squares approach of [14].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "In the next section, we propose an approximate influence weighted subsampling algorithm which combines the approximate leverage computation of [8] and the randomized least squares approach of [14].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 6,
      "context" : "Leverage scores are typically uniform [7, 13] for sub-Gaussian data.",
      "startOffset" : 38,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "Leverage scores are typically uniform [7, 13] for sub-Gaussian data.",
      "startOffset" : 38,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "We compare these results to recent results from [4, 12] suggesting that consistent estimation requires knowledge about ⌃w.",
      "startOffset" : 48,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "We compare these results to recent results from [4, 12] suggesting that consistent estimation requires knowledge about ⌃w.",
      "startOffset" : 48,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "More recently, [5] show that incomplete knowledge about this quantity results in a biased estimator where the bias is proportional to the uncertainty about ⌃w.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "there are no corruptions) the above error reduces to the least squares result (see for example [4]).",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "We note that the bound in Theorem 1 is similar to the bound in [5] for an estimator where all data points are corrupted (i.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "See the Supplementary Information for further discussion, where the relevant results from [5] are provided in Section SI.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "7 Experimental results We compare IWS-LS against the methods SRHT-LS [14], ULURU [7].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "7 Experimental results We compare IWS-LS against the methods SRHT-LS [14], ULURU [7].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "Since SRHT-LS is equivalent to LEV-LS [9] the comparison will highlight the difference between importance sampling according to the two difference types of regression diagnostic in the corrupted model.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 12,
      "context" : "5 and 6 and we provide results on smaller corrupted datasets (to show the performance of IWS-LS) as well as non-corrupted data simulated according to [13] in §SI.",
      "startOffset" : 150,
      "endOffset" : 154
    } ],
    "year" : 2014,
    "abstractText" : "Subsampling methods have been recently proposed to speed up least squares estimation in large scale settings. However, these algorithms are typically not robust to outliers or corruptions in the observed covariates. The concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper. This property of influence – for which we also develop a randomized approximation – motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error. Under a general model of corrupted observations, we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares.",
    "creator" : null
  }
}