{
  "name" : "f60bb6bb4c96d4df93c51bd69dcc15a0.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Tight Continuous Relaxation of the Balanced k-Cut Problem",
    "authors" : [ "Syama Sundar Rangapuram", "Pramod Kaushik Mudrakarta" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Graph-based techniques for clustering have become very popular in machine learning as they allow for an easy integration of pairwise relationships in data. The problem of finding k clusters in a graph can be formulated as a balanced k-cut problem [1, 2, 3, 4], where ratio and normalized cut are famous instances of balanced graph cut criteria employed for clustering, community detection and image segmentation. The balanced k-cut problem is known to be NP-hard [4] and thus in practice relaxations [4, 5] or greedy approaches [6] are used for finding the optimal multi-cut. The most famous approach is spectral clustering [7], which corresponds to the spectral relaxation of the ratio/normalized cut and uses k-means in the embedding of the vertices found by the first k eigenvectors of the graph Laplacian in order to obtain the clustering. However, the spectral relaxation has been shown to be loose for k = 2 [8] and for k > 2 no guarantees are known of the quality of the obtained k-cut with respect to the optimal one. Moreover, in practice even greedy approaches [6] frequently outperform spectral clustering.\nThis paper is motivated by another line of recent work [9, 10, 11, 12] where it has been shown that an exact continuous relaxation for the two cluster case (k = 2) is possible for a quite general class of balancing functions. Moreover, efficient algorithms for its optimization have been proposed which produce much better cuts than the standard spectral relaxation. However, the multi-cut problem has still to be solved via the greedy recursive splitting technique.\nInspired by the recent approach in [13], in this paper we tackle directly the general balanced k-cut problem based on a new tight continuous relaxation. We show that the relaxation for the asymmetric ratio Cheeger cut proposed recently by [13] is loose when the data does not contain k well-separated clusters and thus leads to poor performance in practice. Similar to [13] we can also integrate label information leading to a transductive clustering formulation. Moreover, we propose an efficient algorithm for the minimization of our continuous relaxation for which we can prove monotonic descent. This is in contrast to the algorithm proposed in [13] for which no such guarantee holds. In extensive experiments we show that our method outperforms all existing methods in terms of the\nachieved balanced k-cuts. Moreover, our clustering error is competitive with respect to several other clustering techniques based on balanced k-cuts and recently proposed approaches based on nonnegative matrix factorization. Also we observe that already with small amount of label information the clustering error improves significantly."
    }, {
      "heading" : "2 Balanced Graph Cuts",
      "text" : "Graphs are used in machine learning typically as similarity graphs, that is the weight of an edge between two instances encodes their similarity. Given such a similarity graph of the instances, the clustering problem into k sets can be transformed into a graph partitioning problem, where the goal is to construct a partition of the graph into k sets such that the cut, that is the sum of weights of the edge from each set to all other sets, is small and all sets in the partition are roughly of equal size.\nBefore we introduce balanced graph cuts, we briefly fix the setting and notation. Let G(V,W ) denote an undirected, weighted graph with vertex set V with n = |V | vertices and weight matrix W ∈ Rn×n+ with W = WT . There is an edge between two vertices i, j ∈ V if wij > 0. The cut between two sets A,B ⊂ V is defined as cut(A,B) = ∑ i∈A,j∈B wij and we write 1A for the indicator vector of setA ⊂ V . A collection of k sets (C1, . . . , Ck) is a partition of V if ∪ki=1Ci = V , Ci ∩ Cj = ∅ if i 6= j and |Ci| ≥ 1, i = 1, . . . , k. We denote the set of all k-partitions of V by Pk. Furthermore, we denote by ∆k the simplex {x : x ∈ Rk, x ≥ 0, ∑k i=1 xi = 1}.\nFinally, a set function Ŝ : 2V → R is called submodular if for allA,B ⊂ V , Ŝ(A∪B)+Ŝ(A∩B) ≤ Ŝ(A) + Ŝ(B). Furthermore, we need the concept of the Lovasz extension of a set function.\nDefinition 1 Let Ŝ : 2V → R be a set function with Ŝ(∅) = 0. Let f ∈ RV be ordered in increasing order f1 ≤ f2 ≤ . . . ≤ fn and define Ci = {j ∈ V | fj > fi} where C0 = V . Then S : RV → R given by, S(f) = ∑n i=1 fi ( Ŝ(Ci−1) − Ŝ(Ci) ) , is called the Lovasz extension of Ŝ. Note that\nS(1A) = Ŝ(A) for all A ⊂ V .\nThe Lovasz extension of a set function is convex if and only if the set function is submodular [14]. The cut function cut(C,C), where C = V \\C, is submodular and its Lovasz extension is given by TV(f) = 12 ∑n i,j=1 wij |fi − fj |.\n2.1 Balanced k-cuts\nThe balanced k-cut problem is defined as\nmin (C1,...,Ck)∈Pk k∑ i=1 cut(Ci, Ci) Ŝ(Ci) =: BCut(C1, . . . , Ck) (1)\nwhere Ŝ : 2V → R+ is a balancing function with the goal that all sets Ci are of the same “size”. In this paper, we assume that Ŝ(∅) = 0 and for any C ( V, C 6= ∅, Ŝ(C) ≥ m, for some m > 0. In the literature one finds mainly the following submodular balancing functions (in brackets is the name of the overall balanced graph cut criterion BCut(C1, . . . , Ck)),\nŜ(C) = |C|, (Ratio Cut), (2) Ŝ(C) = min{|C|, |C|}, (Ratio Cheeger Cut), Ŝ(C) = min{(k − 1)|C|, C} (Asymmetric Ratio Cheeger Cut).\nThe Ratio Cut is well studied in the literature e.g. [3, 7, 6] and corresponds to a balancing function without bias towards a particular size of the sets, whereas the Asymmetric Ratio Cheeger Cut recently proposed in [13] has a bias towards sets of size |V |k (Ŝ(C) attains its maximum at this point) which makes perfect sense if one expects clusters which have roughly equal size. An intermediate version between the two is the Ratio Cheeger Cut which has a symmetric balancing function and strongly penalizes overly large clusters. For the ease of presentation we restrict ourselves to these balancing functions. However, we can also handle the corresponding weighted cases e.g., Ŝ(C) = vol(C) =∑ i∈C di, where di = ∑n j=1 wij , leading to the normalized cut[4].\n3 Tight Continuous Relaxation for the Balanced k-Cut Problem\nIn this section we discuss our proposed relaxation for the balanced k-cut problem (1). It turns out that a crucial question towards a tight multi-cut relaxation is the choice of the constraints so that the continuous problem also yields a partition (together with a suitable rounding scheme). The motivation for our relaxation is taken from the recent work of [9, 10, 11], where exact relaxations are shown for the case k = 2. Basically, they replace the ratio of set functions with the ratio of the corresponding Lovasz extensions. We use the same idea for the objective of our continuous relaxation of the k-cut problem (1) which is given as\nmin F=(F1,...,Fk),\nF∈Rn×k+\nk∑ l=1 TV(Fl) S(Fl) (3)\nsubject to : F(i) ∈ ∆k, i = 1, . . . , n, (simplex constraints) max{F(i)} = 1, ∀i ∈ I, (membership constraints) S(Fl) ≥ m, l = 1, . . . , k, (size constraints)\nwhere S is the Lovasz extension of the set function Ŝ and m = minC(V, C 6=∅ Ŝ(C). We have m = 1, for Ratio Cut and Ratio Cheeger Cut whereas m = k − 1 for Asymmetric Ratio Cheeger Cut. Note that TV is the Lovasz extension of the cut functional cut(C,C). In order to simplify notation we denote for a matrix F ∈ Rn×k by Fl the l-th column of F and by F(i) the i-th row of F . Note that the rows of F correspond to the vertices of the graph and the j-th column of F corresponds to the set Cj of the desired partition. The set I ⊂ V in the membership constraints is chosen adaptively by our method during the sequential optimization described in Section 4.\nAn obvious question is how to get from the continuous solution F ∗ of (3) to a partition (C1, . . . , Ck) ∈ Pk which is typically called rounding. Given F ∗ we construct the sets, by assigning each vertex i to the column where the i-th row attains its maximum. Formally,\nCi = {j ∈ V | i = arg max s=1,...,k Fjs}, i = 1, . . . , k, (Rounding) (4)\nwhere ties are broken randomly. If there exists a row such that the rounding is not unique, we say that the solution is weakly degenerated. If furthermore the resulting set (C1, . . . , Ck) do not form a partition, that is one of the sets is empty, then we say that the solution is strongly degenerated.\nFirst, we connect our relaxation to the previous work of [11] for the case k = 2. Indeed for symmetric balancing function such as the Ratio Cheeger Cut, our continuous relaxation (3) is exact even without membership and size constraints.\nTheorem 1 Let Ŝ be a non-negative symmetric balancing function, Ŝ(C) = Ŝ(C), and denote by p∗ the optimal value of (3) without membership and size constraints for k = 2. Then it holds\np∗ = min (C1,C2)∈P2 2∑ i=1 cut(Ci, Ci) Ŝ(Ci) .\nFurthermore there exists a solution F ∗ of (3) such that F ∗ = [1C∗ ,1C∗ ], where (C ∗, C∗) is the optimal balanced 2-cut partition.\nNote that rounding trivially yields a solution in the setting of the previous theorem.\nA second result shows that indeed our proposed optimization problem (3) is a relaxation of the balanced k-cut problem (1). Furthermore, the relaxation is exact if I = V .\nProposition 1 The continuous problem (3) is a relaxation of the k-cut problem (1). The relaxation is exact, i.e., both problems are equivalent, if I = V .\nThe row-wise simplex and membership constraints enforce that each vertex in I belongs to exactly one component. Note that these constraints alone (even if I = V ) can still not guarantee that F corresponds to a k-way partition since an entire column of F can be zero. This is avoided by the column-wise size constraints that enforce that each component has at least one vertex.\nIf I = V it is immediate from the proof that problem (3) is no longer a continuous problem as the feasible set are only indicator matrices of partitions. In this case rounding yields trivially a partition. On the other hand, if I = ∅ (i.e., no membership constraints), and k > 2 it is not guaranteed that rounding of the solution of the continuous problem yields a partition. Indeed, we will see in the following that for symmetric balancing functions one can, under these conditions, show that the solution is always strongly degenerated and rounding does not yield a partition (see Theorem 2). Thus we observe that the index set I controls the degree to which the partition constraint is enforced. The idea behind our suggested relaxation is that it is well known in image processing that minimizing the total variation yields piecewise constant solutions (in fact this follows from seeing the total variation as Lovasz extension of the cut). Thus if |I| is sufficiently large, the vertices where the values are fixed to 0 or 1 propagate this to their neighboring vertices and finally to the whole graph. We discuss the choice of I in more detail in Section 4.\nSimplex constraints alone are not sufficient to yield a partition: Our approach has been inspired by [13] who proposed the following continuous relaxation for the Asymmetric Ratio Cheeger Cut\nmin F=(F1,...,Fk),\nF∈Rn×k+\nk∑ l=1\nTV(Fl)∥∥Fl − quantk−1(Fl)∥∥1 (5) subject to : F(i) ∈ ∆k, i = 1, . . . , n, (simplex constraints)\nwhere S(f) = ∥∥f − quantk−1(f)∥∥1 is the Lovasz extension of Ŝ(C) = min{(k − 1)|C|, C} and quantk−1(f) is the k−1-quantile of f ∈ Rn. Note that in their approach no membership constraints and size constraints are present.\nWe now show that the usage of simplex constraints in the optimization problem (3) is not sufficient to guarantee that the solution F ∗ can be rounded to a partition for any symmetric balancing function in (1). For asymmetric balancing functions as employed for the Asymmetric Ratio Cheeger Cut by [13] in their relaxation (5) we can prove such a strong result only in the case where the graph is disconnected. However, note that if the number of components of the graph is less than the number of desired clusters k, the multi-cut problem is still non-trivial.\nTheorem 2 Let Ŝ(C) be any non-negative symmetric balancing function. Then the continuous relaxation\nmin F=(F1,...,Fk),\nF∈Rn×k+\nk∑ l=1 TV(Fl) S(Fl) (6)\nsubject to : F(i) ∈ ∆k, i = 1, . . . , n, (simplex constraints) of the balanced k-cut problem (1) is void in the sense that the optimal solution F ∗ of the continuous problem can be constructed from the optimal solution of the 2-cut problem and F ∗ cannot be rounded into a k-way partition, see (4). If the graph is disconnected, then the same holds also for any non-negative asymmetric balancing function.\nThe proof of Theorem 2 shows additionally that for any balancing function if the graph is disconnected, the solution of the continuous relaxation (6) is always zero, while clearly the solution of the balanced k-cut problem need not be zero. This shows that the relaxation can be arbitrarily bad in this case. In fact the relaxation for the asymmetric case can even fail if the graph is not disconnected but there exists a cut of the graph which is very small as the following corollary indicates.\nCorollary 1 Let Ŝ be an asymmetric balancing function and C∗ = arg min C⊂V cut(C,C) Ŝ(C) and suppose that φ∗ := (k−1) cut(C ∗,C∗)\nŜ(C∗) + cut(C\n∗,C∗)\nŜ(C∗) < min(C1,...,Ck)∈Pk ∑k i=1 cut(Ci,Ci) Ŝ(Ci) . Then there exists\na feasible F with F1 = 1C∗ and Fl = αl1C∗ , l = 2, . . . , k such that ∑k l=2 αl = 1, αl > 0 for (6)\nwhich has objective ∑k i=1 TV(Fi) S(Fi) = φ∗ and which cannot be rounded to a k-way partition.\nTheorem 2 shows that the membership and size constraints which we have introduced in our relaxation (3) are essential to obtain a partition for symmetric balancing functions. For the asymmetric\nbalancing function failure of the relaxation (6) and thus also of the relaxation (5) of [13] is only guaranteed for disconnected graphs. However, Corollary 1 indicates that degenerated solutions should also be a problem when the graph is still connected but there exists a dominating cut. We illustrate this with a toy example in Figure 1 where the algorithm of [13] for solving (5) fails as it converges exactly to the solution predicted by Corollary 1 and thus only produces a 2-partition instead of the desired 3-partition. The algorithm for our relaxation enforcing membership constraints converges to a continuous solution which is in fact a partition matrix so that no rounding is necessary."
    }, {
      "heading" : "4 Monotonic Descent Method for Minimization of a Sum of Ratios",
      "text" : "Apart from the new relaxation another key contribution of this paper is the derivation of an algorithm which yields a sequence of feasible points for the difficult non-convex problem (3) and reduces monotonically the corresponding objective. We would like to note that the algorithm proposed by [13] for (5) does not yield monotonic descent. In fact it is unclear what the derived guarantee for the algorithm in [13] implies for the generated sequence. Moreover, our algorithm works for any non-negative submodular balancing function.\nThe key insight in order to derive a monotonic descent method for solving the sum-of-ratio minimization problem (3) is to eliminate the ratio by introducing a new set of variables β = (β1, . . . , βk).\nmin F=(F1,...,Fk),\nF∈Rn×k+ , β∈R k +\nk∑ l=1 βl (7)\nsubject to : TV(Fl) ≤ βlS(Fl), l = 1, . . . , k, (descent constraints) F(i) ∈ ∆k, i = 1, . . . , n, (simplex constraints) max{F(i)} = 1, ∀i ∈ I, (membership constraints) S(Fl) ≥ m, l = 1, . . . , k. (size constraints)\nNote that for the optimal solution (F ∗, β∗) of this problem it holds TV(F ∗l ) = β ∗ l S(F ∗ l ), l = 1, . . . , k (otherwise one can decrease β∗l and hence the objective) and thus equivalence holds. This is still a non-convex problem as the descent, membership and size constraints are non-convex. Our algorithm proceeds now in a sequential manner. At each iterate we do a convex inner approximation of the constraint set, that is the convex approximation is a subset of the non-convex constraint set, based on the current iterate (F t, βt). Then we optimize the resulting convex optimization problem and repeat the process. In this way we get a sequence of feasible points for the original problem (7) for which we will prove monotonic descent in the sum-of-ratios.\nConvex approximation: As Ŝ is submodular, S is convex. Let stl ∈ ∂S(F tl ) be an element of the sub-differential of S at the current iterate F tl . We have by Prop. 3.2 in [14], (s t l)ji = Ŝ(Cli−1) − Ŝ(Cli), where ji is the i th smallest component of F tl and Cli = {j ∈ V | (F tl )j > (F tl )i}. Moreover, using the definition of subgradient, we have S(Fl) ≥ S(F tl ) + 〈stl , Fl − F tl 〉 = 〈stl , Fl〉.\nFor the descent constraints, let λtl = TV(F tl ) S(F tl )\nand introduce new variables δl = βl − λtl that capture the amount of change in each ratio. We further decompose δl as δl = δ+l − δ − l , δ + l ≥ 0, δ − l ≥ 0. Let M = maxf∈[0,1]n S(f) = maxC⊂V Ŝ(C), then for S(Fl) ≥ m,\nTV(Fl)− βlS(Fl) ≤ TV(Fl)− λtl 〈 stl , Fl 〉 − δ+l S(Fl) + δ − l S(Fl)\n≤ TV(Fl)− λtl 〈 stl , Fl 〉 − δ+l m+ δ − l M\nFinally, note that because of the simplex constraints, the membership constraints can be rewritten as max{F(i)} ≥ 1. Let i ∈ I and define ji := arg maxj F tij (ties are broken randomly). Then the membership constraints can be relaxed as follows: 0 ≥ 1−max{F(i)} ≥ 1− Fiji =⇒ Fiji ≥ 1. As Fij ≤ 1 we get Fiji = 1. Thus the convex approximation of the membership constraints fixes the assignment of the i-th point to a cluster and thus can be interpreted as “label constraint”. However, unlike the transductive setting, the labels for the vertices in I are automatically chosen by our method. The actual choice of the set I will be discussed in Section 4.1. We use the notation L = {(i, ji) | i ∈ I} for the label set generated from I (note that L is fixed once I is fixed). Descent algorithm: Our descent algorithm for minimizing (7) solves at each iteration t the following convex optimization problem (8).\nmin F∈Rn×k+ ,\nδ+∈Rk+, δ −∈Rk+\nk∑ l=1 δ+l − δ − l (8)\nsubject to : TV(Fl) ≤ λtl 〈 stl , Fl 〉 + δ+l m− δ − l M, l = 1, . . . k, (descent constraints)\nF(i) ∈ ∆k, i = 1, . . . , n, (simplex constraints) Fiji = 1, ∀(i, ji) ∈ L, (label constraints)〈 stl , F t l 〉 ≥ m, l = 1, . . . , k. (size constraints)\nAs its solution F t+1 is feasible for (3) we update λt+1l = TV(F t+1l )\nS(F t+1l ) and st+1l ∈ ∂S(F t+1 l ), l =\n1, . . . , k and repeat the process until the sequence terminates, that is no further descent is possible as the following theorem states, or the relative descent in ∑k l=1 λ t l is smaller than a predefined . The following Theorem 3 shows the monotonic descent property of our algorithm. Theorem 3 The sequence {F t} produced by the above algorithm satisfies ∑k l=1 TV(F t+1l )\nS(F t+1l ) <∑k\nl=1 TV(F tl ) S(F tl ) for all t ≥ 0 or the algorithm terminates.\nThe inner problem (8) is convex, but contains the non-smooth term TV in the constraints. We eliminate the non-smoothness by introducing additional variables and derive an equivalent linear programming (LP) formulation. We solve this LP via the PDHG algorithm [15, 16]. The LP and the exact iterates can be found in the supplementary material."
    }, {
      "heading" : "4.1 Choice of membership constraints I",
      "text" : "The overall algorithm scheme for solving the problem (1) is given in the supplementary material. For the membership constraints we start initially with I0 = ∅ and sequentially solve the inner problem (8). From its solution F t+1 we construct a P ′k = (C1, . . . , Ck) via rounding, see (4). We repeat this process until we either do not improve the resulting balanced k-cut or P ′k is not a partition. In this case we update It+1 and double the number of membership constraints. Let (C∗1 , . . . , C ∗ k) be the currently optimal partition. For each l ∈ {1, . . . , k} and i ∈ C∗l we compute\nb∗li = cut ( C∗l \\{i}, C∗l ∪ {i} ) Ŝ(C∗l \\{i}) + min s6=l cut ( C∗s ∪ {i}, C∗s \\{i} ) Ŝ(C∗s ∪ {i})\n(9)\nand define Ol = {(π1, . . . , π|C∗l |) | b ∗ lπ1 ≥ b∗lπ2 ≥ . . . ≥ b ∗ lπ|C∗ l | }. The top-ranked vertices in Ol correspond to the ones which lead to the largest minimal increase in BCut when moved from C∗l to another component and thus are most likely to belong to their current component. Thus it is\nnatural to fix the top-ranked vertices for each component first. Note that the rankings Ol, l = 1, . . . , k are updated when a better partition is found. Thus the membership constraints correspond always to the vertices which lead to largest minimal increase in BCut when moved to another component. In Figure 1 one can observe that the fixed labeled points are lying close to the centers of the found clusters. The number of membership constraints depends on the graph. The better separated the clusters are, the less membership constraints need to be enforced in order to avoid degenerate solutions. Finally, we stop the algorithm if we see no more improvement in the cut or the continuous objective and the continuous solution corresponds to a partition."
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate our method against a diverse selection of state-of-the-art clustering methods like spectral clustering (Spec) [7], BSpec [11], Graclus1 [6], NMF based approaches PNMF [18], NSC [19], ONMF [20], LSD [21], NMFR [22] and MTV [13] which optimizes (5). We used the publicly available code [22, 13] with default settings. We run our method using 5 random initializations, 7 initializations based on the spectral clustering solution similar to [13] (who use 30 such initializations). In addition to the datasets provided in [13], we also selected a variety of datasets from the UCI repository shown below. For all the datasets not in [13], symmetric k-NN graphs are built with Gaussian weights exp ( − s‖x−y‖ 2 min{σ2x,k,σ 2 y,k} ) , where σx,k is the k-NN distance of point x. We chose the parameters s and k in a method independent way by testing for each dataset several graphs using all the methods over different choices of k ∈ {3, 5, 7, 10, 15, 20, 40, 60, 80, 100} and s ∈ {0.1, 1, 4}. The best choice in terms of the clustering error across all the methods and datasets, is s = 1, k = 15.\nIris wine vertebral ecoli 4moons webkb4 optdigits USPS pendigits 20news MNIST\n# vertices 150 178 310 336 4000 4196 5620 9298 10992 19928 70000 # classes 3 3 3 6 4 4 10 10 10 20 10\nQuantitative results: In our first experiment we evaluate our method in terms of solving the balanced k-cut problem for various balancing functions, data sets and graph parameters. The following table reports the fraction of times a method achieves the best as well as strictly best balanced k-cut over all constructed graphs and datasets (in total 30 graphs per dataset). For reference, we also report the obtained cuts for other clustering methods although they do not directly minimize this criterion in italic; methods that directly optimize the criterion are shown in normal font. Our algorithm can handle all balancing functions and significantly outperforms all other methods across all criteria. For ratio and normalized cut cases we achieve better results than [7, 11, 6] which directly optimize this criterion. This shows that the greedy recursive bi-partitioning affects badly the performance of [11], which, otherwise, was shown to obtain the best cuts on several benchmark datasets [23]. This further shows the need for methods that directly minimize the multi-cut. It is striking that the competing method of [13], which directly minimizes the asymmetric ratio cut, is beaten significantly by Graclus as well as our method. As this clear trend is less visible in the qualitative experiments, we suspect that extreme graph parameters lead to fast convergence to a degenerate solution.\nOurs MTV BSpec Spec Graclus PNMF NSC ONMF LSD NMFR\nRCC-asym Best (%) 80.54 25.50 23.49 7.38 38.26 2.01 5.37 2.01 4.03 1.34Strictly Best (%) 44.97 10.74 1.34 0.00 4.70 0.00 0.00 0.00 0.00 0.00\nRCC-sym Best (%) 94.63 8.72 19.46 6.71 37.58 0.67 4.03 0.00 0.67 0.67Strictly Best (%) 61.74 0.00 0.67 0.00 4.70 0.00 0.00 0.00 0.00 0.00\nNCC-asym Best (%) 93.29 13.42 20.13 10.07 38.26 0.67 5.37 2.01 4.70 2.01Strictly Best (%) 56.38 2.01 0.00 0.00 2.01 0.00 0.00 0.67 0.00 1.34\nNCC-sym Best (%) 98.66 10.07 20.81 9.40 40.27 1.34 4.03 0.67 3.36 1.34Strictly Best (%) 59.06 0.00 0.00 0.00 1.34 0.00 0.00 0.00 0.00 0.00\nRcut Best (%) 85.91 7.38 20.13 10.07 32.89 0.67 4.03 0.00 1.34 1.34Strictly Best (%) 58.39 0.00 2.68 2.01 8.72 0.00 0.00 0.00 0.00 0.67\nNcut Best (%) 95.97 10.07 20.13 9.40 37.58 1.34 4.70 0.67 3.36 0.67Strictly Best (%) 61.07 0.00 0.00 0.00 4.03 0.00 0.00 0.00 0.00 0.00\nQualitative results: In the following table, we report the clustering errors and the balanced k-cuts obtained by all methods using the graphs built with k = 15, s = 1 for all datasets. As the main goal\n1Since [6], a multi-level algorithm directly minimizing Rcut/Ncut, is shown to be superior to METIS [17], we do not compare with [17].\nis to compare to [13] we choose their balancing function (RCC-asym). Again, our method always achieved the best cuts across all datasets. In three cases, the best cut also corresponds to the best clustering performance. In case of vertebral, 20news, and webkb4 the best cuts actually result in high errors. However, we see in our next experiment that integrating ground-truth label information helps in these cases to improve the clustering performance significantly.\nIris wine vertebral ecoli 4moons webkb4 optdigits USPS pendigits 20news MNIST\nBSpec Err(%) 23.33 37.64 50.00 19.35 36.33 60.46 11.30 20.09 17.59 84.21 11.82BCut 1.495 6.417 1.890 2.550 0.634 1.056 0.386 0.822 0.081 0.966 0.471\nSpec Err(%) 22.00 20.22 48.71 14.88 31.45 60.32 7.81 21.05 16.75 79.10 22.83BCut 1.783 5.820 1.950 2.759 0.917 1.520 0.442 0.873 0.141 1.170 0.707\nPNMF Err(%) 22.67 27.53 50.00 16.37 35.23 60.94 10.37 24.07 17.93 66.00 12.80BCut 1.508 4.916 2.250 2.652 0.737 3.520 0.548 1.180 0.415 2.924 0.934\nNSC Err(%) 23.33 17.98 50.00 14.88 32.05 59.49 8.24 20.53 19.81 78.86 21.27BCut 1.518 5.140 2.046 2.754 0.933 3.566 0.482 0.850 0.101 2.233 0.688\nONMF Err(%) 23.33 28.09 50.65 16.07 35.35 60.94 10.37 24.14 22.82 69.02 27.27BCut 1.518 4.881 2.371 2.633 0.725 3.621 0.548 1.183 0.548 3.058 1.575\nLSD Err(%) 23.33 17.98 39.03 18.45 35.68 47.93 8.42 22.68 13.90 67.81 24.49BCut 1.518 5.399 2.557 2.523 0.782 2.082 0.483 0.918 0.188 2.056 0.959\nNMFR Err(%) 22.00 11.24 38.06 22.92 36.33 40.73 2.08 22.17 13.13 39.97 failBCut 1.627 4.318 2.713 2.556 0.840 1.467 0.369 0.992 0.240 1.241 -\nGraclus Err(%) 23.33 8.43 49.68 16.37 0.45 39.97 1.67 19.75 10.93 60.69 2.43BCut 1.534 4.293 1.890 2.414 0.589 1.581 0.350 0.815 0.092 1.431 0.440\nMTV Err(%) 22.67 18.54 34.52 22.02 7.72 48.40 4.11 15.13 20.55 72.18 3.77BCut 1.508 5.556 2.433 2.500 0.774 2.346 0.374 0.940 0.193 3.291 0.458\nOurs Err(%) 23.33 6.74 50.00 16.96 0.45 60.46 1.71 19.72 19.95 79.51 2.37BCut 1.495 4.168 1.890 2.399 0.589 1.056 0.350 0.802 0.079 0.895 0.439\nTransductive Setting: We evaluate our method against [13] in a transductive setting. As in [13], we randomly sample either one label or a fixed percentage of labels per class from the ground truth. We report clustering errors and the cuts (RCC-asym) for both methods for different choices of labels. For label experiments their initialization strategy seems to work better as the cuts improve compared to the unlabeled case. However, observe that in some cases their method seems to fail completely (Iris and 4moons for one label per class).\nLabels Iris wine vertebral ecoli 4moons webkb4 optdigits USPS pendigits 20news MNIST\n1 MTV Err(%) 33.33 9.55 42.26 13.99 35.75 51.98 1.69 12.91 14.49 50.96 2.45BCut 3.855 4.288 2.244 2.430 0.723 1.596 0.352 0.846 0.127 1.286 0.439\nOurs Err(%) 22.67 8.99 50.32 15.48 0.57 45.11 1.69 12.98 10.98 68.53 2.36BCut 1.571 4.234 2.265 2.432 0.610 1.471 0.352 0.812 0.113 1.057 0.439\n1% MTV Err(%) 33.33 10.67 39.03 14.29 0.45 48.38 1.67 5.21 7.75 40.18 2.41BCut 3.855 4.277 2.300 2.429 0.589 1.584 0.354 0.789 0.129 1.208 0.443\nOurs Err(%) 22.67 6.18 41.29 13.99 0.45 41.63 1.67 5.13 7.75 37.42 2.33BCut 1.571 4.220 2.288 2.419 0.589 1.462 0.354 0.789 0.128 1.157 0.442\n5% MTV Err(%) 17.33 7.87 40.65 14.58 0.45 40.09 1.51 4.85 1.79 31.89 2.18BCut 1.685 4.330 2.701 2.462 0.589 1.763 0.369 0.812 0.188 1.254 0.455\nOurs Err(%) 17.33 6.74 37.10 13.99 0.45 38.04 1.53 4.85 1.76 30.07 2.18BCut 1.685 4.224 2.724 2.461 0.589 1.719 0.369 0.811 0.188 1.210 0.455\n10% MTV Err(%) 18.67 7.30 39.03 13.39 0.38 40.63 1.41 4.19 1.24 27.80 2.03BCut 1.954 4.332 3.187 2.776 0.592 2.057 0.377 0.833 0.197 1.346 0.465\nOurs Err(%) 14.67 6.74 33.87 13.10 0.38 41.97 1.41 4.25 1.24 26.55 2.02BCut 1.960 4.194 3.134 2.778 0.592 1.972 0.377 0.833 0.197 1.314 0.465"
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented a framework for directly minimizing the balanced k-cut problem based on a new tight continuous relaxation. Apart from the standard ratio/normalized cut, our method can also handle new application-specific balancing functions. Moreover, in contrast to a recursive splitting approach [24], our method enables the direct integration of prior information available in form of must/cannotlink constraints, which is an interesting topic for future research. Finally, the monotonic descent algorithm proposed for the difficult sum-of-ratios problem is another key contribution of the paper that is of independent interest."
    } ],
    "references" : [ {
      "title" : "Lower bounds for the partitioning of graphs",
      "author" : [ "W.E. Donath", "A.J. Hoffman" ],
      "venue" : "IBM J. Res. Develop.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1973
    }, {
      "title" : "Partitioning sparse matrices with eigenvectors of graphs",
      "author" : [ "A. Pothen", "H.D. Simon", "K.-P. Liou" ],
      "venue" : "SIAM J. Matrix Anal. Appl.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1990
    }, {
      "title" : "Fast spectral methods for ratio cut partitioning and clustering",
      "author" : [ "L. Hagen", "A.B. Kahng" ],
      "venue" : "In ICCAD,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1991
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "A. Ng", "M. Jordan", "Y. Weiss" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "Weighted graph cuts without eigenvectors: A multilevel approach",
      "author" : [ "I. Dhillon", "Y. Guan", "B. Kulis" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "A tutorial on spectral clustering",
      "author" : [ "U. von Luxburg" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "On the quality of spectral separators",
      "author" : [ "S. Guattery", "G. Miller" ],
      "venue" : "SIAM J. Matrix Anal. Appl.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1998
    }, {
      "title" : "Total variation and Cheeger cuts",
      "author" : [ "A. Szlam", "X. Bresson" ],
      "venue" : "In ICML, pages 1039–1046,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "An inverse power method for nonlinear eigenproblems with applications in 1spectral clustering and sparse PCA",
      "author" : [ "M. Hein", "T. Bühler" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Beyond spectral clustering - tight relaxations of balanced graph cuts",
      "author" : [ "M. Hein", "S. Setzer" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Convergence and energy landscape for Cheeger cut clustering",
      "author" : [ "X. Bresson", "T. Laurent", "D. Uminsky", "J.H. von Brecht" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Multiclass total variation clustering",
      "author" : [ "X. Bresson", "T. Laurent", "D. Uminsky", "J.H. von Brecht" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Learning with submodular functions: A convex optimization perspective",
      "author" : [ "F. Bach" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "A first-order primal-dual algorithm for convex problems with applications to imaging",
      "author" : [ "A. Chambolle", "T. Pock" ],
      "venue" : "J. of Math. Imaging and Vision,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Diagonal preconditioning for first order primal-dual algorithms in convex optimization",
      "author" : [ "T. Pock", "A. Chambolle" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "A fast and high quality multilevel scheme for partitioning irregular graphs",
      "author" : [ "G. Karypis", "V. Kumar" ],
      "venue" : "SIAM J. Sci. Comput.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1998
    }, {
      "title" : "Linear and nonlinear projective nonnegative matrix factorization",
      "author" : [ "Z. Yang", "E. Oja" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Nonnegative matrix factorization for combinatorial optimization: Spectral clustering, graph matching, and clique finding",
      "author" : [ "C. Ding", "T. Li", "M.I. Jordan" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2008
    }, {
      "title" : "Orthogonal nonnegative matrix tri-factorizations for clustering",
      "author" : [ "C. Ding", "T. Li", "W. Peng", "H. Park" ],
      "venue" : "In KDD,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "Clustering by left-stochastic matrix factorization",
      "author" : [ "R. Arora", "M.R. Gupta", "A. Kapila", "M. Fazel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Clustering by nonnegative matrix factorization using graph random walk",
      "author" : [ "Z. Yang", "T. Hao", "O. Dikmen", "X. Chen", "E. Oja" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "A combined evolutionary search and multilevel optimisation approach to graph-partitioning",
      "author" : [ "A.J. Soper", "C. Walshaw", "M. Cross" ],
      "venue" : "J. of Global Optimization,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2004
    }, {
      "title" : "Constrained 1-spectral clustering",
      "author" : [ "S.S. Rangapuram", "M. Hein" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The problem of finding k clusters in a graph can be formulated as a balanced k-cut problem [1, 2, 3, 4], where ratio and normalized cut are famous instances of balanced graph cut criteria employed for clustering, community detection and image segmentation.",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "The problem of finding k clusters in a graph can be formulated as a balanced k-cut problem [1, 2, 3, 4], where ratio and normalized cut are famous instances of balanced graph cut criteria employed for clustering, community detection and image segmentation.",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "The problem of finding k clusters in a graph can be formulated as a balanced k-cut problem [1, 2, 3, 4], where ratio and normalized cut are famous instances of balanced graph cut criteria employed for clustering, community detection and image segmentation.",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "The problem of finding k clusters in a graph can be formulated as a balanced k-cut problem [1, 2, 3, 4], where ratio and normalized cut are famous instances of balanced graph cut criteria employed for clustering, community detection and image segmentation.",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "The balanced k-cut problem is known to be NP-hard [4] and thus in practice relaxations [4, 5] or greedy approaches [6] are used for finding the optimal multi-cut.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "The balanced k-cut problem is known to be NP-hard [4] and thus in practice relaxations [4, 5] or greedy approaches [6] are used for finding the optimal multi-cut.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "The balanced k-cut problem is known to be NP-hard [4] and thus in practice relaxations [4, 5] or greedy approaches [6] are used for finding the optimal multi-cut.",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "The balanced k-cut problem is known to be NP-hard [4] and thus in practice relaxations [4, 5] or greedy approaches [6] are used for finding the optimal multi-cut.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "The most famous approach is spectral clustering [7], which corresponds to the spectral relaxation of the ratio/normalized cut and uses k-means in the embedding of the vertices found by the first k eigenvectors of the graph Laplacian in order to obtain the clustering.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "However, the spectral relaxation has been shown to be loose for k = 2 [8] and for k > 2 no guarantees are known of the quality of the obtained k-cut with respect to the optimal one.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "Moreover, in practice even greedy approaches [6] frequently outperform spectral clustering.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "This paper is motivated by another line of recent work [9, 10, 11, 12] where it has been shown that an exact continuous relaxation for the two cluster case (k = 2) is possible for a quite general class of balancing functions.",
      "startOffset" : 55,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "This paper is motivated by another line of recent work [9, 10, 11, 12] where it has been shown that an exact continuous relaxation for the two cluster case (k = 2) is possible for a quite general class of balancing functions.",
      "startOffset" : 55,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "This paper is motivated by another line of recent work [9, 10, 11, 12] where it has been shown that an exact continuous relaxation for the two cluster case (k = 2) is possible for a quite general class of balancing functions.",
      "startOffset" : 55,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "This paper is motivated by another line of recent work [9, 10, 11, 12] where it has been shown that an exact continuous relaxation for the two cluster case (k = 2) is possible for a quite general class of balancing functions.",
      "startOffset" : 55,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "Inspired by the recent approach in [13], in this paper we tackle directly the general balanced k-cut problem based on a new tight continuous relaxation.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "We show that the relaxation for the asymmetric ratio Cheeger cut proposed recently by [13] is loose when the data does not contain k well-separated clusters and thus leads to poor performance in practice.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 12,
      "context" : "Similar to [13] we can also integrate label information leading to a transductive clustering formulation.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 12,
      "context" : "This is in contrast to the algorithm proposed in [13] for which no such guarantee holds.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "The Lovasz extension of a set function is convex if and only if the set function is submodular [14].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "[3, 7, 6] and corresponds to a balancing function without bias towards a particular size of the sets, whereas the Asymmetric Ratio Cheeger Cut recently proposed in [13] has a bias towards sets of size |V | k (Ŝ(C) attains its maximum at this point) which makes perfect sense if one expects clusters which have roughly equal size.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "[3, 7, 6] and corresponds to a balancing function without bias towards a particular size of the sets, whereas the Asymmetric Ratio Cheeger Cut recently proposed in [13] has a bias towards sets of size |V | k (Ŝ(C) attains its maximum at this point) which makes perfect sense if one expects clusters which have roughly equal size.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 5,
      "context" : "[3, 7, 6] and corresponds to a balancing function without bias towards a particular size of the sets, whereas the Asymmetric Ratio Cheeger Cut recently proposed in [13] has a bias towards sets of size |V | k (Ŝ(C) attains its maximum at this point) which makes perfect sense if one expects clusters which have roughly equal size.",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 12,
      "context" : "[3, 7, 6] and corresponds to a balancing function without bias towards a particular size of the sets, whereas the Asymmetric Ratio Cheeger Cut recently proposed in [13] has a bias towards sets of size |V | k (Ŝ(C) attains its maximum at this point) which makes perfect sense if one expects clusters which have roughly equal size.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 3,
      "context" : ", Ŝ(C) = vol(C) = ∑ i∈C di, where di = ∑n j=1 wij , leading to the normalized cut[4].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "The motivation for our relaxation is taken from the recent work of [9, 10, 11], where exact relaxations are shown for the case k = 2.",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "The motivation for our relaxation is taken from the recent work of [9, 10, 11], where exact relaxations are shown for the case k = 2.",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "The motivation for our relaxation is taken from the recent work of [9, 10, 11], where exact relaxations are shown for the case k = 2.",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "First, we connect our relaxation to the previous work of [11] for the case k = 2.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "Simplex constraints alone are not sufficient to yield a partition: Our approach has been inspired by [13] who proposed the following continuous relaxation for the Asymmetric Ratio Cheeger Cut",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 12,
      "context" : "For asymmetric balancing functions as employed for the Asymmetric Ratio Cheeger Cut by [13] in their relaxation (5) we can prove such a strong result only in the case where the graph is disconnected.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "(e) Figure 1: Toy example illustrating that the relaxation of [13] converges to a degenerate solution when applied to a graph with dominating 2-cut.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "(a) 10NN-graph generated from three Gaussians in 10 dimensions (b) continuous solution of (5) from [13] for k = 3, (c) rounding of the continuous solution of [13] does not yield a 3-partition (d) continuous solution found by our method together with the vertices i ∈ I (black) where the membership constraint is enforced.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "(a) 10NN-graph generated from three Gaussians in 10 dimensions (b) continuous solution of (5) from [13] for k = 3, (c) rounding of the continuous solution of [13] does not yield a 3-partition (d) continuous solution found by our method together with the vertices i ∈ I (black) where the membership constraint is enforced.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : "balancing function failure of the relaxation (6) and thus also of the relaxation (5) of [13] is only guaranteed for disconnected graphs.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 12,
      "context" : "We illustrate this with a toy example in Figure 1 where the algorithm of [13] for solving (5) fails as it converges exactly to the solution predicted by Corollary 1 and thus only produces a 2-partition instead of the desired 3-partition.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "We would like to note that the algorithm proposed by [13] for (5) does not yield monotonic descent.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "In fact it is unclear what the derived guarantee for the algorithm in [13] implies for the generated sequence.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "2 in [14], (s t l)ji = Ŝ(Cli−1) − Ŝ(Cli), where ji is the i th smallest component of F t l and Cli = {j ∈ V | (F t l )j > (F t l )i}.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 14,
      "context" : "We solve this LP via the PDHG algorithm [15, 16].",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "We solve this LP via the PDHG algorithm [15, 16].",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "We evaluate our method against a diverse selection of state-of-the-art clustering methods like spectral clustering (Spec) [7], BSpec [11], Graclus1 [6], NMF based approaches PNMF [18], NSC [19], ONMF [20], LSD [21], NMFR [22] and MTV [13] which optimizes (5).",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "We evaluate our method against a diverse selection of state-of-the-art clustering methods like spectral clustering (Spec) [7], BSpec [11], Graclus1 [6], NMF based approaches PNMF [18], NSC [19], ONMF [20], LSD [21], NMFR [22] and MTV [13] which optimizes (5).",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "We evaluate our method against a diverse selection of state-of-the-art clustering methods like spectral clustering (Spec) [7], BSpec [11], Graclus1 [6], NMF based approaches PNMF [18], NSC [19], ONMF [20], LSD [21], NMFR [22] and MTV [13] which optimizes (5).",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 17,
      "context" : "We evaluate our method against a diverse selection of state-of-the-art clustering methods like spectral clustering (Spec) [7], BSpec [11], Graclus1 [6], NMF based approaches PNMF [18], NSC [19], ONMF [20], LSD [21], NMFR [22] and MTV [13] which optimizes (5).",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 18,
      "context" : "We evaluate our method against a diverse selection of state-of-the-art clustering methods like spectral clustering (Spec) [7], BSpec [11], Graclus1 [6], NMF based approaches PNMF [18], NSC [19], ONMF [20], LSD [21], NMFR [22] and MTV [13] which optimizes (5).",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 19,
      "context" : "We evaluate our method against a diverse selection of state-of-the-art clustering methods like spectral clustering (Spec) [7], BSpec [11], Graclus1 [6], NMF based approaches PNMF [18], NSC [19], ONMF [20], LSD [21], NMFR [22] and MTV [13] which optimizes (5).",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 20,
      "context" : "We evaluate our method against a diverse selection of state-of-the-art clustering methods like spectral clustering (Spec) [7], BSpec [11], Graclus1 [6], NMF based approaches PNMF [18], NSC [19], ONMF [20], LSD [21], NMFR [22] and MTV [13] which optimizes (5).",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 21,
      "context" : "We evaluate our method against a diverse selection of state-of-the-art clustering methods like spectral clustering (Spec) [7], BSpec [11], Graclus1 [6], NMF based approaches PNMF [18], NSC [19], ONMF [20], LSD [21], NMFR [22] and MTV [13] which optimizes (5).",
      "startOffset" : 221,
      "endOffset" : 225
    }, {
      "referenceID" : 12,
      "context" : "We evaluate our method against a diverse selection of state-of-the-art clustering methods like spectral clustering (Spec) [7], BSpec [11], Graclus1 [6], NMF based approaches PNMF [18], NSC [19], ONMF [20], LSD [21], NMFR [22] and MTV [13] which optimizes (5).",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 21,
      "context" : "We used the publicly available code [22, 13] with default settings.",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "We used the publicly available code [22, 13] with default settings.",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "We run our method using 5 random initializations, 7 initializations based on the spectral clustering solution similar to [13] (who use 30 such initializations).",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "In addition to the datasets provided in [13], we also selected a variety of datasets from the UCI repository shown below.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "For all the datasets not in [13], symmetric k-NN graphs are built with Gaussian weights exp ( − s‖x−y‖ 2 min{σ2 x,k,σ 2 y,k} ) , where σx,k is the k-NN distance of point x.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 6,
      "context" : "For ratio and normalized cut cases we achieve better results than [7, 11, 6] which directly optimize this criterion.",
      "startOffset" : 66,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "For ratio and normalized cut cases we achieve better results than [7, 11, 6] which directly optimize this criterion.",
      "startOffset" : 66,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "For ratio and normalized cut cases we achieve better results than [7, 11, 6] which directly optimize this criterion.",
      "startOffset" : 66,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "This shows that the greedy recursive bi-partitioning affects badly the performance of [11], which, otherwise, was shown to obtain the best cuts on several benchmark datasets [23].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "This shows that the greedy recursive bi-partitioning affects badly the performance of [11], which, otherwise, was shown to obtain the best cuts on several benchmark datasets [23].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 12,
      "context" : "It is striking that the competing method of [13], which directly minimizes the asymmetric ratio cut, is beaten significantly by Graclus as well as our method.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "As the main goal (1)Since [6], a multi-level algorithm directly minimizing Rcut/Ncut, is shown to be superior to METIS [17], we do not compare with [17].",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 16,
      "context" : "As the main goal (1)Since [6], a multi-level algorithm directly minimizing Rcut/Ncut, is shown to be superior to METIS [17], we do not compare with [17].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "As the main goal (1)Since [6], a multi-level algorithm directly minimizing Rcut/Ncut, is shown to be superior to METIS [17], we do not compare with [17].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 12,
      "context" : "is to compare to [13] we choose their balancing function (RCC-asym).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "Transductive Setting: We evaluate our method against [13] in a transductive setting.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "As in [13], we randomly sample either one label or a fixed percentage of labels per class from the ground truth.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 23,
      "context" : "Moreover, in contrast to a recursive splitting approach [24], our method enables the direct integration of prior information available in form of must/cannotlink constraints, which is an interesting topic for future research.",
      "startOffset" : 56,
      "endOffset" : 60
    } ],
    "year" : 2014,
    "abstractText" : "Spectral Clustering as a relaxation of the normalized/ratio cut has become one of the standard graph-based clustering methods. Existing methods for the computation of multiple clusters, corresponding to a balanced k-cut of the graph, are either based on greedy techniques or heuristics which have weak connection to the original motivation of minimizing the normalized cut. In this paper we propose a new tight continuous relaxation for any balanced k-cut problem and show that a related recently proposed relaxation is in most cases loose leading to poor performance in practice. For the optimization of our tight continuous relaxation we propose a new algorithm for the difficult sum-of-ratios minimization problem which achieves monotonic descent. Extensive comparisons show that our method outperforms all existing approaches for ratio cut and other balanced k-cut criteria.",
    "creator" : null
  }
}