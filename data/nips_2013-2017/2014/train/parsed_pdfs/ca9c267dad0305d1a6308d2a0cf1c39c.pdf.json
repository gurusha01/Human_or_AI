{
  "name" : "ca9c267dad0305d1a6308d2a0cf1c39c.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Multiple Tasks in Parallel with a Shared Annotator",
    "authors" : [ "Haim Cohen" ],
    "emails" : [ "hcohen@tx.technion.ac.il", "koby@ee.technion.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A triumph of machine learning is the ability to predict many human aspects: is certain mail spam or not, is a news-item of interest or not, does a movie meet one’s taste or not, and so on. The dominant paradigm is supervised learning, in which the main bottleneck is the need to annotate data. A common protocol is problem centric: first collect data or inputs automatically (with low cost), and then pass it on to a user or an expert to be annotated. Annotation can be outsourced to the crowed by a service like Mechanical Turk, or performed by experts as in the Linguistic data Consortium. Then, this data may be used to build models, either for a single task or many tasks. This approach is not making optimal use of the main resource - the annotator - as some tasks are harder than others, yet we need to give the annotator the (amount of) data to be annotated for each task a-priori . Another aspect of this problem is the need to adapt systems to individual users, to this end, such systems may query the user for the label of some input, yet, if few systems will do so independently, the user will be flooded with queries, and will avoid interaction with those systems. For example, sometimes there is a need to annotate news items from few agencies. One person cannot handle all of them, and only some items can be annotated, which ones? Our setting is designed to handle exactly this problem, and specifically, how to make best usage of annotation time.\nWe propose a new framework of online multi-task learning with a shared annotator. Here, algorithms are learning few tasks simultaneously, yet they receive feedback using a central mechanism that trades off the amount of feedback (or labels) each task receives. We derive a specific algorithm based on the good-old Perceptron algorithm, called SHAMPO (SHared Annotator for Multiple PrOblems) for binary classification and analyze it in the mistake bound model, showing that our algorithm may perform well compared with methods that observe all annotated data. We then show how to reduce few contextual bandit problems into our framework, and provide specific bounds for such\nsettings. We evaluate our algorithm with four different datasets for OCR , vowel prediction (VJ) and document classification, and show that it can improve performance either on average over all tasks, or even if their output is combined towards a single shared task, such as multi-class prediction. We conclude with discussion of related work, and few of the many routes to extend this work."
    }, {
      "heading" : "2 Problem Setting",
      "text" : "We study online multi-task learning with a shared annotator. There are K tasks to be learned simultaneously. Learning is performed in rounds. On round t, there are K input-output pairs (xi,t, yi,t) where inputs xi,t ∈ Rdi are vectors, and labels are binary yi,t ∈ {−1,+1}. In the general case, the input-spaces for each task may be different. We simplify the notation and assume that di = d for all tasks. Since the proposed algorithm uses the margin that is affected by the vector norm, there is a need to scale all the vectors into a ball. Furthermore, no dependency between tasks is assumed.\nOn round t, the learning algorithm receives K inputs xi,t for i = 1, . . . ,K, and outputs K binary-labels ŷi,t, where ŷi,t ∈ {−1,+1} is the label predicted for the input xi,t of task i. The algorithm then chooses a task Jt ∈ {1, . . . ,K} and receives from an annotator the true-label yJt,t for that task Jt. It does not observe any other label.\nThen, the algorithm updates its models, and proceeds to the next round (and inputs). For easing calculations below, we denote by K indicators Zt = (Z1,t, . . . , ZK,t) the identity of the task which was queried on round t, and set ZJt,t = 1 and Zi,t = 0 for i 6= Jt. Clearly, ∑ i Zi,t = 1. Below, we define the notation Et−1 [x] to be the conditional expectation E [x|Z1, ...Zt−1] given all previous choices. Illustration of a single iteration of multi-task algorithms is shown in Fig. 1. The top panel shows the standard setting with shared annotator, that labels all inputs, which are fed to the corresponding algorithms to update corresponding models. The bottom panel shows the SHAMPO algorithm, which couples labeling annotation and learning process, and synchronizes a single annotation per round. At most one task performs an update per round (the annotated one).\nWe focus on linear-functions of the form f(x) = sign(p) for a quantity p = w>x, w ∈ Rd, called the margin. Specifically, the algorithm maintains a set of K weight vectors. On round t, the algorithm predicts ŷi,t = sign(p̂i,t) where p̂i,t = w>i,t−1xi,t. On rounds for which the label of some task Jt is queried, the algorithm, is not updating the models of all other tasks, that is, we have wi,t = wi,t−1 for i 6= Jt. We say that the algorithm has a prediction mistake in task i if yi,t 6= ŷi,t, and denote this event by Mi,t = 1, otherwise, if there is no mistake we setMi,t = 0. The goal of the algorithm is to minimize the cumulative number of mistakes, ∑ t ∑ iMi,t. Models are also evaluated using the Hinge-loss. Specifically, let ui ∈ Rd be some vector associated with task i. We denote the Hinge-loss of it, with respect to some input-output by, `γ,i,t(ui) = ( γ − yi,tu>i xi,t ) +\n, where, (x)+ = max{x, 0}, and γ > 0 is some parameter. The cumulative loss over all tasks and a sequence of n inputs, is, Lγ,n = Lγ({ui}) = ∑n t=1 ∑K i=1 `γ,i,t(ui). We also use the following expected hinge-loss\nover the random choices of the algorithm, L̄γ,n = L̄{ui} = E [∑n t ∑K i=1Mi,tZi,t`γ,i,t(ui) ] . We proceed by describing our algorithm and specifying how to choose a task to query its label, and how to perform an update."
    }, {
      "heading" : "3 SHAMPO: SHared Annotator for Multiple Problems",
      "text" : "We turn to describe an algorithm for multi-task learning with a shared annotator setting, that works with linear models. Two steps are yet to be specified: how to pick a task to be labeled and how to perform an update once the true label for that task is given.\nTo select a task, the algorithm uses the absolute margin |p̂i,t|. Intuitively, if |p̂i,t| is small, then there is uncertainty about the labeling of xi,t, and vise-versa for large values of |p̂i,t|. Similar argument\nwas used by Tong and Koller [22] for picking an example to be labeled in batch active learning. Yet, if the model wi,t−1 is not accurate enough, due to small number of observed examples, this estimation may be rough, and may lead to a wrong conclusion. We thus perform an explorationexploitation strategy, and query tasks randomly, with a bias towards tasks with low |p̂i,t|. To the best of our knowledge, exploration-exploitation usage in this context of choosing an examples to be labeled (eg. in settings such as semi-supervised learning or selective sampling) is novel and new. We introduce b ≥ 0 to be a tradeoff parameter between exploration and exploitation and ai ≥ 0 as a prior for query distribution over tasks. Specifically, we induce a distribution over tasks,\nPr [Jt = j]= aj ( b+ |p̂j,t|−minKm=1 |p̂m,t| )−1 Dt for Dt= K∑ i=1 ai ( b+ |p̂i,t|−min m |p̂m,t| )−1 . (1)\nParameters: b, λ, ai ∈ R+ for i = 1, . . . ,K Initialize: wi,0 = 0 for i = 1, . . . ,K for t = 1, 2, ..., n do\n1. Observe K instance vectors, xi,t, (i = 1, . . . ,K). 2. Compute margins p̂i,t = w>i,t−1xi,t. 3. Predict K labels, ŷi,t = sign(p̂i,t). 4. Draw task Jt with the distribution:\nPr [Jt = j] = aj ( b+ |p̂j,t| −minKm=1 |p̂m,t| )−1 Dt ,\nDt = ∑ i ai ( b+ |p̂i,t| − K min m=1 |p̂m,t| )−1 .\n5. Query the true label ,yJt,t ∈ {−1, 1}. 6. Set indicator MJt,t = 1 iff yJt,tp̂i,t ≤ 0 (Error) 7. Set indicator AJt,t = 1 iff 0 < yJt,tp̂i,t ≤ λ (Small\nmargin) 8. Update with the perceptron rule:\nwJt,t = wJt,t−1 + (AJt,t +MJt,t) yJt,t xJt,t (2) wi,t = wi,t−1 for i 6= Jt\nend for Output: wi,n for i = 1, . . . ,K.\nFigure 2: SHAMPO: SHared Annotator for Multiple PrOblems.\nClearly, Pr [Jt = j] ≥ 0 and∑ j Pr [Jt = j] = 1. For b = 0 we have Pr [Jt = j] = 1 for the task with minimal margin, Jt = arg minKi=1 |p̂i,t|, and for b→∞ the distribution is proportional to the prior weights, Pr [Jt = j] = aj/( ∑ i ai). As noted above we denote by Zi,t = 1 iff i = Jt. Since the distribution is invariant to a multiplicative factor of ai we assume 1 ≤ ai∀i. The update of the algorithm is performed with the aggressive perceptron rule, that is wJt,t = wJt,t−1 + (AJt,t + MJt,t) yJt,t xJt,t and wi,t = wi,t−1 for i 6= Jt. we define Ai,t , the aggressive update indicator introducing and the aggressive update threshold, λ ∈ R > 0 such that, Ai = 1 iff 0 < yi,tp̂i,t ≤ λ, i.e, there is no mistake but the margin is small, and Ai,t = 0 otherwise. An update is performed if either there is a mistake (MJi,t = 0) or the margin is low (AJi,t = 1). Note\nthat these events are mutually exclusive. For simplicity of presentation we write this update as, wi,t = wi,t−1 +Zi,t (Ai,t +Mi,t)yi,t xi,t. Although this notation uses labels for all-tasks, only the label of the task Jt is used in practice, as for other tasks Zi,t = 0.\nWe call this algorithm SHAMPO for SHared Annotator for Multiple PrOblems. The pseudo-code appears in Fig. 2. We conclude this section by noting that the algorithm can be incorporated with Mercer-kernels as all operations depend implicitly on inner-product between inputs."
    }, {
      "heading" : "4 Analysis",
      "text" : "The following theorem states that the expected cumulative number of mistakes that the algorithm makes, may not be higher than the algorithm that observes the labels of all inputs.\nTheorem 1 If SHAMPO algorithm runs on K tasks with K parallel example pair sequences (xi,1, yi,1), ...(xi,n, yi,n) ∈ Rd × {−1, 1}, i = 1, ...,K with input parameters 0 ≤ b, 0 ≤ λ ≤ b/2, and prior 1 ≤ ai∀i, denote by X = maxi,t ‖xi,t‖, then, for all γ > 0, all ui ∈ Rd and all n ≥ 1\nthere exists 0 < δ ≤ ∑K i=1 ai, such that,\nE [ K∑ i=1 n∑ t=1 Mi,t ] ≤ δ γ [( 1 + X2 2b ) L̄γ,n + ( 2b+X2 )2 U2 8γb ] + ( 2 λ b − 1 ) E [ K∑ i=1 n∑ t=1 aiAi,t ] .\nwhere we denote U2 = ∑K i=1 ‖ui‖ 2. The expectation is over the random choices of the algorithm.\nDue to lack of space, the proof appears in Appendix A.1 in the supplementary material. Few notes on the mistake bound: First, the right term of the bound is equals zero either when λ = 0 (as Ai,t = 0) or λ = b/2. Any value in between, may yield an strict negative value of this term, which in turn, results in a lower bound. Second, the quantity L̄γ,n is non-increasing with the number of tasks. The first terms depends on the number of tasks only via δ ≤ ∑ i ai. Thus, if ai = 1 (uniform prior) the quantity δ ≤ K is bounded by the number of tasks. Yet, when the hardness of the tasks is not equal or balanced, one may expect δ to be closer to 1 than K, which we found empirically to be true. Additionally, the prior ai can be used to make the algorithm focus on the hard tasks, thereby improving the bound. While δ multiplying the first term can be larger, the second term can be lower. A task i which corresponds to a large value of ai will be updated more in early rounds than tasks with low ai. If more of these updates are aggressive, the second term will be negative and far from zero.\nOne can use the bound to tune the algorithm for a good value of b for the non aggressive case (λ = 0), by minimizing the bound over b. This may not be possible directly since L̄γ,n depends implicitly on the value of b1. Alternatively, we can take a loose estimate of L̄γ,n, and replace it with Lγ,n (which is ∼ K times larger). The optimal value of b can now be calculated, b = X 2\n2\n√ 1 +\n4γLγ,n U2X2 . Substituting this value in the bound of Eq. (1) with Lγ,n leads to the following bound, E [∑K\ni=1 ∑n t=1Mi,t ] ≤ δγ [ Lγ,n + U2X2 2γ + U2 2γ √ 1 + 4γLγ,n U2X2 ] , which has the same\ndependency in the number of inputs n as algorithm that observes all of them.\nWe conclude this section by noting that the algorithm and analysis can be extended to the case that more than single query is allowed per task. Analysis and proof appears in Appendix A.2 in the supplementary material."
    }, {
      "heading" : "5 From Multi-task to Contextual Bandits",
      "text" : "Although our algorithm is designed for many binary-classification tasks, it can also be applied in two settings of contextual bandits, when decoupling exploration and exploitation is allowed [23, 3]. In this setting, the goal is to predict a label Ŷt ∈ {1, . . . , C} given an input xt. As before, the algorithm works in rounds. On round t the algorithm receives an input xt and gives as an output multicalss label Ŷt ∈ {1, . . . , C}. Then, it queries for some information about the label via a single binary “yes-no” question, and uses the feedback to update its model. We consider two forms of questions. Note that our algorithm subsumes past methods since they also allow the introduction of a bias (or prior knowledge) towards some tasks, which in turn, may improve performance."
    }, {
      "heading" : "5.1 One-vs-Rest",
      "text" : "The first setting is termed one-vs-rest. The algorithm asks if the true label is some label Ȳt ∈ {1, . . . , C}, possibly not the predicted label, i.e. it may be the case that Ȳt 6= Ŷt. Given the response whether Ȳt is the true label Yt, the algorithm updates its models. The reduction we perform is by introducing K tasks, one per class. The problem of the learning algorithm for task i is to decide whether the true label is class i or not. Given the output of all (binary) classifiers, the algorithm generates a single multi-class prediction to be the single label for which the output of the corresponding binary classifier is positive. If such class does not exist, or there are more than one classes as such, a random prediction is used, i.e., given an input xt we define Ŷt = arg maxi ŷi,t, where ties are broken arbitrarily. The label to be queried is Ȳt = Jt, i.e. the problem index that SHAMPO is querying. We analyze the performance of this reduction as a multiclass prediction algorithm.\n1Similar issue appears also after the discussion of Theorem 1 in a different context [7].\nCorollary 2 Assume the SHAMPO algorithm is executed as above with K = C one-vs-rest problems, on a sequence (x1, Y1), ...(xn, Yn) ∈ Rd × {1, ..., C}, and input parameter b > 0 and prior 1 ≤ ai∀i. Then for all γ > 0 and all ui ∈ Rd, there exist 0 < δ ≤ ∑C i=1 ai\nsuch that the expected number of multi-class errors is bounded as follows E [∑ t[[Yt 6= Ŷt]] ] ≤\nδ γ\n[( 1 + X 2\n2b\n) L̄γ,n + (2b+X2) 2 U2\n8γb\n] + ( 2λb − 1 ) E [∑K i=1 ∑n t=1 aiAi,t ] ,where [[I]] = 1 if the pred-\nicate I is true, and zero otherwise. The corollary follows directly from Thm. 1 by noting that, [[Yt 6= Ŷt]] ≤ ∑ iMi,t. That is, there is a multiclass mistake if there is at least one prediction mistake of one of the one-vs-rest problems. The closest setting is contextual bandits, yet we allow decoupling of exploration and exploitation. Ignoring this decoupling, the Banditron algorithm [17] is the closest to ours, with a regret of O(T 2/3). Hazan et al [16] proposed an algorithm withO( √ T ) regret but designed for the log loss, with coefficient that may be very large, and another [9] algorithm has O( √ T ) regret with respect to prediction mistakes, yet they assumed stochastic labeling, rather than adversarial."
    }, {
      "heading" : "5.2 One-vs-One",
      "text" : "In the second setting, termed by one-vs-one, the algorithm picks two labels Ȳ +t , Ȳ − t ∈ {1 . . . C}, possibly both not the predicted label. The feedback for the learner is three-fold: it is yJt,t = +1 if the first alternative is the correct label, Ȳ +t = Yt, yJt,t = −1 if the second alternative is the correct label, Ȳ −t = Yt, and it is yJt,t = 0 otherwise (in this case there is no error and we set MJt,t = 0). The reduction we perform is by introducing K = ( C 2 ) problems, one per pair of classes. The goal of the learning algorithm for a problem indexed with two labels (y1, y2) is to decide which is the correct label, given it is one of the two. Given the output of all (binary) classifiers the algorithm generates a single multi-class prediction using a tournament in a round-robin approach [15]. If there is no clear winner, a random prediction is used. We now analyze the performance of this reduction as a multiclass prediction algorithm.\nCorollary 3 Assume the SHAMPO algorithm is executed as above, with K = ( C 2 ) one-vs-one problems, on a sequence (x1, Y1), ...(xn, Yn) ∈ Rd × {1, ..., C}, and input parameter b > 0 and prior 1 ≤ ai∀i . Then for all γ > 0 and all ui ∈ Rd, there exist 0 < δ ≤ ∑(C2) i=1 ai such\nthat the expected number of multi-class errors can be bounded as follows E [∑ t[[Yt 6= Ŷt]] ] ≤\n2\n((C2)−1)/2+1\n{ δ γ [( 1 + X 2 2b ) L̄γ,n + (2b+X2) 2 U2 8γb ] + ( 2λb − 1 ) E [∑K i=1 ∑n t=1 aiAi,t ]} .\nThe corollary follows directly from Thm. 1 by noting that, [[Yt 6= Ŷt]] ≤ 2((C2)−1)/2+1 ∑(C2) i=1Mi,t. Note, that the bound is essentially independent of C as the coefficient in the bound is upper bounded by 6 for C ≥ 3.\nWe conclude this section with two algorithmic modifications, we employed in this setting. Currently, when the feedback is zero, there is no update of the weights, because there are no errors. This causes the algorithm to effectively ignore such examples, as in these cases the algorithm is not modifying any model, furthermore, if such example is repeated, a problem with possibly “0” feedback may be queried again. We fix this issue with one of two modifications: In the first one, if the feedback is zero, we modify the model to reduce the chance that the chosen problem, Jt, would be chosen again for the same input (i.e. not to make the same wrongchoice of choosing irrelevant problem again). To this end, we modify the weights a bit, to increase the confidence (absolute margin) of the model for the same input, and replace Eq. (2) with, wJt,t = wJt,t−1 + [[yJt,t 6= 0]] yJt,t xJt,t + [[yJt,t = 0]]ηŷJt,txJt,t , for some η > 0. In other words, if there is a possible error (i.e. yJt,t 6= 0) the update follows the Perceptron’s rule. Otherwise, the weights are updated such that the absolute margin will increase, as |w>Jt,txJt,t| = |(wJt,t−1 + ηŷJt,txJt,t) >xJt,t| = |w>Jt,t−1xJt,t + ηsign(w > Jt,t−1xJt,t)‖xJt,t‖\n2| = |w>Jt,t−1xJt,t|+ η‖xJt,t‖\n2 > |w>Jt,t−1xJt,t|. We call this method one-vs-one-weak, as it performs weak updates for zero feedback. The second alternative is not to allow 0 value feedback, and if this is the case, to set the label to be either +1 or−1, randomly. We call this method one-vs-one-random.\n6 Experiments\n10 −6 10 −4 10 −2\n10 0\n10 2\n10\n15\n20\n25\n30\n35\nb [log]\nE rr\no r\n[% ]\ntotal queried\n(a) Training mistakes vs b (b) Test error vs no. of queries\ntraining examples and 236, 680 test examples. We created binary tasks from these multi-class datasets using two reductions: One-vs-Rest setting and One-vs-One setting. For example, in both USPS and MNIST there are 10 binary one-vs-rest tasks and 45 binary one-vs-one tasks. The NLP document classification include of spam filtering, news items and news-group classification, sentiment classification, and product domain categorization. A total of 31 binary prediction tasks over all, with a total of 252, 609 examples, and input dimension varying between 8, 768 and 1, 447, 866. Details of the individual binary tasks can be found elsewhere [8]. We created an eighth collection, named MIXED, which consists of 40 tasks: 10 random tasks from each one of the four basic datasets (one-vs-one versions). This yielded eight collections (USPS, MNIST and VJ; each as one-vs-rest or one-vs-one), document classification and mixed. From each of these eight collections we generated between 6 to 10 combinations (or problems), each problem was created by sampling between 2 and 8 tasks which yielded a total of 64 multi-task problems. We tried to diversify problems difficulty by including both hard and easy binary classification problems. The hardness of a binary problem is evaluated by the number of mistakes the Perceptron algorithm performs on the problem.\nWe evaluated two baselines as well as our algorithm. Algorithm uniform picks a random task to be queried and updated (corresponding to b → ∞), exploit which picks the tasks with the lowest absolute margin (i.e. the “hardest instance”), this combination corresponds to b ≈ 0 of SHAMPO. We tried for SHAMPO 13 values for b, equally spaced on a logarithmic scale. All algorithms made a single pass over the training data. We ran two version of the algorithm: plain version, without aggressiveness (updates on mistakes only, λ = 0) and an Aggressive version λ = b/2 (we tried lower values of λ as in the bound, but we found that λ = b/2 gives best results), both with uniform prior (ai = 1). We used separate training set and a test set, to build a model and evaluate it.\nResults are evaluated using 2 quantities. First, the average test error (over all the dataset combinations) and the average score. For each combination we assigned a score of 1 to the algorithm with the lowest test error, and a score of 2, to the second best, and all the way up to a score of 6 to the algorithm with the highest test error.\nMulti-task Binary Classification : Fig. 3(a) and Fig. 3(b) show the test error of the three algorithms on two of document classification combinations, with four and eight tasks. Clearly, not only SHAMPO performs better, but it does so on each task individually. (Our analysis above bounds the total number of mistakes over all tasks.) Fig. 3(c) shows the average test error vs b using the one-vs-one binary USPS problems for the three variants of SHAMPO: non-aggressive (called plain), aggressive and aggressive with prior.Clearly, the plain version does worse than both the aggressive version and the non-uniform prior version. For other combinations the prior was not always improving results. We hypothesise that this is because our heuristic may yield a bad prior which is not focusing the algorithm on the right (hard) tasks.\nResults are summarized in Table 1. In general exploit is better than uniform and aggressive is better than non-aggressive. Aggressive SHAMPO yields the best results both evaluated as average (over tasks per combination and over combinations). Remarkably, even in the mixed dataset (where tasks are of different nature: images, audio and documents), the aggressive SHAPO improves over uniform (4.45% error) and the aggressive-exploit baseline (2.75%), and achieves a test error of 2.06%.\nNext, we focus on the problems that the algorithm chooses to annotate on each iteration for various values of b. Fig. 4(a) shows the total number of mistakes SHAMPO made during training time on MNIST , we show two quantities: fraction of mistakes over all training examples (denoted by “total” - blue) and fraction of mistakes over only queried examples (denoted by “queried” - dashed red). In pure exploration (large b) both quantities are the same, as the choice of problem to be labeled is independent of the problem and example, and essentially the fraction of mistakes in queried examples is a good estimate of the fraction of mistakes over all examples. The other extreme is when performing pure exploitation (low b), here the fraction of mistakes made on queried examples went up, while the overall fraction of mistakes went down. This indicates that the algorithm indeed focuses its queries on the harder inputs, which in turn, improves overall training mistake. There is a sweet point b ≈ 0.01 for which SHAMPO is still focusing on the harder examples, yet reduces the total fraction of training mistakes even more. The existence of such tradeoff is predicted by Thm. 1.\nAnother perspective of the phenomena is that for values of b ∞ SHAMPO focuses on the harder examples, is illustrated in Fig. 4(b) where test error vs number of queries is plotted for each problem for MNIST. We show three cases: uniform, exploit and a mid-value of b ≈ 0.01 which tradeoffs exploration and exploitation. Few comments: First, when performing uniform querying, all problems have about the same number of queries (266), close to the number of examples per problem (12, 000), divided by the number of problems (45). Second, when having a tradeoff between exploration and exploitation, harder problems (as indicated by test error) get more queries than easier problems. For example, the four problems with test error greater than 6% get at least 400 queries, which is about twice the number of queries received by each of the 12 problems with test error less than 1%. Third, as a consequence, SHAMPO performs equalization, giving the harder problems more labeled data, and as a consequence, reduces the error of these problems, however, is not increasing the error of the easier problems which gets less queries (in fact it reduces the test error of all 45 problems!). The tradeoff mechanism of SHAMPO, reduces the test error of each problem\nby more than 40% compared to full exploration. Fourth, exploits performs similar equalization, yet in some hard tasks it performs worse than SHAMPO. This could be because it overfits the training data, by focusing on hard-examples too much, as SHAMPO has a randomness mechanism.\nIndeed, Table 1 shows that aggressive SHAMPO outperforms better alternatives. Yet, we claim that a good prior may improve results. We compute prior over the 45 USPS tasks, by running the perceptron algorithm on 1000 examples and computing the number of mistakes. We set the prior to be proportional to this number. We then reran aggressive SHAMPO with prior, comparing it to aggressive SHAMPO with no prior (i.e. ai = 1). Aggressive SHAMO with prior achieves average error of 1.47 (vs. 2.73 with no prior) on 1-vs-1 USPS and 4.97 (vs 4.93) on one-vs-rest USPS, with score rank of 1.0 (vs 2.9) and 1.7 (vs 2.0) respectively. Fig. 3(c) shows the test error for a all values of b we evaluated. A good prior is shown to outperform the case ai = 1 for all values of b.\nReduction of Multi-task to Contextual Bandits Next, we evaluated SHAMPO as a contextual bandit algorithm, by breaking a multi-class problem into few binary tasks, and integrating their output into a single multi-class problem. We focus on the VJ data, as there are many examples, and linear models perform relatively well [18]. We implemented all three reductions mentioned in Sec. 5.2, namely, one-vs-rest, one-vs-one-random which picks a random label if the feedback is zero, one-vs-one-weak (which performs updates to increase confidence when the feedback is zero), where we set η = 0.2, and the Banditron algorithm [17]. The one-vs-rest reduction and the Banditron have a test error of about 43.5%, and the one-vs-one-random of about 42.5%. Finally, one-vs-oneweak achieves an error of 39.4%. This is slightly worst than PLM [18] with test error of 38.4% (and higher than MLP with 32.8%), yet all of these algorithms observe only one bit of feedback per example, while both MLP and PLM observe 3 bits (as class identity can be coded with 3 bits for 8 classes). We claim that our setting can be easily used to adapt a system to individual user, as we only need to assume the ability to recognise three words, such as three letters. Given an utterance of the user, the system may ask: “Did you say (a) ’a’ like ’bad’ (b) ’o’ like in ’book’) (c) none”. The user can communicate the correct answer with no need for a another person to key in the answer."
    }, {
      "heading" : "7 Related Work and Conclusion",
      "text" : "In the past few years there is a large volume of work on multi-task learning, which clearly we can not cover here. The reader is referred to a recent survey on the topic [20]. Most of this work is focused on exploring relations between tasks, that is, find similarities dissimilarities between tasks, and use it to share data directly (e.g. [10]) or model parameters [14, 11, 2]. In the online settings there are only a handful of work on multi-task learning. Dekel et al [13] consider the setting where all algorithms are evaluated using a global loss function, and all work towards the shared goal of minimizing it. Logosi et al [19] assume that there are constraints on the predictions of all learners, and focus in the expert setting. Agarwal et al [1] formalize the problem in the framework of stochastic convex programming with few matrix regularization, each captures some assumption about the relation between the models. Cavallanti et al [4] and Cesa-Bianci et al [6] assume a known relation between tasks which is exploited during learning. Unlike these approaches, we assume the ability to share an annotator rather than data or parameters, thus our methods can be applied to problems that do not share a common input space.\nOur analysis is similar to that of Cesa-Bianchi et al [7], yet they focus in selective sampling (see also [5, 12]), that is, making individual binary decisions of whether to query, while our algorithm always query, and needs to decide for which task. Finally, there have been recent work in contextual bandits [17, 16, 9], each with slightly different assumptions. To the best of our knowledge, we are the first to consider decoupled exploration and exploitation in this context. Finally, there is recent work in learning with relative or preference feedback in various settings [24, 25, 26, 21]. Unlike this work, our work allows again decoupled exploitation and exploration, and also non-relevant feedback.\nTo conclude, we proposed a new framework for online multi-task learning, where learners share a single annotator. We presented an algorithm (SHAMPO) that works in this settings and analyzed it in the mistake-bound model. We also showed how learning in such a model can be used to learn in contextual-bandits setting with few types of feedback. Empirical results show that our algorithm does better for the same price. It focuses the annotator on the harder instances, and is improving performance in various tasks and settings. We plan to integrate other algorithms to our framework, extend it to other settings, investigate ways to generate good priors, and reduce multi-class to binary also via error-correcting output-codes."
    } ],
    "references" : [ {
      "title" : "Matrix regularization techniques for online multitask learning",
      "author" : [ "Alekh Agarwal", "Alexander Rakhlin", "Peter Bartlett" ],
      "venue" : "Technical Report UCB/EECS-2008-138, EECS Department,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Convex multi-task feature learning",
      "author" : [ "Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Decoupling exploration and exploitation in multi-armed bandits",
      "author" : [ "Orly Avner", "Shie Mannor", "Ohad Shamir" ],
      "venue" : "In ICML,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Linear algorithms for online multitask classification",
      "author" : [ "Giovanni Cavallanti", "Nicolò Cesa-Bianchi", "Claudio Gentile" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Robust bounds for classification via selective sampling",
      "author" : [ "Nicolo Cesa-Bianchi", "Claudio Gentile", "Francesco Orabona" ],
      "venue" : "In ICML",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Incremental algorithms for hierarchical classification",
      "author" : [ "Nicolò Cesa-Bianchi", "Claudio Gentile", "Luca Zaniboni" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Worst-case analysis of selective sampling for linear classification",
      "author" : [ "Nicolò Cesa-Bianchi", "Claudio Gentile", "Luca Zaniboni" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Confidence-weighted linear classification for text categorization",
      "author" : [ "Koby Crammer", "Mark Dredze", "Fernando Pereira" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Multiclass classification with bandit feedback using adaptive regularization",
      "author" : [ "Koby Crammer", "Claudio Gentile" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Learning multiple tasks using shared hypotheses",
      "author" : [ "Koby Crammer", "Yishay Mansour" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Frustratingly easy semi-supervised domain adaptation",
      "author" : [ "III Hal Daumé", "Abhishek Kumar", "Avishek Saha" ],
      "venue" : "DANLP",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Robust selective sampling from single and multiple teachers",
      "author" : [ "Ofer Dekel", "Claudio Gentile", "Karthik Sridharan" ],
      "venue" : "In COLT,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Online multitask learning",
      "author" : [ "Ofer Dekel", "Philip M. Long", "Yoram Singer" ],
      "venue" : "In COLT,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    }, {
      "title" : "Regularized multi–task learning",
      "author" : [ "Theodoros Evgeniou", "Massimiliano Pontil" ],
      "venue" : "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "Round robin classification",
      "author" : [ "Johannes Fürnkranz" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2002
    }, {
      "title" : "Newtron: an efficient bandit algorithm for online multiclass prediction",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Efficient bandit algorithms for online multiclass prediction",
      "author" : [ "Sham M Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "How to lose confidence: Probabilistic linear machines for multiclass classification",
      "author" : [ "Hui Lin", "Jeff Bilmes", "Koby Crammer" ],
      "venue" : "In Tenth Annual Conference of the International Speech Communication Association,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Online multi-task learning with hard constraints",
      "author" : [ "Gábor Lugosi", "Omiros Papaspiliopoulos", "Gilles Stoltz" ],
      "venue" : "In COLT,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Sinno Jialin Pan", "Qiang Yang" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Online learning with preference",
      "author" : [ "Pannagadatta K. Shivaswamy", "Thorsten Joachims" ],
      "venue" : "feedback. CoRR,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Support vector machine active learning with application sto text classification",
      "author" : [ "Simon Tong", "Daphne Koller" ],
      "venue" : "In ICML,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2000
    }, {
      "title" : "Piecewise-stationary bandit problems with side observations",
      "author" : [ "Jia Yuan Yu", "Shie Mannor" ],
      "venue" : "In ICML,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "The k-armed dueling bandits problem",
      "author" : [ "Yisong Yue", "Josef Broder", "Robert Kleinberg", "Thorsten Joachims" ],
      "venue" : "In COLT,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    }, {
      "title" : "The k-armed dueling bandits problem",
      "author" : [ "Yisong Yue", "Josef Broder", "Robert Kleinberg", "Thorsten Joachims" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Beat the mean bandit",
      "author" : [ "Yisong Yue", "Thorsten Joachims" ],
      "venue" : "In ICML, pages 241–248,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "was used by Tong and Koller [22] for picking an example to be labeled in batch active learning.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 22,
      "context" : "Although our algorithm is designed for many binary-classification tasks, it can also be applied in two settings of contextual bandits, when decoupling exploration and exploitation is allowed [23, 3].",
      "startOffset" : 191,
      "endOffset" : 198
    }, {
      "referenceID" : 2,
      "context" : "Although our algorithm is designed for many binary-classification tasks, it can also be applied in two settings of contextual bandits, when decoupling exploration and exploitation is allowed [23, 3].",
      "startOffset" : 191,
      "endOffset" : 198
    }, {
      "referenceID" : 6,
      "context" : "Similar issue appears also after the discussion of Theorem 1 in a different context [7].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "Ignoring this decoupling, the Banditron algorithm [17] is the closest to ours, with a regret of O(T ).",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : "Hazan et al [16] proposed an algorithm withO( √ T ) regret but designed for the log loss, with coefficient that may be very large, and another [9] algorithm has O( √ T ) regret with respect to prediction mistakes, yet they assumed stochastic labeling, rather than adversarial.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "Hazan et al [16] proposed an algorithm withO( √ T ) regret but designed for the log loss, with coefficient that may be very large, and another [9] algorithm has O( √ T ) regret with respect to prediction mistakes, yet they assumed stochastic labeling, rather than adversarial.",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : "Given the output of all (binary) classifiers the algorithm generates a single multi-class prediction using a tournament in a round-robin approach [15].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "Details of the individual binary tasks can be found elsewhere [8].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "We focus on the VJ data, as there are many examples, and linear models perform relatively well [18].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 17,
      "context" : "This is slightly worst than PLM [18] with test error of 38.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "The reader is referred to a recent survey on the topic [20].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "Dekel et al [13] consider the setting where all algorithms are evaluated using a global loss function, and all work towards the shared goal of minimizing it.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : "Logosi et al [19] assume that there are constraints on the predictions of all learners, and focus in the expert setting.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "Agarwal et al [1] formalize the problem in the framework of stochastic convex programming with few matrix regularization, each captures some assumption about the relation between the models.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "Cavallanti et al [4] and Cesa-Bianci et al [6] assume a known relation between tasks which is exploited during learning.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "Cavallanti et al [4] and Cesa-Bianci et al [6] assume a known relation between tasks which is exploited during learning.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "Our analysis is similar to that of Cesa-Bianchi et al [7], yet they focus in selective sampling (see also [5, 12]), that is, making individual binary decisions of whether to query, while our algorithm always query, and needs to decide for which task.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "Our analysis is similar to that of Cesa-Bianchi et al [7], yet they focus in selective sampling (see also [5, 12]), that is, making individual binary decisions of whether to query, while our algorithm always query, and needs to decide for which task.",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : "Our analysis is similar to that of Cesa-Bianchi et al [7], yet they focus in selective sampling (see also [5, 12]), that is, making individual binary decisions of whether to query, while our algorithm always query, and needs to decide for which task.",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "Finally, there have been recent work in contextual bandits [17, 16, 9], each with slightly different assumptions.",
      "startOffset" : 59,
      "endOffset" : 70
    }, {
      "referenceID" : 15,
      "context" : "Finally, there have been recent work in contextual bandits [17, 16, 9], each with slightly different assumptions.",
      "startOffset" : 59,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "Finally, there have been recent work in contextual bandits [17, 16, 9], each with slightly different assumptions.",
      "startOffset" : 59,
      "endOffset" : 70
    }, {
      "referenceID" : 23,
      "context" : "Finally, there is recent work in learning with relative or preference feedback in various settings [24, 25, 26, 21].",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 24,
      "context" : "Finally, there is recent work in learning with relative or preference feedback in various settings [24, 25, 26, 21].",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 25,
      "context" : "Finally, there is recent work in learning with relative or preference feedback in various settings [24, 25, 26, 21].",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "Finally, there is recent work in learning with relative or preference feedback in various settings [24, 25, 26, 21].",
      "startOffset" : 99,
      "endOffset" : 115
    } ],
    "year" : 2014,
    "abstractText" : "We introduce a new multi-task framework, in which K online learners are sharing a single annotator with limited bandwidth. On each round, each of the K learners receives an input, and makes a prediction about the label of that input. Then, a shared (stochastic) mechanism decides which of the K inputs will be annotated. The learner that receives the feedback (label) may update its prediction rule, and then we proceed to the next round. We develop an online algorithm for multitask binary classification that learns in this setting, and bound its performance in the worst-case setting. Additionally, we show that our algorithm can be used to solve two bandits problems: contextual bandits, and dueling bandits with context, both allow to decouple exploration and exploitation. Empirical study with OCR data, vowel prediction (VJ project) and document classification, shows that our algorithm outperforms other algorithms, one of which uses uniform allocation, and essentially achieves more (accuracy) for the same labour of the annotator.",
    "creator" : null
  }
}