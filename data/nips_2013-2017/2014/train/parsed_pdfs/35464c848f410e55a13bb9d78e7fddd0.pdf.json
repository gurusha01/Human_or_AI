{
  "name" : "35464c848f410e55a13bb9d78e7fddd0.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning with Fredholm Kernels",
    "authors" : [ "Qichao Que", "Mikhail Belkin", "Yusu Wang" ],
    "emails" : [ "que@cse.ohio-state.edu", "mbelkin@cse.ohio-state.edu", "yusu@cse.ohio-state.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized Fredholm integral equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call Fredholm kernels. We proceed to discuss the “noise assumption” for semi-supervised learning and provide both theoretical and experimental evidence that Fredholm kernels can effectively utilize unlabeled data under the noise assumption. We demonstrate that methods based on Fredholm learning show very competitive performance in the standard semi-supervised learning setting."
    }, {
      "heading" : "1 Introduction",
      "text" : "Kernel methods and methods based on integral operators have become one of the central areas of machine learning and learning theory. These methods combine rich mathematical foundations with strong empirical performance. In this paper we propose a framework for supervised and unsupervised learning as an inverse problem based on solving the integral equation known as the Fredholm problem of the first kind. We develop regularization based algorithms for solving these systems leading to what we call Fredholm kernels.\nIn the basic setting of supervised learning we are given the data set (xi, yi), where xi ∈ X, yi ∈ R. We would like to construct a function f : X → R, such that f(xi) ≈ yi and f is “nice enough” to generalize to new data points. This is typically done by choosing f from a class of functions (a Reproducing Kernel Hilbert Space (RKHS) corresponding to a positive definite kernel for the kernel methods) and optimizing a certain loss function, such as the square loss or hinge loss.\nIn this paper we formulate a new framework for learning based on interpreting the learning problem as a Fredholm integral equation. This formulation shares some similarities with the usual kernel learning framework but unlike the standard methods also allows for easy incorporation of unlabeled data. We also show how to interpret the resulting algorithm as a standard kernel method with a non-standard data-dependent kernel (somewhat resembling the approach taken in [13]).\nWe discuss reasons why incorporation of unlabeled data may be desirable, concentrating in particular on what may be termed “the noise assumption” for semi-supervised learning, which is related but distint from manifold and cluster assumption popular in the semi-supervised learning literature. We provide both theoretical and empirical results showing that the Fredholm formulation allows for efficient denoising of classifiers.\nTo summarize, the main contributions of the paper are as follows: (1) We formulate a new framework based on solving a regularized Fredholm equation. The framework naturally combines labeled and unlabeled data. We show how this framework can be expressed as a kernel method with a non-standard data-dependent kernel.\n(2) We discuss “the noise assumption” in semi-supervised learning and provide some theoretical evidence that Fredholm kernels are able to improve performance of classifiers under this assumption. More specifically, we analyze the behavior of several versions of Fredholm kernels, based on combining linear and Gaussian kernels. We demonstrate that for some models of the noise assumption, Fredholm kernel provides better estimators than the traditional data-independent kernel and thus unlabeled data provably improves inference.\n(3) We show that Fredholm kernels perform well on synthetic examples designed to illustrate the noise assumption as well as on a number of real-world datasets.\nRelated work. Kernel and integral methods in machine learning have a large and diverse literature (e.g., [12, 11]). The work most directly related to our approach is [10], where Fredholm integral equations were introduced to address the problem of density ratio estimation and covariate shift. In that work the problem of density ratio estimation was expressed as a Fredholm integral equation and solved using regularization in RKHS. This setting also relates to a line of work on on kernel mean embedding where data points are embedded in Reproducing Kernel Hilbert Spaces using integral operators with applications to density ratio estimation and other tasks [5, 6, 7]. A very interesting recent work [9] explores a shrinkage estimator for estimating means in RKHS, following the SteinJames estimator originally used for estimating the mean in an Euclidean space. The results obtained in [9] show how such estimators can reduce variance. There is some similarity between that work and our theoretical results presented in Section 4 which also show variance reduction for certain estimators of the kernel although in a different setting. Another line of related work is the class of semi-supervised learning techniques (see [15, 2] for a comprehensive overview) related to manifold regularization [1], where an additional graph Laplacian regularizer is added to take advantage of the geometric/manifold structure of the data. Our reformulation of Fredholm learning as a kernel, addressing what we called “noise assumptions”, parallels data-dependent kernels for manifold regularization proposed in [13]."
    }, {
      "heading" : "2 Fredholm Kernels",
      "text" : "We start by formulating learning framework proposed in this paper. Suppose we are given l labeled pairs (x1, y1), . . . , (xl, yl) from the data distribution p(x, y) defined on X × Y and u unlabeled points xl+1, . . . , xl+u from the marginal distribution pX(x) on X . For simplicity we will assume that the feature space X is a Euclidean space RD, and the label set Y is either {−1, 1} for binary classification or the real line R for regression. Semi-supervised learning algorithms aim to construct a (predictor) function f : X → Y by incorporating the information of unlabeled data distribution. To this end, we introduce the integral operator KpX associated with a kernel function k(x, z). In our setting k(x, z) does not have to be a positive semi-definite (or even symmetric) kernel.\nKpX : L2 → L2 and KpXf(x) = ∫ k(x, z)f(z)pX(z)dz, (1)\nwhere L2 is the space of square-integrable functions. By the law of large numbers, the above operator can be approximated using unlabeled data from pX as\nKp̂Xf(x) = 1\nl + u l+u∑ i=1 k(x, xi)f(xi).\nThis approximation provides a natural way of incorporating unlabeled data into algorithms. In our Fredholm learning framework, we will use functions in KpXH = {KpXf : f ∈ H}, where H is an appropriate Reproducing Kernel Hilbert Space (RKHS) as classification or regression functions. Note that unlike RKHS, this space of functions, KpXH, is density dependent. In particular, this now allows us to formulate the following optimization problem for semi-supervised classification/regression in a way similar to many supervised learning algorithms: The Fredholm learning framework solves the following optimization problem1:\nf∗ = arg min f∈H\n1\nl l∑ i=1 ((Kp̂Xf)(xi)− yi)2 + λ‖f‖2H, (2)\n1We will be using the square loss to simplify the exposition. Other loss functions can also be used in Eqn 2.\nThe final classifier is c(x) = (Kp̂Xf∗) (x), where Kp̂X is the operator defined above. Eqn 2 is a discretized and regularized version of the Fredholm integral equation KpXf = y, thus giving the name of Fredholm learning framework.\nEven though at a first glance this setting looks similar to conventional kernel methods, the extra layer introduced by Kp̂X makes significant difference, in particular, by allowing the integration of information from unlabeled data distribution. In contrast, solutions to standard kernel methods for most kernels, e.g., linear, polynomial or Gaussian kernels, are completely independent of the unlabeled data. We note that our approach is closely related to [10] where a Fredholm equation is used to estimated the density ratio for two probability distributions.\nThe Fredholm learning framework is a generalization of the standard kernel framework. In fact, if the kernel k is the δ-function, then our formulation above is equivalent to the Regularized Kernel Least Squares equation f∗ = arg minf∈H 1l ∑l i=1(f(xi) − yi)2 + λ‖f‖2H. We could also replace the L2 loss in Eqn 2 by other loss functions, such as hinge loss, resulting in a SVM-like classifier.\nFinally, even though Eqn 2 is an optimization problem in a potentially infinite dimensional function space H, a standard derivation, using the Representer Theorem (See full version for details), yields a computationally accessible solution as follows:\nf∗(x) = 1\nl + u l+u∑ j=1 kH(x, xj)vj , v = ( KTl+uKl+uKH + λI )−1 KTl+uy, (3)\nwhere (Kl+u)ij = k(xi, xj) for 1 ≤ i ≤ l, 1 ≤ j ≤ l + u, and (KH)ij = kH(xi, xj) for 1 ≤ i, j ≤ l + u. Note that Kl+u is a l × (l + u) matrix. Fredholm kernels: a convenient reformulation. In fact we will see that Fredholm learning problem induces a new data-dependent kernel, which we will refer to as Fredholm kernel2. To show this connection, we use the following identity, which can be easily verified:(\nKTl+uKl+uKH + λI )−1 KTl+u = K T l+u ( Kl+uKHK T l+u + λI )−1 .\nDefine KF = Kl+uKHKTl+u to be the l × l kernel matrix associated with a new kernel defined by\nk̂F (x, z) = 1\n(l + u)2 l+u∑ i,j=1 k(x, xi)kH(xi, xj)k(z, xj), (4)\nand we consider the unlabeled data are fixed for computing this new kernel. Using this new kernel k̂F , the final classifying function from Eqn 3 can be rewritten as:\nc∗(x) = 1\nl + u l+u∑ i=1 k(x, xi)f ∗(xi) = l∑ s=1 k̂F (x, xs)αs, α = (KF + λI) −1 y.\nBecause of Eqn 4 we will sometimes refer to the kernels kH and k as the “inner” and “outer” kernels respectively. It can be observed that this solution is equivalent to a standard kernel method, but using a new data dependent kernel k̂F , which we will call the Fredholm kernel, since it is induced from the Fredholm problem formulated in Eqn 2.\nProposition 1. The Fredholm kernel defined in Eqn 4 is positive semi-definite as long as KH is positive semi-definite for any set of data x1, . . . , xl+u.\nThe proof is given in the full version. The “outer” kernel k does not have to be either positive definite or even symmetric. When using Gaussian kernel for k, discrete approximation in Eqn 4 might be unstable when the kernel width is small, so we also introduce the normalized Fredholm kernel,\nk̂NF (x, z) = l+u∑ i,j=1 k(x, xi)∑ n k(x, xn) kH(xi, xj) k(z, xj)∑ n k(z, xn) . (5)\nIt is easy to check that the resulting Fredholm kernel k̂NF is still symmetric positive semi-definite. Even though Fredholm kernel was derived using L2 loss here, it could also be derived when hinge loss is used, which will be explained in full version.\n2 We note that the term Fredholm Kernel has been used in mathematics ([8], page 103) and also in a different learning context [14]. Our usage represents a different object."
    }, {
      "heading" : "3 The Noise Assumption and Semi-supervised Learning",
      "text" : "In order for unlabeled data to be useful in classification tasks it is necessary for the marginal distribution of the unlabeled data to contain information about the conditional distribution of the labels. Several ways in which such information can be encoded has been proposed including the “cluster assumption” [3] and the “manifold assumption” [1]. The cluster assumption states that a cluster (or a high density area) contains only (or mostly) points belonging to the same class. That is, if x1 and x2 belong to the same cluster, the corresponding labels y1, y2 should be the same. The manifold assumption assumes that the regression function is smooth with respect to the underlying manifold structure of the data, which can be interpreted as saying that the geodesic distance should be used instead of the ambient distance for optimal classification. The success of algorithms based on these ideas indicates that these assumptions do capture certain characteristics of real data. Still, better understanding of unlabeled data may still lead to progress in data analysis.\nThe noise assumption. We propose to formulate a new assumption, the “noise assumption”, which is that in the neighborhood of every point, the directions with low variance (for the unlabeled data) are uninformative with respect to the class labels, and can be regarded as noise. While intuitive, as far as we know, it has not been explicitly formulated in the context of semi-supervised learning algorithms, nor applied to theoretical analysis.\nNote that even if the noise variance is small along a single direction, it could still significantly decrease the performance of a supervised learning algorithm if the noise is high-dimensional. These accumulated non-informative variations in particular increase the difficulty of learning a good classifier when the amount of labeled data is small. The first figure on right illustrates the issue of noise with two labeled points. The seemingly optimal classification boundary (the red line) differs from the correct one (in black) due to the noisy variation along the y axis for the two labeled points. Intuitively unlabeled data shown in the right panel of Figure 1 can be helpful in this setting as low variance directions can be estimated locally such that algorithms could suppress the influences of the noisy variation when learning a classifier.\nConnection to cluster and manifold assumptions. The noise assumption is compatible with the manifold assumption within the manifold+noise model. Specifically, we can assume that the functions of interest vary along the manifold and are constant in the orthogonal direction. Alternatively, we can think of directions with high variance as “signal/manifold” and directions with low variance as “noise”. We note that the noise assumption does not require the data to conform to a low-dimensional manifold in the strict mathematical sense of the word. The noise assumption is orthogonal to the cluster assumption. For example, Figure 1 illustrates a situation where data has no clusters but the noise assumption applies."
    }, {
      "heading" : "4 Theoretical Results for Fredholm Kernels",
      "text" : "Non-informative variation in data could degrade traditional supervised learning algorithms. We will now show that Fredholm kernels can be used to replace traditional kernels to inject them with “noise-suppression” power with the help of unlabeled data. In this section we will present two views to illustrate how such noise suppression can be achieved. Specifically, in Section 4.1) we show that under certain setup, linear Fredholm kernel suppresses principal components with small variance. In Section 4.2) we prove that under certain conditions we are able to provide good approximations to the “true” kernel on the hidden underlying space.\nTo make our arguments more clear, we assume that there are infinite amount of unlabelled data; that is, we know the marginal distribution of data exactly. We will then consider the following continuous versions of the un-normalized and normalized Fredholm kernels as in Eqn 4 and 5:\nkUF (x, z) = ∫ ∫ k(x, u)kH(u, v)k(z, v)p(u)p(v)dudv (6)\nkNF (x, z) =\n∫ ∫ k(x, u)∫\nk(x,w)p(w)dw kH(u, v) k(z, v)∫ k(z, w)p(w)dw p(u)p(v)dudv. (7)\nNote, in the above equations and in what follows, we sometimes write p instead of pX for the marginal distribution when its choice is clear from context. We will typically use kF to denote appropriate normalized or unnormalized kernels depending on the context."
    }, {
      "heading" : "4.1 Linear Fredholm kernels and inner products",
      "text" : "For this section, we consider the unormalized Fredholm kernel, that is kF = kUF . If the “outer” kernel k(u, v) is linear, i.e. k(u, v) = 〈u, v〉, the resulting Fredholm kernel can be viewed as an inner product. Specifically, the un-normalized Fredholm kernel from Eqn 6 can be rewritten as:\nkF (x, z) = x TΣF z, where ΣF = ∫ ∫ ukH(u, v)v T p(u)p(v)dudv.\nThus kF (x, z) is simply an inner product which depends on both the unlabeled data distribution p(x) and the “inner” kernel kH. This inner product re-weights the standard norm in feature space based on variances along the principal directions of the matrix ΣF . We show that for the model when unlabeled data is sampled from a normal distribution this kernel can be viewed as a “soft thresholding” PCA, suppressing the directions with low variance. Specifically, we have the following3\nTheorem 2. Let kH(x, z) = exp ( −‖x−z‖ 2\n2t\n) and assume the distribution pX for unlabeled data is\na single multi-variate normal distribution, N(µ, diag(σ21 , . . . , σ 2 d)). We have\nΣF = ( D∏ d=1 √ t 2σ2d + t )( µµT + diag ( σ41 2σ21 + t , . . . , σ4D 2σ2D + t )) .\nAssuming that the data is mean-subtracted, i.e. µ = 0, we see that xTΣF z re-scales the projections along the principal components when computing the inner product; that is, the rescaling factor for the i-th principal direction is √\nσ4i 2σ2i+t .\nNote that this rescaling factor σ 4 i\n2σ2i+t ≈ 0 when σ2i t. On the other hand when σ2i t, we\nhave that σ 4 i 2σ2i+t ≈ σ 2 i\n2 . Hence t can be considered as a soft threshold that eliminates the effects of principal components with small variances. When t is small the rescaling factors are approximately proportional to diag(σ21 , σ 2 2 , . . . , σ 2 D), in which case ΣF is is proportional to the covariance matrix of the data XXT ."
    }, {
      "heading" : "4.2 Kernel Approximation With Noise",
      "text" : "We have seen that one special case of Fredholm kernel could achieve the effect of principal components re-scaling by using linear kernel as the “outer” kernel k. In this section we give a more general interpretation of noise suppression by the Fredholm kernel. First, we give a simple senario to provide some intuition behind the definition of Fredholm kernle. Consider a standard supervised learning setting which uses the solution f∗ = arg minf∈H 1 l ∑l i=1(f(xi)−yi)2+λ‖f‖2H as the classifier. Let ktargetH denote the ideal kernel that we intend to use on the clean data, which we call the target kernel from now on. Now suppose what we have are two noisy labelled points xe and ze for “true” data x̄ and z̄, i.e. xe = x̄ + εx, ze = z̄ + εz . The evaluation of ktargetH (xe, ze) can be quite different from the true signal ktargetH (x̄, z̄), leading to an suboptimal final classifier (the red line in Figure 1 (a)). On the other hand, now consider the Fredholm kernel from Eqn 6 (or similarly from Eqn 7): kF (xe, ze) = ∫ ∫ k(xe, u)p(u) · kH(u, v) · k(ze, v)p(v)dudv, and set the outer kernel k to be the Gaussian kernel, and the inner kernel kH to be the same as target kernel ktargetH . We can think of kF (xe, ze) as an averaging of kH(u, v) over all possible pairs of data u, v, weighted by k(xe, u)p(u) and k(ze, v)p(v) respectively. Specifically, points\n3The proof of this and other results can be found in the full version.\nthat are close to xe (resp. ze) with high density will receive larger weights. Hence the weighted averages will be biased towards x̄ and z̄ respectively (which presumably lie in high density regions around xe and ze). The value of kF (xe, ze) tends to provide a more accurate estimate of kH(x̄, z̄). See the right figure for an illustration where the arrows indicate points with stronger influences in the computation of kF (xe, ze) than kH(xe, ze). As a result, the classifier obtained using the Fredholm kernel will also be more resilient to noise and closer to the optimum.\nThe Fredholm learning framework is rather flexible in terms of the choices of kernels k and kH. In the remainder of this section, we will consider a few specific scenarios and provide quantitative analysis to show the noise robustness of the Fredholm kernel. Problem setup. Assume that we have a ground-truth distribution over the subspace spanned by the first d dimension of the Euclidean space RD. We will assume that this distribution is a single Gaussian N(0, λ2Id). Suppose this distribution is corrupted with Gaussian noise along the orthogonal subspace of dimension D − d. That is, for any “true” point x̄ drawn from N(0, λ2Id), its observation xe is drawn from N(x̄, σ2ID−d). Since the noise lies in a space orthogonal to data distribution, this means that any observed point, labelled or unlabeled, is sampled from pX = N(0, diag(λ 2Id, σ 2ID−d). We will show that Fredholm kernel provides a better approximation to the “original” kernel given unlabeled data than simply computing the kernel of noisy points. We choose this basic setting to be able to state the theoretical results in a clean manner. Even though this is a Gaussian distribution over a linear subspace with noise, this framework has more general implications since local neighborhoods of manifolds are (almost) linear spaces.\nNote: In this section we use normalized Fredholm kernel given in Eqn 7, that is kF = kNF for now on. Un-normalized Fredholm kernel displays similar behavior, while the bounds are trickier.\nLinear Kernel. First we consider the case where the target kernel ktargetH (u, v) is the linear kernel, ktargetH (u, v) = u T v. We will set kH in Fredholm kernel to also be linear, and k to be the Gaussian kernel k(u, v) = e− ‖u−v‖2\n2t We will compare kF (xe, ze) with the target kernel on the two observed points, that is, with ktargetH (xe, ze). The goal is to estimate k target H (x̄, z̄). We will see that (1) both kF (xe, ze) and (appropriately scaled) kH(xe, ze) are unbiased estimators of k target H (x̄, z̄), however (2) the variance of kF (xe, ze) is smaller than that of k target H (xe, ze), making it a more precise estimator. Theorem 3. Suppose the probability distribution for the unlabeled data pX = N(0, diag(λ2Id, σ2ID−d)). For Fredholm kernel defined in Eqn 7, we have\nExe,ze(k target H (xe, ze)) = Exe,ze\n(( t+ λ2\nλ2\n)2 kF (xe, ze) ) = x̄T z̄\nMoreover, when λ > σ, Varxe,ze\n(( t+λ2\nλ2\n)2 kF (xe, ze) ) < Varxe,ze(k target H (xe, ze)).\nRemark: Note that we have a normalization constant for the Fredholm kernel to make it an unbiased estimator of x̄T z̄. In practice, choosing normalization is subsumed in selecting the regularization parameter for kernel methods.\nThus we can see the Fredholm kernel provides an approximation of the “true” linear kernel, but with smaller variance compared to the actual linear kernel on noisy data.\nGaussian Kernel. We now consider the case where the target kernel is the Gaussian kernel: ktargetH (u, v) = exp ( −‖u−v‖ 2 2r ) . To approximate this kernel, we will set both k and kH to be Gaussian kernels. To simplify the presentation of results, we assume that k and kH have the same kernel width t. The resulting Fredholm kernel turns out to also be a Gaussian kernel, whose kernel width depends on the choice of t.\nOur main result is the following. Again, similar to the case of linear kernel, the Fredholm estimation kF (xe, ze) and k target H (xe, ze) are both unbiased estimator for the target k target H (x̄, z̄) up to a constant; but kF (xe, ze) has a smaller variance. Theorem 4. Suppose the probability distribution for the unlabeled data pX = N(0, diag(λ2Id, σ2ID−d)). Given the target kernel k target H (u, v) = exp ( −‖u−v‖ 2 2r ) with kernel width r > 0, we can choose t, given by the equation t(t+λ 2)(t+3λ2) λ4 = r, and two scaling\nconstants c1, c2, such that Exe,ze(c −1 1 k target H (xe, ze)) = Exe,ze(c −1 2 kF (xe, ze)) = k target H (x̄, z̄). and when λ > σ, we have Varxe,ze(c −1 1 k target H (xe, ze)) > Varxe,ze(c −1 2 kF (xe, ze)).\nRemark. In practice, when applying kernel methods for real world applications, optimal kernel width r is usually unknown and chosen by cross-validation or other methods. Similarly, for our Fredholm kernel, one can also use cross-validation to choose the optimal t for kF ."
    }, {
      "heading" : "5 Experiments",
      "text" : "Using linear and Gaussian kernel for k or kH respectively, we will define three instances of the Fredholm kernel as follows.\n(1) FredLin1: k(x, z) = xT z and kH(x, z) = exp ( −‖x−z‖ 2\n2r\n) .\n(2) FredLin2: k(x, z) = exp ( −‖x−z‖ 2\n2r\n) and kH(x, z) = xT z.\n(3) FredGauss: k(x, z) = kH(x, z) = exp ( −‖x−z‖ 2\n2r\n) .\nFor the kernels in (2) and (3) that use the Gaussian kernel as outside kernel k we can also define their normalized version, which we will denote by by FredLin2(N) and FredGauss(N) respectively.\nSynthetic examples. Noise and cluster assumptions.\nTo isolate the ability of Fredholm kernels to deal with noise from the cluster assumption, we construct two synthetic examples that violate the cluster assumption, shown in Figure 2. The figures show first two dimensions, with multi-variate Gaussian noise with variance σ2 = 0.01 in R100 added. The classification boundaries are indicated by the color. For each class, we provide several labeled points and large amount of unlabeled data. Note that the classification boundary in the “circle” example is non-linear.\nWe compare Fredholm kernel based classifier with RLSC (Regularized Least Squares Classifier), and two widely used semisupervised methods, the transductive support vector machine and LapRLSC. Since the examples violate the cluster assumption, the two existing semi-supervised learning algorithms, Transductive SVM and LapRLSC, should not gain much from the unlabeled data. For TSVM, we use the primal TSVM proposed in [4], and we will use the implementation of LapRLSC given in [1]. Different numbers of labeled points are given for each class, together with another\n2000 unlabeled points. To choose the optimal parameters for each method, we pick the parameters based on their performance on the validation set, while the final classification error is computed on the held-out testing data set. Results are reported in Table 1 and 2, in which Fredholm kernels show clear improvement over other methods for synthetic examples in term of classification error.\nReal-world Data Sets. Unlike artificial examples, it is usually difficult to verify whether certain assumptions are satisfied in real-world problems. In this section, we examine the performance of Fredholm kernels on several real-world data sets and compare it with the baseline algorithms mentioned above. Linear Kernels. Here we consider text categorization and sentiment analysis, where linear methods are known to perform well. We use the following data (represented by TF-IDF features): (1) 20 news group: it has 11269 documents with 20 classes, and we select the first 10 categories for our experiment. (2) Webkb: the original data set contains 7746 documents with 7 unbalanced classes, and we pick the two largest classes with 1511 and 1079 instances respectively. (3) IMDB movie review: it has 1000 positive reviews and 1000 negative reviews of movie on IMDB.com. (4) Twitter sentiment data from Sem-Eval 2013: it contains 5173 tweets, with positive, neural and negative sentiment. We combine neutral and negative classes to set up a binary classification problem. Results are reported in Table 3. In Table4, we use WebKB as an example to illustrate the change of the performance as number of labeled points increases.\nGaussian Kernel. We test our methods on hand-written digit recognition. The experiment use subsets of two handwriting digits data sets MNIST and USPS: (1) the one from MNIST contains 10k digits in total with balanced examples for each class, and the one for USPS is the original testing set containing about 2k images. The pixel values are normalized to [0, 1] as features. Results are reported in Table 5. In Table 6, we show that as we add additional Gaussian noise to MNIST data, Fredholm kernels start to show significant improvement.\nAcknowledgments. The work was partially supported by NSF Grants CCF-1319406 and RI 1117707. We thank the anonymous NIPS reviewers for insightful comments."
    } ],
    "references" : [ {
      "title" : "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples",
      "author" : [ "Mikhail Belkin", "Partha Niyogi", "Vikas Sindhwani" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Semi-Supervised Learning",
      "author" : [ "Oliver Chapelle", "Bernhard Schölkopf", "Alexander Zien", "editors" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Cluster kernels for semi-supervised learning",
      "author" : [ "Oliver Chapelle", "Jason Weston", "Bernhard Schölkopf" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Semi-supervised classification by low density separation",
      "author" : [ "Oliver Chapelle", "Alexander Zien" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Covariate shift by kernel mean matching",
      "author" : [ "Arthur Gretton", "Alex Smola", "Jiayuan Huang", "Marcel Schmittfull", "Karsten Borgwardt", "Bernhard Schölkopf" ],
      "venue" : "Dataset shift in machine learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Conditional mean embeddings as regressors",
      "author" : [ "S. Grünewälder", "G. Lever", "L. Baldassarre", "S. Patterson", "A. Gretton", "M. Pontil" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Smooth operators",
      "author" : [ "Steffen Grunewalder", "Gretton Arthur", "John Shawe-Taylor" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Kernel mean shrinkage estimators",
      "author" : [ "Krikamol Muandet", "Kenji Fukumizu", "Bharath Sriperumbudur", "Arthur Gretton", "Bernhard Schölkopf" ],
      "venue" : "arXiv preprint arXiv:1405.5505,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Inverse density as an inverse problem: the fredholm equation approach",
      "author" : [ "Qichao Que", "Mikhail Belkin" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Learning with kernels: Support vector machines, regularization, optimization, and beyond",
      "author" : [ "Bernhard Schölkopf", "Alexander J Smola" ],
      "venue" : "MIT press,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2001
    }, {
      "title" : "Kernel methods for pattern analysis",
      "author" : [ "John Shawe-Taylor", "Nello Cristianini" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "Beyond the point cloud: from transductive to semi-supervised learning",
      "author" : [ "Vikas Sindhwani", "Partha Niyogi", "Mikhail Belkin" ],
      "venue" : "In Proceedings of the 22nd International Conference on Machine Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "Binet-cauchy kernels on dynamical systems and its application to the analysis of dynamic scenes",
      "author" : [ "SVN Vishwanathan", "Alexander J Smola", "René Vidal" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Semi-supervised learning literature survey",
      "author" : [ "Xiaojin Zhu" ],
      "venue" : "Technical report, Computer Science, University of Wisconsin-Madison,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "We also show how to interpret the resulting algorithm as a standard kernel method with a non-standard data-dependent kernel (somewhat resembling the approach taken in [13]).",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 8,
      "context" : "The work most directly related to our approach is [10], where Fredholm integral equations were introduced to address the problem of density ratio estimation and covariate shift.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "This setting also relates to a line of work on on kernel mean embedding where data points are embedded in Reproducing Kernel Hilbert Spaces using integral operators with applications to density ratio estimation and other tasks [5, 6, 7].",
      "startOffset" : 227,
      "endOffset" : 236
    }, {
      "referenceID" : 5,
      "context" : "This setting also relates to a line of work on on kernel mean embedding where data points are embedded in Reproducing Kernel Hilbert Spaces using integral operators with applications to density ratio estimation and other tasks [5, 6, 7].",
      "startOffset" : 227,
      "endOffset" : 236
    }, {
      "referenceID" : 6,
      "context" : "This setting also relates to a line of work on on kernel mean embedding where data points are embedded in Reproducing Kernel Hilbert Spaces using integral operators with applications to density ratio estimation and other tasks [5, 6, 7].",
      "startOffset" : 227,
      "endOffset" : 236
    }, {
      "referenceID" : 7,
      "context" : "A very interesting recent work [9] explores a shrinkage estimator for estimating means in RKHS, following the SteinJames estimator originally used for estimating the mean in an Euclidean space.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 7,
      "context" : "The results obtained in [9] show how such estimators can reduce variance.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "Another line of related work is the class of semi-supervised learning techniques (see [15, 2] for a comprehensive overview) related to manifold regularization [1], where an additional graph Laplacian regularizer is added to take advantage of the geometric/manifold structure of the data.",
      "startOffset" : 86,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "Another line of related work is the class of semi-supervised learning techniques (see [15, 2] for a comprehensive overview) related to manifold regularization [1], where an additional graph Laplacian regularizer is added to take advantage of the geometric/manifold structure of the data.",
      "startOffset" : 86,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "Another line of related work is the class of semi-supervised learning techniques (see [15, 2] for a comprehensive overview) related to manifold regularization [1], where an additional graph Laplacian regularizer is added to take advantage of the geometric/manifold structure of the data.",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "Our reformulation of Fredholm learning as a kernel, addressing what we called “noise assumptions”, parallels data-dependent kernels for manifold regularization proposed in [13].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 8,
      "context" : "We note that our approach is closely related to [10] where a Fredholm equation is used to estimated the density ratio for two probability distributions.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "2 We note that the term Fredholm Kernel has been used in mathematics ([8], page 103) and also in a different learning context [14].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "Several ways in which such information can be encoded has been proposed including the “cluster assumption” [3] and the “manifold assumption” [1].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "Several ways in which such information can be encoded has been proposed including the “cluster assumption” [3] and the “manifold assumption” [1].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : "For TSVM, we use the primal TSVM proposed in [4], and we will use the implementation of LapRLSC given in [1].",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 0,
      "context" : "For TSVM, we use the primal TSVM proposed in [4], and we will use the implementation of LapRLSC given in [1].",
      "startOffset" : 105,
      "endOffset" : 108
    } ],
    "year" : 2014,
    "abstractText" : "In this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized Fredholm integral equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call Fredholm kernels. We proceed to discuss the “noise assumption” for semi-supervised learning and provide both theoretical and experimental evidence that Fredholm kernels can effectively utilize unlabeled data under the noise assumption. We demonstrate that methods based on Fredholm learning show very competitive performance in the standard semi-supervised learning setting.",
    "creator" : null
  }
}