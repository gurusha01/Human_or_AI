{
  "name" : "fa83a11a198d5a7f0bf77a1987bcd006.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Covariance shrinkage for autocorrelated data",
    "authors" : [ "Daniel Bartz", "Klaus-Robert Müller" ],
    "emails" : [ "daniel.bartz@tu-berlin.de", "klaus-robert.mueller@tu-berlin.de" ],
    "sections" : [ {
      "heading" : "1 Introduction and Motivation",
      "text" : "Covariance matrices are a key ingredient in many algorithms in signal processing, machine learning and statistics. The standard estimator, the sample covariance matrix S, has appealing properties in the limit of large sample sizes n: its entries are unbiased and consistent [HTF08]. On the other hand, for sample sizes of the order of the dimensionality p or even smaller, its entries have a high variance and the spectrum has a large systematic error. In particular, large eigenvalues are overestimated and small eigenvalues underestimated, the condition number is large and the matrix difficult to invert [MP67, ER05, BS10]. One way to counteract this issue is to shrink S towards a biased estimator T (the shrinkage target) with lower variance [Ste56],\nCsh := (1− λ)S + λT,\nthe default choice being T = p−1trace(S)I, the identity multiplied by the average eigenvalue. For the optimal shrinkage intensity λ?, a reduction of the expected mean squared error is guaranteed [LW04]. Model selection for λ can be done by cross-validation (CV) with the known drawbacks: for (i) problems with many hyperparameters, (ii) very high-dimensional data sets, or (iii) online settings which need fast responses, CV can become unfeasible and a faster model selection method is required. A popular alternative to CV is Ledoit and Wolf’s analytic shrinkage procedure [LW04] and more recent variants [CWEH10, BM13]. Analytic shrinkage directly estimates the shrinkage intensity which minimizes the expected mean squared error of the convex combination with a negligible computational cost, especially for applications which rely on expensive matrix inversions or eigendecompositions in high dimensions.\nAll of the above algorithms assume i.i.d. data. Real world time series, however, are often non-i.i.d. as they possess pronounced autocorrelation (AC). This makes covariance estimation in high dimensions even harder: the data dependence lowers the effective sample size available for constructing the estimator [TZ84]. Thus, stronger regularization λ will be needed. In Figure 1 the simple case of an autoregressive model serves as an example for an arbitrary generative model with autocorrelation.\nThe Figure shows, for three levels of autocorrelation (left), the population and sample eigenvalues (middle): with increasing autocorrelation the sample eigenvalues become more biased. This bias is an optimistic measure for the quality of the covariance estimator: it neglects that population and sample eigenbasis also differ [LW12]. Comparing sample eigenvalues to the population variance in the sample eigenbasis, the bias is even larger (right).\nIn practice, violations of the i.i.d. assumption are often ignored [LG11, SBMK13, GLL+14], although Sancetta proposed a consistent shrinkage estimator under autocorrelation [San08]. In this paper, we contribute by showing in theory, simulations and on real world data, that (i) ignoring autocorrelations for shrinkage leads to large estimation errors and (ii) for finite samples Sancetta’s estimator is still substantially biased and highly sensitive to the number of incorporated time lags. We propose a new bias-corrected estimator which (iii) outperforms standard shrinkage and Sancetta’s method under the presence of autocorrelation and (iv) is robust to the choice of the lag parameter."
    }, {
      "heading" : "2 Shrinkage for autocorrelated data",
      "text" : "Ledoit and Wolf derived a formula for the optimal shrinkage intensity [LW04, SS05]:\nλ? =\n∑ ij Var ( Sij )\n∑ ij E [( Sij − Tij )2] . (1) The analytic shrinkage estimator λ̂ is obtained by replacing expectations with sample estimates:\nVar ( Sij ) −→ V̂ar ( Sij ) = 1\nn2 n∑ s=1 ( xisxjs − 1 n n∑ t=1 xitxjt )2 (2)\nE [( Sij − Tij )2] −→ Ê [(Sij − Tij)2] = (Sij − Tij)2, (3) where xit is the tth observation of variable i. While the estimator eq. (3) is unbiased even under a violation of the i.i.d. assumption, the estimator eq. (2) is based on\nVar\n( 1\nn n∑ t=1 xitxjt\n) i.i.d. = 1\nn Var (xitxjt) .\nIf the data are autocorrelated, cross terms cannot be ignored and we obtain\nVar\n( 1\nn n∑ t=1 xitxjt\n) = 1\nn2 n∑ s,t=1 Cov(xitxjt, xisxjs)\n= 1\nn Cov(xitxjt, xitxjt) +\n2\nn n−1∑ s=1 n− s n Cov(xitxjt, xi,t+sxj,t+s)\n=: 1\nn Γij(0) +\n2\nn n−1∑ s=1 Γij(s) (4)\nFigure 2 illustrates the effect of ignoring the cross terms for increasing autocorrelation (larger ARcoefficients, see section 3 for details on the simulation). It compares standard shrinkage to an oracle shrinkage based on the population variance of the sample covariance1. The population variance of S\n1calculated by resampling.\nincreases because the effective sample size is reduced [TZ84], yet the standard shrinkage variance estimator eq. (2) does not increase (outer left). As a consequence, for oracle shrinkage the shrinkage intensity increases, for the standard shrinkage estimator it even decreases because the denominator in eq. (1) grows (middle left). With increasing autocorrelation, the sample covariance becomes a less precise estimator: for optimal (stronger) shrinkage more improvement becomes possible, yet standard shrinkage does not improve (middle right). Looking at the variance estimates in the sample eigendirections for AR-coefficients of 0.7, we see that the bias of standard shrinkage is only marginally smaller than the bias of the sample covariance, while oracle shrinkage yields a substantial bias reduction (outer right).\nSancetta-estimator An estimator for eq. (4) was proposed by [San08]:\nΓ̂Sanij (s) := 1\nn n−s∑ t=1 (xitxjt − Sij) (xi,t+sxj,t+s − Sij) , (5)\nV̂ar ( Sij )San,b := 1\nn\n( Γ̂Sanij (0) + 2 n−1∑ s=1 κ(s/b)Γ̂Sanij (s) ) , b > 0,\nwhere κ is a kernel which has to fulfill Assumption B in [And91]. We will restrict our analysis to the truncated kernel κTR(x) = {1 for |x| ≤ 1, 0 otherwise} to obtain less cluttered formulas2. The kernel parameter b describes how many time lags are taken into account.\nThe Sancetta estimator behaves well in the high dimensional limit: the main theoretical result states that for (i) a fixed decay of the autocorrelation, (ii) b, n → ∞ and (iii) b2 increasing at a lower rate than n, the estimator is consistent independently of the rate of p (for details, see [San08]). This is in line with the results in [LW04, CWEH10, BM13]: as long as n increases, all of these shrinkage estimators are consistent.\nBias of the Sancetta-estimator In the following we will show that the Sancetta-estimator is suboptimal in finite samples: it has a non-negligible bias. To understand this, consider a lag s large enough to have Γij(s) ≈ 0. If we approximate the expectation of the Sancetta-estimator, we see that it is biased downwards:\nE [ Γ̂Sanij (s) ] ≈ E\n[ 1\nn n−s∑ t=1 ( xitxjtxi,t+sxj,t+s − S2ij\n)] .\n≈ n− s n\n( E2 [Sij ]− E [ S2ij ])\n= −n− s n Var (Sij) < 0.\nBias-corrected (BC) estimator We propose a bias-corrected estimator for the variance of the entries in the sample covariance matrix:\nΓ̂BCij (s) := 1\nn n−s∑ t=1 ( xitxjtxi,t+sxj,t+s − S2ij ) , (6)\nV̂ar ( Sij )BC,b := 1\nn− 1− 2b+ b(b+ 1)/n\n( Γ̂BCij (0) + 2 n−1∑ s=1 κTR(s/b)Γ̂ BC ij (s) ) , b > 0.\n2in his simulations, Sancetta uses the Bartlett kernel. For fixed b, this increases the truncation bias.\nThe estimator Γ̂BCij (s) is very similar to Γ̂ San ij (s), but slightly easier to compute. The main difference is the denominator in V̂ar ( Sij )BC,b : it is smaller than n and thus corrects the downwards bias."
    }, {
      "heading" : "2.1 Theoretical results",
      "text" : "It is straightforward to extend the theoretical results on the Sancetta estimator ([San08], see summary above) to our proposed estimator. In the following, to better understand the limitations of the Sancetta estimator, we will provide a complementary theoretical analysis on the behaviour of the estimator for finite n.\nOur theoretical results are based on the analysis of a sequence of statistical models indexed by p. Xp denotes a p × n matrix of n observations of p variables with mean zero and covariance matrix Cp. Yp = R > p Xp denotes the same observations rotated in their eigenbasis, having diagonal covariance Λp = R > p CpRp. Lower case letters x p it and y p it denote the entries of Xp and Yp, respectively\n3. The analysis is based on the following assumptions: Assumption 1 (A1, bound on average eighth moment). There exists a constant K1 independent of p such that\n1\np p∑ i=1 E[(xpi1) 8] ≤ K1.\nAssumption 2 (A2, uncorrelatedness of higher moments). Let Q denote the set of quadruples {i,j,k,l} of distinct integers.∑\ni,j,kl,l∈Qp Cov 2[ypi1y p j1, y p k,1+sy p l,1+s]\n|Qp| = O\n( p−1 ) ,\nand\n∀s :\n∑ i,j,kl,l∈Qp Cov [ (ypi1y p j1) 2, (ypk,1+sy p l,1+s) 2 ]\n|Qp| = O\n( p−1 ) ,\nhold. Assumption 3 (A3, non-degeneracy). There exists a constant K2 such that\n1\np p∑ i=1 E[(xpi1) 2] ≥ K2.\nAssumption 4 (A4, moment relation). There exist constants α4, α8, β4 and β8 such that\nE[y8i ] ≤ (1 + α8)E2[y4i ], E[y4i ] ≤ (1 + α4)E2[y2i ], E[y8i ] ≥ (1 + β8)E2[y4i ], E[y4i ] ≥ (1 + β4)E2[y2i ].\nRemarks on the assumptions A restriction on the eighth moment (assumption A1) is necessary because the estimators eq. (2), (3), (5) and (6) contain fourth moments, their variances therefore contain eighths moments. Note that, contrary to the similar assumption in the eigenbasis in [LW04], A1 poses no restriction on the covariance structure [BM13]. To quantify the effect of averaging over dimensions, assumption A2 restricts the correlations of higher moments in the eigenbasis. This assumption is trivially fulfilled for Gaussian data, but much weaker (see [LW04]). Assumption A3 rules out the degenerate case of adding observation channels without any variance and assumption A4 excludes distributions with arbitrarily heavy tails.\nBased on these assumptions, we can analyse the difference between the Sancetta-estimator and our proposed estimator for large p: Theorem 1 (consistency under “fixed n”-asympotics). Let A1, A2, A3, A4 hold. We then have\n1\np2 ∑ ij Var (Sij) = Θ(1)\n3We shall often drop the sequence index p and the observation index t to improve readability of formulas.\nE ∥∥∥∥∥∥ 1p2 ∑ ij ( V̂ar San,b (Sij)−Var (Sij) )∥∥∥∥∥∥ 2 = ( BiasSan,b + BiasSan,bTR )2 +O ( ∑ j γ 2 j ( ∑ j γj) 2 )\nE ∥∥∥∥∥∥ 1p2 ∑ ij ( V̂ar BC,b (Sij)−Var (Sij) )∥∥∥∥∥∥ 2 = ( BiasBC,bTR )2 +O ( ∑ j γ 2 j ( ∑ j γj) 2 )\nwhere the γi denote the eigenvalues of C and\nBiasSan,b := − 1 p2 ∑ ij\n{ 1 + 2b− b(b+ 1)/n\nn Var (Sij)−\n4\nn3 b∑ s=1 n∑ t=n−s n∑ u=1 Cov [xitxjt, xiuxju]\n}\nBiasSan,bTR := − 1 p2 2 n ∑ ij n∑ s=b+1 n− s n Cov [xitxjt, xi,t+sxj,t+s]\nBiasBC,bTR := − 1 p2 2\nn− 1− 2b+ b(b+1)n ∑ ij n−1∑ s=b+1 Cov [xitxjt, xi,t+sxj,t+s]\nProof. see the supplemental material.\nRemarks on Theorem 1 (i) The mean squared error of both estimators consists of a bias and a variance term. Both estimators have a truncation bias which is a consequence of including only a limited number of time lags into the variance estimation. When b is chosen sufficiently high, this term gets close to zero. (ii) The Sancetta-estimator has an additional bias term which is smaller than zero in each dimension and therefore does not average out. Simulations will show that, as a consequence, the Sancetta-estimator has a strong bias which gets larger with increasing lag parameter b. (iii) The variance of both estimators behaves as O( ∑ i γ 2 i / ( ∑ i γi) 2 ): the more the variance of the data is spread over the eigendirections, the smaller the variance of the estimators. This bound is minimal if the eigenvalues are identical. (iv) Theorem 1 does not make a statement on the relative sizes of the variances of the estimators. Note that the BC estimator mainly differs by a multiplicative factor > 1, hence the variance is larger, but not relative to the expectation of the estimator."
    }, {
      "heading" : "3 Simulations",
      "text" : "Our simulations are based on those in [San08]: We average over R = 50 multivariate Gaussian AR(1) models\n~xt = A~xt−1 + ~ t,\nwith parameter matrix4 A = ψAC · I , with ψno AC = 0, ψlow AC = 0.7, and ψhigh AC = 0.95 (see Figure 1). The innovations it are Gaussian with variances σ2i drawn from a log-normal distribution\n4more complex parameter matrices or a different generative model do not pose a problem for the biascorrected estimator. The simple model was chosen for clarity of presentation.\nwith mean µ = 1 and scale parameter σ = 0.5. For each model, we generate K = 50 data sets to calculate the std. deviations of the estimators and to obtain an approximation of p−2 ∑ ij Var (Sij).\nSimulation 1 analyses the dependence of the estimators on the dimensionality of the data. The number of observations is fixed at n = 250 and the lag parameter b chosen by hand such that the whole autocorrelation is covered5: bno AC = 10, blow AC = 20 and bhigh AC = 90. Figure 3 shows that the standard shrinkage estimator is unbiased and has low variance in the no AC-setting, but under the presence of autocorrelation it strongly underestimates the variance. As predicted by Theorem 1, the Sancetta estimator is also biased; its remains stays constant for increasing dimensionality. Our proposed estimator has no visible bias. For increasing dimensionality the variances of all estimators decrease. Relative to the average estimate, there is no visible difference between the standard deviations of the Sancetta and the BC estimator.\nSimulation 2 analyses the dependency on the lag parameter b for fixed dimensionality p = 200 and number of observations n = 250. In addition to variance estimates, Figure 4 reports shrinkage intensities and the percentage improvement in absolute loss (PRIAL) over the sample covariance matrix:\nPRIAL ( C{pop., shr, San., BC} ) =\nE‖S−C‖ − E‖C{pop., shr, San., BC} −C‖ E‖S−C‖ .\nThe three quantities show very similar behaviour. Standard shrinkage performs well in the no ACcase, but is strongly biased in the autocorrelated settings. The Sancetta estimator is very sensitive to the choice of the lag parameter b. For low AC, the bias at the optimal b is small: only a small number of biased terms are included. For high AC the optimal b is larger, the higher number of biased terms causes a larger bias. The BC-estimator is very robust: it performs well for all b large enough to capture the autocorrelation. For very large b its variance increases slightly, but this has practically\n5for b < 1, optimal in the no AC-setting, Sancetta and BC estimator are equivalent to standard shrinkage.\nno effect on the PRIAL. An interesting aspect is that our proposed estimator even outperforms shrinkage based on the the population Var (Sij) (calculated by resampling). This results from the correlation of the estimator V̂ar ( Sij )BC,b\nwith the sample estimate eq. (3) of the denominator in eq. (1)."
    }, {
      "heading" : "4 Real World Data: Brain Computer Interface based on Motor Imagery",
      "text" : "As an example of autocorrelated data we reanalyzed a data set from a motor imagery experiment. In the experiment, brain activity for two different imagined movements was measured via EEG (p = 55 channels, 80 subjects, 150 trials per subject, each trial with ntrial = 390 measurements [BSH+10]). The frequency band was optimized for each subject and from the class-wise covariance matrices, 1-3 filters per class were extracted by Common Spatial Patterns (CSP), adaptively chosen by a heuristic (see [BTL+08]). We trained Linear Discriminant Analysis on log-variance features.\nTo improve the estimate of the class covariances on these highly autocorrelated data, standard shrinkage, Sancetta shrinkage, cross-validation and and our proposed BC shrinkage estimator were applied. The covariance structure is far from diagonal, therefore, for each subject, we used the average of the class covariances of the other subjects as shrinkage target [BLT+11]. Shrinkage is dominated by the influence of high-variance directions [BM13], which are pronounced in this data set. To reduce this effect, we rescaled, only for the calculation of the shrinkage intensities, the first five principal components to have the same variance as the sixth principal component.\nWe analyse the dependency of the four algorithms on the number of supplied training trials. Figure 5 (upper row) shows results for an optimized time lag (b = 75) which captures well the autocorrelation of the data (outer left). Taking the autocorrelation into account makes a clear difference (middle left/right): while standard shrinkage outperforms the sample covariance, it is clearly outperformed by the autocorrelation-adjusted approaches. The Sancetta-estimator is slightly worse than our proposed estimator. The shrinkage intensities (outer right) are extremely low for standard shrinkage and the negative bias of the Sancetta-estimator shows clearly for small numbers of training trials. Figure 5 (lower row) shows results for a too large time lag (b = 300). The performance of the Sancetta-estimator strongly degrades as its shrinkage intensities get smaller, while our proposed estimator is robust to the choice of b, only for the smallest number of trials we observe a small degradation in performance. Figure 6 (left) compares our bias-corrected estimator to the four other approaches for 10 training trials: it significantly outperforms standard shrinkage and Sancetta shrinkage for both the larger (b = 300, p ≤ 0.01) and the smaller time lag (b = 75, p ≤ 0.05).\nAnalytic shrinkage procedures optimize only the mean squared error of the covariance matrix, while cross-validation directly optimizes the classification performance. Yet, Figure 5 (middle) shows that for small numbers of training trials our proposed estimator outperforms CV, although the difference is not significant (see Fig. 6). For larger numbers of training trials CV performs better. This shows that the MSE is not a very good proxy for classification accuracies in the context of CSP: for optimal MSE, shrinkage intensities decrease with increasing number of observations. CV shrinkage intensities instead stay on a constant level between 0.1 and 0.15. Figure 6 (right) shows that the three shrinkage approaches (b = 300) have a huge performance advantage over cross-validation (10 folds/10 parameter candidates) with respect to runtime."
    }, {
      "heading" : "5 Discussion",
      "text" : "Analytic Shrinkage estimators are highly useful tools for covariance matrix estimation in timecritical or computationally expensive applications: no time-consuming cross-validation procedure is required. In addition, it has been observed that in some applications, cross-validation is not a good predictor for out-of-sample performance [LG11, BKT+07]. Its speed and good performance has made analytic shrinkage widely used: it is, for example, state-of-the-art in ERP experiments [BLT+11]. While standard shrinkage assumes i.i.d. data, many real world data sets, for example from video, audio, finance, biomedical engineering or energy systems clearly violate this assumption as strong autocorrelation is present. Intuitively this means that the information content per data point becomes lower, and thus the covariance estimation problem becomes harder: the dimensionality remains unchanged but the effective number of samples available decreases. Thus stronger regularization is required and standard analytic shrinkage [LW04] needs to be corrected.\nSancetta already moved the first step into this important direction by providing a consistent estimator under i.i.d. violations [San08]. In this work we analysed finite sample sizes and showed that (i) even apart from truncation bias —which results from including a limited number of time lags— Sancetta’s estimator is biased, (ii) this bias is only negligible if the autocorrelation decays fast compared to the length of the time series and (iii) the Sancetta estimator is very sensitive to the choice of lag parameter.\nWe proposed an alternative estimator which is (i) both consistent and —apart from truncation bias— unbiased and (ii) highly robust to the choice of lag parameter: In simulations on toy and real world data we showed that the proposed estimator yields large improvements for small samples and/or suboptimal lag parameter. Even for optimal lag parameter there is a slight but significant improvement. Analysing data from BCI motor imagery experiments we see that (i) the BCI data set possesses significant autocorrelation, that (ii) this adversely affects CSP based on the sample covariance and standard shrinkage (iii) this effect can be alleviated using our novel estimator, which is shown to (iv) compare favorably to Sancetta’s estimator."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was also supported by the National Research Foundation grant (No. 2012-005741) funded by the Korean government. We thank Johannes Höhne, Sebastian Bach and Duncan Blythe for valuable discussions and comments."
    } ],
    "references" : [ {
      "title" : "Heteroskedasticity and autocorrelation consistent covariance matrix estimation",
      "author" : [ "Donald WK Andrews" ],
      "venue" : "Econometrica: Journal of the Econometric Society,",
      "citeRegEx" : "Andrews.,? \\Q1991\\E",
      "shortCiteRegEx" : "Andrews.",
      "year" : 1991
    }, {
      "title" : "Invariant common spatial patterns: Alleviating nonstationarities in brain-computer interfacing",
      "author" : [ "Benjamin Blankertz", "Motoaki Kawanabe", "Ryota Tomioka", "Friederike Hohlefeld", "Klaus-Robert Müller", "Vadim V Nikulin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Blankertz et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Blankertz et al\\.",
      "year" : 2007
    }, {
      "title" : "Single-trial analysis and classification of ERP components – a tutorial",
      "author" : [ "Benjamin Blankertz", "Steven Lemm", "Matthias Treder", "Stefan Haufe", "Klaus-Robert Müller" ],
      "venue" : null,
      "citeRegEx" : "Blankertz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Blankertz et al\\.",
      "year" : 2011
    }, {
      "title" : "Generalizing analytic shrinkage for arbitrary covariance structures",
      "author" : [ "Daniel Bartz", "Klaus-Robert Müller" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Bartz and Müller.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bartz and Müller.",
      "year" : 2013
    }, {
      "title" : "Spectral Analysis of Large Dimensional Random Matrices",
      "author" : [ "Zhidong Bai", "Jack William Silverstein" ],
      "venue" : null,
      "citeRegEx" : "Bai and Silverstein.,? \\Q2010\\E",
      "shortCiteRegEx" : "Bai and Silverstein.",
      "year" : 2010
    }, {
      "title" : "Neurophysiological predictor of SMRbased",
      "author" : [ "Benjamin Blankertz", "Claudia Sannelli", "Sebastian Halder", "Eva M Hammer", "Andrea Kübler", "KlausRobert Müller", "Gabriel Curio", "Thorsten Dickhaus" ],
      "venue" : "BCI performance. Neuroimage,",
      "citeRegEx" : "Blankertz et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Blankertz et al\\.",
      "year" : 2010
    }, {
      "title" : "Optimizing spatial filters for robust EEG single-trial analysis",
      "author" : [ "Benjamin Blankertz", "Ryota Tomioka", "Steven Lemm", "Motoaki Kawanabe", "Klaus-Robert Müller" ],
      "venue" : "Signal Processing Magazine, IEEE,",
      "citeRegEx" : "Blankertz et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Blankertz et al\\.",
      "year" : 2008
    }, {
      "title" : "Shrinkage algorithms for MMSE covariance estimation",
      "author" : [ "Yilun Chen", "Ami Wiesel", "Yonina C Eldar", "Alfred O Hero" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "Chen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2010
    }, {
      "title" : "Random matrix theory",
      "author" : [ "Alan Edelman", "N. Raj Rao" ],
      "venue" : "Acta Numerica,",
      "citeRegEx" : "Edelman and Rao.,? \\Q2005\\E",
      "shortCiteRegEx" : "Edelman and Rao.",
      "year" : 2005
    }, {
      "title" : "MNE software for processing MEG and EEG data",
      "author" : [ "Alexandre Gramfort", "Martin Luessi", "Eric Larson", "Denis A. Engemann", "Daniel Strohmeier", "Christian Brodbeck", "Lauri Parkkonen", "Matti S. Hämäläinen" ],
      "venue" : null,
      "citeRegEx" : "Gramfort et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gramfort et al\\.",
      "year" : 2014
    }, {
      "title" : "The Elements of Statistical Learning",
      "author" : [ "Trevor Hastie", "Robert Tibshirani", "Jerome Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2008
    }, {
      "title" : "Regularizing common spatial patterns to improve BCI designs: unified theory and new algorithms",
      "author" : [ "Fabien Lotte", "Cuntai Guan" ],
      "venue" : "Biomedical Engineering, IEEE Transactions on,",
      "citeRegEx" : "Lotte and Guan.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lotte and Guan.",
      "year" : 2011
    }, {
      "title" : "A well-conditioned estimator for large-dimensional covariance matrices",
      "author" : [ "Olivier Ledoit", "Michael Wolf" ],
      "venue" : "Journal of Multivariate Analysis,",
      "citeRegEx" : "Ledoit and Wolf.,? \\Q2004\\E",
      "shortCiteRegEx" : "Ledoit and Wolf.",
      "year" : 2004
    }, {
      "title" : "Nonlinear shrinkage estimation of large-dimensional covariance matrices",
      "author" : [ "Olivier Ledoit", "Michael Wolf" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Ledoit and Wolf.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ledoit and Wolf.",
      "year" : 2012
    }, {
      "title" : "Distribution of eigenvalues for some sets of random matrices",
      "author" : [ "Vladimir A. Marčenko", "Leonid A. Pastur" ],
      "venue" : "Mathematics of the USSR-Sbornik,",
      "citeRegEx" : "Marčenko and Pastur.,? \\Q1967\\E",
      "shortCiteRegEx" : "Marčenko and Pastur.",
      "year" : 1967
    }, {
      "title" : "Sample covariance shrinkage for high dimensional dependent data",
      "author" : [ "Alessio Sancetta" ],
      "venue" : "Journal of Multivariate Analysis,",
      "citeRegEx" : "Sancetta.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sancetta.",
      "year" : 2008
    }, {
      "title" : "Robust spatial filtering with beta divergence",
      "author" : [ "Wojciech Samek", "Duncan Blythe", "Klaus-Robert Müller", "Motoaki Kawanabe" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Samek et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Samek et al\\.",
      "year" : 2013
    }, {
      "title" : "A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics",
      "author" : [ "Juliane Schäfer", "Korbinian Strimmer" ],
      "venue" : "Statistical Applications in Genetics and Molecular Biology,",
      "citeRegEx" : "Schäfer and Strimmer.,? \\Q2005\\E",
      "shortCiteRegEx" : "Schäfer and Strimmer.",
      "year" : 2005
    }, {
      "title" : "Inadmissibility of the usual estimator for the mean of a multivariate normal distribution",
      "author" : [ "Charles Stein" ],
      "venue" : "In Proc. 3rd Berkeley Sympos. Math. Statist. Probability,",
      "citeRegEx" : "Stein.,? \\Q1956\\E",
      "shortCiteRegEx" : "Stein.",
      "year" : 1956
    }, {
      "title" : "The interpretation and estimation of effective sample size",
      "author" : [ "H. Jean Thiébaux", "Francis W. Zwiers" ],
      "venue" : "Journal of Climate and Applied Meteorology,",
      "citeRegEx" : "Thiébaux and Zwiers.,? \\Q1984\\E",
      "shortCiteRegEx" : "Thiébaux and Zwiers.",
      "year" : 1984
    } ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "The accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms. In high dimensional settings the sample covariance is known to perform poorly, hence regularization strategies such as analytic shrinkage of Ledoit/Wolf are applied. In the standard setting, i.i.d. data is assumed, however, in practice, time series typically exhibit strong autocorrelation structure, which introduces a pronounced estimation bias. Recent work by Sancetta has extended the shrinkage framework beyond i.i.d. data. We contribute in this work by showing that the Sancetta estimator, while being consistent in the high-dimensional limit, suffers from a high bias in finite sample sizes. We propose an alternative estimator, which is (1) unbiased, (2) less sensitive to hyperparameter choice and (3) yields superior performance in simulations on toy data and on a real world data set from an EEG-based Brain-Computer-Interfacing experiment.",
    "creator" : null
  }
}