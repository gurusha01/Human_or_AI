{"title": "Bandit Convex Optimization: Towards Tight Bounds", "abstract": "Bandit Convex Optimization (BCO) is a fundamental framework for decision making under uncertainty, which generalizes many problems from the realm of online and statistical learning. While the special case of linear cost functions is well understood, a gap on the attainable regret for BCO with nonlinear losses remains an important open question. In this paper we take a step towards understanding the best attainable regret bounds for BCO: we give an efficient and near-optimal regret algorithm for BCO with strongly-convex and smooth loss functions. In contrast to previous works on BCO that use time invariant exploration schemes, our method employs an exploration scheme that shrinks with time.", "id": "c399862d3b9d6b76c8436e924a68c45b", "authors": ["Elad Hazan", "Kfir Levy"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper details an approach to online bandit optimization for smooth strongly convex losses. This setting is in contrast to the online \"full information\" setting, where the gradient is made available to learner, instead feedback is received only through point-wise evaluations of the function. The authors describe a meta-algorithm that, through careful sampling strategies, constructs and unbiased approximation of the gradient and subsequently supplies it an optimization routine for the full information setting. The authors provide theoretical analysis for this approach and contrast the resulting bound O(T^{1/2}) with existing approaches. \n\nThe authors consider an important problem in online optimization, the paper is well written and each point is both clear and precise, and the final result is a clear improvement over existing methods. The proofs are straightforward and easy to follow and do not contain any obvious errors. \n\nThe paper hints at an interesting potential for future work, in the form of either an improved O(T^{1/2}) upper bound for smooth, or strictly convex losses or, alternatively, an \\Omega(T^{2/3}) lower bound. \n\n Authors present algorithm for online bandit optimization for smooth strictly convex losses with improved regret bound, paper is clear and concise, strong accept.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors consider the online convex optimization with bandit feedback. Until now we know rates of convergence in T^{2/3} when the loss functions are convex and either smooth or strongly convex . The authors make both assumption and prove convergence in T^{1/2}.\n\nThe paper is incremental. The algorithm is a mere adaptation of existing ones (Abernethy, Hazan & Rakhlin or Saha & Tewari) and the techniques of proof are almost exactly the same (basically, one inequality is improved using the strong convexity assumption).\n\nThe true interesting question would be whether we could remove any assumption (apart from Lipschitz) and still get T^{1/2}  This is another paper on online convex optimization with several assumptions on the loss functions. I find it is rather incremental.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In this paper, the authors have proposed a new algorithm for Bandit Convex Optimization (BCO) with strongly-convex and smooth loss functions, which uses an exploration scheme that shrinks with time. The authors have also proved that the proposed algorithm achieves an \\tilde{O}(\\sqrt{T}) regret (see Theorem 10), which matches the existing best lower bound if we ignore the logarithm factors. The technical analysis of this paper looks correct.\n\nThe result of this paper is significant in the sense that the authors have not only provided a tighter regret bound for BCO with strongly-convex and smooth loss functions (from \\tilde{O}(T^{2/3}) to \\tilde{O}(T^{1/2})), but more importantly, this improved regret bound matches the lower bound in (Shamir 2013). Thus, in general, I vote for acceptance of this paper. However, I vote for marginal acceptance instead of strong acceptance since I think the paper is not well-written, specifically,\n\n1) There are many lemmas in the paper, but when reading the paper (without looking at the appendix), it is not clear which ones are existing results and which ones are proved by the authors. This makes it a little bit hard to evaluate the technical significance of the paper.\n\n2) The authors spend too much space to review the existing results, and the main result of this paper starts at the end of page 6.\n\n3) Some symbols in the paper are not well defined, for example, \\delta in Eqn(4) is not defined. Some results are not very clear, for instance, the result of Lemma 9 should hold for any \\omega, but the authors do not explicitly state that.\n\n4) The authors should rewrite Algorithm 2: there should be no loop in Algorithm 2, and \\Nabla h_t (x_t)'s should be inputs to the algorithm.\n\n\n\n The result of the paper is significant, but the paper is not well-written and requires rewriting.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
