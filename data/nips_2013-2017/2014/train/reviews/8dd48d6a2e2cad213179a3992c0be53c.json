{"title": "Robust Bayesian Max-Margin Clustering", "abstract": "We present max-margin Bayesian clustering (BMC), a general and robust framework that incorporates the max-margin criterion into Bayesian clustering models, as well as two concrete models of BMC to demonstrate its flexibility and effectiveness in dealing with different clustering tasks. The Dirichlet process max-margin Gaussian mixture is a nonparametric Bayesian clustering model that relaxes the underlying Gaussian assumption of Dirichlet process Gaussian mixtures by incorporating max-margin posterior constraints, and is able to infer the number of clusters from data. We further extend the ideas to present max-margin clustering topic model, which can learn the latent topic representation of each document while at the same time cluster documents in the max-margin fashion. Extensive experiments are performed on a number of real datasets, and the results indicate superior clustering performance of our methods compared to related baselines.", "id": "8dd48d6a2e2cad213179a3992c0be53c", "authors": ["Changyou Chen", "Jun Zhu", "Xinhua Zhang"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The paper introduces max-margin Bayesian clustering (BMC) that extends Bayesian clustering techniques to include the max-margin criterion. This includes, for example, the Dirichlet process max-margin Gaussian mixture that relaxes the underlying Gaussian assumption of Dirichlet process Gaussian mixtures by incorporating max-margin posterior constraints, and is able to infer the number of clusters from data. The resulting techniques (DPMMGM and a further one classed MMCTM) are compared to a variety of other techniques in several numerical experiments.\n\nThe paper combines two clustering approaches: Deterministic and Bayesian clustering. Deterministic approaches can easily handle constraints, but lack the ability to infer the number of clusters. Bayesian techniques can infer the number of clusters, but not handle constraints such as the max-margin. \n\nQuality:\n\nThe resulting BMC algorithm is claimed to be the first extension of regularized Bayes inference to the unsupervised clustering task. Furthermore, the DPMMGM algorithm uses the max-margin constraints to relax the Gaussian assumptions of the standard DPGMM, which is a strong new capability.\n\nClarity:\n\n(pg 4, 184) \u201cThe max-margin constraints take effects in the model via \\tilde\\phi_i\u2019s in (8).\u201d Perhaps this claim was better illustrated in earlier work, but why this statement is true is not made sufficiently clear in this paper. The authors need to provide better support for these claims.\n\n(pg 4, 208) \u201cNote that DPMMGM does not need the complicated class balance constraints [6] because the Gausses in the pseudo likelihood would balance the clusters to some extent.\u201d Not sure that \u201cGausses\u201d is an appropriate wording. This statement ending of \u201cto some extent\u201d is not sufficiently precise for this type of publication.\n\n(pg 6, 298) controll is misspelled\n\n(pg6, 284) Fig 2 shows that the two algorithms generate different results, but the discussion provided here does not give sufficient indication of why the result of the DPMMGM is better. DPGMM is claimed to be more fragmented, but why is that significant here, and, if anything, the DPGMM appears to be a better fit to the data, so what has been lost in switching to the DPMMGM?\n\n(pg6, 295) \u201cthus driving the data points to collapse as well.\u201d Pls clarify this statement. Please also expand this discussion to connect the intuition to the figure 3 (a)-(j). There are a lot of figures, a lot of trends, and each figure is difficult to interpret with the lines and clusters. The conclusion \u201cthe results indeed follow our intuition,\u201d is far less obvious than the authors claim.\n\nOriginality:\n\nWhile it would appear that the ideas are original, much of section 3 on robust Bayesian max-margin clustering follows [17] very closely. In particular the key steps between equations (6)-(7)-(8) reuse the techniques introduced in [17]. This suggests the possibility that the core ideas are in the prior work, and the authors are just re-applying them here to a new model formulation, which calls into question the novelty and importance of this work. Without further clarification, this influences the impact score. \n\nSignificance:\n\nThe extensive numerical results show that DPMMGM outperforms DPGMM, and similarly with MMCTM vs. SVM and S3VM, which appear to be significant results. \n\nHowever, while the paper contains a lot of material on model development, there appear to be very few theoretical statements on the performance or convergence of the algorithms.\n Interesting paper topic. Resulting algorithms do well in the numerical comparisons, that seem extensive. But further clarification compared to the technical approach in [17] is required.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose a clustering framework in which max-margin constraints are invoked such that clusters are defined as a tradeoff between modeling the data density and separating observations in terms of cluster specific latent projections based on regularized Bayesian inference. The method is derived for standard DPGMM as well as topic modeling based on the cluster based topic model (CTM) forming the proposed DPMMGMM and MMCTM and the utility of the approach demonstrated on a synthetic dataset as well as several real datasets demonstrating that the ground truth information is better recovered when compared to standard non-parametric mixture modeling (DPGMM and cluster based topic modeling, CTM) as quantified respectively in terms of normalized mutual information and accuracy.\n\nThe framework is interesting and invoking separation of clusters is both reasonable and seems to provide significant utility which may warrant publication. The proposed framework extends the regularized Bayesian inference framework and the non-trivial implementation details are provided in the accompanying appendix. \n\nMy main criticism has to do with the tuning of the regularization strength c and margin l. Clearly as also discussed in the paper these parameters have great influence on the results. However, their suggested tuning is a bit ad hoc and it would improve the paper to inspect their cross-validated tuning versus the proposed heuristic forming DPMMGMM*. Furthermore, it is unclear what denotes the MMCTM* framework it is just stated in the paper that this constitutes the optimal parameter setting - but how was this found (do you mean using the same number of topics as for CTM - Please clarify)? For the topic modeling results in particular it seems that the choice of c=9 and l=0.1 is a rather informed setting and it is unclear how this setting was selected. This should also be clarified or at least the influence of these choices be better accounted for as the tuning of these parameters for the approach seems to be somewhat of a key issue for performance.\n\nMinor comments:\nPlease clarify what the lines denote in Figure 3. I assume they are representing \\eta_k.\n\nGausses -> Gaussians\nin the generate process -> in the generative process\nneeded to sampled -> needed to be sampled\nto jointly sampled (y_i,s_i) -> to jointly sample (y_i,s_i)\nmany of the the sampling formula are -> many of the sampling formulas are\n\n\n\n\n\n\n An interesting framework for separating clusters in generative models that appears to have utility in extracting ground truth structure. However, adequate tuning of tradeoff and marging parameters seems to be essential.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a fully Bayesian framework which enables max-margin constraints to be incorporated for clustering models. The method leverages the existing Regularized Bayesian inference technique from Zhu et al. but extends it to unsupervised settings. The authors demonstrate the efficacy of the new approach using two different clustering models: a DP- based GMM and a cluster-topic model, both with integrated max-margin constraints. \n\nThe paper is well written with clear description and details of the new max-margin clustering approach and the model variants. It would be useful if you can expand on the posterior inference part a bit more (more details on sampling complexity) since practical performance highly depends on the efficiency of the sampler. The authors also compare the two models to baselines systems and analyze sensitivity to the hyper parameters, which is nice.\n\nInstead of using the the heuristic for model selection, would it be possible to make it non-parametric & estimate this during inference (perhaps your heuristic could be used as an informative prior)? \n\nClarification: Is \u201cp\u201d the dimensionality of the latent space? So the time complexity for DPGMM has an extra cubic factor in p.\n\nIt is encouraging to see that adding max-margin constraints improves clustering quality when using unsupervised Bayesian models. The datasets in Table 1 are still small and I wonder if the new method can scale to practical settings when the data & dimensionality is really large. Perhaps some of the recent work on fast sampling using randomization and other approximation techniques could be used to speed up inference further. It will be nice to address this somewhere in the paper (either as future work or in related work).\n\nAre the accuracy differences in Table 2 statistically significant (e.g., S3VM vs. MMCTM for L=5,10)? Please provide stat sig. numbers.\n\n The paper is well written and extends the RegBayes framework to unsupervised settings. The authors demonstrate the utility of the new approach with two different clustering models showing that adding max-margin constraints improve clustering quality.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
