{"title": "Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature", "abstract": "We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in probabilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-)parameters (for example to compute marginal likelihood or a partition function). MCMC has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefficiency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples.", "id": "e94f63f579e05cb49c05c2d050ead9c0", "authors": ["Tom Gunter", "Michael A. Osborne", "Roman Garnett", "Philipp Hennig", "Stephen J. Roberts"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Overview: this paper presents a fast alternative to MC methods for approximating intractable integrals. \n\nThe main idea behind Bayesian quadrature is to exploit assumptions and regularities in the likelihood surface, something which pure Monte Carlo ignores. \n\nThe authors in this paper model the square root of the integrand (the likelihood/prior) as a Gaussian Process. Samples are then drawn according to some criterion - in this case, samples are chosen to the location of the maximal expected posterior variance of the integrand. Intuitively, this is a location where the model knows the least about the value of the integrand, and stands to gain a lot of information. \n\nImportantly, they stress the computational benefits of their BQ active sampling method over standard Monte Carlo techniques. \n\nThe authors then approximate integrals for a variety of problems, including marginal likelihood calculation for GP regression and GP classification marginal likelihoods. \n\nQuality - \nThis paper is technically sound: the problem is well motivated, the method is well described and their approach does a good job when compared to other methods for numerical integration. \n\nClarity- \nThe paper is very well written and organized. The authors do a good job conveying all aspects of the analysis. They describe Bayesian Quadrature (and numerical integration in the first place), as well as existing approaches similar to theirs in a clear way. They differentiate their own method by very clearly laying out their contributions. They do a great job explaining their approach, and the process of going from problem to solution. \n\nOriginality- \nThey present their method as a way to improve (both speed and some accuracy) existing methods for Bayesian quadrature. They stress two contributions: the square root GP and \u2018fast active sampling\u2019. \n\nThe square root GP seems to be another way to model a positive function (the likelihood), and one that is typically less explored. The authors also do a great job describing two ways to cope with the intractability of inference given a non-linear transformation of a GP (linearization and moment matching). \n\nSignificance-\nThe authors describe an alternative tool to compute a marginal likelihood - an extremely difficult and important task. The utility of such a tool is based on its speed, accuracy, and simplicity of implementation. This paper lays out an alternative solution to this common problem - one that is competitive in speed, accuracy, and simplicity. However, it remains unclear how significant this particular paper will be (or how much followup research it will inspire). What are some future directions of research made possible by this contribution? \n\nQuestions and comments: \n- Figure 1: In this example, are the hyperparameters of the GP learned? If the covariance is something like a squared exponential, How does the length-scale cope with the crazy section?\n- Line 208: how does the log transform compare to the square root transform. It seems somewhat clear that an unconstrained GP will more poorly, how well does another type of constrained GP perform? \n- Line 264: Why is the variance of the log transform worse (worse enough to make the whole scheme worse)? \n- Line 303: Is the task here just to integrate over the mixture of gaussians? \n- Fig 8+9: Maybe put the converged values in a table? It\u2019s hard to compare L, M and AIS here. \n\n This is a good, technically sound paper describing a new method to perform Bayesian Quadrature.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper explores the application of Bayesian quadrature to Bayesian inference. This doubly Bayesian strategy was developed by Osborne et al. (NIPS 2012). The present work builds on those ideas and introduces some further innovations. The general approach is to build a probabilistic model of the posterior surface of some Bayesian inference problem, to allow (\"meta\") inference over quantities of interest, such as the partition function. The first innovation here is to use a squared Gaussian Process to model the likelihood (Osborne et al., used an exponentiated Gaussian Process) and the second innovation is to choose evaluate the posterior at points of high uncertainty. As before, various approximations must be made to make inference tractable.\n\nThese are valid innovations and the authors demonstrate their effectiveness. But since the obvious (and stated) precedent is that of Osborne et al., I would like to have seen \"WSABI\" experimentally compared against an exponentiated GP rather than just compared against the un-warped Bayesian Monte Carlo.\n\nOne pedantic point: some people find it grating to have the term \"95% confidence interval\" used to describe a region containing 95% of the posterior mass since it invites confusion with frequentist confidence intervals. You could consider avoiding the term. I also think the title of the paper is confusing and possibly misleading. You're using the term \"sampling\" in a very different sense from its usual meaning in the context of Bayesian inference. Something more explicit like \"Squared Gaussian Processes for Fast Bayesian Quadrature\" could be more appropriate.\n\n Some worthwhile innovations over recent work using Bayesian quadrature for Bayesian inference.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents a Bayesian quadrature approach that uses square root transform and active learning technique. Two approximation schemes for the likelihood Gaussian process prior are explored -- linearization and moment matching. A simple active sampling method (without integration) is also used to increase speed.\n\nThe paper is well written. I am not an expert in BQ, so the paper is more like an educational read to me. The only part concerns me is that experimental section. The author didn't compare the method with other BQ methods. It is hard for me to judge if the method is indeed superior when it is only compared to slow MCMC methods. It would also be ideal for the authors to post the number of point it requires in order to achieve the same accuracy as MCMC methods (BMC and SMC at least).\n\n  A well written paper in BQ, but the experimental section can use some work.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
