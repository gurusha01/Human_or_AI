{"title": "Learning From Weakly Supervised Data by The Expectation Loss SVM (e-SVM) algorithm", "abstract": "In many situations we have some measurement of confidence on <code>positiveness for a binary label. The</code>positiveness\" is a continuous value whose range is a bounded interval. It quantifies the affiliation of each training data to the positive class. We propose a novel learning algorithm called \\emph{expectation loss SVM} (e-SVM) that is devoted to the problems where only the ``positiveness\" instead of a binary label of each training sample is available. Our e-SVM algorithm can also be readily extended to learn segment classifiers under weak supervision where the exact positiveness value of each training example is unobserved. In experiments, we show that the e-SVM algorithm can effectively address the segment proposal classification task under both strong supervision (e.g. the pixel-level annotations are available) and the weak supervision (e.g. only bounding-box annotations are available), and outperforms the alternative approaches. Besides, we further validate this method on two major tasks of computer vision: semantic segmentation and object detection. Our method achieves the state-of-the-art object detection performance on PASCAL VOC 2007 dataset.\"", "id": "3a066bda8c96b9478bb0512f0a43028c", "authors": ["Jun Zhu", "Junhua Mao", "Alan L. Yuille"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The authors propose an expectation loss svm (e-svm) that can handle continuous attributes of positive instances in a learning problem and apply the model to learn visual segmentation classifiers under weak supervision. The methodology is validated for both semantic segmentation and object detection with promising results. The paper addresses an important problem of visual learning under weak supervision, the presentation is clear and well-motivated and the experiments are interesting and well executed. Although the technical content is an extension of the standard multiple instance learning framework (by replacing a binary with a soft weighting of positives), it applies in both the observed and the latent setting, and represents a valid contribution for NIPS. An expectation loss svm method that can handle continuous attributes of positive instances in a learning problem and apply it to learn visual segmentation classifiers under weak supervision. Good motivation, clear presentation, relevant results for semantic segmentation and object detection.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes a modification to the SVM learning algorithm that deals with weakly supervised data. Specifically, a per-example weight (between 0 and 1) is assigned to each training example. Then the loss function of the SVM is modified to take into account this weight. The method is then extended to the case when the per-example weight is not observed (i.e., is a latent variable).\n\nThe method is well described and the experimental results indicate that, for the semantic segmentation and object detection tasks, there may be value is using such an approach over treating each example with equal weight. However, I fail to see the difference between the proposed method and existing \"example dependent costs\", which are widely known and used in the machine learning community. SVMlight, for example, provides such a mechanism.\n\nThe extension to the case when the per-example weight is not observed is interesting but straightforward. In this case the problem is non-convex and no theoretical analysis is given. Indeed a strong regularization term is used suggesting that the per-example weights do not deviate far from their initial values. Can the authors comment?\n\nMinor comment: The claim on L050 that \"it is also not a standard regression problem since the positiveness belongs to a bounded interval [0, 1]\". Logistic regression is very much standard and is bounded to [0, 1]. \n\nMinor spelling mistake in the abstract: \"continues\" -> \"continuous\"\n The paper proposes a modified SVM learning algorithm in which the loss function is modified by a per-example weight. However, example dependent costs are already widely used in machine learning.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary: \nThe authors present a scheme for semantic labeling and object detection. The former is accomplished by generating candidate segments and extracting features for these; and then regressing on the Intersection-over-Union overlap ratio between a candidate segment and pixel-accurate ground truth. In the case of object detection, only weaker labels in terms of bounding boxes are available and the precise spatial extent is used as latent variable. Regression is performed using a weighted SVM and a weighted transductive SVM, respectively. Results are on par with the state of the art in semantic labeling, and the best currently known for object detection on VOC 2007.\n\nOriginality:\nAlgorithmic novelty is limited, and the authors unfortunately do not cite pertinent previous work. In particular, the weighted SVM is well-known (e.g. [R1,R2,R4] below, not cited). The authors put a sum constraint on the weights, which may be novel (I did not conduct an extensive literature search here). The \"latent e-SVM\" that the authors propose is a weighted version of a transductive SVM (e.g. [R3], not cited). \n\n[R1] Suykens et al. Weighted least squares support vector machines: robustness and sparse approximation, Neurocomputing, 2002\n[R2] X.Yang et al. Weighted Support Vector Machine for Data Classification, IJCNN, 2005\n[R3] V. Vapnik, \u00e2\u20ac\u0153Statistical Learning Theory\u00e2\u20ac\u009d, 1998\n[R4] M.Lapin et al, Learning Using Privileged Information: SVM+ and Weighted SVM, Neural Networks, 2014\n\nSignificance:\nOutperforming the recent CNN results by a \"conventional\" learning approach is a feat and of great interest to the community. \n\nQuality:\nCrucial references to previous work are missing (see above). The object detection framework involves quite a bit of engineering, but not outside the norm. The default method for regression on the unit interval is (regularized) logistic regression, so these results should be reported as baseline to support the use of a sum-constrained weighted SVM. Authors should give more details on how they picked regularization coefficients \\lambda_W and \\lambda_R. The main strength of the paper are the good empirical results. \n\n\n\nClarity:\nThe title and abstract do not summarize the core contribution of the paper well. The abstract also insinuates that regression targets from the unit interval are somehow less informative than mere binary class memberships. This is not true, because the latter can always be obtained from the former by thresholding. For a NIPS audience, the paper spends rather too much space (up to and including page 4) on small variations of well-known algorithms, and passes somewhat quickly over the all-important feature engineering details (lines 254ff). \n\nMinor comments \nLine 19: continues => continuous\nLine 94: boxes => box\nLine 98: AP: Abbreviation not introduced\nLine 350: SVM => SVC\nLine 350: with every different values => for all threshold values\n\n-----\n\nI have upped my quality score by one, assuming the authors will not be defensive about the relation to previous work, but rather comment extensively on it and connections to it.  Great results, deficient references to earlier work, little algorithmic novelty. A paper that should certainly be published, possibly after some rewriting, and possibly rather at a computer vision conference.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
