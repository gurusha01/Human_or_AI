{"title": "Parallel Direction Method of Multipliers", "abstract": "We consider the problem of minimizing block-separable convex functions subject to linear constraints. While the Alternating Direction Method of Multipliers (ADMM) for two-block linear constraints has been intensively studied both theoretically and empirically, in spite of some preliminary work, effective generalizations of ADMM to multiple blocks is still unclear. In this paper, we propose a parallel randomized block coordinate method named Parallel Direction Method of Multipliers (PDMM) to solve the optimization problems with multi-block linear constraints. PDMM randomly updates some blocks in parallel, behaving like parallel randomized block coordinate descent. We establish the global convergence and the iteration complexity for PDMM with constant step size. We also show that PDMM can do randomized block coordinate descent on overlapping blocks. Experimental results show that PDMM performs better than state-of-the-arts methods in two applications, robust principal component analysis and overlapping group lasso.", "id": "42a0e188f5033bc65bf8d78622277c4e", "authors": ["Huahua Wang", "Arindam Banerjee", "Zhi-Quan Luo"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper considers the problem of minimizing block-separable functions subject to linear constraints. A variant of ADMM (called PDMM) is proposed to deal with the multiple blocks case in this paper: it randomly selects K blocks of the primal variable to update and uses a backward step to refine the dual variable; the process to update the primal variable can be implemented in parallel. The sublinear convergence rate is proved for the proposed method. This method generalized several variants of ADMM.\n\nThis paper is well written and easy to follow. I am positive to accept this paper. My questions and comments are given below:\n\n1) The algorithm description in (5) and (6) is not clear enough. Do you update all coordinates of y^{t+1} and \\hat{y}^{t+1} or just a single block \"i\"? My understanding is that you only update a block of the dual variable \"y\", but I did not find how to select \"i\". This should be clarified in the revision.\n\n2) In Theorem 2, I believe that \"\\sum_{i=1}^I\" was missing behind of \"{\" from your proof.\n\n3) More discussion about Theorem should be included. Theorem 2 (the main result in this paper) provides the convergence rate of the proposed method PDMM. First, PDMM generalizes several variants of ADMM. A natural question is if Theorem 2 (by properly choose parameters like K, J) is consistent with the convergence rates of those variants. Second, since you split the transformation matrix A into multiple blocks, what is the optimal way to split it from your theorem?\n\n4) I am curious of the comparison on the tensor completion problem (\"tensor completion for estimating missing values in visual data, 2012\"), which also has the multiple block structure. Do you have any clue which variant of ADMM is optimal? \n\nMinors:\n\n1) In (9), a space is missing behind of \"min\"\n\n2) Line 217, remove the space before \"Section\"  I am positive to accept this paper. More discussion after Theorem 2 is expected.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a parallel direction method of multipliers (PDMM) to minimize block-separable convex functions subject to linear constraints. In contrast with ADMM, which update primal variables using a Gauss-Seidel manner but limited to two blocks, this work proposes to update the primal blocks in the Jacobian manner with multiple blocks. The main result of the paper is the introduction of the dual backward step, which compensate the limited information propagation in the Jacobian updates by effectively reducing the step size for the dual updates. Similar to ADMM, O(1/T) type convergence results are established for PDMM in this paper.\n\nIt is shown (in supplementary materials) that two previous methods, sADMM and PJADMM are special cases of the proposed method, which give better understanding the relationship between different methods in the landscape of decomposition methods based on augmented Lagrangian. Experiments results demonstrated the effectiveness of the PDMM method as compared with ADMM and other variants. The results are a little counter intuitive for me, since in general Jacobian type methods would give slow convergence compared with Gauss-Seidel type of methods. It would be good to elaborate on this point.  The dual backward step in PDMM to ensure convergence seem to be new and key to obtain convergence under general conditions as ADMM. The proposed method gives more alternatives and possibilities for distributed optimization.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a randomized parallel version of ADMM, which can handle palatalization with multiple blocks. In each iteration, PADMM picks K random blocks, updates the block primal vector, and then updates the dual vector via a backward step. The authors illustrate the importance of using a backward step, as it makes the dual update more conservative, enabling global convergence. The algorithm also allows adding a proximal term to the primal update, making a part of optimization problems easier to solve. The authors give theoretical analysis on the algorithm, establishing its global convergence and its iteration complexity. When there are totally J blocks, and when the PADMM algorithm randomly picks K blocks at one time, the convergence rate of the algorithm is O(J/(TK)) after T iterations of update.\n\nThe main advantage of the PADMM algorithm is (1) it allows full parallelization for the primal step and (2) it allows relatively large update stepsize comparing to other methods. In particular, PADMM can be faster than sADMM as it allows greater stepsizes. It is more flexible than PJADMM, and more parallelizable than GSADMM. The authors have evaluated the algorithm on robust principal component analysis and\noverlapping group lasso. The experimental results are quite promising. The PADMM algorithm achieves the desired accuracy with less computation time. Nevertheless, the authors haven't reported the performance of PADMM with parallel implementation. Comparing to traditional ADMM, it is less convenient to tune the three parameters of PADMM.\n\nOverall, this is an interesting contribution to solving the ADMM-type problems, where multiple constraints are provided. The algorithm has the same convergence rate as ADMM, but it allows parallelization and it exhibits promising practical performance.  Overall, this is an interesting contribution to solving the ADMM-type problems. The paper is well written. Both the theoretical and empirical parts are solid.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
