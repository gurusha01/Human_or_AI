{"title": "Multi-Class Deep Boosting", "abstract": "We present new ensemble learning algorithms for multi-class classification. Our algorithms can use as a base classifier set a family of deep decision trees or other rich or complex families and yet benefit from strong generalization guarantees. We give new data-dependent learning bounds for convex ensembles in the multi-class classification setting expressed in terms of the Rademacher complexities of the sub-families composing the base classifier set, and the mixture weight assigned to each sub-family. These bounds are finer than existing ones both thanks to an improved dependency on the number of classes and, more crucially, by virtue of a more favorable complexity term expressed as an average of the Rademacher complexities based on the ensemble\u2019s mixture weights. We introduce and discuss several new multi-class ensemble algorithms benefiting from these guarantees, prove positive results for the H-consistency of several of them, and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L1-regularized counterparts.", "id": "7bb060764a818184ebb1cc0d43d382aa", "authors": ["Vitaly Kuznetsov", "Mehryar Mohri", "Umar Syed"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Summary of the paper: \n\nThe paper introduces a new class of multi-class boosting algorithms with base learners that are regularized based on their Rademacher complexities. The experiments show great improvement over other multi-class approaches in the literature and on the non-regularized version of the proposed algorithm.\n\nDetailed remarks:\n\nFor the title: in general, the term \"deep\" in deep learning is used for saying that the representation is hierarchical in a compositional sense. Trees are obviously not deep in this sense, and so just because you are using large trees in boosting (a quite common setup in practice), it will not make the approach \"deep\". You still use a single \"hidden layer\" of trees, combined linearly, this is definitely a shallow architecture. Thus I strongly object to the title, it is misleading.\n\nLines 042-043: In practice, boosted trees almost always outperform boosted stumps \\cite{CaNi06, Keg14}, and when the validated tree sizes are obtained by proper hyper-parameter optimization, they can be quite large, the same order as found in your experiments. Moreover, when boosting multi-class Hamming trees in AB.MH, \\cite{Keg14} also found that on most of the data sets there is very little overfitting, basically one can boost trees of several tens of inner nodes for ten thousand iterations (see, for example, pendigits or letters, two sets on which most algorithms are tested), without increasing the test error. So, the statement of \"boosting has been observed to overfit in practice\", derived from 15 year-old papers, should be revised. Sometimes it overfits, sometimes it doesn't, and basically we don't know when it does and why it does when it does.\n\nLines 046-047: To my knowledge, the first paper proposing adaptive regularization of base classifiers is \\cite{KeWa04}. The intuitive idea is the same and the final algorithm is not that different either (coefficients have to be shrunk by a quantity related to the empirical complexity of the weak classifier).\n\nAlthough we cannot expect that a conference paper surveys all multiclass boosting algorithms, the paper should at least mention those that seem to be state of the art: AOSO \\cite{SuReZh12}, ABC \\cite{Li09,Li09a}, Gao-Koller's iterative weak learner in hinge-boost \\cite{GaKo11}, and AB.MH with Hamming trees \\cite{Keg14} (it seems to me that the trees in this latter are quite similar to those used in this submission).\n\nThe experimental setup. \n\nI was honestly stunned reading this: \n\n\"We recorded the parameter tuple that had the lowest average error across all 10 runs, and this average error and the standard deviation of the error is reported in Table 1 and Table 2, along with the average number of trees and the average size of the trees in the ensembles.\" \n\nYou're validating on the test set, something that we teach to our students never to do. The consequence is twofold. First, I cannot compare the errors to those available in the literature. Some of the errors (e.g. on pendigit) looked suspiciously low, 3-4 times lower than I've ever seen, that's when I started to check your experimental setup. Second, the empirical comparison of the algorithms you tested is tainted. It is obvious that if you take an algorithm and you add hyper-parameters (the AB.MR -> L1 AB.MR -> MDeepBoostSum chain), the minimum test error can only decrease. The ballpark range of the \"improvements\" is very much in line with this view: you simply harvested the fact that the minimum of a larger sample is smaller than the minimum of a smaller sample, even if they come from the same distribution.\n\nNow, I know that this seems like a detail for a theoretician, but for people using these algorithms what you are claiming is important. We have tested a lot of ways of regularizing the weak learners up to about ten years ago (you referred to some of the works), it never worked, more precisely, we didn't seem to need it. There were some indications that it could help on small data sets \\cite{KeWa04}, but, exactly because of the small size of the sets, results were inconclusive. If you now claim that it is not the case, the experimental validation has to be rock solid. \n\nMy suggestion is that you make an attempt to redo the experiments doing proper double cross validation during the rebuttal period, and show us the new results. If they are non-conclusive (that is, the regularized version doesn't beat the standard algorithm), I would say the paper could still be accepted, but the message has to be altered to something like \"here is an interesting-looking algorithm with some strong theoretical justifications, but regularization doesn't work\".\n\nProviding an open source implementation of the algorithm would be a great way to make the experiments reproducible and to let people use the proposed techniques and to build on them.\n\nPseudocode: for those who would like to implement the method starting from the pseudocode, it would be helpful to point towards the definitions of the quantities in the caption: \\Lambda and S_t for t = 1 are undefined.\n\n\n@inproceedings{KeWa04,\n\tAddress = {Vancouver, Canada},\n\tAuthor = {K\\'{e}gl, B. and Wang, L.},\n\tBooktitle = {Advances in Neural Information Processing Systems},\n\tPages = {665--672},\n\tPublisher = {The MIT Press},\n\tTitle = {Boosting on manifolds: adaptive regularization of base classifiers},\n\tVolume = {17},\n\tYear = {2004}}\n\n@inproceedings{CaNi06,\n\tAuthor = {Caruana, R. and Niculescu-Mizil, A.},\n\tBooktitle = {Proceedings of the 23rd International Conference on Machine Learning},\n\tPages = {161--168},\n\tTitle = {An Empirical Comparison of Supervised Learning Algorithms},\n\tYear = {2006}}\n\n@inproceedings{KeBu09,\n\tAddress = {Montreal, Canada},\n\tAuthor = {K\\'{e}gl, B. and Busa-Fekete, R.},\n\tBooktitle = {International Conference on Machine Learning},\n\tPages = {497--504},\n\tTitle = {Boosting products of base classifiers},\n\tVolume = {26},\n\tYear = {2009}}\n\n@inproceedings{Li09,\n\tAuthor = {Li, P.},\n\tBooktitle = {International Conference on Machine Learning},\n\tTitle = {{ABC}-{B}oost: Adaptive Base Class Boost for Multi-class Classification},\n\tYear = {2009}}\n\n@techreport{Li09a,\n\tAuthor = {Li, P.},\n\tInstitution = {Arxiv preprint},\n\tNumber = {arXiv:0908.4144},\n\tTitle = {{ABC-LogitBoost} for Multi-class Classification},\n\tYear = {2009}}\n\n@inproceedings{GaKo11,\n\tAuthor = {Gao, T. and Koller, D.},\n\tBooktitle = {International Conference on Machine Learning},\n\tTitle = {Multiclass boosting with hinge loss based on output coding},\n\tYear = {2011}}\n\n@inproceedings{SuReZh12,\n\tAuthor = {Sun, P. and Reid, M. D. and Zhou, J.},\n\tBooktitle = {International Conference on Machine Learning (ICML)},\n\tTitle = {{AOSO-LogitBoost}: Adaptive One-Vs-One {LogitBoost} for Multi-Class Problem},\n\tYear = {2012}}\n\n@inproceedings{Keg14,\n\tAbstract = { We train vector-valued decision trees within the framework of AdaBoost.MH. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length $K$ and label-independent scalar classifier.},\n\tAuthor = {K\\'{e}gl, B.},\n\tBooktitle = {International Conference on Learning Representations},\n\tTitle = {The return of {AdaBoost.MH}: multi-class {H}amming trees},\n\tUrl = {http://arxiv.org/abs/1312.6086},\n\tYear = {2014}}\n\n----\n\nI looked at the new experimental results and indeed they are more reasonable now. As expected, errors increased significantly (eg., on letters and pendigits the errors increased by 6-7 standard deviations). In this light your answer \n\n\"1) While it is of course possible to overestimate the performance of a learning algorithm by optimizing hyperparameters on the test set, this concern is less valid when the size of the test set is large relative to the \"complexity\" of hyperparameter space (as any generalization bound will attest). Note that our experiments varied only three hyperparameters over a large data set.\" \n\nlooks strange. You just proved that my concern was highly relevant. \n\nIf I look at your results now, I see no significant improvement by the added regularization which contradicts the main message of the paper (differences are in the 1-2 std range, completely compatible by statistical fluctuation). On the other hand, your results using AB.MR and AB.MR-L1 are really good, you do something in your tree-building procedure which seems to work better than what other people are doing (unfortunately, you don't give details on your procedure). \n\nI'm sympathizing with you in the sense that I know that simply describing a state-of-the-art algorithm without any theoretical results will never be accepted at NIPS, but still, at this point your theoretical results are irrelevant for a practitioner (it's not your theory that makes the algorithm good), and your practical results are irrelevant for a theoretician (for the same reason: it's not your practical results that make the theory interesting or relevant). It's two papers in one with a false conclusion. I'd be happy to accept the paper if you were honest about it.  I like the idea and algorithm itself, but considering the current state of the experimental setup the decision should be a clear rejection.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary: \nThe authors proposes a multi-class extension of Deep Boosting framework of Cortes and Mohri. First, the authors prove a generalization bound of linear combination of base hypotheses where each base hypotheses belong to\ndifferent sets with different Radmacher complexities. The bound improves the standard multi-class generalization bound of Koltchinskii and Panchenko. Then, motivated by the generalization bound, the authors formulate an optimization problem which minimizes an upper bound of the generalization error. The proposed algorithm is a coordinate-descent style algorithm solving the optimization problem. The experimental results show that the proposed algorithm with decision trees as base hypotheses outperforms standard multi-class boosting algorithms such as AdaBoost.MR and its variants.\n\n\n\nComments:\n\nThe optimization problem is well motivated by the improved generalization bound of the authors. Also, the formulation naturally implies an coordinate-descent algorithm to solve it. Interestingly enough, the derived criterion to choose a weak hypothesis is the sum of the weighted error and the complexity of the hypothesis class. \n\nThe experimental result seems to show the effectiveness of the proposed method as well. But, it would be better, as done in the previous ICML'14 paper on Deep Boosting, to use validation sets to optimize parameters. \n\n\nBasically, the formulation is \"primal\", i.e, minimizing losses (say exponential or logistic loss), similar to AdaBoost or LogitBoost. \nAnother formulation is based on \"dual\" view of boosting. For example, AdaBoost is motivated by minimizing the relative entropy from the last distribution on the sample under some linear constraints (see, Kivinen and Warmuth COLT99). Further investigations following the dual view are found in TotalBoost(Warmuth et al. ICML06), SoftBoost(Warmuth et al. NIPS07) and ERLPBoost (Warmuth et al. ALT08). The dual view for the proposed algorithm might be interesting and it might deepen the understanding of the algorithm.\n\nAfter viewing the authors's comments:\nSince the authors report the new experimental results based on reviewers' suggestion, I would raise my evaluation.\n I think the theoretical contribution is sufficient enough for the community of NIPS.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a multi-class extension to the recently proposed ensemble learning algorithm called DeepBoost. It provides new data-dependent bounds for convex ensembles in multi-class classification setup.\n\nThe paper reads well and technical derivations are correct Further it provides data-dependent bounds which are tighter than existing bounds due to the explicit dependency on the mixture weights in the convex combination. The special case of the bound derived in theorem 1, also leads to an important result as leads to a linear dependency on the number of classes. It improves the existing bounds from Koltchinskii and Panchenko, wherein this dependency is quadratic. In this sense, the bound is more useful, particularly for large number of classes. The authors also design optimization objectives and multi-class DeepBoosting algorithms which give good performance on UCI datasets as compared to the multi-class version of Adaboost and logistic regression.\n\nHowever some parts of the paper need to be expanded and clarified especially the relation to the existing paper Deep Boosting in ICML, 2014 which makes the proposed approach somehow incremental as well as the experimental setup. Indeed, the proof techniques, design of the objective function, and developed algorithms are very similar in flavor to this existing work. This limits the novelty of this current work to certain extent. Another point which was not clear was that authors say in Line 143 (in section 3.1) that generalization error of $f$ and $f/\\rho$ is same. In my opinion, $\\rho$ has the interpretation of margin from definitions developed in equation (2), and hence it is not immediately clear why these two will admit the same generalization error. Moreover, it seems that the authors are using the labels on the test set to find the hyper-parameters of the algorithm. In the ICML paper on binary classification, the experimental setting does look different, in that paper, there is a separate validation set on which the parameter values are chosen, and not directly on the test set. The authors should clarify on this point as well.\n +The paper provides data-dependent bounds which are tighter than existing bounds due to the explicit dependency on the mixture weights in the convex combination.+ The special case of the bound derived in theorem 1, also leads to an important result as leads to a linear dependency on the number of classes.+ The authors also design optimization objectives and multi-class DeepBoosting algorithms which give good performance on UCI datasets- This work is somewhat incremental on the existing paper Deep Boosting in ICML, 2014- Some points need to be clarified (experimental setup, clear novelty with respect to Deep Boosting paper in ICML, 2014)", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
