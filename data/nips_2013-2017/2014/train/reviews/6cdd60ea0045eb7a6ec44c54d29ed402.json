{"title": "Robust Logistic Regression and Classification", "abstract": "We consider logistic regression with arbitrary outliers in the covariate matrix. We propose a new robust logistic regression algorithm, called RoLR, that estimates the parameter through a simple linear programming procedure. We prove that RoLR is robust to a constant fraction of adversarial outliers. To the best of our knowledge, this is the first result on estimating logistic regression model when the covariate matrix is corrupted with any performance guarantees. Besides regression, we apply RoLR to solving binary classification problems where a fraction of training samples are corrupted.", "id": "6cdd60ea0045eb7a6ec44c54d29ed402", "authors": ["Jiashi Feng", "Huan Xu", "Shie Mannor", "Shuicheng Yan"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The authors consider logistic regression when the data are contaminated with outliers. No assumptions are made on the outliers, they need not even follow any distribution, which can happen for example when a video is corrupted by sensor errors. \n\nThe authors provide an innovative algorithm, RoLR, and provide performance guarantees. The algorithm is simple to describe but uses sophisticated ideas, and the mathematics behind the performance guarantees is impressive. The simulations are promising. \nSpecific comments:\nL135: Why does $\\beta^*$ have unit length?\nLine 157: X_i is p-dimensional, so how is the square and the absolute value defined?\n\nThe paper is of high quality and clearly written. It is to be seen how the method will perform for real data, but regardless of this the theoretical advance is important. \n\n The paper is innovative and the mathematics is of high quality.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The manuscript proposes a new method for robust logistic regression with a focus on dealing with outliers with high leverage. Outliers are assumed to come from an arbitrary and unknown distribution, but the number of outliers is assumed to be known ahead of time, which is a theoretically convenient, but practically slightly more troubling assumption.\n\nThe method is based on maximising the sum of y*x'beta, but only summed over the observations which contribute in absolute terms the least to the objective function, thus outliers with large leverage are excluded. This can be elegantly translated into a linear programming problem.\n\nThe authors derive some risk bounds. In common with most of these bounds, they are probably too loose to have a practical quantitative use, but are useful for qualitative interpretation.\n\nThe manuscript contains a small simulation study in which the proposed method compares very favourably with classical logistic regression.\n\nA few more detailed comments:\n\n- Why do you need the preprocessing step? Observations with large ||x|| should also be among the observations with large y*x'beta and thus omitted by the algorithm. Also T is decreasing in n (and tends to 0 in the limit), so the more data we have, the more observations are thrown out by the preprocessing. For example, what proportion of observations is removed by the preprocessing step in the simulation?\nAlso, I am surprised that T is not chosen based on n or lambda. \n\n- On a related point, it might be worth looking at how well logistic regression does after the same preprocessing is carried out.\n\n- Given the preprocessing and the set-up of the method, situations where (like in the simulation shown) the outliers have much larger variance in the covariates are quite favourable to the proposed method (they are very likely to trigger the preprocessing criterion or lead to a large contribution to the objective function and are thus omitted). What happens is sigma_o is decreased, so that outliers look less like obvious outliers and are more likely to sneak into the n smallest contributions (and thus not excluded by the method)? Will the method perform better or worse?\n\n- One important question is how much worse the method performs when compared to logistic regression when there are no outliers. The very left end of figure 2 suggests no big difference in terms of estimating beta, but a more substantial difference when looking at the misclassification rate. Essentially, what price do we have to pay for robustness?\n\n- In practice, how do I choose n? How sensitive is the method to the choice of n?\n\n\n\n\n\n\nThe manuscript is well written and clearly structured.\n The manuscript, which is well written, proposes a new method for performing robust logistic regression, which is essentially based on spotting observation with large leverage. The bit that impressed me the most is how the authors turn the problem of finding the observation contributing the least to the objective function into a linear programming problem.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper introduce a robust logistic regression under arbitrary outliers. As the title of the paper implies, the method is robust to outliers, and this point is investigated in terms of theory as well as experiments. \n\nThe theoretical result to outliers (theorem 1 and remark 1) is impressive. The estimation algorithm is also nice, which consists of just thresholding and linear programming. However, in practical points of view, the method includes some weakness, which is that the user has to set the number of outliers in advance. This makes the method weaker in practice because it is difficult to know the number in advance. If author(s) discussed ideas to overcome this problem, the paper would be better. In addition, the author(s) should mention how the performance varies if the incorrect number of outliers were set in the method. \n The paper is well-written, and some theoretical result is impressive. I think that this work has an impact on machine learning community.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
