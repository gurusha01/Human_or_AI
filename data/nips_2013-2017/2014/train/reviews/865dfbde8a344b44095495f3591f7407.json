{"title": "Stochastic variational inference for hidden Markov models", "abstract": "Variational inference algorithms have proven successful for Bayesian analysis in large data settings, with recent advances using stochastic variational inference (SVI). However, such methods have largely been studied in independent or exchangeable data settings. We develop an SVI algorithm to learn the parameters of hidden Markov models (HMMs) in a time-dependent data setting. The challenge in applying stochastic optimization in this setting arises from dependencies in the chain, which must be broken to consider minibatches of observations. We propose an algorithm that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects. We demonstrate the effectiveness of our algorithm on synthetic experiments and a large genomics dataset where a batch algorithm is computationally infeasible.", "id": "865dfbde8a344b44095495f3591f7407", "authors": ["Nick Foti", "Jason Xu", "Dillon Laird", "Emily Fox"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The authors adapt SVI of Hoffman, Blei, Wang and Paisley (2013) to hidden\nmarkov models (HMM).\nPrevious applications of SVI consider minibatches of complete data items.\nSVIHMM differs in that minibatches consist of minibatches of subsequences.\nThis is an interesting idea, where some external observations are used to\nseed the messages on each side of the subsequence.\nThe authors propose a heuristic for determining how many observations need to be added\non each side of the subsequence. It is a bit surprising that this works since\none might imagine there might be long term dependence in the messages.\nDoesn't GrowBuf terminate immediately if S^new = S^old?\nIs there a relationship between the optimal tau and the second largest\neigenvalue of A?\n\nThe paper quality itself is low, unfortunately: there are missing figures and tables from\nexperiments (e.g., table 4 line 355 and the timing experiments).\nThe introduction and review is rather long: it is not until page 4 that we get\nto the material of the paper. Consequently, too much is placed in the\nsupplement. GrowBuf would appear key to understanding the paper, but is only\npresented in the supplement.\nI find it hard to assess the FBR rate quoted: DBN has a FDR of 0.999038, whilst\nthis method yields 0.999026. This difference appears really rather slight,\nbut perhaps SVIHMM is faster than the DBN result? No results were presented.\nAlso, how much noise is there in the estimate of the FDR? A mostly straightforward adaptation of SVI to HMMs with a heuristic for training on subsequences. Presentation is incomplete.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents stochastic variational inference (SVI) for HMMs. Unlike LDA, in HMMs the chain-based dependence structure poses problems for SVI. Subsampling in each iteration cannot assume independence between series data, and skipping any data outside the chain can lead to errors. (Network models present somewhat different difficulties, replacing chain-based with pair-wise dependence. Subsampling in this setting has been handled in prior work (Gopalan et al. (2012)).)\n\nIn this work, the authors show how to scale the noisy subsample gradients for unbiased estimates and bound error induced by updating only variables corresponding to subchain samples. They augment subsampled subchains with extra observations to achieve this. They argue that this buffering and the forward-backward computations can be done incrementally, and therefore efficiently. They apply their algorithm to a large genomics data set, and demonstrate similar performance to VB.\n\npros:\n\nthe paper is written well; contributions through developing an SVI algorithm are clear and significant.\n\ncons:\n\n- my main criticism of this work is that a comparison to Johnson et al. (2014) is not provided. That paper develops an algorithm for large collections of independent series. surely, the SVI algorithm in your paper makes better (and correct) assumptions; but it's hard to know if these additional complexities are worth it, in practice, when running on real data. comparison to an EM algorithm is insufficient, as your gains could derive from using SVI. \n\n- You mention \"significant gains\" in computation for the genomic data compared to the EM algorithm, but no runtime evidence is provided. \n\n- computational complexity of the batch vs. SVI algorithm is not mentioned (although it's evaluated); i assume the SVI is still has quadratic per-iteration complexity in terms of states? \n\n- what is the worst-case cost when L is set poorly? the algorithm requires a good choice of subchain lengths (L) and number of chains per mini-batch. this is not unusual for SVI algorithms, where mini-batch size can plays a key role (Hoffman et al., (2010)). however, choosing a good L is critical, as it may cause the \"buffering\" algorithm to add too many observations for a good approximation. the authors state that this is not the case, but it's not convincing, as it's in the case of simulated data. \n The paper presents a thorough development of SVI for the HMM model, that respects the chain-based dependencies in long time-series data. Prior work has dealt with somewhat different challenges posed by dependencies in network data; or prior work has simply treated considered only collections of independent time-series. The authors demonstrate improvements over the EM algorithm on a large genomics data set, and study decisions within the algorithm using simulated data.One drawback is they have not presented a comparison with prior SVI algorithms (Johnson et al. (2014)) for collections of independent time-series data.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper provides an innovative method for solving HMM models using SVI. The algorithm samples subchains in sequence data, but also addresses the issue of dependency breaking at the edges of subchains. Specifically, this problem is resolved by expanding the scope of a buffer around the target subchain until the subchain posteriors, across different length buffers, converge. The experiments show that their method is much more efficient than the existing methods and applicable to the real data. This paper provides insights for applying online learning algorithms to time-dependent models. \n\nMinor comments:\n\nEquation (1), p(y1|x1) is mising phi parameter\n\n\nLine 355 \u201c Table 4\u201d -> Table 1\n\nTable 1, What\u2019s the predictive log-likelihood in this experiment? What\u2019s the held-out percentage? What\u2019s the setting of hyper-parameters in this case? Why was ||A - A0||_F not compared as well in this experiment? The error is hard to tell in the log-predictive case.\n\nFor all these experiments, the author only mentioned the hyper-parameter setting for k, what about other hyper-parameters?\n\nPage 8\nIn Human chromatin segmentation experiment, what\u2019s the runtime for DBN? An important advance in dealing with large scale HMM inference.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
