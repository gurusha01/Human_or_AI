{"title": "Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning", "abstract": "The combination of modern Reinforcement Learning and Deep Learning approaches holds the promise of making significant progress on challenging applications requiring both rich perception and policy-selection. The Arcade Learning Environment (ALE) provides a set of Atari games that represent a useful benchmark set of such applications. A recent breakthrough in combining model-free reinforcement learning with deep learning, called DQN, achieves the best real-time agents thus far. Planning-based approaches achieve far higher scores than the best model-free approaches, but they exploit information that is not available to human players, and they are orders of magnitude slower than needed for real-time play. Our main goal in this work is to build a better real-time Atari game playing agent than DQN. The central idea is to use the slow planning-based agents to provide training data for a deep-learning architecture capable of real-time play. We proposed new agents based on this idea and show that they outperform DQN.", "id": "8bb88f80d334b1869781beb89f7b73be", "authors": ["Xiaoxiao Guo", "Satinder Singh", "Honglak Lee", "Richard L. Lewis", "Xiaoshi Wang"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Review of submission 1706:\nDeep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning\n\nSummary: \n\nA fast, deep NN is trained to play ALE Atari games, where the teacher is a high-quality, but slow, traditional game planner. This works better than a recent method [19] (here called \"DQN\") using temporal difference-based reinforcement learning for a deep NN function approximator with the same architecture.\n\nComments:\n\nInteresting work. I like the simplicity of the basic approach. It is good that somebody implemented this. \n\nAbstract and text are rather verbose though - I suggest to greatly shorten this, maybe in the style of the brief summary above.\n\nText on previous work: \"Over the last decade, deep learning (e.g., [13, 12, 18, 8]; see [7] for a survey) has emerged as a powerful technique for learning feature representations from data (again, this is in a stark contrast to the conventional way of hand-crafting features by domain experts).\"\n\nThe text above is rather misleading, and its references are rather unbalanced - they almost exclusively refer to recent papers from a few related groups, without pointing out that successful deep learning goes back much further. For example, the cited \"survey [7]\" focuses on certain results since 2006. However, deep learning of feature representations in neural networks (and similar systems) is much older - a recent comprehensive survey http://arxiv.org/abs/1404.7828 summarizes deep learning methods since 1965.\n\nGeneral recommendation:\n\nThe submission is interesting although it confirms what one might expect: slow but good traditional game players can train much faster deep networks in supervised fashion to outperform similar deep networks trained by more general reinforcement learning methods. Publishable, provided a more balanced account of deep learning history is provided. \n\n\n\n\n\n\n\n The submission is interesting although it confirms what one might expect: slow but good traditional game players can train much faster deep networks in supervised fashion to outperform similar deep networks trained by more general reinforcement learning methods. Publishable, provided a more balanced account of deep learning history is provided.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Paper proposes to use neural networks (generic function approximators)\nto learn mapping coming from UCT. UCT is a Monte Carlo method, which\nconsiders multiple trajectories of game. Monte Carrlo methods are\nexpensive due to consideration of exponential number of trajectories.\nNeural networks trained on such mapping allow to retrive results\nwithin orders of magnitude faster time.\n\nPaper is difficult to understand due to many abbreviations. However,\noverall concept is quite simple. It seems that this technique could be\nused to speed up any Monte Carlo players, but paper describes only\nresults for UCT. It would be nice, if you would show for other Monte\nCarlo methods that this approach works.\n\nPlease add some mathematical equations to make paper more clear.\n\nBTW: Would it be possible to train a single model to play all the\ngames ? Would it help across games ? Results are compelling, however presentation can be improved. Authorsshould include a some mathematical formulas, and fix figures (e.g.figure number 4 is quite to read, and understand).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary.\n\nThis paper studies a number of variations on the topic of training a deep network using data generated by a Monte-Carlo Tree Search (MCTS) agent. The paper focuses on the Atari 2600 platform and is motivated by the observation that, while MCTS performs extremely well on Atari 2600 games, it is also too computationally expensive to be used in a realistic setting. The authors provide empirical results on a number of Atari 2600 games.\n\nOverall.\n\nTo me, the main contribution of this paper is to propose to ``compile'' the UCT value function (or policy, according to the algorithm used) into a deep network. The paper is clear, and the results of good quality, but the work lacks in significance. While there are some nice results -- and performance improvements on all but one game -- I feel the topic is simply not sufficiently explored. There is little insight gained into how these results might carry over to other domains, or significant algorithmic improvements that could result from this work.\n\n\nA few major hurdles:\n . The UCC-I algorithm, described here, is very reminiscent of the DAgger algorithm of Ross et al. (2011). It would probably be good to discuss the relationship. Do we expect better sampling guarantees than their approach?\n . Are UCR/UCC really valid competitors to previous learning approaches? They still require full access to the simulator.\n . The fact that the empirical results rely on game-specific tunings somewhat devalues said results. Are the results so different when a uniform environment parametrization is used?\n\n\nThere are a number of research directions which I believe could improve the paper:\n . Further studying how well a policy/value function can be summarized into a deep network. Which is easier? Your results hint that it is better to summarize the policy. Does this tell us something about the nature of Atari 2600 environments?\n . What about performing regression on the empirical return, rather than the UCT value? Is there a sense in which one is better than the other?\n . The UCT agent doesn't use \"features\", but these might emerge from the \"policy compilation\". A related idea was studied by Cobo et al. (2013) in learning features from expert data. Can something like this be investigated here as well? \n . How is partial observability handled? The optimal stimuli plots hint at some of this, but it still seems an understudied question.\n\n\nMinor points:\n . line 120: \"two broad classes of approaches\": this seems to suggest no other way would be valid. Can this be rephrased? \n . line 136-143: it would be nice if the related work section discussed how the non-deep network features differ from the new stuff. For example: is the main distinction that the visual features are learned, or that we are using a deep architecture?\n . line 136: I believe cite [4] is incorrect in the bibliography. Other cites also seemed incorrectly numbered. Overall an interesting idea. What I find missing most from this paper is a main thesis -- a cogent theory to be investigated. A number of decent results are provided, but these would need to be studied more thoroughly to warrant acceptance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
