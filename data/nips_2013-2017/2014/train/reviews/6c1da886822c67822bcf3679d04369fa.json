{"title": "Controlling privacy in recommender systems", "abstract": "Recommender systems involve an inherent trade-off between accuracy of recommendations and the extent to which users are willing to release information about their preferences. In this paper, we explore a two-tiered notion of privacy where there is a small set of <code>public'' users who are willing to share their preferences openly, and a large set of</code>private'' users who require privacy guarantees. We show theoretically and demonstrate empirically that a moderate number of public users with no access to private user information already suffices for reasonable accuracy. Moreover, we introduce a new privacy concept for gleaning relational information from private users while maintaining a first order deniability. We demonstrate gains from controlled access to private user preferences.", "id": "6c1da886822c67822bcf3679d04369fa", "authors": ["Yu Xin", "Tommi Jaakkola"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper proposes a privacy-preserving mechanism for recommender systems\nwhich are based on the popular matrix factorization. Specifically, the\nauthors propose splitting users into disjoint groups of public and private\nusers. Private users do not share any information (ratings) with the\nsystem while public users share all of their information. The authors show\nthat under certain technical conditions one can bound the estimation\naccuracy of the item features based on the number of observed ratings.\nThis estimate can further be used to bound the reconstruction error of the\nprivate users. The authors further demonstrate the empirical performance\nof their approach using the Movielens 10M dataset.\n\nPrivacy in recommender systems is an important problem that we are only\nbeginning to explore. As far as I know most formal previous approaches\nrely on differential privacy. This paper considers that even the\nrecommender engine is not safe. This seems like a reasonable practical\nsetting. In that setting the authors develop an interesting framework.\nThe formalism developed in this paper is clear and seems sound as are the\nderivations. The results (bounding the error of the item factors and\nconsequently the errors of the reconstruction) are interesting. It would\nbe nice to give a bit more intuition about Theorem 3.5.\n\nOverall the paper is well written and easy to follow.\n\nThe experiments are also reasonable. They demonstrate what is needed and\nthe comparisons to other methods is reasonably convincing.\n\nIn the first set of experiments, it would be good to clarify what the\nlabel \"Percentage of Users\" mean. I understood it to mean the percentage\nof all users that were public.\n\nThe second set of experiments uses 100 public users and up to 400 private\nusers. It is a bit unclear why the authors are not reporting results with\nmore private uses (the dataset contains 10K users). What happens in that\nsetting? Do any of the DP methods (especially LAP eps=5) reach similar\nperformance as PMC and PMD. Was there a reason for stopping at 400 private\nusers? It would be good to show that PMC and PMD do well in a less\nsynthetic setting. It would also be nice to provide results in the case\nwhere the private users aren't the ones that have necessarily consumed the\nmost items.\n Good paper which introduces an interesting way to preserve privacy inmatrix-factorization-based recommender systems. Both theoretical analysisand empirical results seem sound.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper addresses the high sparsity problem of user-item ratings in recommendation systems, when applying matrix factorization technique. Some users own a large number of ratings (called public users in the paper), whereas many users have very few ratings (called private users). A main reason for the latter case is that such private users intend to protect their privacy.\n\nFirst, The authors provided theoretical guarantees of reasonably good performance of recommendation systems, when only a small set of public users are available. Then, they defined mechanisms for private protection for both discrete and continuous rating values to exploit private users in order to improve the models initially built on public users' ratings. The mechanisms add private users by maintaining the second order statistics of the user ratings.\nFinally, they tested the proposed private preserving mechanisms on the MovieLens 10M data set. The experimental results confirmed the promised guarantees and showed improvements by exploring private users.\n\nThis paper is clearly written. The novelty of the paper lies in the theoretical proof of the performance guarantee and the definition of the private protection mechanisms. Experiments are somewhat limited A paper with novel ideas, supported by proofs and experimental results", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes an EM-like algorithm for matrix completion applied to recommendation systems; parameter estimation is constrained in order to maintain privacy guarantees. The authors modify the typical matrix completion with trace-norm regularization to only estimate the item features.\n\n\nQuality:\nThe paper illustrates a nice use of privacy aware learning for the application of recommendation systems. They show that a small number of public users having a large number of ratings can provide sufficient overlap with private data to enable good accuracy. They use the private data to estimate covariances, while keeping a particular marginal distribution that helps maintain privacy.\n\nClarity:\nThe paper is well written. A few things could have been elaboarated:\n- It's not quite clear how their method compares to previous methods, e.g. [10], either experimentally or in terms of privacy guarantees.\n- It would be nice to show summary statistics/plots of the marginal distributions to help illustrate the affects of their technique.\n\nSignificance:\nThis paper belongs to an important class of algorithms that allow one to choose between privacy and accuracy. If data privacy continues to be in the public spotlight, this paper could be a nice addition to that field.\n\nOriginality:\nTo this reader's knowledge, their approach is novel, borrowing from common techniques in privacy aware learning. The paper illustrates a nice application of privacy aware learning to recommendation systems. Further experiments would strengthen the reader's understanding of how the algorithm performs, whether it meets its privacy goals, and how it compares to previous methods.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
