{"title": "Bayesian Nonlinear Support Vector Machines and Discriminative Factor Modeling", "abstract": "A new Bayesian formulation is developed for nonlinear support vector machines (SVMs), based on a Gaussian process and with the SVM hinge loss expressed as a scaled mixture of normals. We then integrate the Bayesian SVM into a factor model, in which feature learning and nonlinear classifier design are performed jointly; almost all previous work on such discriminative feature learning has assumed a linear classifier. Inference is performed with expectation conditional maximization (ECM) and Markov Chain Monte Carlo (MCMC). An extensive set of experiments demonstrate the utility of using a nonlinear Bayesian SVM within discriminative feature learning and factor modeling, from the standpoints of accuracy and interpretability", "id": "b7892fb3c2f009c65f686f6355c895b5", "authors": ["Ricardo Henao", "Xin Yuan", "Lawrence Carin"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The authors \"kernelize\" a recent Bayesian SVM by use of Gaussian Processes. For the resulting hierarchical GP model two forms of approximate inference are proposed: An MCMC sampler and an EM algorithm.\nA sparse (FITC) approximation is proposed as a further speedup.\nAnother contribution is a GPLVM model for the Bayesian kernel SVM.\n\nIn most places the paper is fairly clear in terms of writing.\n\nConceptually, most of the proposed extensions to the Bayesian SVM are pretty standard. But together they yield an interesting set of contributions.\nDue to the large number of different contributions and extensions, each of these falls a bit short in terms of experiments. Despite assessing prediction quality on a range of data sets, some additional empirical validation beyond bare prediction errors and timing would be nice to assess properties of the model in terms of modeling and inference. The topic and the proposed Bayesian kernel SVM are interesting and fairly relevant. I would have wished for some better intuition and empirical assessment of the properties of the model and differences to related models.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The author(s) present a method for combining the Bayes/GP and max-margin approaches to learning, initially by extending earlier work of [5]. They develop a detailed model combining Bayesian methods and SVM-related methods in a way that is clear, comprehensive and novel. They obtain algorithms for inference and prediction based on several statistical methods, including an algorithm incorporating a factor model, and give extensive experimental results. \n\nQuality\n-------\n\nThis is an excellent paper, presenting a new and apparently powerful collection of algorithms which seamlessly and elegantly integrate several state-of-the-art methods. While attempts to produce Bayesian versions of the SVM have a considerable history --- and I was somewhat surprised that no mention was made of the work of Peter Sollich et al., which I believe was the earliest --- this seems a cleaner and more satisfactory approach than those I have previously seen.\n\nClarity \n-------\n\nThe paper is clear, concise and well-written.\n\nOriginality\n-----------\n\nThe work demonstrates considerable originality.\n\nSignificance\n------------\n\n A very good paper indeed.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper mainly extends the Bayesian linear SVM in [5] to a nonlinear version, and further combines the Bayesian nonlinear SVM with factor models.\nThe extension from linear to nonlinear however is quite trivial by simple adopting the standard kernel tricks. The resulted nonlinear version involves more complicated inference problem since it will also learn the kernel function parameters.\nThe combination with factor models is produced by taking the two objectives together, while kernels are produced on the factor representations.\n\nThere is not much novelity in terms of model extension and combination strategies. The overall learning problem is in fact a quite complicated non-convex optimization problem.\nUnder the probabilistic Bayesian framework, some inference procedures are introduced to perform learning but there is no analysis about the complexity of the overall inference procedure.\n\nThe experiments are limited to using Gaussian kernels. Is it possible to use other types of nonlinear kernels? Will it affect the inference algorithm?\n\nThe datasets used in the experiments are too small (see Table 1). Large scale experiments need to be conducted.\nMoreover, the authors only compared the proposed approach to SVM and GPC methods.\nConsidering the tasks addressed in this paper are simple binary classification tasks, why not compare to more advanced state-of-art methods?\n\nThe authors motivate the work from the perspective of discriminative feature-learning models, which is a very general topic.\nI do not feel related works on this topic have been sufficiently discussed in the related work section.\n This paper extends previous works from linear to nonlinear models.The experiments are insufficient to demonstrate the efficacy of the approach.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes an extension of the previous Bayesian formulation of conventional Support Vector Machines (SVMs). The previous Bayesian formulation proposed a likelihood of the form of Gaussian mixture, and the authors extend the formulation using the priors on the scale-mean parameter. This formulation results in a skewed Laplace likelihood which is different from the previous hinge loss. In addition, the Gaussian process extension of the Gaussian prior on the weight vectors presents a nonlinear SVM formulation. Two different optimization method is proposed for optimization.\n\nThe paper shows a nice example of extending the conventional SVM framework to a Bayesian setting using a well-defined formulation with prior distributions. According to the authors, the previous work [4] showed a connection between an infinite Gaussian mixture formulation and a hinge loss likelihood producing the SVM solution, but the formulation was improper due to the flat prior. The authors show how the formulation can be extended and how the inference can be performed using a well-defined formulation. The results show how the likelihood changed from the hinge-type likelihood to the skewed Laplacian. The explanation is clear and I enjoyed reading the manuscript. One thing I did not understand is the derivation of the predictive distribution in Eq. (11). Does the supplementary material include the derivation? I want a detailed derivation of Eq. (11) either in the paper or in the supplementary if the paper is accepted.\n The paper is in general clear and the proposed formulation is correct. This paper can be a nice material for most of NIPS readers showing an example of Bayesian formulation of SVM.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
