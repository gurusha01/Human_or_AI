{"title": "Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics", "abstract": "We present a policy search method that uses iteratively refitted local linear models to optimize trajectory distributions for large, continuous problems. These trajectory distributions can be used within the framework of guided policy search to learn policies with an arbitrary parameterization. Our method fits time-varying linear dynamics models to speed up learning, but does not rely on learning a global model, which can be difficult when the dynamics are complex and discontinuous. We show that this hybrid approach requires many fewer samples than model-free methods, and can handle complex, nonsmooth dynamics that can pose a challenge for model-based techniques. We present experiments showing that our method can be used to learn complex neural network policies that successfully execute simulated robotic manipulation tasks in partially observed environments with numerous contact discontinuities and underactuation.", "id": "6766aa2750c19aad2fa1b32f36ed4aee", "authors": ["Sergey Levine", "Pieter Abbeel"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Summary:\nThe paper presents a sample-efficient policy search algorithm for large, continuous reinforcement learning problems. In contrast to existing model-based policy search algorithms, the approach presented in this paper tries to learn local models in form of linear Gaussian controllers. Given the information (rollouts) from these linear local models, a global, nonlinear policy can then be learned using an arbitrary parametrization scheme. The so-called Guided Policy Search approach alternates between (local) trajectory optimization and (global) policy search in an iterative fashion. In their experiments, the authors show that the approach outperforms various state-of-the-art Policy Search methods, e.g., REPS, PILCO etc. Experiments where conducted in (mostly 2D) dynamics simulations involving the continuous control of multi-linked agents. \n\nQuality:\nThis is a well-written paper that addresses a topic which is relevant to the NIPS. Policy search algorithms have gained significant popularity in recent years and the work presented in this paper adds various important and interesting insights. Training a global model from locally linear models in a supervised fashion is an excellent idea! The results presented in the experiments section show that the approach can outperform well-known PS methods. However, it is at times difficult to judge the quality of the method since the proposed experiments are new and (to the best of my knowledge) have not been used by other researchers. Deisenroth et al. showed that PILCO can learn the \"Cart-Pole Swing-up\" task within a few trials. Can GPS do the same? I also wonder whether the local models can influence each other during dynamics fitting. For this not to happen, you need to ensure that the rollouts are only generated in the timeframe in which a particular local model is the expert. After reading the paper several times, it was still unclear to me how this is achieved. Especially when training Neural Networks, conflicts between several local datasets can have an extremely negative effect. That being said, I really like the fact that different types of policy representations can be used. While they have been out of fashion for controls, this paper clearly shows that (trained the right way) neural networks can we powerful tools for continuous problems.\nClarity:\nAll sections of the paper are clear and understandable. The authors did a very good job explaining the reasons behind the each choice made in the development of the algorithm. I particularly like that information about practical/empirical choices (e.g. parameter values, how to speed up learning) is shared with the reader. \nOriginality:\nThe paper bears several similarities with well-known PS algorithms, in particular REPS. The use of KL divergence when learning policies was made popular by the REPS algorithm. Using supervised learning bears some resemblance to model-based RL. However, I think that these similarities are mostly superficial. At its core, the GPS approach differs significantly from exisiting PS approaches. In constract to model-based RL, the approach here learns the Policy in a supervised learning fashion rather than a model of the environment. Using local models has various beneficial effects, e.g. simpler training, stability of the system, convergence etc. \nSignificance:\nI think that the paper makes a significant and valuable contribution to the Policy Search and Reinforcement Learning community. While the approach builds upon similar, existing methods, it has various interesting new ideas. I recommend the publication of this paper.  This is a well-written paper that introduces a method for sample-efficient learning of policies in unknown environments. Turning policy search into a supervised learning problem is a very interesting new research direction and will hopefully lead to more insights into this problem.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary of the paper:\nThe paper proposes a trajectory optimization method derived from iLQG, where instead of using the linearization of a known model, the linearized dynamics is acquired from samples. In order to ensure proper convergence in iLQG a backtracking line search can be implemented in the forward pass to ensure that the new trajectory does not differ too much from the previous one. In this paper, since the dynamics is unknown the line search is replaced by the addition of a constraint on the KL divergence between the previous and the new trajectory distribution during optimization to constrain the change in the new trajectory. A GMM model is used as a prior for the sampled unknown dynamics. Moreover, a parametrized policy is learned through guided policy search using the trajectory optimization framework with a neural network to approximate the policy. Experiments on 3 different simplified robotics problem as well as comparisons with other methods are provided to support the claims of the paper.\n\nComments:\nThe paper is interesting and well-written and in general it addresses an important topic for robotics research. In terms of the novelty of the approach, the main contribution of the paper is to show how iterative LQG can be used in a model-free context while ensuring convergence by replacing the backtracking line-search with a KL divergence constraint. While this is an interesting approach, one could argue that this technical improvement is rather incremental given knowledge on iLQG methods and linear Gaussian approximation of the dynamics. The guided policy search approach is also interesting but is also related to previous work, for example the work of Mordatch et al. (RSS 2014). One good aspect of the experiments is that it nicely shows that the proposed approach converges faster than other methods, even without the use of GMM as a prior model.\n\nOne comment with the use of a GMM prior is that this prior somehow already constitutes a model and therefore it is expected that the proposed approach performs close to model-based trajectory optimization methods. Indeed, it is mentioned in 3.2 that large mixtures that modeled the dynamics with high details produced the best results. This is to be expected since this basically already provide a good dynamic model. One question that arises is how is this GMM constructed? (i.e. what is the method to sample the potentially high dimensional space while ensuring safety on a real robot?). Additional comments on the likelihood that such an approach could scale to more realistic robot models where such GMM construction would be extremely challenging would also be useful. \n\nAnother possible important limitation of the approach is that it already requires an example demonstration for the walking task, which is only a 2D walking task. It is well-known that 3D walking is much more difficult than 2D walking (where relatively simple feedback control solutions are known to exist for this type of walking, e.g. the work of M. Spong or R. Gregg or J. Grizzle on the topic). Having an example demonstration means that it is already possible to solve the problem in some sense. It would be interesting that the paper comments on this issue. It also raises the question on how this method can really scale to more complicated problems (e.g. 3D walking or swimming with more than 2 joints). Indeed, assuming that the GMM prior is precise, it is surprising that the optimizer does not find a walking solution since the iLQG method can find such solutions with 3D walking and many more DOFs.\n\nAnother comment concerns the comparison with iLQG for the tasks involving contacts. What is the contact model used in this evaluations and what is the contact model used by the iLQG algorithm? Tassa and colleagues have shown very good results with tasks involving contacts and many DOFs using iLQG, one of the reason residing in the choice of a contact model that is more \u201cfriendly\u201d with iLQG optimization techniques. For a fair comparison, the paper should compare the tasks using this contact model for iLQG. Overall the paper is interesting and well written and the comparison with other learning approaches is very useful. My main concerns are related to 1) the use of KL divergence for line search which is a rather incremental change of iLQG and 2) scalability issues of the approach for more realistic scenarios.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary:\n\nThe authors propose the combined use of trajectory optimization and policy learning for systems with unknown dynamics approximated by linear local models. The proposed trajectory optimization algorithm is based on DDP/iLQG methods. The local linear models are assumed Gaussian and obtained from samples returned by the previous policy solution. The paper introduces the iLQG-based method such that the approximated local models can be used. Optimization with approximated models is made possible by limiting the changes of the modified iLQG solution using KL-divergence as a constraint. The paper also presents the use of their trajectory optimization method within the framework of Guided Policy Search previously proposed in [11] such that richer parameterized policies can be learned by iteratively swapping between the trajectory optimization and the policy learning steps. Results are evaluated in simulation.\n\nQuality:\n\nThe paper is of high quality. It is clearly written. Sometimes the paper relies too much on prose (see comments below) and support for a few statements are not clearly found. The paper is of high interest for the robotics and robot learning community especially due to the \"unknown dynamics\" feature, however, large part of the merit is attributed to the fact that it builds upon the the contributions of [11, 12, 13]. The evaluation of the method is of high quality and extensive and several relevant state-of-the-art methods are selected for comparison. The simulated tasks are interesting and show in a clear way the differences in performance. \n\nClarity and Questions:\n\nCertain parts of the paper are not clear and although he supporting information may be somewhere in the paper it is not always easy to find.\n. Please, define the use of \"unknown dynamics\" and \"hybrid\" right at the introduction. Unknown dynamics inevitably induces the \"model-free\" image, but the paper does not seem to consider itself as a model-free method. At the same time, the idea of creating a model from samples is usually regarded as model-based (such as PILCO). I personally would not introduce the new term \"unknown dynamics\" but rather use something like \"local Gaussian models\" instead.\n. In line 52, \"parameterized policy never needs to be executed on the real system\" needs clarification. What I understand from this sentence is that you would use the Gaussian controller during the whole learning process, and only at the end learn the parameterized policy based on what the Gaussian controller found (?). In that case it seems that you are not using the GPS framework as the parameterized policy is not being influenced by the trajectory optimization and vice-versa.\n. Line 59, the conclusion \"...transforms a discontinuous deterministic problem into a smooth stochastic one\" is very strong but the intuition that lead to it is not convincing. Is there a better way to explain it? A related request: since simulations with contact are difficult to model and local models do not capture discontinuity the paper would become much stronger with experimental results on a tasks with real physical contact or impact. \n. Although the GMM for background dynamics distribution seems to be very beneficial (in lines 400-401 it even seems to be intrinsic part of the method), it is only in Section 3.2 that GMMs are firstly mentioned. Until this point the whole method is explained as if there is no GMM. This can be a little confusing as at this point I do not know if GMM is part of the method or GMM is an extra feature that improves the method. As it seems to be always a good thing, why not present the method as unknown dynamics approximated by a mixture of Gaussians?\n. How does the system behave under saturation? Any real physical robot will have actuation limits. Does it compromise the induction of Gaussian trajectory distributions if the commands from the controller are saturated?\n. The fact that the iLQG with a true model performs worse than your method (which builds on iLQG but uses approximated linear models) is counter-intuitive. How it is possible?\n\nOriginality and Significance:\n\nThe paper is incremental in relation to the works of [11,12,13] although the contribution it adds is significant. The idea of being able to optimize trajectories and design a controller for an unknown system dynamics is of high interest for the robotics community, which was not previously addressed by [11,12,13].\n\n\n\n\n This paper is, in general, well written and easy to understand. It addresses the ability to apply the GPS framework [11,12,13] on systems with unknown dynamics by creating local Gaussian models. Although the paper is incremental in relation to existing work, the whole idea is of high interest for the robotics community.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
