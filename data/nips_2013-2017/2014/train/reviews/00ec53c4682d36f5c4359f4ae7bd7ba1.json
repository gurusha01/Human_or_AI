{"title": "Two-Stream Convolutional Networks for Action Recognition in Videos", "abstract": "We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. We also aim to generalise the best performing hand-crafted features within a data-driven learning framework. Our contribution is three-fold. First, we propose a two-stream ConvNet architecture which incorporates spatial and temporal networks. Second, we demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance in spite of limited training data. Finally, we show that multi-task learning, applied to two different action classification datasets, can be used to increase the amount of training data and improve the performance on both. Our architecture is trained and evaluated on the standard video actions benchmarks of UCF-101 and HMDB-51, where it is competitive with the state of the art. It also exceeds by a large margin previous attempts to use deep nets for video classification.", "id": "00ec53c4682d36f5c4359f4ae7bd7ba1", "authors": ["Karen Simonyan", "Andrew Zisserman"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Summary:\nThis paper proposes a model for solving discriminative tasks with video inputs. The\nmodel consists of two convolutional nets. The input to one net is an appearance\nframe. The input to the second net is a stack of densely computed optical flow\nfeatures. Each pathway is trained separately to classify its input. The\nprediction for a video is obtained by taking a (weighted) average of the\npredictions made by each net.\n\nThe model is evaluated on two datasets. The results are impressive and match\n(or come close to) the state-of-the-art methods which use intensively\nhand-crafted features.\n\n\nStrengths:\n- This model is a simple application of convolutional nets that gets good results.\n- Previous deep learning models have not tried to use optical flow (or other\nhand-crafted features) for vision tasks, preferring to learn all features from\npixels directly. This paper shows that at least for action recognition, optical\nflow fields are a useful input representation. This is an important contribution.\n\nWeaknesses:\n- The model does not take into account the overall sequence of\nactions over the entire video but only models fixed length (L=10) adjacent frames.\n\nQuality:\nThe experiments are well-designed. It would be more insightful if the authors also include some analysis of the error modes, for example, are their some classes or group of classes that the model is unable to classify well ? Is it possible to characterize the kind of videos on which the model does not work well ?\n\nClarity:\nThe paper is clearly written. The model is well explained.\n\nOriginality:\nThe application of a convolutional net to optical flow features is novel.\n\nSignificance:\nThis approach could have a significant impact on the research in using videos and motion for various vision problems. The model is a simple application of convolutional nets to video data and gets very promising results. The use of optical flow features as inputs to a conv net is a novel contribution.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a new neural network architecture for classifying videos of human actions. The model combines the predictions of two convolutional neural networks: one trained on single video frames and the other trained on short sequences of dense optical flow images. The model significantly outperforms other ConvNet-based approaches to action recognition and matches the current state-of-the-art on two standard video classification datasets.\n\nWhile the paper essentially applies the standard ConvNet image classification pipeline to a new kind of data (dense optical flow frames) and combines this with a single frame ConvNet, the experiments are thorough and the results are impressive. It was good to see several different ways of training a ConvNet on optical flow compared with the results clearly showing the benefits of using optical flow as input.\n\nI have only a few minor comments:\n- It is not clear why the spatial network does so much better than the models from [13] given that the architectures are similar.\n- It would be interesting to see a comparison of different sampling schemes for the frames fed into the spatial stream. This is a good empirical paper that will be of interest to anyone working on video classification with ConvNets.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes a few new methods for video classification with deep learning, namely two-stream formulation, and multi-task learning. The methodology, implementations, procedures were described in good detail. The paper is clear, well written, and presents a fair case for the proposed methods by showing good results. \n\nThese methods are novel. Although the paper adds improvements to the framework of [13], the two-stream framework is very interesting, because it shows that the optical flow component adds significant improvements. This is a result many researchers will be interested in. The paper is clearly presented, at the same time it seems to be split across multiple contributions as opposed to having a single theme. Finally, the results offer quite impressive improvements upon [13], and competitive with state-of-the-art. \n\nSome suggestions to improve the paper: \n1. It seems most of the significance is concentrated on the two-stream and incorporation of optical flow, the abstract and introduction could have a more focused theme. \n2. Missing citations in deep learning: \nLearning Hierarchical Spatio-temporal Features for Action Recognition with Independent Subspace Analysis. CVPR 2011. \nMissing citations in computer vision: \nEvaluation of local spatio-temporal features for action recognition. BMVC, 2010.\n The paper seems to offer significant practical contributions, with some interesting ideas that complements the main ideas in deep learning. I would recommend to accept the paper at NIPS 2014, conditional upon that the authors provide the suggested improvements.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
