{"title": "Multitask learning meets tensor factorization: task imputation via convex optimization", "abstract": "We study a multitask learning problem in which each task is parametrized by a weight vector and indexed by a pair of indices, which can be e.g, (consumer, time). The weight vectors can be collected into a tensor and the (multilinear-)rank of the tensor controls the amount of sharing of information among tasks. Two types of convex relaxations have recently been proposed for the tensor multilinear rank. However, we argue that both of them are not optimal in the context of multitask learning in which the dimensions or multilinear rank are typically heterogeneous. We propose a new norm, which we call the scaled latent trace norm and analyze the excess risk of all the three norms. The results apply to various settings including matrix and tensor completion, multitask learning, and multilinear multitask learning. Both the theory and experiments support the advantage of the new norm when the tensor is not equal-sized and we do not a priori know which mode is low rank.", "id": "6d9cb7de5e8ac30bd5e8734bc96a35c1", "authors": ["Kishan Wimalawarne", "Masashi Sugiyama", "Ryota Tomioka"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "In this paper, the authors study the problem of learning a tensor for the purpose of linear multi-task learning. The authors propose a new weighted version of a previously proposed tensor norm (called \"latent trace norm\") and show that the introduced rescaling yields better bounds on the excess risk as well as improved recovery performance on some datasets. The paper is well written and organized, and the proposed rescaling can potentially have a significant impact in practice, although a more extensive experimental evaluation would have been desirable. The technical results seem to be appropriate and correctly proven. Literature coverage seems to be sufficient. It may be worth noticing that all the tensor norms studied in this paper can be also seen as particular cases of a more general class of tensor penalties introduced in the recent paper \nA. Argyriou, F. Dinuzzo. A Unifying View of Representer Theorems. ICML 2014\n\n The paper carries out a study of some recently proposed tensor norms, and a new weighted version of one of them, illustrating the advantages of such reweighting both theoretically and empirically. The paper is well-written, organized, and of potential significance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In this paper, the authors proposed a new norm, called scaled latent trace norm, to relax the convex condition for the tensor multilinear rank. Both theoretical and experimental results are presented to show that the advantage of the scaled latent trace norm especially when the multitask learning task has inhomogeneous dimensions and there is no priori knowledge about which mode is low rank.\n\nStrengths about this paper:\n1) This paper is well written.\n2) The authors develop nice theorems that show the upper bound of the error between the empirical risk and the true risk in different scenarios in which overlapped trace norm, latent norm, and scaled latent norm are involved respectively.\n3) The authors also list all sample complexity for matrix completion, multitask learning, and multilinear multitask learning to compare the results.\n4) The authors provide corresponding experimental results to show that the scaled latent norm performs better when multitask learning involves inhomogeneous dimensions.\n\nSome aspects to clarify/improve:\n1) Latent trace norm is studied extensively in R. Tomioka and T.suzuki (2013), and the contribution of this paper is to develop \"scaled\" latent trace norm. The difference is not that big. \n2) From the results (Table 2), the sample complexity of the scaled latent trace norm is a little better than that of the latent trace norm. In tensor completion, it's hard to tell whether scaled latent trace norm is always better than that of the latent trace norm.\n3) The upper bound of all scenarios involved in three norms are all proportional to the number of training samples to the power of negative 1/2, which makes them not essentially distinguished. In general a good paper with nice theoretical results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "After reading the authors feedback, I think I have misunderstood the \"mode\" in the experiment, which is exactly what I was requested. I have modified my score correspondingly. \n\n===\n\nIn this paper the authors studied an approach for multi-task learning, which represent the (linear) models of the tasks into a tensor, and enforce a low-rank structure in the tensor to model the task relatedness, in which the knowledge is transferred among the tasks. The existing approaches such as latent trace norm failed to address the problem where the task dimensions are heterogeneous, and this paper proposed an improve version of latent trace norm, which normalizes along each dimension. The authors derived error bounds for the two existing tensor norms and the proposed one, relating them to the expected dual norm and showing the expected dual norm for the three norms. It is really nice to have a unified comparison over different norms as shown in the paper. Finally, the authors showed experimental results on synthetic data as well as two real-world data. Overall, this is a good paper with interesting theoretical analysis, and below are my comments of the paper. \n\nMy main concern of this paper is the usefulness of the tensor multi-task learning. Indeed we can have some \u201cfeatures X aspects X customers\u201d model to explore the relatedness, and the question is that: is it necessary to go beyond two-dimensional case. For any tensor model, we can collapse the model into a flat \u201cfeatures X task\u201d matrix and enforce a low rank to enforce the task relatedness. As one may argue this cannot capture some part of the information as did in tensor formulations, it rare to see how tensor can indeed increase the performance. Even if it helps a little bit, will people pay a large amount of additional time cost for such increase in performance? The lacking of the comparison to simple matrix trace norm in the experiments somehow exaggerated my concern. \n\nThe core novelty of this paper is the scaled version of a traditional latent trace norm, by adding a scaling factor to the unfolding along each mode. This suggests that we should treat each task (and the unfolding) differently due to some reasons. The idea is somehow related to another paper trying to address a similar problem, the \u201cMultivariate Regression with Calibration\u201d by Liu et. al. The authors may want to see if this paper can be related to that one. It is interesting if the authors can show some connection between the two. \n\nWhat is the algorithm used in this paper to solve the formulation with the new norm? Is it the same as the one used in the original latent tensor norm? What is the complexity of algorithm? In the case where scaled latent trace norm performs better than the original version, how much time does it take to converge? \n\nAs mentioned before, for multi-task learning, everything use tensor can also be used by flat trace norm. The authors should at least compare to these types of \u201cflat\u201d multi-task learning methods, e.g., the trace norm, in terms of performance and efficiency. Also, it is weird to see that the authors use different evaluation (explained variance) metric for the school data. There are many multi-task learning papers that report MSE/NMSE for school data. The authors can use MSE on school data as well to make the paper consistent.\n\nSome minor comments:\n1. Notation \u201cW_{(k)}^{(k)}\u201d in (2) and later on is very confusing. It has not been defined before its use. \n2. Typo \u201cTuring\u201d=>\u201cTurning\u201d in page 5, 10 lines from the bottom.  In the paper the authors proposed a scaled latent trace norm for multi-task learning and provided both theoretical analysis and some empirical evaluation. The paper looks good and I have some concerns on the impact of this paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
