{"title": "Orbit Regularization", "abstract": "We propose a general framework for regularization based on group majorization. In this framework, a group is defined to act on the parameter space and an orbit is fixed; to control complexity, the model parameters are confined to lie in the convex hull of this orbit (the orbitope). Common regularizers are recovered as particular cases, and a connection is revealed between the recent sorted 1 -norm and the hyperoctahedral group. We derive the properties a group must satisfy for being amenable to optimization with conditional and projected gradient algorithms. Finally, we suggest a continuation strategy for orbit exploration, presenting simulation results for the symmetric and hyperoctahedral groups.", "id": "f670ef5d2d6bdf8f29450a970494dd64", "authors": ["Renato Negrinho", "Andre Martins"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Summary: The authors re-explain regularization in optimization problems as a constraint of the type \"the parameters ${\\bf w}$ must belong to the convex set $O$\" where the convex set \"O\" is obtained as the convex hull of all the points of the form $g.v$ where $v$ is some fix vector, $g$ an element from a group and $.$ is a (linear) group action of element $g$ on vector $v$.\n\nMore concretely, their main contributions are as follows. (A) they explain how several regularizations can be obtained from their framework. For example, the ball associated to the L1 norm can be explained as the convex hull of the points obtained by flipping the sign and permuting the components of the vector $(1,0,0,..,0)$; (B) they show that given a seed $v$ and a group action associated to a group $G$, the notion of \"$w$ is a member of the convex set $O_G(v)$\" can be seen as \"$v$ is smaller than $w$\" under a pre-order; (C) they show that if $-v$ belongs to convex set $O$ then $O$ can be seen as the ball of an atomic norm (as defined in Chandra et al.); (D) they show that the L1-sorted norm equals the dual of the norm associated to the signed-pertumation orbitope; (E) they show how to reinterpret the main steps of conditional and projected gradient algorithms in the language of orbitopes and give a procedure to compute projections onto orbitopes. (F) they provide an heuristic algorithm that iteratively morphs the shape of the ball-norm associated to the regularizer, generalizing the idea of regularization paths.\n\nQuality: There are no technical mistakes in the paper. The idea of morphing the shape of the regularizer's ball-norm is the most interesting idea in my opinion. In this regard, it would be good if the authors could clarify the following. Homotopy methods build complete regularization paths that, after being computed, are used in combination with, for example, cross-validation to find the right amount of regularization to perform. I do not understand why the continuation algorithm stops \"at (the) point regularization is not having any effect\". It would also appreciate if the authors could say a few words about how the continuation algorithm would performs when $\\epsilon = 0$. In other words, the shape of the ball-norm is changed by its size is kept constant. Does the algorithm converge ?\n\nProposition 10 has a trivial pictorial explanation that might be good to include for the sake of clarity. In particular, taking the dual of the norm associated to singed-permutations corresponds to transforming the edges in the ball-norm of Fig. 1-right to vertices and transforming vertices to edges. This leads immediately to the ball-norm of the sorted L1-norm, that can be seen as the intersection of the ball-norms of all weighted L1-norms obtained by permuting the coefficients $w$.\n\nIn Prop. 3 the authors show that (under some conditions) orbit regularizers can be seen as atomic norms. It would be good to explain when/how atomic norms can be seen as orbit regularizers. \n\nClarity: The paper is overall very well written and clear. Here are a few minor things that can be improved. In Line 071 there is a parenthesis missing. In line 244-246 subscripts are missing in $m({\\bf w},{\\bf v})$, having them would be better. The quality of the pictures should be improved. Are pictures vector format? When I print the paper they look blurred. It would be very useful to have numbers in the references, [1], [2], etc. In Fig. 4, what is the scale in the y-axis referring too?\n\nSignificance: I find the idea of morphing the shape of the regularizer's ball-norm potentially interesting. Unfortunately, the numerical results do not clearly show that the continuation algorithm, as is, leads to significantly better performance. In Fig. 5 only one simple example is analyzed. Also, the results from Fig. 4 seem inconclusive. It would be good to report the number of iterations it takes for the continuation algorithm to converge. \n\nOriginality: The idea of re-explaining regularization using orbitopes is new. The idea of a continuation algorithm that iteratively morphs the shape of ball-norms in addition to scaling them is, as far as I can tell, new. The paper is well written and has no technical mistakes but most of the contributions consist of re-explaining previously introduced ideas in a different language (orbitopes).Their continuation algorithm is the contribution that most clearly allows one to actually do something in a different way (maybe leading to improved solutions over other algorithms) but, unfortunately, the algorithm comes with no guarantees and the numerical results are a bit lacking.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper provides a new insight on sparsity inducing regularizations. The authors imported the notion of group-majorization and showed that several well-known regularizations are recovered by properly introducing a group G and a seed vector v. The authors also provided gradient-based optimization methods and a regularization path heuristic.\n\nOverall, the paper is well-written and the main idea is clear. Characterization of sparsity inducing regularizations is important since sparsity is fundamental in recent machine learning, and this work would help us for deeper understanding.\n\nThe use of the orbitope would be an unique point of this research. However, the discussion on its utility seems not sufficient. As stated in Corollary 4, the orbitope and the atomic norm are relevant, and the premutahedra and sorted l1-norms discussed in the paper are both atomic norms. The orbitope and the atomic norm may be different in general, but how does this difference brings us to a new insight? In particular, can we find practically useful new regularizations with a help of the orbitope, in which any existing studies could not? Further discussion on this point will be beneficial. I think the regularization path heuristic in Section 6 would be one advantage of the orbitope. This heuristic allows us to adaptively tune the regularization, which will not be available with the atomic norm. This research provides a new way to interpret sparsity inducing regularizations using the orbitope. The content is interesting, although the advantage of the use of the orbitope is yet unclear.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper investigates a group-theoretic view of penalized likelihood functions in vector spaces. It is shown that many commonly used regularization terms, such as the L_1-, L_2-, and L_infty-norm, are specific instantiations of what is here called \"orbit regularization\". The generic view is shown to suggest also new reasonable forms of regularization, which admit nice properties when it comes to optimization using conditional and projected gradient algorithms.\n\nQuality:\n\nThe work is of very good quality. The relevant literature, both old and recent, is well cited. \n\nClarity:\n\nThe presentation is excellent. The notation is carefully chosen and adheres the usual conventions. The definitions are clear. The paper is pleasant and relatively easy to read, even if the subject is quite abstract. \n\nThe paper does not adhere to the NIPS section headings and referencing styles.\nLine 035: \"klowledge\". In Def 5, Prop 6, etc., consider placing the period (in bold) outside the parentheses, or just remove it.\n\nOriginality:\n\nIt seems that the proposed view is new to the machine learning community. However, much of the underlying mathematics does not look that new: for example, Propositions 11 and 12 are attributed to earlier works as old as Eaton (1984) and Hardy et al. (1952). The paper could better crystallize the its contributions. \n\nSignificance:\n\nThe significance of the work is unclear. In particular, it is not clear how the work advances the state of the art when it comes to the practice of machine learning. In general, it is nice to have several computationally efficient forms of regularization available. On the other hard, there is no objective way to select \"the best\" among them for a given learning problem. So, just generating new forms will not lead to very significant advancements in the field. \n Well presented, carefully typed, view that may contribute to better understanding of likelihood regularization in \"linearly flavored\" machine learning problems.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
