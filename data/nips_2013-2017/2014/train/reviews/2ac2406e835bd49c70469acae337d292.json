{"title": "Spectral Learning of Mixture of Hidden Markov Models", "abstract": "In this paper, we propose a learning approach for the Mixture of Hidden Markov Models (MHMM) based on the Method of Moments (MoM). Computational advantages of MoM make MHMM learning amenable for large data sets. It is not possible to directly learn an MHMM with existing learning approaches, mainly due to a permutation ambiguity in the estimation process. We show that it is possible to resolve this ambiguity using the spectral properties of a global transition matrix even in the presence of estimation noise. We demonstrate the validity of our approach on synthetic and real data.", "id": "2ac2406e835bd49c70469acae337d292", "authors": ["Cem Subakan", "Johannes Traa", "Paris Smaragdis"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Summary: The authors consider the problem of learning a mixture of Hidden Markov Models. The authors first suggest using a spectral learning algorithm to learn a set of parameters for a hidden Markov model, and then provide a method for resolving the permutation ambiguity in the transition matrix to recover it\u2019s underlying block-diagonal structure.\n\n\nMajor Comments:\n\n1. I found this paper to be very well written for the most part. The experimental results section could be fleshed out a bit. In particular the \n\n2. The authors rely on the fact that a mixture of Hidden Markov Models can be expressed as a single HMM. One of the main reasons, if not the primary reason, for learning a mixture of HMMs as opposed to a single HMM directly is that the block diagonal structure of the mixture of HMMs is sparse. This sparseness may be imposed on problems that are deemed to be too large to learn with a full model, especially if the problem is large and the amount of training data is small. However, in this paper, the author\u2019s algorithm relies on *first* accurately estimating the full HMM parameters, then reversing the permutation in order to find the block diagonal structure. For most real-world problems this seems completely untenable. Do the author\u2019s have suggestions for when their algorithm might be appropriate and when it might not be? \n\n3. Again, in practice, the recovered transition matrix is not likely to be sparse. Is there any way to guarantee that the transition matrix learned by the spectral algorithms suggested in this paper will be (close to) sparse? I suspect that there may be many HMM models with full transition matrices that are close to the sparse model with block diagonal structure that the authors are looking for. It is not at all obvious that the matrix that is learned via [1,2] will be sparse with very small amounts of noise.\n\n4. I am really worried that the experimental results do not accurately reflect the reality of noise in the transition matrix. For example, in Section 4.1, the authors apply their algorithm to a transition matrix that has been permuted by noise sampled from a Dirichlet distribution\u2026 but this is a paper about learning mixtures of HMM parameters, not just depermuting matrices. Shouldn\u2019t the authors have sampled observation data from a mixture of HMMs and then learned all of the parameters back from the *observed* data? This would give a much more accurate idea of how noise in observations and small quantities of data are actually reflected in the learned transition matrix, and how hard it is to depermute the matrix in the presence of this noise.\n\n5. The experiment in Section 4.2 is starting to get at what I was hoping for in the previous section, but there is nowhere near enough information to evaluate the quality of the experiment. In particular, I have no idea what the mixture of HMMs actually looked like. The actual parameters of the transition and observation matrix matter *a lot* for evaluating how easy or hard it is to learn the model from data. I would like to know more about the specifics of these parameters (the authors could put it in an appendix if they feel that space is at a premium).\n\n6. In table 1, why is EM initialized with spectral learning ever doing worse than spectral learning alone? Shouldn\u2019t EM only improve the results?\n\n\n\nMinor Comments:\n\n1. I believe that the subscript of \\Lambda in the bottom right corner of the first matrix in the proof in Section 3.2.1 should be K, not 2.\n\n2. Text and numbers are *way* too small in the figures.\n The paper has some interesting ideas and good theoretical results. I am worried that the method is less likely to work on real datasets in practice.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a spectral algorithm for learning a mixture of hidden Markov models (MHMM). It shows that an existing spectral algorithm for learning HMMs (Anandkumar et al., 2012) can be used off-the-shelf by formulating an MHMM as a special case of HMM. It proposes a stabilization-based solution for parameter permutation by introducing an assumption that each HMM's transition matrix has a single eigenvalue of value 1. \n\nThere seem to be some issues with notation. The authors definitely need to show more awareness of related work by providing references (e.g., initializing EM with spectral estimates is already explored in Chaganty and Liang (2013)). Experiments are quite basic, and I strongly recommend reporting results in a table format rather than the graphical format in Figure 4.\n\nThat said, the proposed scheme to dodge the problem of parameter permutation is interesting and intuitive. The authors also provide an analysis for the case with estimation noise, which leads to an intriguing usage of eigenvalues (Equation (7)) in the algorithm. \n\nComments: \n\n- I was initially confused by the terms in Corollary 1. It'd be good to define 1_J^T (as a row vector of length J filled with ones) and elaborate a bit on the structure of lim_{e->inf} bar{A}^e (that it's JK x JK block diagonal where the columns within each J x J block are the same). \n\n- Generally, please define each vector/matrix along with its dimensions when it's first introduced. It will immensely help the presentation. \n\n- I find Figure 4 very hard to read: please replace with tables. Also, I'm not sure why EM is performing so badly in Table 1. Typically, even with random initialization, with enough iterations it beats spectral methods (despite the local optima problem). A standard practice is to report the best (not average) accuracy of EM: please include that result. \n\n- Nit: It might help readers if the paper conforms to the established notation of Hsu et al. (2009). Namely, use T for transition matrices (not A), use n, m for numbers of observation and hidden states (not L, J). \n The paper proposes an algorithm for learning a mixture of hidden Markov models. This algorithm consists of an existing spectral algorithm for learning hidden Markov models followed by a novel stabilization step for undoing parameter permutation.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors present spectral method for learning a mixture of HMMs.\nThe algorithm would be a trivial variant of the standard Hsu, Kakade & Zhang\nalgorithm, except that the transition matrix recovered is an arbitrary permutation of\nthe true one, mixing the various components. The authors propose a depermutation algorithm\nthat seems to work reasonably well.\n\nOccasionally there seem to be a number or little typos. e.g.:\n\"For each e, identify of eigenvalues that contain all of the energy.\"\nPlease proof read more carefully.\n\nThe paper could do a better job of putting this work into context, perhaps relating to papers such as:\nChaganty and Liang. Spectral experts for estimating mixtures of linear regressions. In\nInternational Conference on Machine Learning (ICML), 2013.\n The authors present spectral method for learning a mixture of HMMs, addressing the key question ofwho to \"de-permute\" the results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
