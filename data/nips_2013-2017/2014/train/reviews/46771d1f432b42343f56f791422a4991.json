{"title": "On Communication Cost of Distributed Statistical Estimation and Dimensionality", "abstract": "", "id": "46771d1f432b42343f56f791422a4991", "authors": ["Ankit Garg", "Tengyu Ma", "Huy Nguyen"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper investigates the communication cost of distributed estimation for d-dimensional spherical Gaussian distribution with unknown mean and unitary covariance, where the joint distribution is assumed to be a product distribution of each coordinate. The authors generalize previous works on the one-dimensional case in [4] by proposing upper and lower bounds for d-dimensional data on two communication schemes, interactive and simultaneous communication settings, for achieving minimax squared loss. The results establish the tradeoffs between dimensionality and communication cost for distributed estimation. In addition, improved bounds are derived when the unknown mean is s-sparse.\n \nAlthough the model is somewhat restrictive (for example, the joint distribution is a product distribution of each coordinate and the covariance matrix is known), this paper offers new insights on the communication cost of distributed estimation and protocol design. The results are technically solid and several rule-of-thumb protocols have been proposed with an aim of achieving the communication lower bounds.\n\n\nA minor concern for this reviewer was the paper's weakness in presentation. First, the introduction is too lengthy and, second, there is no conclusion section. It is suggested that the authors refine the introduction part and provide a conclusion section to summarize the contributions. It was also somewhat irritating that the notations in the main paper and the supplementary file were not consistent, making it difficult to use the supplementary file for clarification of certain points in the paper. For example, Lemmas 1 and 2 in the paper are exactly the same as Lemmas 3 and 4 in the supplementary file. This paper studies bounds on communication cost in distributed estimation and several protocols are proposed to achieve the communication lower bounds. The authors provide solid analysis on the subject and the results are significant.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper explores the connection between dimensionality and communication cost in distributed learning problems. In this problem, the samples from the unknown distribution are distributed among m different machines. The goal is to estimate the mean at the optimal minimax rate while communicating as few bits as possible. This paper shows that in this setting, the communication cost scales linearly in the number of dimensions. Applying this result to previous lower bounds for one dimension in the interactive setting [4] and to the improved bounds for the simultaneous setting, this paper proves new lower bounds of \\Omega(md/ log(m)) and \\Omega(md) for the bits of communication needed to achieve the minimax squared loss, in the interactive and simultaneous settings respectively. Besides, this paper demonstrates an interactive protocol achieving the minimax squared loss with O(md) bits of communication. Given the strong lower bounds in the general setting, this paper initiates the study of the distributed parameter estimation problems with structured parameters. Specifically, when the parameter is known to be s-sparse, it shows a protocol achieving the minimax squared loss with high probability and with communication cost proportional to s rather than the dimension d of the ambient space.\n\nQuality: The authors need to assume that the readers have no knowledge on the problem in [4].\n\nClarity: In Section 2, definitions are missing for several notions:\n0. What is the definition of s-sparse?\n1. In the definition of R(\\hat{\\theta},\\theta), why the expectaion is over \\hat{\\theta},X,\\theta? How is the mean \\vec{\\theta}(Y) of Y obtained? Why is Y in {\\cal X}^n? Is there any example for transcript?\n2. I cannot follow below Private/Public Randomness. For example,\na. What is private and public randomness? What is a protocol? Give definitions to them. \nb. Why can the public randomness be shared among the machines before the start of the protocol?\nc. Why does the protocol work well on average over all public randomness?\nd. Why can the machine use private randomness to hide information from other machines in a protocol?\ne. The definition ``(\\Pi,\\vec{\\theta}) solves T(d,m,n,\\sigma^2,{\\cal D}_\\theta^d) with C and R.\n\nOiginality: I believe that new lower and upper bounds are derived for the interactive and simultaneous settings, and that the negative result is obtained. It is fair that the authors explain how difficult the derivation fom [4] is.\n\nSignificance: I do not understand why ''the communication cost scales linearly in the number of dimensions\" is a news. In fact, ituitively, this seems to be obvious while they mathematically derived the property. The authors need to explain why the result is different from what they expected. \n The authors should assume that the readers are not familiar with the material. Some introductory explanation could be used. I admit that the idea is novel.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper consider the problem of estimating the mean of a d-dimensional normal distribution with unit covariance, given nm samples available at m machines. The authors study the trade-off between communication complexity and dimensionality.\nSeveral results are derived:\n- Communication complexity cannot be reduced by processing jointly the various dimensions.\n- It can be reduced for sparse means.\n- Improved upper bound in the scalar case.\n- Improved lower bound under simultaneous protocols.\n\nI have a few minor suggestions:\n1) At the beginning \\theta is not a random variable, but it becomes a random variable after Definition 1. This is a standard device in minimax theory, but it is worth emphasizing the passage. Also, before definition 1, the authors introduce the conditional mutual information given theta. Strictly speaking makes sense only for theta a random variable (although taking a constant value with probability 1).\n\n2) In Corollary 3.1. The minimax rate for the stated problem is not d\\sigma^2/(nm). Thing of the case in which \\sigma^2 = 1000000, n = m = d = 1. Then the estimator \\hat{\\theta} = 0 can do better than that. The statement must be qualified.\n\n3) In Protocol 2, last two lines. I believe that Y_i in the argument of \\hat{\\theta}_i should not have subscript.\n A very nice papers with several interesting ideas.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
