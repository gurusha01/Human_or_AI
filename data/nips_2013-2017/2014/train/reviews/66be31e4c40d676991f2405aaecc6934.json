{"title": "Learning with Pseudo-Ensembles", "abstract": "We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.", "id": "66be31e4c40d676991f2405aaecc6934", "authors": ["Philip Bachman", "Ouais Alsharif", "Doina Precup"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The authors formalize the notion of learning with pseudo-ensemble, under which several existing learning methods can be explained, e.g., dropout in deep neural networks. The authors further present the PEV regularization, which encourages the robustness of learned model (activations of hidden units in different layers in the case of deep layered neural networks) under perturbation of the model space. Since no label information is used in the PEV regularization, it naturally generalizes to semi-supervised learning setup. The authors compare the method to several baseline methods under different learning scenarios, supervised, semi-supervised and transfer learning and show improvement. \n\nThe Pseudo-Ensemble notion generalizes previous work on robust learning w.r.t. perturbation in the input space (Burge & Scholkopf 1997, Chapelle et. al. 2000, Maaten et. al. 2013, Wager et. al. 2013). It is a straight-forwarding extension. The PEV regularization is interesting by itself, and is able to match the performance of dropout under supervised learning setup, and significantly outperform in the scenarios of semi-supervised and transfer learning. \n\nThe authors try to connect the PEV regularization to dropout in section 4.1. The pseudo-ensemble objective introduced in eq.(1) explains dropout in the limiting case. However, it is less clear from the writings that learning with the PEV regularization actually approximates dropout. It is not very convincing to draw the conclusion that discouraging co-adaption is the reason of success for both PEV regularization and dropout simply by verifying the performance (accuracy numbers) of the two model is similar. \n\nStarting from eq. (1) , a more natural formulation for the regularization would be to penalize the variance of distribution of the output layer rather than summing over all different layers from 2 to d as shown in eq. (3). Could the authors explain why it is done in the current formulation and how much performance deterioration would be observed if we drop the penalties on the hidden layers. \n\n\nSmall comment:\n1. Figure 1 is not explained in the text. \n\n2. Based on Table 2, even with the PEV regularization, DAE pre-training (PEV+) was still able to significantly improve the performance of the learned model, which seems to suggest that the PEV regularization is less effective in terms of utilizing unlabeled data.  The work introduces an interesting regularization for learning with multi-layer neural networks, motivated by dropout and pseudo-ensemble learning. The regularization can also be applied to other classes of models. The authors try to connect directly the PEV regularization to dropout in section 4.1, but the connection is not clear from the writing. Though a straight-forward method, the PEV regularization offers satisfying performance. It could be of interest to a subset of the community to learn about it.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In essence, this paper considers the generalization of dropout to other classes of models by perturbing the parameters of the corresponding source model. For instance, in a the simple case of a mixture model, one could add appropriate noise to the means of the components or to their covariance matrices. The paper introduces a regularizer to ensure robustness of the pseudo-ensemble of child models with respect to the noise process used to perturb the parent model and generate the child models. The general topic of the paper is of significant interest to the NIPS community and in my opinion worth presenting, although overall it is a somewhat straightforward generalization of dropout. The ideas are supported by several informative simulations on standard benchmark datasets.\n\nSome specific comments:\n\n1)The first two sentences are too vague to make sense, especially in relation to the role of the variables x and y. For instance, is one trying to derive approximations of p(x,y) that are conditioned on x?\n\n2) The statement \"Many useful methods could be developed....by generating perturbations beyond the iid masking noise that has been considered for neural networks\" is not correct. Even in the original paper by the Hinton group the noise was not iid since the dropout probability used in the input layer (data) was lower (e.g. 0.2) than in the other layers (e.g. 0.5). Furthermore, examples of non-iid cases were analyzed in reference [1] which should be augmented with, or replaced by, its more extended version (Artificial Intelligence, 210, 78\u2013122, 2014). This reference considers also other forms of noise, such as adding Gaussian noise to the activity of the units, and shows how they the fall under the same framework. This is highly related to the theme of this paper.\n\n3) This is a minor point but the author may want to consider changing the terminology. The term \"pseudo-ensemble\" is perhaps not ideal since \"pseudo\" has a slightly negative connotation, whereas the point to convey is that this is a new approach to learning in its own right.\n\n4) The remark \"While dropout is well supported empirically, its mode of action is not well understood outside the limited context of linear models\" is not entirely correct. In the non-linear case, the ensemble properties of dropout in deep non-linear neural networks are reasonably well understood, and so are its regularization properties, as described in the reference given above.\n\n5) This is another minor point but there are a few typos. The paper should be run through a spell-checker. See, for instance, the last line of page 6 (\"northe images in the target domain\").\n\n6) The paper is well supported by several informative experiments on different benchmark datasets.\n\n\n In essence this paper presents an incremental generalization of dropout. This is currently a hot topic for the NIPS audience. It is supported by a set of interesting simulations.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes the notion of a pseudo-ensemble, that of dependent child models ensembled together, which is an nice and unified way of several related techniques prominently including dropout. Then presented the pseudo ensemble variance regularizer and tested it with convincing empirical results. The presented technique seems much easier to apply to multi-layer, semi-supervised setting than the technique of [23].\n\nSome objections are that the notion of this pseudo-ensemble as an expectation is already in references [5] and [23], so the contribution of naming it and generalizing to multi-layer is not a significant leap. The authors made an effort of gaining more understanding by writing down the boosty and baggy forms, but nothing was really done with them and the connection to boosting is vague. Finally, while it is okay to introduce an regularizer and show that it is empirically successful, it would be nice if it corresponds to, or is approximating something that we understand. The structure of paper would have me believe that this is done, as sections 1-3 talks about a general framework, and section 4 should be testing it. Instead, section 4 introduces a new regularizer that has little to do with the previous sections. I would have given the paper a higher score based on its empirical strength and the simplicity of the proposed regularizer compared to the alternatives, if not for the fancy presentation that made it harder to read but did not add much insights. Please explain in the response if I am wrong on this and I am willing to change my scores.\n\nDetailed comments:\nthe formalism of f(x; \\xi) is much more general than the situation the paper seek to deal with: that of subsampling child models through \\xi. Without alluding to that \\xi represents something like dropout, this formalism does not mean much. Unfortunately, this seems hard to remedy without making the notations more complicated, so perhaps it is okay.\n\nLine 106 \"Note how the left objective moves the loss L inside the expectation over noise\"...\nthis contradicts the equation, where the loss L is outside the expectation over noise\n\nEq 2): baggy PE does not seem to make sense to me, are you summing over i? or is one of the expectations supposed to be taken over i as well?\n\nWhile there is some intuitive resemblance (final prediction is made by the sum of weak learners, though the sum is weighted in boosting), it is unclear to me how these boosty PE concretely connect to boosting. \nThe flavor of sequentially adding more classifiers is not there, nor is the boosting weights assigned to each example. Can you make this more concrete? If not, perhaps it would be good to say this is how you define boosty in the first place and the reader should not be looking for rigorous connections.\n\nIn line 129-140, you compared a few existing approaches and say that they only deal with the input space. I do not think this is entirely true. My perspective on this:\nA: [5] Learning with MCFs: non-convex (convex in some cases) lower bound by\nmoving the expectation inside the log but not inside the exp.\nB: [23] adaptive regularization: delta method (second-order central limit)\nexpansion (relies on 4th moments small). This has the advantage of giving an explicit regularizer that's interpretable. Non-convex.\nC: Wang and Manning, Fast Dropout: relies on central limit\ntheorem and sum of noise converging to Gaussian. Non-convex and deals with model noise as well as input noise.\nD: Baldi and Sadowski, Understanding dropout paper, analyzes geometric mean, also deals with model noise. \n\n\nEq 3): why is there a subscript on the variance notation? it is the same operation regardless of i (correction: not so in line 210: is it worth the subscript though). \n\nYou should perhaps call \\mathcal{V} scale-invariant variance penalty, so the reader does not think you are talking about the actual variance. The font helps, but you still said its variance. \n\nWhat is the motivation of this regularization method? can you show that this converges to [23] for linear models somehow? or can you provide more understanding by somehow deriving/approximate it instead of just defining it? This paper presented the pseudo ensemble variance regularizer and tested it with convincing empirical results. The PEV regularization technique seems much easier to apply to multi-layer, semi-supervised setting than the technique of [23]. However, the regularizer is unmotivated theoretically and very little is done with the conceptual discussions of boosting and bagging while the presentation would lead readers to believe otherwise.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary:\nThis paper proposes a regularization technique for neural nets where the model\nis encouraged to reduce the variance of each hidden layer representation over\ndropout noise being added to the layers below. This idea is generalized to\n``pseudo-ensemble\" models where other kinds of perturbations can be used.\n\nThe main contribution of this paper is the variance regularizer. Experiments\nare done on MNIST (supervised and semi-supervised) and NIPS'11 transfer\nlearning dataset (CIFAR-100, TinyImages) using standard neural nets with\ndropout perturbations. The authors also experiment with the Stanford Sentiment\nTreebank dataset using Recursive Neural Tensor Nets with other kinds of\nperturbations. The experiments show that this regularizer works the same or\nbetter than using the perturbations alone.\n\nStrengths-\n- The model gets promising results in harsh situations where there is very\nlittle labelled data.\n- The experiments are well chosen to highlight the applicability of this method\nto different models and datasets.\n\nWeaknesses -\n- Some parts of the paper seem somewhat superfluous. It's not clear what the\ndiscussion about Baggy/Boosty PE adds to the paper (assuming that the main\npoint is the variance regularizer).\n- Some crucial details about the experiments should be included. Those are\nmentioned below.\n\nThe authors should mention / discuss -\n(1) How many noise samples were used to compute the different variances ?\n(2) Was back-prop done through each dropped-out model or just the clean one ?\n\n(3) One of the major problems of dropout is that it slows down training. This\napproach probably further exacerbates this problem by requiring that one must\ndo multiple forward and (back ?) props per gradient update (with/without noise,\nor with different noise samples to compute the variance). It would be good to\nanalyze how much of a slow-down we get, if any, by making a plot of\ntraining/test error vs time (as opposed to number of epochs).\n\n(4) What was the stopping criterion for the semi-supervised MNIST experiments ?\nThe previous section mentions \"We trained all networks for 1000 epochs with no\nearly-stopping.\" Does that also apply to the semi-supervised experiments ? If\nyes, was it kept 1000 epochs even for 100 labelled cases ? It seems likely that\n500-500 or 1000-1000 nets would overfit massively on the labelled data sizes\nconsidered here, even with dropout, by the end of 1000 epochs for reasonable\nlearning rates. Is this true ? I think it is very important to know if early\nstopping with a validation set was needed because in small data regimes, large\nvalidation sets are hard to find.\n\n(5) If multiple (say $n$) forward and backprops are done per gradient update in\nPEV, would it be fair to compare models after running for a fixed number of\nepochs ? Wouldn't each PEV update be equivalent to roughly $n$ regular SDE\nupdates ?\n\n(6) For the semi-supervised case, did each mini-batch contain a mixture of\nlabelled and unlabelled training cases ? If yes, what fraction were labelled ?\n\n(7) Consider comparing with SDE+ in Table 1 ?\n\n(8) Was the same architecture and dropout rate used for SDE and PEV in Table 1\n? If yes, is that a fair comparison ? May be it's possible that for SDE, a\nsmaller net or the same net with higher dropout would work better ? It is clear\nthat PEV is a ``stronger\" regularizer, so we should probably also let SDE be\nstrong in its own way (by having higher dropout rate).\n\n\nQuality:\nThe experiments are well-designed. Some more explanations and comparisons, as\nasked for above, will add to the quality.\n\nClarity:\nThe paper is well-written barring minor typos.\n\nOriginality:\nThe variance regularizer is a novel contribution.\n\nSignificance:\nThis paper could have a strong impact on people working with small datasets.\n  This paper proposes an interesting way of regularizing models. The experiments are convincing, but the paper can be improved by adding some more details and clarifications.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
