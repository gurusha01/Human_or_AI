{"title": "Inference by Learning: Speeding-up Graphical Model Optimization via a Coarse-to-Fine Cascade of Pruning Classifiers", "abstract": "We propose a general and versatile framework that significantly speeds-up graphical model optimization while maintaining an excellent solution accuracy. The proposed approach, refereed as Inference by Learning or IbyL, relies on a multi-scale pruning scheme that progressively reduces the solution space by use of a coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with classic computer vision related MRF problems, where our novel framework constantly yields a significant time speed-up (with respect to the most efficient inference methods) and obtains a more accurate solution than directly optimizing the MRF. We make our code available on-line.", "id": "c7635bfd99248a2cdef8249ef7bfbef4", "authors": ["Bruno Conejo", "Nikos Komodakis", "Sebastien Leprince", "Jean Philippe Avouac"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The paper is aiming to speed up optimization for the of graphical model MAP problem. The authors suggest a coarse to fine approach and clearly motivate their work. The method suggested by the authors combines label pruning for each variable with sequential optimization of the pruned, yet finer problem. \n\nAs far as i can tell, the work is original and correct. One point i think was not clearly pointed out is whether the proposed method can be applied to graph structure with no apparent regularity (in contrast to grid graphs resulting from images).\n\nAnother interesting question i am curious about is whether this method could be used inside a learning procedure.\n\nGiven my limited knowledge of the related works it is hard for me to asses the significance of this work.  The papers proposes a coarse to fine method to accelerate MAP inference. The method seems interesting and sound,. In addition experimental result are also supportive.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The manuscript \u201cSpeeding-up Graphical Model Optimization via a Coarse-to-fine Cascade of Pruning Classifiers\u201d introduces a framework to improve MRF Optimization through a cascade of classifiers to progressively prune the search space while going from coarse to fine-grained models.\n\nThe newly introduced pruning scheme is novel, interesting and seems to provide good solution in practice while being faster than other approaches. There are several parameters pre-set (e.g., C = 10, 5 scales, rho). It is not clear, how robust the method is regarding the value of the parameters, but since they are set the same way for all evaluation data sets, they are probably not too over-trained (although line 294 states that C was set to 10, since it gave good enough accuracy (implying that the value was found looking at the test data)).\n\nThe manuscript is clearly written and easily accessible. The speed-up of the new method is considerable which is why it could have a significant impact on the field.\n\nMinor typos:\nLine 90: different classifiers per scale are used (not \u201cis used\u201d)\nLines 102-103: with a fine-grained pruning of the labels (not \u201cfined-grained\u201d)\nLine 155: one with too much in the heading\nLine 363: all aggressiveness factors except lambda (not \u201cexpect lambda\u201d)\nLine 370: illustrated in column (d,e,f) (not \u201c(d,e,g)\u201d)\n The newly introduced pruning scheme is novel, interesting and seems to provide good solution in practice while being faster than other approaches. The speed-up of the new method is considerable which is why it could have a significant impact on the field.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper combines a coarse-to-fine cascade with pruning classifiers to speed up the optimization of the MRF. \n\nAlthough the greedy method that is used to prune the solution space seems interesting, the paper does not have sufficient theoretical and empirical evidence to support the argument that the heuristic works. Also, the type of MRFs which may benefit from the method is limited to the ones whose solutions are piecewise smooth. \n\nSome part of the paper is not clear enough. For example, in the description of the pruning matrix, it is not clear enough what \"active\" means. The pruning matrix updating of the algorithm is also not very clear. I would suggest the author provide a toy example to illustrate the algorithm better. On the other hand, the proposed approach part of sec 1 is too long and some contents may be redundant. Figure 2 is also not very informative. \n\nThe experiment is interesting but also not clear enough. First of all, there are three algorithms which are compared, but there is only one curve shown in fig 1 (Since there are three algorithms in comparison, only showing the ratio is not sufficient). Secondly, since the energy ratio is between the current energy and the lowest computed energy by any strategy, why is it less than 1 in many cases? \n\n** After author feedback: \nThe authors clarify some of the confusions in the original paper. I expect the authors to correct all the typos and address all the concerns by the reviewers. I move my rating from 4 to 5, because there is still no theoretical justification for the pruning method. \nA few more comments about the experiments: \n- In column(d) of fig 1, both two curves in the 3rd row have less than 96% agreement, while in the paper it says \"the agreement is never worse than 96%\". \n- Consider moving fig 2 to appendix. Add more experiments or analysis on hyperparameter choosing.  Interesting heuristics. Lack of clarity. Weak in theoretical and empirical support.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
