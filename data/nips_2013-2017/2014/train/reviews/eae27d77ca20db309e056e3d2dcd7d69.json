{"title": "A Unified Semantic Embedding: Relating Taxonomies and Attributes", "abstract": "We propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work which only utilized them as side information, we explicitly embed the semantic entities into the same space where we embed categories, which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics, we enforce each category to be generated as a sparse combination of a supercategory + attributes, with an additional exclusive regularization to learn discriminative composition. The proposed reconstructive regularization guides the discriminative learning process to learn a better generalizing model, as well as generates compact semantic description of each category, which enables humans to analyze what has been learned.", "id": "eae27d77ca20db309e056e3d2dcd7d69", "authors": ["Sung Ju Hwang", "Leonid Sigal"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The paper presents a multi-task learning based technique for jointly accommodating information conveyed by mid-level attributes and category semantics. A sparse coding approach is utilized to infer discriminative information to identify categories while permitting generative modeling to reason out new categories. The method is evaluated on the Animals with attributes dataset and shows improved classification performance, with an emphasis on one/few-shot learning circumstances.\n\nThe paper, as mentioned in the manuscript, is a culmination of different existing ideas pertaining to semantic models, discriminative embedding, multitask learning and sparse learning. While the paper does argue how they are different, in my opinion there isn't much contribution in terms of a new approach/algorithm.\n\nMoreover, the paper seems to overly use/rely on recent \"terminologies\" in the name of attributes/semantics, while in crux all its doing is taking different pieces of information about the entities (object categories, here) one is interested in. While one can argue, for instance, attributes are generic while category specific information is not, the approach is more like an hierarchical one where such information can be integrated. In that spirit, existing deep learning methods do the same thing without having to handcode these pieces of information. Also of relevance is the coarse-to-fine strategies that vision community has looked at for many years. Besides dealing with these features, the claim of jointly performing discriminative and generative modeling dates back atleast three decades in the pattern recognition literature eg [Ref1-4]. And in doing do, the paper resorts to existing sparse coding schemes - while it argues [14] learns dictionary in unsupervised way, there are efforts that learn dictionaries discriminatively (eg. [Ref5-6]). Thus there is not much new/interesting contribution in the paper.\n\n\n\nReferences:\n[Ref1] P. A. Devijver and J. Kittler, Pattern Recognition: A Statistical Approach (Prentice-Hall International, Englewood Cliffs, NJ, 1980).\n[Ref2] K. Fukunaga, Introduction to Statistical Pattern Recognition, 2nd Ed. (Academic Press, New York, 1990. \n[Ref3] R. Schalkoff, Pattern Recognition: Statistical, Structural and Neural Approaches (John Wiley & Sons, New York, 1992)\n[Ref4] Lasserre, Julia A., Christopher M. Bishop, and Thomas P. Minka. \"Principled hybrids of generative and discriminative models.\" Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on. Vol. 1. IEEE, 2006.\n[Ref5] Jiang, Zhuolin, Zhe Lin, and Larry S. Davis. \"Learning a discriminative dictionary for sparse coding via label consistent K-SVD.\" Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, 2011.\n[Ref6] Yang, Meng, D. Zhang, and Xiangchu Feng. \"Fisher discrimination dictionary learning for sparse representation.\" Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 2011. A system paper that pieces together several existing concepts, while what it really does is covered by several aspects of existing literature that are not referred to (some are given in references). Hence the paper does not have any significant contribution to support its acceptance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This work presents a method for modeling object categories, supercategories, and attributes in a shared space. This is made possible by an appropriately labeled dataset: Animal with Attributes. The main motivation is that the category-supercategory and category-attribute relationships provide generative regularization for the category-only discriminative learning. \n\n[strength] The work is well motivated in the introduction and related work. I recommend some mention of recent deep methods in the \"sparse coding\" subsection.\n\n[suggestion] Figure 1 could more efficiently use available space by type-setting text to the right of the figure.\n\n[strength] The approach is clearly laid out component by component, with clear mathematical notation, appropriate level of detail, and a sufficient explanation of the numerical implementation.\n\n[strength] Evaluation considers a fair set of baselines, and uses two different underlying features to demonstrate that the method is not overfit to a particular feature space. It is unfrotunate that only the AWA dataset is considered, but perhaps there are no other possibilities for this involved task.\n\n[weakness] Little discussion of the computational concerns of this method. The optimization looks tricky, and I would appreciate knowing details of the training and test runtimes. The proposed unified semantic embedding model is well motivated, clearly and sequentially formulated, and properly evaluated -- demonstrating superiority -- on one super-labeled dataset. The paper would further benefit through discussing computational concerns.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper introduces methodology to learn a semantic space for object categorization, where semantic entities like supercategories and attributes are used to constrain the resulting (discriminative) embedding. The main idea is to (sparsely) represent a category by means of its super-category + a combination of attributes (tiger=striped feline). An advantage of the method is the capacity to generate compact semantic descriptions for the learnt categories. The paper is clearly presented, the motivation is sound and the good results well-emphasize the superiority of the propose techniques. While some of the principles of designing cost functions for discriminative embeddings with good inter-class separation have been described elsewhere (e.g. [7], [14]), the authors here present novel ways to instantiate such ideas in order to connect categories, supercategories and attributes.\n\n- Other Comments\n\nThe introduction is a bit confusing. Too many elements (concepts) float around without a clear buildup. On lines 077-079 it is unclear what the generative and the discriminative objectives are until later. The combination of precise terminology (e.g. generative/discriminative) and imprecise anchoring in the particular context makes the text more difficult to follow.\n\nLine 145: S(z_i,\u2026) not S(z,\u2026)\n\nThe models are non-convex. How is the initialization performed?\n A compact semantic space model that learns a discriminative space for object categorization by leveraging constraints from supercategories and attributes. Improved results on AWA dataset with the additional advantage of a model that can generate a human-interpretable decomposition of categories.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
