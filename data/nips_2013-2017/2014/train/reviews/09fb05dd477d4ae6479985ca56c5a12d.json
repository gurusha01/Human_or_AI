{"title": "LSDA: Large Scale Detection through Adaptation", "abstract": "A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a &gt;7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at", "id": "09fb05dd477d4ae6479985ca56c5a12d", "authors": ["Judy Hoffman", "Sergio Guadarrama", "Eric S. Tzeng", "Ronghang Hu", "Jeff Donahue", "Ross Girshick", "Trevor Darrell", "Kate Saenko"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper addresses the issue of object detection, in particular the challenge of obtaining bounding boxes on a scale similar to which category labels exist for object categorization. The authors side-step this challenge by proposing to adapt object classifiers for the detection task. Their algorithm is fairly simple and straightforward, which is not a bad thing in itself. Their experimental protocol uses 100 categories for training (with both category labels and bounding boxes), and tests on 100 left-out categories. These left-out categories had only category labels. The results obtained were reasonably good, filling about 35% of the gap between an oracle and a baseline on the 100 left-out categories.\n\nQuality and clarity: This is a good quality, clearly-written paper.\n\nOriginality: This paper combines two existing ideas related to adaptation: adapting object classifiers for detection, and adapting from one set of categories to another. To the best of this reviewer's knowledge, this is a novel and interesting idea -- but not ground-breaking.\n\nSignificance: The main contribution of this paper is the novel (but not ground-breaking) approach. Because results-wise, it is hard to judge the long-term significance, although they will likely be surpassed later by more sophisticated method. (This is to be expected). The main issue with the results is that they inevitably sacrifice performance (mAP) by using fewer bounding boxes, and there is no objective way to judge if this trade-off is worth it.\n\n(As a side note, it is not clear if the issue being tackled is really an important long-term challenge. Yes, bounding boxes are more \"expensive\" to obtain than category labels. But who is to say that throwing resources at the problem won't fix it? Not too many years ago, having millions of images labeled with 20,000 object categories seemed almost impossible...)\n\nDetailed issues that should be addressed:\n- In Figure 3a, which methods do (or do not) use category labels to help with detection, other than their method and R-CNN? The authors should state this clearly, to be clear about an apples-to-apples comparison.\n- My judgment of how good the proposed approach is, is based on the the blue bars in Figure 3b. DNN is about 1/3 between baseline and oracle. Do the authors agree that this is one of the main results? If yes, please highlight it, not the 78% (line 341) -- which is not quite the right number for 2 reasons: 1) should judge based only left-out categories, and 2) should not simply be a percentage of oracle performance (should take baseline into account). Analogous problem for the claim of 50% improvement (line 425).\n- Authors should devote a table or a sub-section to clearly stating the differences from R-CNN, which seems to be the closest competing alternative. Currently, these statements seem to be scattered or not as prominent as they could be. I would suggest \"sacrificing\" Figure 6, which are simply anecdotal examples.\n\n\nMinor issues (no need to respond)\n- Is the proposed method called \"DNN\" or \"DDA\"???\n- Typo on line 431 (\"classi?ers\") Overall, this is a reasonably good paper proposing a reasonably novel approach. I believe the paper deserves to be accepted, and to be seen by the NIPS community. While the results will almost certainly be surpassed soon, since the method is simple (fine, this being the first paper to take this approach), the long-term significance of the paper (and the approach in particular) remains to be seen.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors proposed a framework to adapt deep neural networks trained from image classification tasks to object detection tasks. Evaluation on the ImageNet LSVRC-2013 dataset demonstrates that the proposed method ranks 4th in the objects detection challenge. This paper is clearly presented and generally easy to follow, and the proposed CNN structure is interesting and simple to implement. Over all, the quality of the paper is good. However, there are two concerns that the authors could improve. \n\nFirst, as the main contribution of this work, the adaption method proposed in this paper needs better justification. The details of underlying motivations are not presented in a principled fashion. For example in Section 3.2 (Page 5 Line 231), it is not clear why the weights for adapted detection task is a summation of W_j^c and the W_{avg}. The physical meaning of this \u2018offset parameter\u2019 is not explained. As a consequence, it is also not clear why the kNN variant is better.\n\nSecond, in comparing with other methods, the users used part of the validation set, but not the test set, making the comparison not fair. The authors mentioned in footnote (Line 322) that RCNN has similar performance on val2 set, but due to the small size of this set, the performance gain is not statistically justified.\n The authors proposed a framework to adapt deep neural networks trained from image classification tasks to object detection tasks. The proposed method has comparable performance as the state-of-the-art model, but the comparisons are on different settings, and the proposed adaptation method needs better explanation.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a method for adapting a Krizhevsky CNN network that is trained for classification (without bounding box annotations) to work for object detection, given a small number of classes that have annotated bounding boxes. At train time, the approach 1) trains a CNN for classification on images of all classes, 2) trains a CNN on image regions on classes labeled with bounding boxes, while adding a negative background region class, and 3) adds the background class to the full CNN model while adapting some of the CNN weights. At test time, the CNN is run on multiple image regions in a test image.\n\nQuality: This is an interesting paper and is fairly compelling that it gets reasonable results. These results are in large part building off the success of Krizhevsky et al.'s model, rather than something than something that is radically new; however, they will be valuable to researchers in computer vision and machine learning. The general approach is simple, but logical. Experiments are promising; however, it is missing comparisons to other baselines:\n1) Use full images as positive examples, and images and regions from other classes as negative examples. Train and fine-tune all layers of the CNN. \n2) Do the same as above, but use a MIL approach, where you iteratively select the highest scoring positive region for each image from set A (instead of the full image)\n3) Train and fine-tune the CNN for classification. At test time, run the classifier and predict a single bounding box at the center of the image (e.g., the mean bounding box for ImageNet)\n\nI am unsure as to whether or not the proposed approach would outperform these baselines. It isn't clear to me that the proposed approach makes more sense than baseline 1, and I think it probably makes less sense that baseline 2. Including the 3rd baseline would be important to convey to the reader how easy/hard the problem and dataset are.\n\nClarity: The paper is clear and well written. The clarity of the experiments section could be improved, in particular in terms elaborating on the explanation of what each method in table 1 refers to (does this mean applying Eq. 1 or 2 to a subset of CNN layers?)\n\nOriginality: The paper is sufficiently novel to be accepted, although the basic approach is simple and specific to certain types of CNN networks.\n\nSignificance: The problem and results are significant and valuable.\n The paper addresses a relevant problem and the fact that decent detection results were obtained on a largescale dataset without bounding box labels is compelling. At the same time, the paper is missing some obvious baselines, and it is very possible that results are capitalizing off of improved results of CNN features rather than due to introducing a better way of training object detectors from weak supervision.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
