{"title": "Sparse Random Feature Algorithm as Coordinate Descent in Hilbert Space", "abstract": "", "id": "459a4ddcb586f24efd9395aa7662bc7c", "authors": ["Ian En-Hsu Yen", "Ting-Wei Lin", "Shou-De Lin", "Pradeep K. Ravikumar", "Inderjit S. Dhillon"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The paper examines the problem of approximating Kernel functions by random features. \nThe main result is that using an L1 regularisation one can use only O(1/\\epsilon) random features that to obtain an \\epsilon accurate approximation to kernel functions.\n\nThe paper develops Sparse random features algorithm which is analogous to functional gradient descent in boosting.\nThe algorithm require O(1/\\epsilon) random features which compares extremely favourably with the state of the art \nwhich requires O1/\\epsilon^2) features.\n\n\n\nDetailed Convergence analysis are presented in the form of theorems. They appear to be correct.\nThe paper is well written.\n\nThis is an elegant result which should be of practical interest for solving large scale problems. \n The proposed Sparse Random features algorithm is based on the idea of using L1 norm regularisation and yields significant improvement over existing work.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper is an incremental work on kernel approximation using Random Features. The proposed algorithm (Sparse Random Features) aims to introduce l1-regularization in the Random Feature algorithm so that the model doesn't grow linearly with the number of features. The authors also show that the proposed algorithm can be seen as a Randomized Coordinate Descent in Hilbert Space.\n\nOverall the paper is easy to read and clear. The work appears to be significant and will allow for solving practical large scale problems more efficiently than current kernel methods. \n\nHowever, I will suggest to make some efforts for a better presentation (analysis) of the experiments and results. There is a noticeable drop in accuracy of the proposed algorithm with Laplacian and Perceptron using the Covtype data, which could be further discussed. In addition to that, it is not clear whether the tables come from cross validation (preferable) or a simple split of the data.\n\nAlthough the narrative is easy to follow and understand, I have the impression that some sentences need proofreading. This is an interesting piece of work with direct impact on machine learning applications. The paper is well written, however, the authors require to improve the experiments section, and perhaps, adding conclusions and future work directions.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents a random feature based approximation algorithm for solving the l1 regularized problem in a (possibly infinite dimensinal) Hilbert space.\n \n\t\t The paper is well written and easy to read. Theorem 2 and its corollaries are interesting and form the key technical contribution.\n \n\t\t Solving l1 regularized problem in Hilbert space was considered ealier (for eg. [1*], which should perhaps be cited). However the proposed random feature algorithm and more importantly, its theoretical analysis are new and non-trivial.\n \n\t\t Comments:\n\t\t 1. Prior work on solving l1 regularized problem in Hilbert spaces perhaps need to be summarized and cited. For e.g. [1*]. Also, alternatives like [2*] may be appropriately discussed.\n\t\t 2. The empirical section may be strengthened by comparing with [1*].\n\t\t 3. The current choices of \\lamda seem arbitrary. Why not cross-validate for accuracy for all algorithms and report support-vectors/sparsity too? I think the current choice makes the values in the plots un-comparable. Or alternatively, provided plots with varying \\lambda.\n\t\t 4. Some discussion on the results is needed, for e.g, performance in case of regression seems to be better than classification. why? etc.\n\t [1*]. S. Rosset et.al. l1 regularization ininfinite dimensional feature spaces. COLT-2007.\n\t [2*]. G. Song et.al. Reproducing kernel banach spaces with the l1 norm. Journal of Applied and Computational Harmonic Analysis.\n  The paper presents interesting theoretical results that establish the performance of a randomized feature based approximation algorithm for solving the l1 problem in a Hilbert space. However, the simulations section can be largely improved.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
