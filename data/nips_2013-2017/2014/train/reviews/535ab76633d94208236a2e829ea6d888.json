{"title": "Reputation-based Worker Filtering in Crowdsourcing", "abstract": "In this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks. Unlike most prior work which has examined this problem under the random worker paradigm, we consider a much broader class of {\\em adversarial} workers with no specific assumptions on their labeling strategy. Our key contribution is the design of a computationally efficient reputation algorithm to identify and filter out these adversarial workers in crowdsourcing systems. Our algorithm uses the concept of optimal semi-matchings in conjunction with worker penalties based on label disagreements, to assign a reputation score for every worker. We provide strong theoretical guarantees for deterministic adversarial strategies as well as the extreme case of {\\em sophisticated} adversaries where we analyze the worst-case behavior of our algorithm. Finally, we show that our reputation algorithm can significantly improve the accuracy of existing label aggregation algorithms in real-world crowdsourcing datasets.", "id": "535ab76633d94208236a2e829ea6d888", "authors": ["Srikanth Jagabathula", "Lakshminarayanan Subramanian", "Ashwin Venkataraman"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This work explores the problem of identifying and filtering out *adversarial* workers in crowdsourced classification problems. A worker is adversarial if his/her voting strategy doesn\u2019t follow the standard random process: vote correctly with probability p and incorrectly with probability 1-p, where p is the worker\u2019s reliability. To solve the problem of adversarial worker filtering, this paper proposes to attach reputation values to workers. The reputation value of a worker is calculated based on if this worker\u2019s votes agree with other workers\u2019 (in particular, this paper proposes to penalize disagreements instead of awarding agreements) The authors propose two versions of the penalty algorithms and prove their properties with different adversarial worker voting strategies. Experiments on both synthetic and real datasets are conducted.\n\nI think this paper addresses a very interesting problem. Most of other papers on label aggregation assume workers\u2019 votes are i.i.d. samples from some distribution. This paper considers different voting strategies and takes a step towards a more realistic setting. However, the theoretical analysis on hard penalty algorithms are not too satisfactory since 1) it\u2019s hard to interpret the results and 2) the authors make a strong assumption in their analysis (i.e., honest workers are perfect). I also have some concerns about the experiment results. (Please see comments below.)\n\nSome detailed comments:\n\n- In the analysis of hard penalty assignment algorithm (and also the simulation), you assume the honest workers are perfect, i.e., give correct votes with probability 1. This seems to be a fairly strong assumption to me. Is this necessary in your analysis? Do you have intuitions on how to relax this assumption?\n\n- The results of Theorem 3 and 4 are represented using the workers\u2019 voting graph B_H. It would help the readers to get intuitions by giving examples to illustrate the bounds.\n\n- In the experiments results on Table 2, the number of users being filtered out is different in each setting. I think you should show all the results (i.e., set the number of filtered users from 1 to 10) since you won\u2019t know the optimal parameters when running your algorithm in real applications.\n\n- In the experiments, your algorithm boosts the performance of ITER (by Karger et. al. 2011) the most. However, ITER assumes all workers complete the same number of tasks. If you want to apply ITER in datasets without this property, you should modify their algorithm by normalizing the messages at every step during the message passing. Otherwise the algorithm performance would just be dominated by the accuracies of workers who complete the most number of tasks. Without this modification, your algorithm is essentially just filtering out low-accuracy workers completing lots of tasks. It doesn\u2019t provide evidences that your algorithm can filter out adversarial workers.\n\nAFTER THE REBUTTAL:\n\nThanks for addressing my concern about the ITER implmentation. I have updated my scores. Please report the updated results if accepted. \n\nOverall, I think this is a nice paper addressing a more reasonable setting. I am still a little bit bothered by the perfect-worker assumption, but the empirical evaluations mitigate this shortcoming. I would vote for accept. This paper addresses a very interesting problem, but the contribution is limited because of the strong assumption. I also have some concerns about the experiments. Overall, I think this is a borderline paper which I don\u2019t have preferences on either accepting or rejecting.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presented an approach to filter-out different adversarial voters in crowd sourcing systems in order to increase the quality of aggregation voting. The paper presents two different algorithm, soft and hard penalty assignment in order to filter out adversarial voters. Theoretical results justifies the proposed algorithms and the experimental results sounds promising.\n\nThis is an interesting problem and it would be nice to have more details about the applications of this problem.\n \nIt would be nice to have a few sentences explaining optimal semi-matching algorithm as it is being used as one of the important components in hard penalty assignment algorithm. \n\nWhy the hard penalty assignment performs consistently better than soft penalty assignment in real data set comparing to synthetic data set? This consistency is not clear for me. \n The problem is interesting and the paper has been written well. It would be nice to have more discussion about the applications where the problem fits.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper looks at the problem of learning the true labels of objects in the context of crowd-sourcing systems and considers broader classes of adversarial voting strategies (and very powerful adversaries) than had been considered before. \nHere, the authors propose novel reputation-based algorithms based on user disagreements and the use of semi-matchings which identify adversarial users. For these algorithms, the authors were able to show that they improve vote aggregation quality for 3 widely used vote aggregation algorithms. In addition, the authors show that their definition of reputation is compatible with the notion or reliability of users and that the algorithm can detect adversarial users (even when \u2018smart\u2019 adversarial strategies are employed), under the assumption that non-adversarial users have high reliability.\n\nFinally, the authors establish bounds on the minimum damage that may be caused by smart adversaries.\n\nOverall, I found the paper to be both interesting and original in that it considers different classes of adversaries than I had seen previously. At the same time, the authors make a number of strong assumptions which may not be realistic, such as (a) the degree to which \u2018honest\u2019 users label objects correctly (see Assumption 1 in Section 3.2), (b) the knowledge of the complete honest user voting pattern by adversarial users and (c) the knowledge of the decision rule being employed by the adversaries. Given these, it is not clear if the results of Theorem 3 and 4 do really have much practical relevance.\n\nThe other point I was curious about was the way the authors motivate not giving *any* credit for agreement (Section 2), since they didn\u2019t want to give any incentive for adversaries to agree with honest users. In a scenario where all votes by honest users are known, this may be sensible (adversaries can simply agree with a set of items whose label they don\u2019t want to influence and which have a large number of honest votes already), but in practical scenarios where adversaries don\u2019t know the voting pattern, it\u2019s not at all clear that no credit should be given for agreement.\n\nDetailed comments:\n- It would be very interesting to see how the results in Section 4 depend on the threshold of 20% placed on the set of users that are being removed. \n An original look at user-filtering in the context of crowd-sourcing. The authors show good practical results and have some interesting theory behind it, but require some strong assumptions for their theoretical results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
