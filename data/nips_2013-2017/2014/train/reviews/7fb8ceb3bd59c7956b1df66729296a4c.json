{"title": "Algorithm selection by rational metareasoning as a model of human strategy selection", "abstract": "Selecting the right algorithm is an important problem in computer science, because the algorithm often has to exploit the structure of the input to be efficient. The human mind faces the same challenge. Therefore, solutions to the algorithm selection problem can inspire models of human strategy selection and vice versa. Here, we view the algorithm selection problem as a special case of metareasoning and derive a solution that outperforms existing methods in sorting algorithm selection. We apply our theory to model how people choose between cognitive strategies and test its prediction in a behavioral experiment. We find that people quickly learn to adaptively choose between cognitive strategies. People's choices in our experiment are consistent with our model but inconsistent with previous theories of human strategy selection. Rational metareasoning appears to be a promising framework for reverse-engineering how people choose among cognitive strategies and translating the results into better solutions to the algorithm selection problem.", "id": "7fb8ceb3bd59c7956b1df66729296a4c", "authors": ["Falk Lieder", "Dillon Plunkett", "Jessica B. Hamrick", "Stuart J. Russell", "Nicholas Hay", "Tom Griffiths"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Summary: This very strong paper proposes a rational model for algorithm selection based on problem features and Bayesian regression. The model is shown to be effective computationally and to better predict human performance than comparable models. \n\nThis paper is the epitome of a strong NIPS paper. The paper is clearly written and addresses an interesting problem. There is both a nice computational result about the algorithm and a cognitive model that is tested with a brief experiment. \n\nMy only substantive concern is that too much is made of the rational component of the approach. A lot of the success (both computationally and as a cognitive model) comes from breaking the problem space into the features rather than the rational inference per se. It would be nice to see this approach pitted against an RL model with function approximation, instead of the simple RL model considered. \n\nMinor points:\n\n-Fig 1 has tiny fonts.\n-line 38: \u201cby specialized algorithms\u201d no a.\n-Equations 11-12 were a bit hard to follow and certainly did not seem simple\n-One of the strongest parts of the paper is the very serious comparison against other algorithms that was undertaken. That is not sufficient usual and should be commended.\n This very strong paper proposes a rational model for algorithm selection based on problem features and Bayesian regression. The model is shown to be effective computationally and to better predict human performance than comparable models.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper is refreshing in its scope and ambition; I've reviewed and read many NIPS papers that are just one tiny idea explored in excruciating detail. However, despite the paper's considerable breadth and the effort placed into evaluation, I feel this paper may walk the line too far to the side of broad and shallow, versus narrow and deep. I lay out specific concerns below that limit the impact of the results presented.\n\nThe area of algorithm selection / metareasoning is key in both AI and cog sci. \nI am surprised that more work hasn't been done in this important area. \n(I don't know the literature, but I am trusting the authors' review which \nmostly consists of decade-old work. They do fail to cite work in cognitive\narchitectures like SOAR and ACT-R which probably doesn't have a lot to\noffer in terms of how people learn metastrategies.)\n\nI am confused by the authors' decision to focus on the domain of sorting\nalgorthms because this domain doesn't lend itself well to the rational\nmetareasoning (RM) model. RM attempts to optimize a combined measure of\nalgorithm accuracy and algorithm time/opportunity cost. However, for sorting\nalgorithms, don't all algorithms achieve perfect accuracy? And as a result,\nisn't the modeling of scoring (Equations 3, 8, 9, and 10) irrelevant?\n\nI also wonder whether there is much benefit of a Bayesian linear regression\napproach over simple ridge regression, given that only parameter means are\nbeing utilized (Equation 12)? That said, the representation selected seems\nvery sensible (using the two features, their logs, and 2nd order polynomial\nterms involving these features and their logs).\n\nThe work claims to contribute both to state-of-the-art AI approaches to\nalgorithm selection and to the psychological theory of metareasoning. I have\nquestions about its contributions to each, which I'll address separately.\n\nWith regard to AI, the authors compare their results to two existing models.\nThe Lagoudakis method doesn't appear to utilize presortedness. As a result,\nit seems more like a straw man than a serious contender. With regard to\nthe comparison to Guo, I had some concern that (based on the text on line 160)\na different measure of presortedness was being used by Guo, but the authors\nassure me in their rebuttal that they use the same representation as Guo.\n\nWith regard to the work's contribution to cognitive science: In human strategy\nselection, the trade off between accuracy and opportunity cost is key, and the\nopportunity cost involved is not only the run time of an algorithm, but the\ncost of selecting an algorithm, as reflected in Simon's notion of satisficing.\nThus, I question selection of sorting algorithms as the most suitable domain\nfor studying human strategy selection. Although the experimental set up\nin Section 5 is elaborate and impressive, the coarse performance statistics\n(proportion of merge sort selection and overall quality of selection)\nhardly make a compelling argument that the RM model is correct. All we\nknow from the experiment is that both the RM model and people are pretty \ngood at selecting strategies, whereas the other models are not. This result \ngives us little insight as to whether the RM model is a good cognitive model.\n\nI couldn't find much detail about training the models used in Section 5, but if\nthese are really meant to be cognitive models they should be trained on the\nsame data that people had available during practice, and the same total number\nof trials. The author rebuttal assures me that the training data is identical.\n\nMINOR COMMENTS:\n\n[093]: A Gaussian distribution may be acceptable for sorting algorithm \nruntimes, but it's probably not the best choice for modeling human response\ntimes. On a range of tasks from simple button presses to memory retrieval to\ncomplex problem solving, reaction times tend to have a long-tailed asymmetric\ndistribution.\n\n[091]: The standard deviation does not appear to be a polynomial in the\nextended features (Equation 7)\n\n[168]: The table caption should explain the performance measure. I believe\nit is the percentage of runs in which the optimal sorting algorithm was\nselected.\n\n[295]: The text says that Equations 13 and 14 suggest the conditions\nunder which merge sort will be chosen over cocktail sort. However, the\ncoefficients on n_comparisons and n_moves in Equation 13 are both smaller\nthan the corresponding coefficients in Equation 14, so it seems to me\nthat cocktail sort should be chosen for all but the very shortest lists. The work addresses an important challenge to AI and to cognitive science. The authors try a straightforward approach to learning strategy selection and get sensible results in a limited domain (selecting a sorting algorithm).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose to model the selection of the appropriate algorithm for a task as a metacognitive process by maximizing the value of computation, defined as the amount of expected utility of acting after having carried out a computation that exceeds the expected utility without carrying out the computation. The authors approximate this value by learning to predict the score and the runtime of an algorithm on the basis of features, which are chosen by Bayesian feature selection. Applying this meta-reasoning to the problem of choosing the algorithm for minimizing the duration of sorting lists shows that the proposed computation performs better than two previously proposed methods, one based on decision trees and one based on recursive algorithm selection using dynamic programming. \nFinally, the authors present a set of behavioral experiments in which human subjects had to sort lists according to either cocktail sort or merge sort by following respective instructions. After estimating from behavioral data the time to carry out individual sorting actions according to the two algorithms, the authors applied this model to the case in which subjects chose the algorithm to sort new sequences. The human behavior was better predicted by the metacognitive strategy compared to previous models of how humans choose to sort.\n\nThis is one of the best papers of the last three years I have been reviewing for Nips. The only recommendation I would like to give the authors for improving the manuscript is to provide more details for the reader on: \n- feature selection by Bayesian model choice,\n- allows us to efficiently compute Bayes factors,\n- Lagoudakis et al.\u2019s method,\n- an explanation for why Lagoudakis et al.\u2019s method performs so much worse. \n\nP6:\n\u201cto an descending list.\u201d\na Great manuscript formalizing how to choose one algorithm for sorting applied to machine sorting and to human performance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
