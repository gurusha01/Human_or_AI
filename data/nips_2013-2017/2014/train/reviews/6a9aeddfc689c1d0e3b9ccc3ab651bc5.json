{"title": "Fast and Robust Least Squares Estimation in Corrupted Linear Models", "abstract": "Subsampling methods have been recently proposed to speed up least squares estimation in large scale settings. However, these algorithms are typically not robust to outliers or corruptions in the observed covariates. The concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper. This property of influence -- for which we also develop a randomized approximation -- motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error. Under a general model of corrupted observations, we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares.", "id": "6a9aeddfc689c1d0e3b9ccc3ab651bc5", "authors": ["Brian McWilliams", "Gabriel Krummenacher", "Mario Lucic", "Joachim M. Buhmann"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper looks at alternatives to LS for solving linear regression\nproblems. Classical statistics will consider a score function as a\nway of weighting observations. But, these would only capture an\nadditive error which is different than Gaussian. This paper instead,\nalso considers the influence of a point. Statisticians for years have\nthrown out influential outliers--but as\tfar as I can find, this\tis the\nfirst automatic\tprocedure that\tuses influence as a way\tof\ndown-weighting a\tregression.\n\nThe probabilistic model\tpresented here is not the driving force\tof the\npaper. \tIn other words,\tit is pretty easy to identify which points are\ncontaminated and which are clean. (This is very easy for large p\nsince the two distributions won't overlap.) So\tif the estimator\npresented was tied to this model, I would be unexcited about the\nresults. But, the estimator is\tfairly natural.\t So the\tmodel is only\nbeing used as an example.\n\n1) Put a line where the accuracy of the MLE for your probabilistic\nmodel would be. You shouldn't end up being close to this line since\nyou aren't assuming your model. But it still is a more useful\ncomparison point than the LS line. If you don't want to bother\twith\ndoing the de-convolution problem, you can assume that the ML has\naccess to the random variables U. Then\tit will\tbe a lower bound on\nhow well it will perform. As p\t--> infinity, this lower bound will\nbecome tight.\n\n2) Make the connection to some of the literature on the use of scoring\nfunctions (Since this goes back to the early 1970's I'm not going to\nsuggest any literature). For example, the aRWS can be thought of as\ndown weighting points by a 1/epsilon^2. If one were to down weight by\n1/epsilon, that would correspond to doing a L-1 regression. So it\nwould be similar to assuming a double exponential distribution for the\nerrors. Your weighting function is even more draconian. So what\ndistribution does it correspond to if you think of it as a score?\n(I'm guessing it looks like a Cauchy distribution--but only\napproximately.) (Note: you are sampling, whereas I'm more used to\nthinking of weights. So it might be that your 1/epsilon^2 sampling is\nrelated to a 1/epsilon weight. If so, then it is approximating a L-1\nregression. This would be very nice to point out if it were true.)\n\n3) Are you claiming to be the first people to suggest weighting by\n1/influence? This might be true--but it is a strong claim.\n\n4) I think what you are doing is using a different regression function\nfor \"large observations\" than you use for \"small observations.\" One\nway to test this and provide a better comparison for LS would be to\ndefine tilde-l as you do and then interact this variable with all your\nX's in your regression. This will allow the LS methods to have access\nto tilde-l. Hence LS should be able to end up with a better estimator\nsince it now can have different slopes for the pure X observations\nthan it has for the X+W observations.\n\n5) There is extensive literature on both errors in variables and\nrobust regressions. At least a few of these 1000+ papers should have\nsomething useful to say. Put in at least some effort into finding\nsomething that connects. That will help the implied claim that your\nmethods are in fact new.\n\n6) PLease look at this 1980's NBER paper:\n\n http://www.nber.org/chapters/c11698.pdf\n\n In particular, equations (15) and (16). This is very close to the estimator you are using. It would be nice if you were to chase down some of the modern references and see if there are any connections to your work.\n\n This paper provides a new alternative to robust regression. Namely, it down weights by the influence of a point. A useful theorem is provided and good alternatives are considered in the empirical section.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "- Summary:\n\tThis paper presents a speed-up proposal for least squares linear regression. Compared to existing approaches the current proposal claims robustness to outliers or corruptions in the measured predictors.\n\n\t- Quality:\n\tThe paper seems technically sound to me. The claims are well supported from a theoretical perspective. From an experimental perspective I was a bit disappointed. The initial motivation of the authors was estimation on large scale settings. Although in terms of artificial data sets the authors have used reasonably large data (n=100,000; p=500), regards the single real world domain it can hardly be considered a large data set by today standards (n=13,000; p=170). Moreover, as mentioned before, there was a single real world data set. This is a bit limiting in terms of experimental results, particularly given the initial motivation immediately outlined in the first sentence of the abstract. Still, the work is very complete from a theoretical perspective, particularly when we take into account the supplemental material that was provided. \n \n\t- Clarity:\n\tI think the paper reads very well. The algorithms and methodological approach are clear and I think it should be easy to reproduce the results of the authors. Still, a bit more details on the Airline delay dataset would be preferable (still an URL is given...). I also do not understand the label on the X-axis of figure 2 (n_{subs})... what is the meaning of this label? On page 5, near Eq. 7, there seems to be some sort of typo (... $4 and $4 ...)\n \n\t- Originality:\n\tThe proposal seems novel to me. Still, the proposal is a combination of previous works ([8] and [14] as the authors refer on section 5) and thus with limited novelty. Nevertheless, the authors are clear about this and identify relevant and related works.\n \n\t- Significance:\n\tThe results reported in the paper are immediately limited by the fact that they are constrained to a specific type of regression method. Moreover, given the limitations on the experimental evaluation mentioned above, I'm afraid it is also not easy to completely assert the impact of these proposals in terms of experimental results. This means that although I think this is an advance on the state of the art of least squares linear regression, the exact impact of this advance is not correctly evaluated by the authors in my opinion, which limits the significance of the paper that was already potentially small by being constrained to a specific regression algorithm.\n\n  This paper presents a very localized contribution that seems theoretically sound but whose impact is limited both because of its focus on a single technique and by a limited experimental evaluation", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary:\n\nThe paper presents an influence reweighted sampling method (IWS-LS) (as well as a residual weighted sampling method, i.e., RWS-LS) for learning large-scale least squares, which is robust with respect to some data corruption, e.g., the particular sub-Gaussian additive noise. Existing approximation methods are adopted to compute the OLS estimate and the leverage scores. Estimation error is analyzed theoretically for IWS-LS. Finally, empirical results are reported on both synthetic and real-world data sets.\n\nComments:\nOverall, the paper is well written. The influence reweighted subsampling method is new. The theoretical and empirical results appear to be sound. \n\nFor the experiments, the dataset with 100,000 samples in a p=500 space is not huge (the real-world Airline delay dataset is even smaller). The computation of the OLS estimate as well as the leverage scores should be feasible (with O(n p^2) complexity), even though it may take minutes or hours, on a standard desktop; and thus the results of exact methods should be included as a baseline. It is also helpful to include the experiment environment. Furthermore, as time efficiency is one very important aspect of large-scale learning, running time should be included. \n\nFor data nosing/corruption, this paper focuses on developing estimators that are robust to the resulting outliers. It would be useful if the authors can discuss on the work that leverages data nosing to actually improve classification/regression. Some recent work includes dropout training for deep neural networks [1,2], learning with marginalized corrupted features [3,4], and etc.\n\nReferences:\n[1] G. Hinton et al., Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580v1, preprint. \n[2] S. Wager, et al., Dropout training as adaptive regularization. NIPS, 2014.\n[3] L. van der Maaten, et al., Learning with marginalized corrupted features. ICML, 2013.\n[4] N. Chen, et al. Dropout Training for Support Vector Machines, AAAI, 2014.\n\nFinally, some typos should be corrected. For example, line 42 \u201cmore more realistic\u201d; line 254 \u201cideas from $4 and $4\u201d. \n The paper is well written. The influence reweighted subsampling method is new. The theoretical and empirical results appear to be sound.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
