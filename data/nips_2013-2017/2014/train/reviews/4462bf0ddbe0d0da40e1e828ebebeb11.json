{"title": "Expectation-Maximization for Learning Determinantal Point Processes", "abstract": "A determinantal point process (DPP) is a probabilistic model of set diversity compactly parameterized by a positive semi-definite kernel matrix. To fit a DPP to a given task, we would like to learn the entries of its kernel matrix by maximizing the log-likelihood of the available data. However, log-likelihood is non-convex in the entries of the kernel matrix, and this learning problem is conjectured to be NP-hard. Thus, previous work has instead focused on more restricted convex learning settings: learning only a single weight for each row of the kernel matrix, or learning weights for a linear combination of DPPs with fixed kernel matrices. In this work we propose a novel algorithm for learning the full kernel matrix. By changing the kernel parameterization from matrix entries to eigenvalues and eigenvectors, and then lower-bounding the likelihood in the manner of expectation-maximization algorithms, we obtain an effective optimization procedure. We test our method on a real-world product recommendation task, and achieve relative gains of up to 16.5% in test log-likelihood compared to the naive approach of maximizing likelihood by projected gradient ascent on the entries of the kernel matrix.", "id": "4462bf0ddbe0d0da40e1e828ebebeb11", "authors": ["Jennifer A. Gillenwater", "Alex Kulesza", "Emily Fox", "Ben Taskar"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper is concerned with learning kernel matrices to be used by determinantal point processes (DPPs). The main contribution of the paper is a reformulation of the objective that outperforms the standard approach of differentiating the logprob with respect to kernel parameters and doing projected gradient ascent (this baseline approach is called K-Ascent (KA) in the paper). The idea is to reformulate the problem within an EM framework. This works by observing that any DPP can be represented as a mixture over \"elementary DPPs\", where there is one elementary DPP for each subset of elements. A variational distribution is instantiated over these subsets, and the algorithm alternates between updating the distribution over these subsets, which is represented as a k-DPP, and taking single gradient steps in the M step. Results show that the EM method outperforms the KA method by quite a bit when KA is initialized naively, and by a little bit in some settings when KA is initialized more intelligently. \n\nOverall, I find the approach interesting, and it seems like the EM method does yield some improvement. The M-step is fairly involved, and the caveats required to make it work are a bit unsatisfying, but there is something to be said for getting it to work.\n\nExperimentally, I would have liked to have seen a bit more analysis on the effect of initialization on EM. For example, if the EM algorithm were initialized with the result of KA, would it improve upon the solution?\n\nI'm also confused by the use of relative log likelihood as a measure to report. Log prob differences reflect ratios of probabilities. Why take ratios of log probabilities? Why not just differences in log probs? I have a hard time interpreting what these numbers mean.\n\nAlso a minor note (sorry for the crankiness!): I find some of the writing style to be annoying. I would prefer the authors don't call their own algorithm elegant, particularly when the elegance is debatable due to the caveat and approximation required in the M step. I also find it unnecessary to bold the text in the introduction. Interesting new approach for learning kernels in DPPs. Experiments could be improved a bit, and the improvements over the basic projected gradient algorithm aren't huge, but overall a pretty good paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents and EM algorithm for learning Determinantal Point Processes. The method combines several nontrivial ingredients: eigendecomposition of the kernel, optimisation on the Stiefel manifold, representation of DPPs as mixtures of elementary DPPs, and lower-bounding the log-likelihood by Jensen 's Inequality. The experiments show favourable behaviour of the proposed approach compared to a simpler \"K-Ascent\" scheme. \n\nQuality: As far as I can tell the method is well-founded, and is believable that the methods should work as stated. However,\nthe evaluation of the method could be improved. The experiments are quite limited and do not compare against alternative approaches for subset selection (e.g. those mentioned in the introduction), so it is not clear how much we benefit from the DPP approach.\n\nClarity: The paper is not particularly clearly written. write-up is somewhat sloppy and hard to follow. Not all notation is defined and sometimes definition comes after using it. The writing is also unnecessarily convoluted at times, mostly because the text is structured to describe a process how one ends up with the finally proposed approach, with side-remarks of possible alternative approaches and problems interleaved with the main thread. I would have preferred to present the developed method as clearly as possible and postpone discussion to a later stage.\n\nOriginality: For me the paper looks quite original, I am not aware of similar work.\n\nSignificance: I think the method derived in the paper presents a solid advance and a useful contribution to subset selection literature.\n\nDetails:\n- p.2. line 81 the marginal kernel K is not defined before use. What kind of kernel qualifies as marginal kernel?\n- p.2. l 100: V is not defined\n- p.3. l 159: \"minimises the change to Frobenius norm\" is unclearly stated. It would be better to say \"chooses the closest (in Frobenius norm) PSD matrix to Q\".\n- p 4. I found this section hard to understand. Rather than referring to [26] it would be nice to outline your (simpler) approach directly so that the reader is not forced to lookup the more complex original method. \n - p.4 line 189: what does \"weight\" refer to?\n- p. 7 line 324. I don't quite get the rationale in (30). Why is m_{ij} not used here? Why would simply using the empirical covariance matrix as the kernel not have worked as initialisation? The paper puts forward a new method for learning Determinantal Point Processes for subset selection. The method is highly non-trivial and I assume it is original. The write-up is not that clear. The experiments do not consider other than DPP methods.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Determinantal Point Processes is a distribution over a fixed ground set that assigns higher probability to diverse sets. DPPs can be parameterized by a positive semidefinite matrix (L). Since learning L is np-hard, only partial learning of L has been discussed as prior work. Learning a scalar vector for L compromises on the diversity dimension of DPP. Previous work have resorted to restricting the parametric form of L. In this paper, the authors propose a learning method that does not restrict parameterization of L. The learning method does not require a projection step as in gradient ascent that can lead to the similarity property being compromised, and compromise on the diversity property. Using EM on eigen-values and eigen-vectors overcomes this disadvantage. They explore some optimization algorithms to solve this without needing to project values. Using projected gradient ascent requires projection of both eigen-values and eigen-vectors. Exploiting the fact that V is full-rank, they avoid the projection of eiger-vectors. Jensen\u2019s inequality is used to lower bound the objective function and construct an EM procedure. \n\nThe learning method is useful as it preserves the diversity property of the DPP as opposed to other previous work where only the quality property has been focused upon. The step-by-step derivation of the EM procedure is insightful. Mapping the constraint to a an optimization over Stiefel manifold to eliminate the projection of eigen-vector and showing that the inverse distribution is a DPP are solid contributions. They test the learning algorithm on both synthetic datasets and a product recommendation task. The experiments support the claim in the paper \u2013the learning method retains the quality and diversity of the DPP, by comparing it with KA. While only two of the top 10 products are replaced by KA by products that are less likely, it still brings out the contribution of the paper and supports the claim made in the beginning. The conclusion is a bit hasty not bringing forward all the contributions of this paper, but overall the paper is written cohesively. The proofs are well-written and the step-by-step reduction of optimization to EM is interesting. \n The paper is written coherently and proofs are well-written. The step-by-step reduction to EM is interesting and well explained. More experiments in real-world datasets can illustrate the importance of the \"diversity\" produced by this algorithm when compared to others in this area.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
