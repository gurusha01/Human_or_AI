{"title": "Learning Generative Models with Visual Attention", "abstract": "Attention has long been proposed by psychologists to be important for efficiently dealing with the massive amounts of sensory stimulus in the neocortex. Inspired by the attention models in visual neuroscience and the need for object-centered data for generative models, we propose a deep-learning based generative framework using attention. The attentional mechanism propagates signals from the region of interest in a scene to an aligned canonical representation for generative modeling. By ignoring scene background clutter, the generative model can concentrate its resources on the object of interest. A convolutional neural net is employed to provide good initializations during posterior inference which uses Hamiltonian Monte Carlo. Upon learning images of faces, our model can robustly attend to the face region of novel test subjects. More importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.", "id": "c22abfa379f38b5b0411bc11fa9bf92f", "authors": ["Charlie Tang", "Nitish Srivastava", "Russ R. Salakhutdinov"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Summary:\n\nThis paper proposes a deep learning based spatial attention mechanism based on a probabilistic generative model. The approach enables identifying novel objects in large images, and hence might allow for better exploitation of unlabeled, uncropped data.\n\nMain Comments:\n\nThis paper takes a solid step towards being able to apply deep learning methods to large uncropped, unlabeled images. The motivation is compelling: previous DL approaches have required \u2018curated\u2019 training data that sticks the object essentially in the center with minimal clutter/occlusions\u2014overcoming this limitation would truly enable learning from unlimited unlabeled data.\n\nThe experimental section contains a number of interesting experiments verifying that the approximate inference method is working, and that HMC is still worthwhile beyond this. The network can perform some notable and rarely addressed tasks such as the ability to learn generative models from large images without labels, and the ability to shift attention by conditioning on various target stimuli.\n\nThe paper claims that the algorithm runs in time O(1) given the size of the image. However initializations farther from the target image are likely to require more approximate inference steps, so there is an implicit dependence on image size. It could be interesting to plot the required number of steps as a function of image size to see this scaling. Also the window patch supplied to the conv net might also need to be increased for large images, so the scaling is not really O(1).\n\nThe paper is clearly written and easy to follow. This paper takes a solid step towards being able to learn deep network models using large uncropped, uncentered, unlabeled images\u2014which could give access to virtually unlimited unlabeled data.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes a generative model for images that uses an attentional mechanism to focus on items (objects) of interest within an image, and devoting the object model just to these attended objects, as opposed to the entire image (as in most deep network approaches). Latent variable representations for the object and its pose (position, size and orientation) are inferred from the image using hamiltonian monte carlo. Performance is demonstrated on the Caltech and CMU-PIE face datasets, showing good performance of the model.\n\nOverall I love this paper. It proposes something that is totally sensible and long overdue in both discriminative and generative models of images - i.e., employing an attentional mechanism. I feel that this is an important advance, and although many aspects of the work could probably be expanded and improved, I think it will generate strong interest at NIPS. \n\nOne area where I think this work could be improved is in learning the transformation model rather than assuming affine translation, scale and rotation. For example 2D projections of 3D objects will generate a richer set of warps/transformations. Also how to deal with occlusion?\n Great paper - a very sensible approach with good results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The goal of this study is to learn generative models of sensory signals in the attended region. The authors proposed a model that could successfully learn a generative model of frontal faces from large-size images that include faces among distractors.\n\nThe manuscript is well written overall. A wide range of researches are surveyed and combined, from neuroscience to computer vision to machine learning. I think the manuscript would be informative and useful to a broad audience.\n\nThe model itself is formulated in a generic way so that it could potentially be applied to a wide range of data. I think, however, the model is based on a couple of key assumptions that significantly limit its applicability. Namely, the model assumes \u201ccanonical image\u201d (variable \u201cv\u201d), and also assumes similarity transformation. Therefore, the proposed approach could build a data generative model of \u201cattended image regions\u201d, but not the underlying, three-dimensional \u201cobjects\u201d whose observations by sensors involve more than just similarity transformation. I think this is the reason why the section 6: experiments only examined frontal faces, not those from different views, nor other objects. This manuscript can be improved if such limitations towards the grand goal are clearly discussed. An interesting approach to a challenging problem of \u201cgenerative model of attended image regions\u201d, by combining several strands of prior researches from different areas. Thought-provoking paper, but I think the proposed model is still fairly limited.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
