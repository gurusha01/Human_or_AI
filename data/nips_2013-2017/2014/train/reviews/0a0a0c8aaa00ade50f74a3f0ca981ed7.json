{"title": "Analog Memories in a Balanced Rate-Based Network of E-I Neurons", "abstract": "The persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies. However, despite decades of theoretical work on the subject, the mechanisms that support the storage and retrieval of memories remain unclear. Previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way. Specifically, some models violate Dale's law (i.e. allow neurons to be both excitatory and inhibitory), while some others restrict the representation of memories to a binary format, or induce recall states in which some neurons fire at rates close to saturation. We propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints. We directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics. The resulting networks operate in the balanced regime, are robust to corruptions of the memory cue as well as to ongoing noise, and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices. Our results constitute a step forward in our understanding of the neural substrate of memory.", "id": "0a0a0c8aaa00ade50f74a3f0ca981ed7", "authors": ["Dylan Festa", "Guillaume Hennequin", "Mate Lengyel"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Summary: The authors present a model of auto-associative memory in a rate-based neural network subject to a battery of biological plausible constraints. Previous models of auto-associative memory have failed to include several key features of real biological networks, namely an adherence to Dale's Law that neurons have a strictly excitatory or inhibitory effect on their projections and the observation that networks can encode memories without relying on units that simply respond at their saturation rate or respond in a binary manner. Memories are encoded in the network via synaptic modifications based on a gradient descent procedure, constrained using a recently published method for ensuring that the linearization of the dynamics around a dynamical system's fixed point is stable. The authors illustrate the effectiveness of their training procedure with simulations, noting that the trained fixed points exhibit slow network dynamics (i.e. they are close to being, but are not exactly, fixed points) and are stable, as desired. The authors note two features of their trained network that are in line with experimentally observed features of cortical networks, namely a distribution of synaptic weights centered at zero with long tails and that an average network unit received approximately equal excitatory and inhibitory synaptic input. The authors identify these features as key components to the networks success in performing robust auto-associative memory when the network is perturbed from a fixed point, even in the presence of stochastic input noise. \n\nReview: This is a clearly-written, novel submission that will be of great interest to the community. The authors nicely review the history of training neural networks to perform auto-associative tasks, highlighting the main contributions and shortcomings of previous work. The network architecture and training procedure are both presented in a clear way and the figures are well-chosen to quickly illustrate the main results of the submission. The authors illustrate the networks performance limitations when network noise is included, which highlights the robust solution the training procedure finds. \n\nOne recommendation would be to clarify the relationship between the chosen training procedure and the stated objectives. As the author's noted other methods have been employed in the past to enforce Dale's Law or achieve graded network memories, with the latter condition seemingly more difficult to enforce. The chosen training procedure achieves the desired goal of building an auto-associative network with non-saturing units, but nothing in the text indicates that the authors chose this method because they believed it was particularly well-suited to this task. Perhaps another network training procedure would achieve the same result? Can anything be said as to why this particular method achieves that goal so effectively?\n\nAlso (although probably beyond the scope of this paper) given a rich history of training networks to perform auto-associative tasks, a comparison of how the trained network performs against previous models in terms of network capacity (number of memories as a function of network units) would surely be of interest to the community. 1-2 sentences: Applying a recent method for ensuring stable fixed points in a dynamical system, the authors introduce a new model of auto-associative memory that conforms to several constraints of real neural networks, most notably the condition that network units are capable of encoding memories without saturating or binary responses.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In this manuscript, the authors design a rate-based neuronal network which is trained to hold a number of stable activity patterns, i.e. memories. The goal is to overcome several shortcomings of previous attractor models, namely violation of Dale's law and saturation of activities in the memory states. For training, the authors use a gradient descent on a cost function which minimizes activity changes in the desired attractors and the Frobenius norm of the weight matrix, and maximizes the stability of attractors. The main results are a weight distribution close to experimental findings and a balance of excitation and inhibition in the attractor states.\n\nThe writing of the manuscript is good. I found the model and the training procedure easy to understand, as well as the presentation of the results. The combination of biological constraints going into the model and the resulting features after training make this paper an interesting contribution to the understanding of attractor neural networks. The authors train a rate-based neuronal network to hold graded and non-saturated attractors in the synaptic weights and inhibitory activities. Constraints imposed on the synapses and neurons for biological plausability lead to a balance of excitation and inhibition on the network level.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper returns to the venerable problem of associative memory in neural networks with a new fresh perspective, adding much new insight to an old problem. The authors observe that much previous work in the field ignored biological aspects such as Dale\u2019s law, used binary memory representations in rate based models, and used non-sparse representations for memories. While all these issues have been dealt with individually in other work, the authors develop a strategy which allows them to address all issues. More concretely, they make use of a recent control-theoretic approach, based on spectral bounding techniques, to construct networks which deal with all these issues. In this approach the authors propose a cost function balancing three terms to achieve the desired goal through gradient descent. The first term in the cost contributes to the fixed point nature of the desired memories, the second term to the stability of the fixed points, and the third term encourages small weights which prevent saturation. By directly enforcing Dale\u2019s principle in the weight parameterization, the authors are able to address all the above shortcomings of previous work, leading to a network where excitations and inhibition are naturally balanced. Interestingly, while they do not enforce saturation in the single neuron firing, this arises naturally through the solution of the optimization problem. Importantly, the authors only make use of the excitatory neurons to implement the memory (which is consistent with the fact it is usually the excitatory neurons that send output to other regions), and view the inhibitory neurons as variables to be optimized over. The authors validate their results through numerical simulations, and demonstrate its noise robustness. In addition to demonstrating the good associative memory properties of the network, the authors are also able to explain experimental findings showing that the trial to trial variability is reduced following stimulus onset (although this seems to be a general feature of associative memories with spontaneous baseline firing). This is explained through convergence to the attractor closets to the initial condition. \n\nIn summary, this is a clear and well written paper. It tackles a classic problem in neural computation using novel control-theoretic tools, and demonstrates how a biologically plausible associative memory may function. They do not propose a biologically plausible learning mechanism at this point, but hint at some recent proposals that may lead to such rules. I found the utilization of the inhibitory neurons as resources to be interesting and beneficial. Overall, the paper presents a clear, concise and important contribution to neural computation. \n\nA few minor issues:\nThe authors use \\gamma=0.04 in eq. (3). It would good to discuss the sensitivity of the results to this choice (although I expect it to be small)\nA log-normal distribution for memory states is proposed on line 115. Please motivate this. \nThe cost function (4) may yield solutions which violate the exact fixed-point nature of the memories. How severe is this violation? This should be quantified and discussed. \nLine 254-256: The statements here were not clear to me. Please clarify.\nThe authors do not mention capacity issues. It would be nice to make some statement, even preliminary, about this important computational aspect. \nIt would be nice if the authors could discuss the computational benefits of Dale\u2019s law. Namely, can this law be shown to enhance computational power under specific constraints? This is not necessarily the case, and may be a result of evolutionary and/or biophysical constraints, but it would be nice to relate to this. \n A clear and well written paper tackling a classic problem in neural computation using novel control-theoretic tools, and demonstrating how a biologically plausible associative memory may function.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
