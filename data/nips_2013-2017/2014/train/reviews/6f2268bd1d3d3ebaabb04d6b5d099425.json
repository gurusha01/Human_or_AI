{"title": "On Integrated Clustering and Outlier Detection", "abstract": "We model the joint clustering and outlier detection problem using an extension of the facility location formulation. The advantages of combining clustering and outlier selection include: (i) the resulting clusters tend to be compact and semantically coherent (ii) the clusters are more robust against data perturbations and (iii) the outliers are contextualised by the clusters and more interpretable. We provide a practical subgradient-based algorithm for the problem and also study the theoretical properties of algorithm in terms of approximation and convergence. Extensive evaluation on synthetic and real data sets attest to both the quality and scalability of our proposed method.", "id": "6f2268bd1d3d3ebaabb04d6b5d099425", "authors": ["Lionel Ott", "Linsey Pang", "Fabio T. Ramos", "Sanjay Chawla"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The paper looks at the problem of combining clustering and outlier detection. It is very well written and easy to read. The authors reuse an earlier facilities location without outliers formulation by Charikar et' al and their main contribution is the solution to the problem formulation. \n\nThe FLO problem was shown to be intractable by the authors of that paper and no approximation algorithm exists that is both i) scalable and ii) comes with guarantees. The FLO paper has well over 100 citations and a quick scan of them shows this to be case. The Lagrange relaxation the author's propose is straight forward and seems reasonable. The analysis of the LR is interesting, the core result that the LP relaxation of FLO is equivalent to the LR relaxation is non-intuitive. \n\nThe experimental results are a nice addition to the paper in that they show the usefulness of the formulation particularly the speed up over the LP relaxation. However, I found the lack of absolute values in the experimental section raised questions. For example in Figure 3, your results show the speedup over LP relaxation but doesn't show how fast the LR method is in absolute time.\n\n The authors take an existing formulation (FLO) show how it is useful for outlier detection and clustering simultaneously. Their main contribution is the LR formulation for the existing objective function and the experimental results showing its usefulness.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper considers an important problem in unsupervised machine learning and optimization, clustering with outlier detection. Clustering has a long history in both theoretical and practical areas. People worked on many types of clustering problems before, such as k-means, k-medians and k-centers in geometric background, and correlation clustering, spectral clustering in graph theory. However, one critical issue for clustering is outlier detection, which could influence the final result significantly. \n\nThe authors present a gradient descent algorithm for clustering with outlier detection. They slightly modify the integer programing model from [8], through adding the outlier part to the constraint. Then they relax the model into a sequence of Lagrange relaxations, and solve it via a gradient descent strategy. \n\nThe experiment considers both of synthetic and real data, and shows the advantages over other two methods. \n\nPositive points:\n\n1. Using Lagrange relaxation is a new idea for outlier detection. \n\n2. The algorithm is clean, and easy to implement, which makes it practical. \n\nNegative points:\n\n1. The theoretical analysis is not enough. For example, in section 4.2, the authors should provide more details for the convergence.\n\n2. More references are needed. In computational statistics, there are many new techniques on trimming outlier for regression and clustering, such as David Mount et al, ``A practical approximation algorithm for the LMS line estimator\" and ``On the least trimmed squares estimator\".\n This paper provides a Lagrange relaxation approach for a hard problem in clustering area. The technique is new, but needs more theoretical analysis.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper describes an approach to clustering which deals automatically with the problem of outlier detection. The whole idea is based on the so-called Facility Location with Outliers (FLO) problem, introduced and studied in theoretical computer science in 2001 (see ref. [8] in the paper). The novelty of the proposed approach lies mainly in the study ot the Lagrangian relaxation of the original (integer) linear programming problem. In particular, the authors show that the Lagrangian relaxation is equivalent to solving the linear relaxation of FLO, and also analyze the convergence properties of the subgradient method used, which makes the algorithm highly scalable. Some experiments on both synthetic and real-world data sets (MINST) are presented.\n\nThe paper is well written and organized, it\u2019s easy to follow and understand, despite many technicalities, and its motivations and goals are quite clear.\n\nAlso, I like the idea of introducing, within the machine learning community, this apparently novel formulation of the clustering problem which, to my knowledge, was originally confined only to the theoretical computer science domain. In regards to the statement made by the authors that \u201cclustering and outlier detection are often studied as separate problems,\u201d however, I\u2019d like to draw the authors\u2019 attention to novel clustering formulations which, abandoning the idea of partitioning the input data set, focus instead on the notion of a cluster itself (see, e.g., M. Pelillo, \u201cWhat is a cluster?\u201d NIPS\u201909 Workshop on Clustering, and references therein). In fact, in these approaches, which are non-partitional by definition, one can see that the problem of clustering and outlier detetion are simply two faces of the same coin.\n\nOne problem with the proposed formulation, though, is that while it doesn\u2019t need as input the number of clusters, which is of course good, it neverthless needs to know the number of outliers, which is even more problematic than knowing the number of clusters. Also, it needs to know the \u201ccost\u201d of creating a new cluster, and here, again, it\u2019s not clear how to define them in practical applications (the choice made in the experiments reported here look quite heuristic).\n\nMy final comment concerns the experimental validation. Indeed, the results presented here do not show a clear and substantial improvement over the other methods used. This is particularly clear in the MNIST experimemts (Table 1) which show that APOC and LR are basically equivalent, and k-means-- is better than the proposed LR algorithm.\n\nMinor comments:\n\n- Definition 2 is indeed a Proposition\n Although the experimental results are not compelling and the method requires a problematic setting of parameters, I think the ideas proposed in this paper are potentially interesting and to some extent novel. Hence, I would be inclined to give the authors the opportunity to present their work at NIPS in the poster format (urging them to possibly provide more experimental evidence of the effectiveness of the approach).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
