{"title": "Learning with Fredholm Kernels", "abstract": "In this paper we propose a framework for supervised and semi-supervised learning based on reformulating the learning problem as a regularized Fredholm integral equation. Our approach fits naturally into the kernel framework and can be interpreted as constructing new data-dependent kernels, which we call Fredholm kernels. We proceed to discuss the noise assumption\" for semi-supervised learning and provide evidence evidence both theoretical and experimental that Fredholm kernels can effectively utilize unlabeled data under the noise assumption. We demonstrate that methods based on Fredholm learning show very competitive performance in the standard semi-supervised learning setting.\"", "id": "35464c848f410e55a13bb9d78e7fddd0", "authors": ["Qichao Que", "Mikhail Belkin", "Yusu Wang"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper describes a framework particularly useful for semi-supervised learning based on Fredholm kernels. The classical supervised learning optimization problem solved in kernel-based methods is extended to incorporate unlabeled information leading to discretized version of the Fredholm integral equation.\n\nQuality\n\nThe paper has high technical quality with well-supported claims by theoretical analysis and convincing experimental results. The proposed formulation leads to a new data-dependent kernel that incorporates unlabeled information. The classifying function differs from the classical represent theorem solution but it is still elegant and easy to compute. \n\nI have few comments: One is related to Equation 3 where the solution of the proposed optimization problem is described. Since the Authors mentioned that the associated kernel need not be PSD, does Equation 3 still hold in this case ? A proof on how Equation 3 was obtained could have been nice to see too. Another comment concerns the experimental section where the Authors chose an \"optimal\" parameter for all methods in the benchmark. How were those parameters determined ? What is meant with \"optimal\" ?\n\n\nClarity\n\nThe paper is clearly written and well-organized in most part. It is easy to follow and the main ideas are explain adequately. There are some typos though for example:\n\n* Line 82: \"... will a Euclidean ...\" -> ... will be a Euclidean ...\"\n* Line 83: \"or\" missing.\n* Section 4 title: \"Fredhom\" -> Fredholm\n* Line 247: \"porportional\" -> proportional\n* Line 283: \"That is, or...\" -> That is, for...\n* Line 352: Sentence needs to be rewritten.\n\n\nOriginality\n\nThe proposed approach is related to a recent NIPS publication ([9]) but it is novel in essence. It is clear how the work differs from this publication and others. \n\nSignificance\n\nThe results are important and significant. There was an significant effort to test the proposed method on several datasets in different application domains. \n Well-written paper with good theoretical basis and convincing experimental section.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors present an interesting new method for supervised and semi-supervised learning using Fredholm kernels. Many aspects of the method are discussed in detail. Overall, I find the paper very interesting.\n\nNevertheless, I still have a few suggestions and questions:\n\n- section 2 (from eq 1 to eq on line 99):\n\nAlso in the Nystrom method one considers an approximation to an integral equation\n(in that case kernel PCA). I think it would be good if the authors could mention this and also explain in what sense the approach is similar or different.\n\n- section 2 eq 2:\n\nIn the area of support vector machines and kernel methods also approaches with operator equations have been presented in literature. It would be good to mention this work and explain in which sense eq 2 is a special case of it (or explain how it differs).\n\n- section 2 line 128 representer theorem:\n\nEq 2 is less common in learning theory. The authors assume that a representer theorem exists. They should prove a representer theorem here or otherwise give a precise reference and explain how eq 2 is a special case of it.\n\n- section 4 eq 8:\n\nThe interpretation as soft-thresholding PCA is nice and interesting.\nHow does it differ from other methods like Fisher kernels and probability product kernels?\n\n- typos:\n\nline 165 Not that\nline 221 achived\nline 341 chossing\nline 672 Table ??\n\n New approach to supervised and unsupervised learning using Fredholm kernels, which are data-dependent.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors proposed a data dependent kernel called Fredholm kernels. The novelty of the kernel is to use unlabeled dataset in natural way. Authors showed that several theoretical property of the kernel and experimentally verified that the kernel compares favorably with existing state-of-the-art method. \n\nThe paper is well written and organized. \n\nDetailed comments: \n1. Is there any systematic approach to select kernel width for freadholm kernel? For example, under some assumption, can you select a kernel width by cross validation?\n2. It would be nice to add some toy experiments to show the case the proposed method might not work. I just want to know in which setting the proposed method is most effective.\n The paper is interesting and is very clearly written. I vote for an acceptance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
