{"title": "On Prior Distributions and Approximate Inference for Structured Variables", "abstract": "We present a general framework for constructing prior distributions with structured variables. The prior is defined as the information projection of a base distribution onto distributions supported on the constraint set of interest. In cases where this projection is intractable, we propose a family of parameterized approximations indexed by subsets of the domain. We further analyze the special case of sparse structure. While the optimal prior is intractable in general, we show that approximate inference using convex subsets is tractable, and is equivalent to maximizing a submodular function subject to cardinality constraints. As a result, inference using greedy forward selection provably achieves within a factor of (1-1/e) of the optimal objective value. Our work is motivated by the predictive modeling of high-dimensional functional neuroimaging data. For this task, we employ the Gaussian base distribution induced by local partial correlations and consider the design of priors to capture the domain knowledge of sparse support. Experimental results on simulated data and high dimensional neuroimaging data show the effectiveness of our approach in terms of support recovery and predictive accuracy.", "id": "74071a673307ca7459bcf75fbd024e09", "authors": ["Oluwasanmi O. Koyejo", "Rajiv Khanna", "Joydeep Ghosh", "Russell Poldrack"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Overview:\n\nThe paper proposes a framework for enforcing structure in Bayesian models via structured prior selection based on the maximum entropy principle. Although the optimal prior may not be tractable, the authors developed an approximation method using submodule optimization. Contructing priors with structured variables is an important topic, so this method should be able to make good impact.\n\n\nQuality\n\nThe paper is technically sound. The authors present some results on both simulated data and functional MRI data to show that the methodology is effective. The strengths and limitations are also discussed. \n\nClarity\n\nOverall, this paper is clearly written and well-organized. \n\nTypeos: Page 4, line 210 and line 211, \u201cCorollary 10\u201d should be \u201cCorollary 6\u201d?\nPlease make sure the references have the same format.\n \n \nOriginality\n \nIt has enough novelty to me if all proofs are correct.\n\nSignificance\n\nThe paper presents an important topic and suggests an approach that is computationally feasible and understandable. \n\n\n\n This is a nice work. I recommend acceptance for this paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper considers the problem of doing Bayesian inference when one would like to impose structural priors that restrict the support of the distribution. \n\nIt begins by motivating incorporating support constraints via the maximum entropy principle, where we would like to impose the constraint but deviate from some base distribution as little as possible, as measured by relative entropy. The distribution that optimizes this criteria is called the information projection of the base distribution onto the constraint set. The paper begins by showing that this principle leads to intuitive results in the case of support restrictions (the information projection of p onto support set A is 1{x \\in A} p(x) / \\int_{x \\in A} p(x) ), and that the notion can be used to describe Bayesian inference, where the posterior arises when the joint distribution is projected onto the support set defined by the observations.\n\nWhen we would like to perform an information projection onto the intersection of several sets, then the paper shows that this can be done by projecting onto each set in sequence in any order and the results are unchanged. \n\nThis motivates an inference procedure in the special case of sparsity restrictions, which leads to the paper's main algorithm. The idea of the algorithm is to structure inference as an inside and outside optimization, where in the inner optimization, a posterior is computed where the set of k entries allowed to be nonzero are fixed, and in the outer optimization, the set of nonzero entries is optimized over. It is shown that this optimization problem can be formulated as a submodular optimization problem, which leads the authors to propose a greedy forward selection strategy for choosing nonzero dimensions. Results against a variety of baselines show the method to work quite well.\n\nThe main pros of the paper are as follows:\n- the writing is very clear, and the arguments appear to be clearly justified; things seem technically correct\n- the formulation in terms of information projections is a clean and interesting way of formalizing things, providing arguments that help better understand what it means to add constraints to models\n- the algorithm appears to work well\n\nThe main con is that there is a lot of technical work to get to results that are not terribly surprising. From a practical perspective, I'm not convinced that Section 2 of the paper is necessary to get to the algorithm in Section 3. It seems that we could have simply observed that the sparsity constraints can be viewed as a union of simple constraint sets, then arrived fairly naturally at the objective that is used in Section 3, and then derived the algorithm in the paper. The fact that J(s) is submodular is easy to see. So while I enjoyed reading the paper, I'm not sure I left with any new tools for imposing prior knowledge in my models. Interesting, well-executed paper, but I was left a bit disappointed not to have gotten more in the way of methods for incorporating structural constraints into my models.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a framework for learning with prior constraints over the parameters and apply it in sparse learning with prior knowledge on the cardinality of the parameter support.\n\n* While this paper offers a new perspective that constraint over parameters can be enforced by projecting in KL-divergence, it does not seem to lead to new interesting theoretical results or new algorithms.\n\n* The proposed algorithm is not clearly presented. An outline, possibly in pseudocode, may be helpful.\n\n* The experimental result shows that the proposed method (Sparse-G) outperforms all other methods but the reason is not clearly explained. Moreover, the Spike-and-slab performs significantly worse than Lasso, which is inconsistent with results in the literature. One possible reason could be the setting of the hyper-parameter.\n\n* The writing is generally clear except: \n - The concept \"structure\" needs to be better explained. \n - Line 310 \"... spike and slab does not return sparse estimates ...\" is confusing as spike-and-slab is proven to be an effective \n prior for sparse learning. The paper proposes a new framework for learning with prior constraint on parameters based projecting distributions in KL-divergence. This is kind of interesting but does not seem to have any very useful theoretical or algorithmic insight.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
