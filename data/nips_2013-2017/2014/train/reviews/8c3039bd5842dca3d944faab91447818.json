{"title": "Factoring Variations in Natural Images with Deep Gaussian Mixture Models", "abstract": "Generative models can be seen as the swiss army knives of machine learning, as many problems can be written probabilistically in terms of the distribution of the data, including prediction, reconstruction, imputation and simulation. One of the most promising directions for unsupervised learning may lie in Deep Learning methods, given their success in supervised learning. However, one of the current problems with deep unsupervised learning methods, is that they often are harder to scale. As a result there are some easier, more scalable shallow methods, such as the Gaussian Mixture Model and the Student-t Mixture Model, that remain surprisingly competitive. In this paper we propose a new scalable deep generative model for images, called the Deep Gaussian Mixture Model, that is a straightforward but powerful generalization of GMMs to multiple layers. The parametrization of a Deep GMM allows it to efficiently capture products of variations in natural images. We propose a new EM-based algorithm that scales well to large datasets, and we show that both the Expectation and the Maximization steps can easily be distributed over multiple machines. In our density estimation experiments we show that deeper GMM architectures generalize better than more shallow ones, with results in the same ballpark as the state of the art.", "id": "8c3039bd5842dca3d944faab91447818", "authors": ["Aaron van den Oord", "Benjamin Schrauwen"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper introduces the idea of deep Gaussian mixture models. A GMM can be seen as consisting of a single isotropic unit norm Gaussian, where each of the components of the mixture consists of applying a different linear transformation to that Gaussian. This idea is extended to the case of a multilayer network, where each node in the network corresponds to a linear transformation, and each route through the network corresponds to a sequence of linear transformations. The number of mixture components is then the number of routes through the network.\n\nThis is a clearly written paper. The idea is both simple and good. Overall I liked the paper. I did think that the experimental results were disappointingly low dimensional, especially considering how much effort was spent in the paper discussing how to make the algorithm computationally tractable.\n\nAlso, I think the proposed algorithm is quite closely related to\nTang, Yichuan, Ruslan Salakhutdinov, and Geoffrey Hinton. \"Deep mixtures of factor analysers.\" International Conference on Machine Learning (2012).\nthis probably deserves a citation and discussion.\n\nI would strongly encourage the authors to release the source code for the experimental results as supplemental material. This makes the paper more convincing, increases citation counts, and encourages a culture of reproducible science which benefits everyone.\n\nMore detailed comments follow:\n\n39 - \"in function\" -> \"as a function\"\n\n73 - \"A_3,1\" -> \"A_1,3\"\n\n89 - You provide good arguments in 312-318 for why this is true for your algorithm. I'm skeptical that EM is more parallelizable than stochastic gradient descent in general though. Maybe phrase this more specifically.\n\n139 - There are an exponential number of paths through the network. Assigning a probability to each of them without factorizing them would become intractable quite quickly -- even for relatively small networks. I'm surprised this factorization isn't needed almost all the time.\n\n199 - This is commonly called the MAP approximation. Why not instead just sample a single nonzero gamma with probability proportional to pi (this is EM, with the E step represented as a sample from the posterior). Unlike when using the MAP approximation, this will give you an unbiased estimate of your learning gradient, and should lead to a higher log likelihood model.\n\n230 - \"in function of\" -> \"as a function of\"\n\n235 - In the plot it never converges to the optimum.\n\n251 - \"and scalable.\" -> \"and in a scalable way.\"\n\n307, 341 - If you want to do quasi-Newton optimization with minibatches, and without hyperparameters, you might try https://github.com/Sohl-Dickstein/Sum-of-Functions-Optimizer\n\n312-318 - Cool. This provides good support for the benefits of EM.\n\n331 - \"have to construction\" -> \"have to construct\"\n\n337 - with using -> using\n\n344 - I strongly suspect this is a side effect of the MAP approximation to the posterior, as opposed to not optimizing fully during the M step. (As a note -- EM can be seen as maximizing a lower bound on the log likelihood. Importantly, that lower bound increases even if the M step is not run to convergence.)\n\nFigure 4 - I don't think this X-axis makes much sense for the Deep GMMs -- there's nothing special about the number of components in the top layer. Number of parameters would probably make a better x-axis.\n\n436 - Author name listed as \"anonymous\".\n The idea is simple, good, and clearly presented. Overall I liked the paper. I thought the experimental results were unexpectedly weak.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes a Deep Gaussian Mixture model (Deep GMM), which generalizes Gaussian mixtures to multiple layers. The key idea is to stack multiple GMM layers on top of each other. One can view Deep GMMs as a generative model where a standard normal random variable is successively transformed through a path in a network of k layers, where a transformation (multiplication by a matrix and adding a bias) is performed at each layer of the network.\n\nOne can then construct an equivalent shallow GMM but with the exponential number of mixture components.\n\nIn general, this is quite an interesting idea and the authors provide various heuristics to speed up EM learning algorithm, including (1) using hard EM and (2) using a \"folding\" trick by folding all the layers above a current layer into a flat \"shallow\" GMM model (although this becomes expensive when considering bottom layers).\n\nHowever, my main concern is that this work is very closely related to the following work on a deep mixture of factor analyzers:\n\nDeep Mixtures of Factor Analysers (ICML 2012)\nYichuan Tang, Ruslan Salakhutdinov and Geoffrey Hinton\n\nespecially given the close connections between GMMs and mixture of factor analyzers. Similar to your construction, deep MFA can be \"folded\" into a shallow MFA and learning can be carried out using EM. One can also pretrain these models layer-by-layer.\n\nI think it would be important to highlight similarities/differences between\nyour work and deep MFA work.\n In general, this is a well-written paper. But given its similarity to the previously publish work, the authors need to clarify their novel contributions.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents a new model called DeepGMM, which is a MoG with an exponential number of components and tied parameters. Each component in the mixture correspond to a path through a fully interconnected multilayer network of affine transformations (where the matrix is the Cholesky of a positive-definite matrix)applied on a identity-covariance multivariate Gaussian. The weights of each component are not tied, although a factorisation is suggested in the paper for very deep networks.\n\nAs the authors comment, their hypothesis is that by tying the parameters of the many components a bigger number of components can be used while avoiding overfitting.\n\nThe authors also propose a hard-EM based training algorithm. For the expectation phase, coordinate descent and several heuristics are recommended to decrease the computational load. For the maximization phase the authors present three options: \nA batch GD method suitable for DeepGMMs with a small number of paths. Unfortunately, the authors give no specific figures.\nA batch GD method suitable for DeepGMMs modelling data of sufficiently small dimensionality\nA SGD method for bigger DeepGMMs\n\nExperimental results on two well-known dataset are presented. All experiments use the second of the aforementioned optimisation techniques.\n\n\nQuality:\nPros: The paper is technically sound and it main hypothesis supported by experimental results. Figure 4 shows by tying parameters it is possible to train a DeepMoG with 2500 effective components that offers superior performance than a untied MoG using 300 (which has more parameters), and using more components in an untied mixture offers no improvement.\n\nCons: The paper only show results on natural images. The inductive bias of a DeepGMM could be specially advantageous on this kind of data.\n\nClarity\nThe paper is well structured and reads well, but there are some typos (parrallizable -> parallelizable, netwerk -> network).\nIn Figure 4 the maximum value achieved is about 154.5 while in Table 1 it is reported as 156.2. Are these different experiments using more data instead of 500 thousand patches? If so it should be specified.\n\nIt would be of interest to report training times, does it take hours, days or weeks to train a model of 8x8 patches?\n\nIn Figure 4, if the goal is to show, the ability to train a MoG with many components without overfitting, it would be more interesting to show the effective number of components instead of the number of components in the top layer.\n\nIf the authors find space it could be interesting to show some samples from the model.\n\nWas the tinyimages dataset resized to 8x8 pixels, if not the likelihods should not be compared to those obtained on BSDS300 as is done on line 397.\n\nOriginality:\nThe particular technique for parameter tying in a GMM presented in this paper is new. Also the training algorithms presented (including heuristics) are of interest.\n\nSignificance:\nThe results are important. Although training and evaluation of densities at test time will have high computational cost, sampling should be very efficient. Also the idea is interesting and can be further built upon.\n The paper presents a new way of tying the parameters of a MoG that allows the authors to obtain state-of-the-art results on patches of natural images. The paper is interesting and easy to read.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
