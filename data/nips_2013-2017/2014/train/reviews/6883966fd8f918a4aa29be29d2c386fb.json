{"title": "Active Regression by Stratification", "abstract": "We propose a new active learning algorithm for parametric linear regression with random design. We provide finite sample convergence guarantees for general distributions in the misspecified model. This is the first active learner for this setting that provably can improve over passive learning. Unlike other learning settings (such as classification), in regression the passive learning rate of O(1/epsilon) cannot in general be improved upon. Nonetheless, the so-called `constant' in the rate of convergence, which is characterized by a distribution-dependent risk, can be improved in many cases. For a given distribution, achieving the optimal risk requires prior knowledge of the distribution. Following the stratification technique advocated in Monte-Carlo function integration, our active learner approaches a the optimal risk using piecewise constant approximations.", "id": "6883966fd8f918a4aa29be29d2c386fb", "authors": ["Sivan Sabato", "Remi Munos"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This work studies the problem of active learning in linear regression.\nUnlike classification, the objective in active regression is merely \nto improve the constant factors in the distribution-dependent rates \nof convergence compared to passive learning, since it is known that \nthe asymptotic dependence on the number of labels typically cannot be \nimproved compared to passive learning, and nor can the worst-case\nvalues of the constant factor. The paper argues that there is a \ndistribution-dependent constant factor in the rate of convergence \nof passive learning, which can sometimes be improved for active \nlearning.\n\nSpecifically, they propose a rejection sampling scheme, which alters\nthe sampling distribution to a more-favorable one, without changing\nthe optimal solution. However, the rejection sampler requires a sort \nof scaling function \\phi as a parameter, and obtaining good performance \nguarantees requires one to set this function carefully, and with some \ndependence on the joint distribution of (X,Y). Since this latter distribution \nis unknown, the algorithm attempts to optimize the choice of \\phi among\npiecewise constant functions, using an estimated linear function from an \ninitial sample. They prove a risk bound for this method, which can be \nmade to approach the ``oracle'' rate (where the optimal \\phi is given),\nand which is provably sometimes superior to the capabilities of passive \nlearning methods.\n\nOverall, this seems to be a solid contribution, which I suspect will have \na wide audience.\n\nMy one main reservation is that I would have liked to see more discussion\nof the dependence on K vs \\rho*_A in Theorem 5.1. There are some terms\nthat are increasing in K, while we would like \\rho*_A to decrease toward\n\\rho*, which presumably requires K to grow. Thus, the trade-off between\nthese two quantities can affect the rates. Some examples to illustrate \nhow we should expect this trade-off to behave, for some reasonable \ndistributions and sensible partition, would be helpful.\n\n\nminor comments:\n \nOne citation that is missing here is Efromovich (2005): Sequential Design\nand Estimation in Heteroscedastic Nonparametric Regression.\nThat work studies active regression as well (though for a nonparametric class),\nand also finds improvements in constant factors based on the degree of \nheteroscedasticity.\n\nI also want to echo a remark of one of the other reviewers, that the notation for \nP_\\phi seems a little strange to me. I believe the intention is that, denoting \nby Q_\\phi the measure having density \\phi with respect to D, we define \nP_\\phi as the distribution of (X/\\sqrt{\\phi(X)},Y/\\sqrt{\\phi(X)}), for \n(X,Y) \\sim Q_\\phi. This P_\\phi should then be well-defined, and \nL(w,D)=L(w,P_\\phi) would then follow via the law of the unconscious statistician.\nWas this the intended meaning of P_\\phi?\n A solid paper on improving the distribution-dependent constant factors in linear regression via active learning.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a new method for actively learning a linear regressor, under \nthe usual least-squares loss function. The method works roughly as follows:\n\n1. Let's say the data lie in R^d. First, a partition of the space is chosen.\nThen, each data point and response value are reweighted in a simple manner that\nis uniform within each cell of the partition and is specified by a single real\nnumber within each cell. The authors show that this reweighting leaves the\nsquared loss of any linear function unchanged.\n\n2. However, different weightings yield different rates of convergence for the\nlinear regressor. Here the authors are very insightful in the way they use\na recent generalization bound for linear regression.\n\n3. Finding the optimal weighting, the one that leads to the best rate of convergence,\nrequires labels. The authors give a parsimonious, active way to do this, and along\nthe way, to estimate the linear regressor.\n\n4. Steps (1)-(3) are all for a particular partition of space. The authors suggest\npicking successively finer partitions as the number of points grows.\n\nLabel complexity bounds are given for steps (1)-(3).\n\nComments:\n\nThis paper has a lot of novel ideas and insights: of particular interest are the\nreweighting method for the distribution and the way in which the new generalization\nbound for regression is exploited. The paper also presents an algorithm that can \nbe made reasonably practical. All in all, this is a significant advance in the state\nof the art in active linear regression.\n\nThere are a few things that one could quibble about:\n\n1. The analysis requires that the \"label noise\" for data point x be bounded by\nO(||x||^2). It would be nice to do away with this. Still, previous work on\nactive regression has made far stronger assumptions on the noise.\n\n2. No method is given for refining the partition, and there is no analysis of\nthe asymptotic rate that would be achieved. This doesn't bother me: in practice,\na reasonable partition could be obtained by hierarchical clustering, for instance.\n A novel and insightful paper that advances the state of the art in active learning forleast-squares linear regression.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper is hard to follow and not clear. It is based on recent results by Hsu et al on loss minimization (although their theorem is stated incorrectly). The paper lacks empirical validation; how we can benefit from active learning in parametric linear regression. The derivation of main theorems are not clear to me (if not wrong). \n\nIn equation 1, ||X||_* has not been defined earlier. \n\nPlease double check theorem 2.1. For sample complexity \"n\" should only appear once. I believe n > c log(n) is not what Hsu et al meant. \n\nThe derivation of L(W,P) = L(W,D) (on page 3) does not seem correct to me. P is a measure and as a result the second line seems bizarre. I think the result is correct though. \n\nIt is not clear what happens to the main theorem of the paper once \\Lambda_D is unbounded. This happens once the distribution is heavy-tail. I think in such situations the gap between Lemma 3.1 and Theorem 5.1 will be huge.\n\nIt is very important to support the claims through a set of experiments. \n\n\n I think this work needs improvement in terms of writing and stating the results. The authors should also make a better job in terms of writing the proofs. They are not clear. A set of real-world experiments are also missing.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
