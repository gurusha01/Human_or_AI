{"title": "Covariance shrinkage for autocorrelated data", "abstract": "The accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms. In high dimensional settings the sample covariance is known to perform poorly, hence regularization strategies such as analytic shrinkage of Ledoit/Wolf are applied. In the standard setting, i.i.d. data is assumed, however, in practice, time series typically exhibit strong autocorrelation structure, which introduces a pronounced estimation bias. Recent work by Sancetta has extended the shrinkage framework beyond i.i.d. data. We contribute in this work by showing that the Sancetta estimator, while being consistent in the high-dimensional limit, suffers from a high bias in finite sample sizes. We propose an alternative estimator, which is (1) unbiased, (2) less sensitive to hyperparameter choice and (3) yields superior performance in simulations on toy data and on a real world data set from an EEG-based Brain-Computer-Interfacing experiment.", "id": "fa83a11a198d5a7f0bf77a1987bcd006", "authors": ["Daniel Bartz", "Klaus-Robert M\u00fcller"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "840:\n\nthis is a very nice paper, with compelling theoretical, simulated, and real data results. i have a few majorish issues, and some minor ones.\n\nmajor\n\n-- one can choose lambda via CV or some theoretical tool. if the theoretical tool has no parameters, it is a clear win. however, there is a truncation parameter here. this manuscript did not convey to me *how* to choose b, and importantly, the extent to which the results are robust to this choice of b. if this method is to be adopted as the de facto standard, some discussion about how to choose b and robustness to that choice is necessary.\n\n-- given that the main justification of using this method over CV is computational time, one might also acknowledge that practitioners always weigh a trade-off between accuracy and time. \nclearly, this method is faster than CV, assuming we have a good way of choosing b. but, how accurate is it? if it is much less accurate, than the improvement in time might not be so useful. for example, in the real data example, we could simply use the average class covariances for the other subjects. this would be fast, parameter free, and maybe just as accurate?\n\nminor\n\n-- in eq 5, b is some constant that satisfies some properties as a function of n? please clarify more formally the assumptions on b. also, please explain b. please define the truncation kernel here.\n\n-- \"we will provide a complementary analysis on the behaviour of the estimator for finite n.\"\n\nperhaps state a 'complementary theoretical analysis', i was led to believe you possibly meant only numerical, which of course, is much weaker.\n\n-- line 206, space missing\n\n-- remarks on thm 1: i would like more explanation of the relative size of the 3 biases. the biases are a function of b, n, s and covariances. some plotting showing the relative magnitude, say, of bias(San) vs bias(BC) would be very helpful. for example, a heatmap showing bias(San)-bias(BC) for fixed n when varying b and s, or fixed function b_n and varying n & s.\n\n-- i don't understand the simulation setting. please explain it more clearly, with equations, the notation for the 'parameter matrix' is unclear to me, what are '/' meant to denote? also, i don't know the abbreviation 'cmp'. if you are just trying to save space, i recommend removing some paragraph breaks, and keep content as clear as possible.\n\n-- a supplementary figure justifying footnote 4 is requested.\n\n-- \"We average over R multivariate \" ok, what do you set R to be for these simulations?\n\n-- i think a better justification for *why* one would want to estimate a covariance matrix from an AR process, rather than the dynamics matrix, is in order. in the end of the manuscript, you demonstrate an important application that totally justifies, but leading up to that, i was wondering. very nice, could become new standard, provided some guidance on choosing b is provided, and demonstration that performance is robust to this choice of b, and accuracy is not so much worse than cross-validation.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose a novel bias-corrected estimator of covariance matrices for autocorrelated data. They provide simulated data as well as a real-world data set on brain-computer interfacing to demonstrate the superior performance of their estimator in comparison to a standard-, a shrinkage-, and the Sancetta estimator.\n\nI believe the authors address a very interesting problem and make an important contribution. At the same time, I find the manuscript rather hard to read and the experimental result on real data not particularly convincing:\n\n* The authors do not really introduce their notation. While most notation is obvious from its context, this makes the manuscript harder to read than it would need to be.\n* I did not quite understand the heuristic fix of the Sancetta estimator in Section 2.\n* Along the same lines, I would be interested in a more detailed explanation of the bias-corrected estimator in (6). As the discussion section is primarily a summary of what the authors have done, it might be shortened to have more space in Section 2?\n* I am missing some of the details of the decoding procedure in Section 4. In particular, how many CSP filters were used for decoding? Which frequency band did the authors use? How did they perform cross-validation?\n* The following is the primary concern I have with this manuscript: It appears to me that the authors use only two trials to estimate the CSP filters and only pick the two most discriminative CSP filters for the plots in Figures 6 and 7. This is not what one would typically do in this setting. I suspect that this choice has been motivated by highlighting the differences between the different estimators. Furthermore, it appears to me from Figure 7 that the differences in performance between the various estimators are not a result of a better estimation of the spatial filters, but rather due to a different ranking of the CSP components. One would typically not use the best two but the best six CSP filters. From my experience, it is quite likely that this set of CSP filters would include filters for left- and right sensorimotor cortex for all estimators. If so, the differences in decoding performances between the estimators are likely to be negligible. In order to be convinced that the bias-corrected estimator outperforms any other estimator, I would like to see decoding differences on CSP filters that at least focus on the same brain regions.\n\n\nTypos and minor comments:\n* I believe the normalization term is missing in (3)?\n* Section 2: \"rate of p\" should be \"rate of n\"?\n* The first sentence of the second paragraph in Section 5 is very hard to parse. Very interesting theoretical work. The experimental results on real data, however, have been tuned to look more impressive than they would be in a realistic setting.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The aim of the paper is to provide an unbiased and consistent method for the accurate estimation of covariance matrices in presence of high dimensional dataset with small number of examples subject to internal autocorrelation that further reduce the effective size of the datasets. The solution is based on a state-of-the-art approach proposed by Sancetta [San08] where covariance matrix is shrinked toward a diagonal matrix with a shrinkage intensity proportional to the variance of the covariance matrix. In this framework the paper proposes an analytical estimate of shrinkage intensity incorporating a bias correction that relates the coefficient with the effective size of data.\n\nThe proposed unbiased variance estimator is an incremental work ([San08]) and it does not provide a strong theoretical novelty. However, the advantage of proposed solution is theoretically sound, and an empirical evaluation on toy examples and on a real EEG dataset shows that the proposed estimate is actually comparable to the one in the original work and it is even better in case of small high-dimensional datasets. A comparison with CV (not just the computational cost) would have been very useful.\n\nTechnically, while being relatively clear, the paper has some flaws, as many times the notation is used without any introduction of its meaning, making it sometimes difficult to follow all the formulations. Moreover, I noticed changes in the formulation along the paper (indexes inversion). It seems that X is interchangeably assumed to be organized by rows or by columns. Finally, the figures are many times difficult to understand because of missing descriptions both in the captions and in the text. The paper proposes an incremental work with a limited originality, nevertheless, the proposed solution presents some advantages which have been proven theoretically and empirically. Technically it is well written but still needs some work to make it clearer, principally correcting some mistakes in the indexes and introducing the formal notation the first time it is used.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
