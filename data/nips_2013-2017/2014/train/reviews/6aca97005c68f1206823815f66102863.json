{"title": "Incremental Local Gaussian Regression", "abstract": "Locally weighted regression (LWR) was created as a nonparametric method that can approximate a wide range of functions, is computationally efficient, and can learn continually from very large amounts of incrementally collected data. As an interesting feature, LWR can regress on non-stationary functions, a beneficial property, for instance, in control problems. However, it does not provide a proper generative model for function values, and existing algorithms have a variety of manual tuning parameters that strongly influence bias, variance and learning speed of the results. Gaussian (process) regression, on the other hand, does provide a generative model with rather black-box automatic parameter tuning, but it has higher computational cost, especially for big data sets and if a non-stationary model is required. In this paper, we suggest a path from Gaussian (process) regression to locally weighted regression, where we retain the best of both approaches. Using a localizing function basis and approximate inference techniques, we build a Gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to LWR. Empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale, and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters.", "id": "6aca97005c68f1206823815f66102863", "authors": ["Franziska Meier", "Philipp Hennig", "Stefan Schaal"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The paper extends Locally Weighting regression (LWR) where the loss function is not a weighted combination of local models but individual data points are weighted by each model. By putting a Gaussian prior on the coefficients of the models, they end up with the local models as Gaussian processes (GP). First the batch version of the algorithm is presented where the centers of the local models is assumed to be fixed. Then, an incremental version is also introduced (in a Bayesian spirit where old posteriors are considered to be the new prior) where new models can also be added.\n\nQuality and Clarity\n\nThe paper is well written, easy to read. The transitions from LWR to LGP is well introduced.\n\nThe experiments present results from a robotic setup by learning the inverse dynamics of a SARCOS arm a KUKA arm. The results show that LGP outperforms LWPR that is considered to be one of the best state-of-the-art methods in inverse dynamics learning. Furthermore, better performance is achieved with less local models.\n\nThe authors mention article 'Local Gaussian process regression for real time' [16] but the connection between the presented method is loosely discussed. Comparison with [16] at the level of the experiments would be also very interesting.\n\nOriginality and Significance\n\nThe new weighting of the local models seems to be original and the experimental results highlight the significance of the presented method.\n The paper extends Locally Weighting regression (LWR) with a different weighting of the local models leading to local Gaussian models. Results show that the presented method outperforms the state-of-the-arm algorithm.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose a regression scheme based on locally weighted regression (LWR). By making links from LWR to Gaussian process (GP) regression, a probabilistic model is formed. The model initially appears to have scalability like a GP (cubic in N) but a variational scheme rescues the method. The updates in the variational scheme mimic the LWR procedure, recovering scalability of the algorithm. \n\nClarity\n--\nThe paper is very well written. It's easy to follow, the notation is simple and consistent, and sufficient detail is presented without being overwhelming. The link from LWR to GP regression is especially neat. Although it's clear that any individual model in the LWR scheme comprises a GP (as any linear Gaussian model), it seesm novel to interpret the entire methodology as such. \n\nIf one section of the paper is unclear, it is section 4. The authors may protest constraints on space, but this section seems somewhat anecdotal in comparison to the thoroughness of the rest of the paper. The presentation of the Algorithm is a help, I suppose. \n\nQuality and significance\n--\nThe paper is technically sound, is of interest to a large section of the NIPS community, and contains solid experiments. The experiments chosen represent interesting challenges and the proposal appears to make a good improvement in terms of speed whilst maintaining accuracy. \n\nI would really like to see a mention of the availability of the implementation, which would enhance the paper further. \n\nI have one quality related complaint: the probabilistic nature of the algorithm is not explored in the experiments. I would have like to have seen the average log-density of held -out data alongside the MSE. Whilst the LWR method might not provide probabilistic estimates, the proposed method and the SSGPR will. Surely in a robotics environment, where decisions have to be made under uncertainty, log p(y*) is a more informative measure than MSE? It is widely known that different GP approximations perform very differently in terms of predictive density (e.g. FITC usually provides conservative predictive density): perhaps the authors could provide a supplementary table with the log density scores? \n\nQueries. \n--\nTo make the variational approximation tractable, you have to introduce uncertainties via the parameters beta. In practise, what did these converge to ? Does this slight change of model have a strong impact?\n\nThe variational updates for the local models are independent, but the beta parameters are global: does this make the mode computationally costly? Do you interlace fewer of these updates with the local updates?\n\nTable 2: The SSGPR method was pre-trained with 200 features, but the LWGPR method was allowed to use around 500 local models. Would it be fair to say that both models are of the same complexity? Does the SSGPR method not do better with more that 200 features: I seem to recall that the method scales cubically in the number of features, so could you not have afforded a few more? In table 3 the discrepancy is more severe, I guess due to the offline-online differences in the procedures.\n\nSummary\n--\nA very well presented paper, enjoyable to read. I have a few technical questions, and my overall score may change depending on the authors rebuttals.  Great presentation, relevant topic, good experiemtns let down by lack of probabilistic quantities in the results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper describes a method for non-parametric model learning from incremental data. The example motivation is learning manipulator models in robotics, where the models are often high-dimensional and highly non-linear. The approach is to apply Gaussian regression over a series of smaller patches (which are also learned by pruning and greedy addition). The key innovation over current techniques is that the proposed method produces a generative probabilistic model and requires little parameter tuning.\n\nThe paper presents convincing test data that their method achieves its goals.\n\nAs minor suggestions for improvement, the authors hint at other benefits of using a fully probabilistic/generative model, it would be nice to explicitly state why a method that is fully probabilistic is important from a controls perspective. Also, it seems like the forgetting and learning rates of LGR are tuning parameters. Is there any way to show or quantify how many fewer and/or how much less sensitive the parameters of the proposed are compared to the state of the art techniques?\n This paper is well written and organized. The need for efficient non-parametric fitting of robotics data is convincing, as is the argument about less need for tuning.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
