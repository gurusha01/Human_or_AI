{"title": "Streaming, Memory Limited Algorithms for Community Detection", "abstract": "In this paper, we consider sparse networks consisting of a finite number of non-overlapping communities, i.e. disjoint clusters, so that there is higher density within clusters than across clusters. Both the intra- and inter-cluster edge densities vanish when the size of the graph grows large, making the cluster reconstruction problem nosier and hence difficult to solve. We are interested in scenarios where the network size is very large, so that the adjacency matrix of the graph is hard to manipulate and store. The data stream model in which columns of the adjacency matrix are revealed sequentially constitutes a natural framework in this setting. For this model, we develop two novel clustering algorithms that extract the clusters asymptotically accurately. The first algorithm is {\\it offline}, as it needs to store and keep the assignments of nodes to clusters, and requires a memory that scales linearly with the network size. The second algorithm is {\\it online}, as it may classify a node when the corresponding column is revealed and then discard this information. This algorithm requires a memory growing sub-linearly with the network size. To construct these efficient streaming memory-limited clustering algorithms, we first address the problem of clustering with partial information, where only a small proportion of the columns of the adjacency matrix is observed and develop, for this setting, a new spectral algorithm which is of independent interest.", "id": "fc528592c3858f90196fbfacc814f235", "authors": ["Se-Young Yun", "marc lelarge", "Alexandre Proutiere"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper proposes an efficient algorithm for clustering networks. The clustering utilizes the stochastic blockmodel and the authors show that the model can be fitted efficiently with partial data and memory constraints. \n\nSpecific Comments:\n\nThe version of the stochastic blockmodel reported is a very simple version of the model. Most applications of the stochastic blockmodel use a different intra cluster probability for each cluster and a different inter cluster probability for each pair. Can the blockmodel used herein be used to approximate the more general type of blockmodel?\n\nThe results appear to be restricted to the case where p and q have similar structure in terms of n because p=a f(n) and q=b f(n). Could the results be extended to other cases? I suspect that they may hold if p tends to dominate q as n gets large?\n\nThe paper would benefit in showing either one or both of the following:\n(a) a large but not huge example that shows how a standard stochastic blockmodel fit compares to the fast algorithm presented herein.\n(b) the results applied to a huge example (however briefly).  This paper proposes an efficient algorithm for clustering networks. The clustering utilizes the stochastic blockmodel and the authors show that the model can be fitted efficiently with partial data and memory constraints.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper studies community detection of stochastic block models in a streaming setting. Motivated by the need to reduce input data size when the number of nodes is large, the author(s) first considered a subsampling technique, where community detection is carried out on a random sample of columns is obtained from the adjacency matrix. The main result is a sharp threshold on the sampling rate. It is shown that below this critical rate, no method can recover the communities with vanishing error rate; and a modified spectral method is provided and proved to be consistent when the sampling rate exceeds the threshold. This result is further used to construct a sub-linear memory streaming algorithm that is consistent.\n\nThis paper addresses an important problem, and is clearly written. The theoretical results are sharp and solid. The algorithms are modifications of previous spectral methods, and their practicality is not obvious and is not demonstrated through data examples. This paper studies network community detection under the subsample and streaming context. It is an early effort in such a topic and is likely to make a good impact.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper constructs and analyzes SBM estimation algorithms for two settings: partial information (where only some columns of the adjacency matrix are observed), and streaming (where columns are observed one at a time). Information-theoretic lower bounds are established, and an asymptotic analysis of the estimation algorithms is conducted. No simulation study was provided, which I find concerning since some of the algorithms have difficult-to-set tuning parameters.\n\nQuality:\nI am not an expert on SBM theory, so I cannot comment on the quality of the proofs - but I did find the presentation to be accessible enough for my level of expertise. The theory appears quite complete, with information-theoretic lower bounds nicely matched by the algorithm upper bounds. I also found it nice how Algorithms 3/4 are simply extensions of 1/2, thus making an explicit connection between the streaming setting and partial-information setting.\n\nMy major concern is the lack of simulation study, and the fact that Algorithms 3 and 4 require tuning parameters (which I worry they will be highly sensitive to). While I am convinced that Algorithms 1 and 2 will work in practice, I cannot say the same about Algorithms 3 and 4.\n\nClarity:\nWell-written in most places, but there are some paragraphs of noticeably lower quality, that are obviously written by a different person.\n\nThe writing did not make the difference between Algorithms 3 (offline streaming) and 4 (online streaming) clear - it looks like the information is there, but scattered throughout Section 4. The authors should provide a summary paragraph stating exactly how those algorithms differ.\n\nOriginality:\nI only have superficial knowledge of the theoretical SBM literature, but this paper addresses settings (partial information; streaming) that are atypical in my experience. I would say this paper is original.\n\nSignificance:\nTo the best of my knowledge, I am not aware of SBM estimation algorithms for the partial information or streaming settings. I would consider this paper to be highly significant, had the authors shown via simulation that the algorithms were actually practical to implement and tune.\n\nMinor comments:\n-On page 8, the authors claim that the supplementary material contains a precise statement on the performance guarantees on their methods vs streaming PCA. I could not find this statement in the supplementary.\n-The abstract does not make it clear that the paper's contribution is solely theoretical, with not even a simulation study. If the paper is accepted as-is, its scope should be made clear in the abstract.\n The paper provides valuable insight into the task of network clustering, under the SBM model with either limited information about the network, or in the streaming setting. However, the lack of even a basic simulation study, coupled with the presence of tuning parameters in Algorithms 3 and 4, lead me to question if the presented algorithms are actually practical.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper describes two related algorithms for finding clusters under the stochastic block model assumption when there is partial information (in the sense that only a fraction of the columns of the adjacency matrix are revealed) or in a data streaming setting (where fewer than the size of the input bits are retained to perform the computation).\n\nPros: \n\nAnother contribution to an area of recent interest, both with respect to data-constrained computation, clustering matrices, etc.\n\nThe main algorithm splits the problem into two steps, which is probably the correct way to do streaming clustering under model assumptions like this stochastic block model. Making that explicit highlights both important points.\n\nCons:\n\nThe algorithm is of theoretical interest, as it makes unrealistically strong assumptions for realistic networks of even moderate size and since it doesn't work at important \"boundaries\" of edge density, e.g., extremely sparse graphs.\n\nClaims with respect to how this work fits into the streaming and community literature are very much overstated, as mentioned below.\n\nThe description of the algorithm seems to not make clear what I think is going on: that under the stochastic block model with large blocks, then input has very nice structure, and so the main goal of the algorithm is to make sure that the sampling doesn't overly concentrate certain quantities in the \"empirical\" estimates. For example, I think that is what the trimming step is doing, and I think that is similar to what Feige and others, Montaneri and others, and probably others have done. \n\nA few other comments. \n\n(1) The problem as stated is unrealistic for what most people would consider community detection in realistic networks. For example, the clusters don't overlap, and the results don't go through when the graphs are extremely sparse (which is typical) or extremely dense (which may arise occasionally). Thus, they are best viewed as contributing to the recent work on spectral-like algorithms for stochastic block (which also suffer from similar drawbacks) and related models under various computational constraints (which is of interest).\n\n(2) No empirical evaluation. This isn't necessarily a bad thing, since if there were it would likely be trivial (in the sense that they would simply show a phase transition which they establish) or overly-idealized (i.e., they would have to \"simulate\" memory limitations, since dealing with realistic memory management is very nontrivial).\n\n(3) The columns are revealed one by one. This is a rather strong assumption in terms of motivations, and it means that it is much easier to obtain much stronger results than if elements or blocks are revealed. For example, much stronger results are obtained in low-rank matrix approximation when columns are sampled than when elements are sampled. More formal data streaming models, e.g., in the theoretical computer science literature consider other data presentation formats.\n\n(4) Finally, if the authors don't know of other related work on clustering or partitioning or community finding, it is for lack of effort. For example, a few quick web searches find many papers, including the following papers:\n\nFENNEL: Streaming Graph Partitioning for Massive Scale Graphs, by Tsourakaki, et al.\n\nOnline Analysis of Community Evolution in Data Streams, by Aggarwal and Yu\n\nStreaming Graph Partitioning for Large Distributed Graphs, by Stanton and Kliot\n\nSparse Cut Projections in Graph Streams, by Das Sarma, Gollapudi and Panigrahy\n\nThese papers are all sufficiently different than the present paper that there is novelty in the present paper and no overlap. But they illustrate that it much more accurate to describe the present paper as an interesting but relatively minor improvement on recent work on stochastic blockmodeling under various well-motivated memory constraints than as the first community detection algorithms in the data stream model or as a paper that introduces the problem of community detection with partial information, both of which are claimed but neither of which is correct.\n Overall a reasonable paper on a topic of interest. If the paper is accepted, I suggest that the authors adjust their claims to be more modest and correct and that they also work on the presentation to highlight their few nice contributions.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
