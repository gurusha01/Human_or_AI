{"title": "Conditional Random Field Autoencoders for Unsupervised Structured Prediction", "abstract": "We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observed data using a feature-rich conditional random field (CRF). Then a reconstruction of the input is (re)generated, conditional on the latent structure, using a generative model which factorizes similarly to the CRF. The autoencoder formulation enables efficient exact inference without resorting to unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate insightful connections to traditional autoencoders, posterior regularization and multi-view learning. Finally, we show competitive results with instantiations of the framework for two canonical tasks in natural language processing: part-of-speech induction and bitext word alignment, and show that training our model can be substantially more efficient than comparable feature-rich baselines.", "id": "b9f94c77652c9a76fc8a442748cd54bd", "authors": ["Waleed Ammar", "Chris Dyer", "Noah A. Smith"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "REPLY TO AUTHOR RESPONSE\n\nThanks for the response. You write: \"It is possible we missed a related paper in the neural network literature. We would be grateful if you could provide citations of neural architectures which used hand-crafted features for unsupervised learning of structured outputs.\" \n\nThis may have misunderstood my point. I was really just making the same point as reviewer_26 (you responded to him or her). \n\nBut as it happens, I know of one branch of work loosely related to your question. Stoyanov et al. (AISTATS 2011, NAACL 2012) showed how to derive the structure of a sum-product feed-forward network by unrolling loopy sum-product belief propagation on a given graphical model. Each potential function in the graphical model can be parameterized in terms of hand-crafted features of its incident variables (e.g., it may be log-linear). These potentials convert into parametric weights of the network, reflecting the hand-crafted features. In addition, the structure of the network reflects the structure of the graphical model. The network is then trained by backprop to minimize prediction error at designated output variables. Is this \"unsupervised\"? Well, like the original graphical model, the resulting feed-forward network learns to predict the latent variables of the graphical model from the input variables in a way that is helpful for predicting the output variables. This could be regarded as \"unsupervised\" learning of a structure over the latent variables given the (input, output) variables. If the graphical model were given an auto-encoder structure where output = input, then their method would result in an unsupervised system very much like yours.\n\nSUMMARY\n\nUnsupervised learning of latent structure y given input x. This is a bottleneck method using an autoencoder p(x | y) p(y | x). Specifically, p(y | x) is a domain-specific CRF, while p(x | y) appears to be a simple independent model \\prod_i p(x_i | y_i). The \n y are compared with human annotations.\n\nIt is efficient to compute the objective (2) and its gradient. The objective is regularized log-loss of the reconstruction, marginalizing over y. \n\nMore generally, the autoencoder is p(\\hat{x} | y,phi) p(y | \\hat{x},x,phi) where \\hat{x} is some ad hoc simplified version of x and phi is side information. \n\nCOMMENTS\n\nThe results are quite good. In particular, the method seems to produce fairly stable improvements across multiple languages (8 for POS tagging, 3 for alignment as evaluated by MT). However, the paper and supplementary material appear to have been written hastily. It is often hard to figure out the details of what was done experimentally. \n\nThe method is formally attractive and might turn out to be an important building block in learning certain kinds of latent structure. (Particularly if reconstructing the input x is only one task in a multi-task learning setting, so that y should also be useful to predict other, supervised properties of the input x. The authors don't discuss this.)\n\nHowever, the authors don't do a good job of explaining why the method will do a good job of unsupervised learning. Their objective is to match human annotations, not just to learn representations that are useful for some other task. The usual difficulty in this setting is that the latent variables end up learning the \"wrong\" properties of the data -- not the properties that the humans annotated. The authors say something about this problem at L067, but they don't really explain why their method should do any better. Nor do they do any analysis of the experimental results to understand what was learned and why. \n\nThere are single-number comparisons with other methods, but no breakdowns or targeted experiments aimed at understanding what is going on. It's just left as a happy mystery. (There are no learning curves, either.) \n\nIt would also be helpful to explore whether the improved results are achieved because they reduce model error or reduce search error. For example, suppose method A (previous) and method B (this paper) find latent distributions p_A and p_B over the latent variables y. If each method is initialized separately at p_A and p_B, then does method A's objective prefer p_A and method B's objective prefer p_B, which implies that the objective is better? Or do they both prefer p_A or both prefer p_B, which suggests that the different stems from a search bias?\n\nThe authors suggest (L262) that their autoencoder architecture is more appropriate in some respects than neural autoencoders, although this assertion is not defended by discussion, empirical comparison, or error analysis. I think the authors are mainly motivated by the fact that the p(y | x) model can build on a long line of efficient, feature-engineered work on supervised structured prediction work in NLP. But there are many architectures that could use such features, including neural architectures.\n\nDETAILED COMMENTS TO AUTHOR\n\nThe p(\\hat{x} | y) model is described only obliquely. I have a good guess what it is, but please spell it out!\n\nWhat exactly are you evaluating? At L172 you mention the distribution p(y | x,\\hat{x},phi), under which, presumably, the probability of a given latent y is proportional to p(\\hat{x} | y,phi) p(y | x,phi). Does this mean that you are evaluating the 1-best y from this distribution? One or many samples from this distrubtion? Something else?\n\nIt's not possible to really understand the features. For example, supplement L103 says that auto+full features include functions of the y_i, but supplement Table 2 doesn't show what those features are. It is also not clear whether \"x_i, x_{i-1}\" in the \"full\" column of that table is talking about conjoined features or is a list of features. And it's not clear whether the full model also includes all of the h&k features or not.\n\nL147: Even before you introduced phi, you already allowed side information for the encoding phase, namely anything in x that was not part of \\hat{x}. So perhaps you want to say that phi is side information that is (also) available in the reconstruction phase?\n\nPossibly relevant is http://www.cs.cmu.edu/~nasmith/papers/gimpel+smith.naacl12b.pdf .\n\nSuggestion on how to handle the supplementary material: Currently this is formatted as a separate paper, which is confusing. Please set it up as appendices to the main paper, following the bibliography, without duplicate material. This will allow you to have a single bibliography, a single set of figure and equation numbers, and crossrefs with the main paper. After running through pdflatex, use pdftk to divide the document into the main paper and the supplement. \n This is an attractive method and the basic idea is quite appropriate for NIPS or ACL. The results appear strong. The downside is that there should be more analysis of why the method should work and why it does work (as well as a more careful description of the experiments, which I think we can trust the authors to fix).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents a CRF trained by auto-encoding through latent\nvariables y. (a) The P(y|x) is parameterized as in traditional CRFs.\n(b) The regeneration P(\\hat{x}|y) is a categorical distribution\nindependent for each \\hat{x}_i and y_i pair. \\hat{x}_i may be a\ntransformation of x_i, such as Brown cluster ids.\n\nThe paper is delightfully sensible and relatively simple (in a good\nway). It falls into the category of papers that makes the reader say,\n\"Why hasn't this been done before; I wish I had thought of it.\"\n\nI like it. The experimental results are positive, and I'm inclined\ntowards acceptance.\n\nThere is no reason this approach couldn't be used in a semi-supervised\nsetting. It would be great to see some results on these lines.\n\nAlong these lines, loosely related work that combines conditional and\ngenerative training with log-linear models (but not necessarly in a\nstructured output setting) includes: (1) Tom Minka. Discriminative\nmodels, not discriminative training. MSR-TR-2005-144, 2005. (2)\nAndrew McCallum, Chris Pal, Greg Druck and Xuerui Wang.\nMulti-Conditional Learning: Generative/Discriminative Training for\nClustering and Classification. AAAI, 2006.\n\nNear the top of page 5 you describe alternative methods that would\nrequire approximations on Z in the reconstruction phase. But I don't\nbelieve you provide empircal comparisons with those methods. Are they\nbeaten by the other methods you do compare against?\n\nOn a related note, reading after Equation (5), and the \"Feature HMM\":\nWhat do you loose by putting a \"multi-feature view\" of x only on the\nP(y|x) side, but not on the P(\\hat{x}|y) side? It would be nice to\nhave some discussion of this.\n\nSection 3.2: You say the goal is being \"coherent and interpretable\".\nWhy is this the goal? You don't evaluate interpretability?\n\nYou picked just a subset of the languages in the CoNLL-X shared task.\nThis seems suspicious. How did you select them? Why not show all\nlanguages?\n\nEspecially since scalability is an advertized advantage of your\napproach, I would have liked to see a graph of test accuracy as the\namount of training data increases.\n\nThe objective function for this model is certainly not convex. It\nwould be nice to see some discussion of local minima, and the extent\nto which you see empirial evidence of problems with respect to local\nminima. What initialization do you use?\n\n\n\n\n\nMinor writing issues:\n\nPage 2: \"offers two ways locations to impose\" -> \"offers two ways for\nlocations to impose\" ?\n\n\"condition on on side information\" -> \"condition on side information\"\n\nPage 6: \"and parsing Though\" -> \"and parsing. Though\"\n\n Well-written paper on a clean idea, with positive experimentalresults. Relatively simple---in a good way---such that I expect it tobe used and have impact.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents an efficient way of doing unsupervised structured prediction by using a CRF as the first layer of an autoencoder. The paper is very clearly written, the idea is of the \"I wish I had thought of this\" kind, and the experimental results in POS tagging and MT alignment are solid and well-discussed. The relationship to previous work is presented fairly with one notable exception: the paper of Suzuki and Isosaki on semisup CRF training at ACL 2008. If you squint a bit, their proposal is actually rather similar to yours. In my own words (their paper is quite a bit harder to read than yours), what Suzuki and Isosaki do is to jointly train a CRF p(y|x) and a generative model p(x'|y) with a loss function that bounds a combination of the error on labeled data and the disagreement between the two models on unlabeled data. Their general approach is way more complicated than yours, and your presentation is leaner and more effective, but I feel that you need to give a careful account of the connection between the two approaches (and take care to look at more recent related work of theirs as well).\n\nYou claim that neural autoencoders cannot learn latent sequential structure without labeled data. That feels like an argument from ignorance. Maybe we/some of us don't know how to do that, but RNNs and LSTMs have at least the potential for doing it. I'd suggest a more measured comparison between these differen types of autoencoders.\n A clearly written, convincing presentation of a simple but effective idea for unsupervised learning of structured predictors. I like this paper a lot, but the connection to some earlier work not cited needs to be sorted out.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
