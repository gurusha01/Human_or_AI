{"title": "Active Learning and Best-Response Dynamics", "abstract": "We consider a setting in which low-power distributed sensors are each making highly noisy measurements of some unknown target function. A center wants to accurately learn this function by querying a small number of sensors, which ordinarily would be impossible due to the high noise rate. The question we address is whether local communication among sensors, together with natural best-response dynamics in an appropriately-de\ufb01ned game, can denoise the system without destroying the true signal and allow the center to succeed from only a small number of active queries. We prove positive (and negative) results on the denoising power of several natural dynamics, and also show experimentally that when combined with recent agnostic active learning algorithms, this process can achieve low error from very few queries, performing substantially better than active or passive learning without these denoising dynamics as well as passive learning with denoising.", "id": "16e6a3326dd7d868cbc926602a61e4d0", "authors": ["Maria-Florina F. Balcan", "Christopher Berlind", "Avrim Blum", "Emma Cohen", "Kaushik Patnaik", "Le Song"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The paper considers the setting of a sensor network (or agents) in a noisy environment that are able to communicate locally. The authors prove that theoretical bounds on the number of active queries can be achieved through simple best response dynamics.\n\nThe paper is very well-written, technically correct, and the synthetic experiment makes the results clear. The theoretical results are novel and I think that the paper deserves to be published to be published at NIPS.\n\nFor the benefit of readers, the paper needs to cite, and relate to, recent papers in NIPS that have presented theoretical guarantees for active learning in the presence of noise:\n1. Near-optimal Bayesian active learning with noisy observations. Golovin et al. NIPS 2010.\n2. Extensions of generalized binary search to group identification and exponential costs. Bellala et al. NIPS 2010. This paper presents novel and interesting theoretical results for a sensor network in equilibrium of a consensus game, obtained through simple best response dynamics, in a noisy environment.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "SUMMARY:\nThis paper combines dynamics from game theory and active learning techniques to solve a problem where a center tries to determine a decision boundary from noisy sensors. Under the assumption that the noise is independent, the authors prove that simple simultaneous or asynchronous game dynamics successfully denoise the system with high probability, given enough sensors. To illustrate the effectiveness of the denoising together with active learning (which is known to perform poorly in the presence of large amounts of noise), the authors give experiments in the plane which show superior results compared with learning without denoising and passive learning with denoising.\n\nCRITIQUE:\nThe results are interesting, the approach is novel, and the exposition is clear. My only concern is the significance. I believe the authors make a good case in general that denoising a collection of sensors, using simple distributed local computations, can be effective enough in some situations to allow active learning to be applied, thereby enabling an efficient sensor query load. However, the bounds given in the analysis leave a lot of room to question the applicability of this approach in practice. Is it really reasonable to have N = 10000 for r = 0.1, as in the experiments? Some rough sense of reasonable settings of these parameters is badly needed to evaluate the utility of this approach. For example, if r = 0.8, then surely one would want a more sophisticated denoising algorithm, since the theoretical guarantees break down, and moreover nodes have unreasonable communication loads. Similarly, if r=0.001, then the communication graph is likely to be disconnected, unless N is even larger. What settings of r, N, eta, etc are reasonable? Likewise, is a uniform distribution reasonable?\n\nAs another high level point, I was surprised not to see more discussion of the 2r band around the separator within which all bets are off -- are we assuming r is so small that this is negligible? For large r, it seems that a slightly more sophisticated distance-weighted approach would be much better here. This (or some similar procedure) is stated in a footnote to perform similarly to the simple majority; intuitively, the two should behave roughly the same away from the boundary, but the distance weighting should perform significantly better near the boundary, even for smaller r. A more fine-grained comparison near the boundary would be nice here.\n\n\nSPECIFIC COMMENTS:\npg 1\n. This paper would be much more accessible if terms such as active learning and agnostic active learning were defined in simple terms after their first use (e.g. \"active learning, a branch of machine learning where...\")\npg 2\n. \"sensors within distance r are connected by an edge\" -- as mentioned above, the relationship between r and d (dim) and N is not discussed much in the text; aside from what values one should expect in practice, it might be good to mention some results from theoretical computer science / graph theory about the connectedness of the communication graph.\npg 3\n. it would help to give an overview of section 3 at the top, since it was not stated prior\npg 7\n. \"Pockets of noise appear to be more difficult to denoise.\" -- the algorithm was not designed for this case, so this is not surprising. One might expect pockets to be less severe in higher dimensions.\npg 8\n. \"A synchronous round\" -- perhaps change to \"One synchronous round\" to avoid confusion with \"asynchronous\"\n Combines consensus dynamics and active learning techniques to solve a problem where a center tries to determine a decision boundary from noisy sensors. Clear, interesting, novel, but needs more justification re significance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors study the following problem. There are N noisy sensors with local communication capabilities uniformly distributed in a region. There is an underlying function separating the region into positive and negative sub-regions. The objective of the external source is to accurately separate the region with as few queries from the source to sensors as possible. \n\nThe main idea is that local communication amongst sensors can be used to de-noise the system before costly communication with the external source. The authors analyze versions of a best-response dynamic that asynchronously or synchronously updates a sensor\u2019s reading to the majority reading of its neighbors. They combine the de-noising with an active learning algorithm from the literature, and show through simulations that this combination outperforms SVMs with de-noising as well as either algorithm prior to de-noising. \n\nThis work is interesting and relevant to NIPS, and is very well-presented. The technical results (both analytical and simulations) are of high quality. \n\nSome minor issues I had:\n\n\u2014 Figure 3 should have y-axes on the same scale so that random noise and pockets of noise can more easily be compared. \n\n\u2014 Based on the simulations, almost all the benefit is coming from de-noising the data. After the data is de-noised, SVM and active learning perform about the same (there are no error bars on the plots, but the difference does not appear to be meaningful regardless of significance). The authors call active learning \u201cour method\u201d, but the real contribution seems to be the de-noising. Hopefully this can be clarified in the paper.\n\n\u2014 De-noising performance was much better on random noise than pockets of noise in simulations, but the pockets of noise model (or generally, noise that is not independent across sensors) seems more realistic. This result somewhat weakens the contribution of the de-noising algorithm.\n\n\u2014 In Figure 2 (right), for synchronous updates with random noise, it looks like there is a slight increase in final noise as the number of rounds increases. This made me wonder: how do results look for 100K rounds with more simulations, and what does the max over final noise look like after each number of rounds? Given that the analytical results were for a single step with independent noise, and that running multiple steps will violate the independent noise assumption, it seems that there could be instances where error in an early step propagates to result in high final noise.\n\n\u2014 Not really an issue with the paper, but for consideration: the motivation of the de-noising algorithm is that local communication is cheap while communication with the external source is expensive. Practically speaking, another alternative would be for the sensors to communicate locally to collect information (without the best-response dynamic), and then have a single message sent to the source with information about all sensor readings. If this approach is impractical for some reason (e.g., it still requires more data to be sent to the external source, albeit by a single sensor), it might be worth further specifying the problem to rule this possibility out.\n A well-presented paper with interesting analytical and simulation-based results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
