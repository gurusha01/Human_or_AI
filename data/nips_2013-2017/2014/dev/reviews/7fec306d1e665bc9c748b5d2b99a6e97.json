{"title": "Probabilistic Differential Dynamic Programming", "abstract": "We present a data-driven, probabilistic trajectory optimization framework for systems with unknown dynamics, called Probabilistic Differential Dynamic Programming (PDDP). PDDP takes into account uncertainty explicitly for dynamics models using Gaussian processes (GPs). Based on the second-order local approximation of the value function, PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces. Different from typical gradient-based policy search methods, PDDP does not require a policy parameterization and learns a locally optimal, time-varying control policy. We demonstrate the effectiveness and efficiency of the proposed algorithm using two nontrivial tasks. Compared with the classical DDP and a state-of-the-art GP-based policy search method, PDDP offers a superior combination of data-efficiency, learning speed, and applicability.", "id": "7fec306d1e665bc9c748b5d2b99a6e97", "authors": ["Yunpeng Pan", "Evangelos Theodorou"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The proposed approach, while straightforward, quite elegantly handles the problem at hand. What prevents this paper from being a clear cut acceptance is the lack of adequate experimental validation.\n\nTypos line 47: draw -> drawn\n\nA more thorough discussion of noise in the exploration step of Algorithm 1 (step 8) would be appreciated. This issue is also not discussed in the experiments section (how much noise was used?).\n\nI also had a few issues with some of the claimed advantages in the paper. Specifically:\n(1) The claim that PDDP has an advantage over PILCO since it does not have to solve non-convex optimization problems seems suspect given the non-convexity of the optimization problem solved in the hyper-parameter tuning step.\n\n(2) The claim that PDDP\u2019s complexity does not scale with the dimensionality of the state seems inaccurate. Even if it doesn\u2019t factor into the computational complexity of the policy learning step, it clearly factors into the computation of the kernel matrices necessary for the GP inferences. This issue should be clarified in the final version.\n\n(3) The claim on line 364 that PDDP keeps a fixed size of data seems only accurate in the special case where I_max = 1 (that is when trajectory collection and optimization are note iterated).\n\nMy biggest complaint with the paper is that the experimental results are not very strong. Firstly, there is no comparison of the total cost for PILCO versus PDDP. Secondly, is it the case that there are no straightforward methods to speedup PILCO? A discussion of this point would be appreciated. Lastly, GPDDP appears to do as well or better than PDDP on every dimension (data efficiency and computational efficiency). This seems to undermine the idea that PDDP has the benefit of doing \"safe exploration\". A discussion of this point would strengthen the experiments section.  The authors present a very nice, approach to combining Gaussian Process Regression and Differential Dynamic Programming that elegantly handles the exploration / exploitation tradeoff. The principal downside of the paper is in the experimental validation.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a DDP formulation for models represented as Gaussian processes. \nThe derivations of the required equations that connects the Gaussian Process model with the DDP framework are provided.\nThe method allows the solution of local optimal trajectory problems for systems whose dynamics are not known in advance.\nAs such this work shares several characteristics with PILCO. The simulated results show that solutions are found much faster than PILCO. \n\nQuality and clarity\n\nAlthough the concept of the paper is strong, the paper needs more work to be done, particularly regarding the presentation of the results.\nPresentation of the results is currently too shallow, especially the discussion, which is basically non-existent for figures 1 and 2.\nClaims such as \"safe exploration\" lacks support and the computational complexity conclusion is questionable.\nThe writing could be refined in general; I am not referring to specific sentences, but it feels this work is lacking some additional number of polishing iterations on the writing.\n\nOriginality and Significance\n\nThe fact that DDP is being proposed as the optimization method has an impact for the DDP/iLQG practitioners and also for the model-based optimization community. Unarguably the presented work shares several similarities with PILCO both in the philosophy and also the methodology. However, while the work of PILCO basically kept the choices of the optimizer open, this work suggest DDP and develops the full framework thus providing a self-contained solution.\nThis work can be much improved if extensive and careful analyses between PDDP and PILCO are provided.\n\nQuestions \n\n. Line 33: \"... Compared to global optimal control approaches, the local optimal DDP shows superior scalability to high-dimensional problems...\" is an incomplete statement. The big trade-off is that DDP solution is only local and prone to provide a poor a local optima. I agree that for high-dimensional problems a local solution is usually the only option. If there was no trade-off there would be no reason for the full dynamic programming.\n. Line 212: The sentence \" ... where the variance of control distribution and the off-diagonal entries of the state covariance matrix can be neglected because of no impact on control performances.\" is not obvious especially regarding the control distribution. Noise in control can have a large impact on how the states integrate over time. It may also influence the final cost. Are you assuming a deterministic system?\n. Line: 294. The conclusion on the computational complexity is precipitated. Yes, although the complexity of the learning policy does not relate directly to the number of states, for the usual case of fully actuated systems, the number of control inputs is proportinal to the number of states which makes the complexity of the algorithm O3.\n. The formulation of the \"safe exploration\" feature is not clear in the text. Although Figure 3(a) is a cartoon that shows the intuition, where is this intuition implemented in the algorithm? \n. Line 322: \"To keep the system stable\" seems to be a bad sentence, unless you are talking about unstable system such as an inverted pendulum. Do you mean \"final zero velocity\"?\n. The variance in (a) seems simply to small, almost a deterministic system. As the GP is learning the system from samples one can infer that the variance during the initial iterations are large and decrease with the number iterations. It would be interesting to see such a plot.\n. I do not see the point of proposing another method (GPDDP) that does not bring any benefit. What is the motivation for that? If what you want to show is the effect of the propagation of uncertainty then you can just say that without the need of introducing the GPDDP as a new second method.\n. The paper must also compare the traditional DDP results when dynamics are fully known in Figs 1 and 2.\n\nMinor questions:\n. Legends for Figure 1 are too small. The y axes of (a) and (c) should be on the same range.\n. Why not showing the costs for PILCO on figures 1(b) and 2(b)?\n The paper provides a self-contained framework for GP-based DDP which is of high interest for the DDP/iLQG learning community. The paper, however, is not clear enough and the evaluation, particularly in regards to PILCO, is insufficient.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose to use a Gaussian Process (GP) dynamics model learned from data together with a DDP-like trajectory optimization algorithm operating on the mean and covariance of the state distribution. The proposed method is compared to PILCO and an ablated variant (using just the mean as the state), and achieves slightly worse results but at a very large improvement in computation time.\n\nIt appears that, besides learning a GP model of the dynamics, the proposed method is simply performing Gaussian belief space planning. This connection is not mentioned by the authors, but seems pretty clear. For example, the paper \"Motion planning under uncertainty using iterative local optimization in belief space\" describes a method that, except for the GP learning phase, appears extremely similar (there are numerous other methods in this general \"family\" of belief space planning techniques). The novelty of the approach therefore would seem to be quite low, as the method is simply taking belief space planning and combining it with learned GP models. Since belief space planning algorithms are typically agnostic to the form of the simulator used, a GP-based dynamics model would be straightforward to combine with any previous belief space planning algorithm, yielding an extremely similar approach.\n\nThere are several other issues with this work. Among previous methods, the authors only compare to PILCO. The body of belief space planning work is completely ignored, though perhaps this is because such work often does not consider learning the model (since any model learning technique can easily be used in conjunction with any belief space planner). However, there is also other work on model-based RL to compare to. Since the authors are claiming computation time as a major advantage of their method, the choice of PILCO doesn't really make sense, since PILCO has exceedingly extravagant computational requirements that are far outside the \"norm\" for model-based RL. Without a comparison to any other method, it's not clear whether the improvement in the tradeoff between computation and sample complexity is actually notable.\n\nThe title of the paper is also a bit too ambitious. There are plenty of papers that discuss the \"probabilistic\" aspects of DDP, including a paper entitled \"Stochastic DDP,\" papers that deal with noise and uncertainty (including state and action dependent noise, etc), and of course the previously mentioned field of belief space planning. Something like Gaussian Process DDP would have been more appropriate, but since the primary contribution of the paper appears to be combining belief space planning with learned GP models, something about \"GP models for belief space planning\" might also make sense.\n\n-----\n\nRegarding the rebuttal: I'm not entirely clear on the distinction that the authors are drawing between their approach and belief-space planning. In belief-space planning, the underlying *true* dynamics is sometimes assumed to be Gaussian (not the same as deterministic, but it's true that E[f(x)] = f(E[x])). But the resulting mean and covariance propagation equations look similar to the ones presented in this paper: stochastic transitions in terms of the state are turned into deterministic transitions in terms of the state mean and covariance. If there is some distinction between them, it should be demonstrated experimentally, as it is not at all obvious from the description whether one method is better or worse.\n\nI do appreciate the clarification with PI^2, though I'm not sure if this algorithm is the best candidate for a comparison, since requiring 2500 iterations to solve pendulum swingup seems quite excessive (see for ex NP-ALP, Reinforcement Learning In Continuous Time and Space Doya '00, and other papers that compare on the pendulum benchmark). Another *model-based* method would be a better candidate here I think, since you are comparing model-based methods with a particular kind of inductive bias (smooth dynamics functions). The proposed method appears to be DDP-based belief space planning combined with a learned GP dynamics model. In light of prior work, this doesn't seem particularly novel. The experimental results also fail to convincingly demonstrate the advantage of the proposed method in terms of the computation cost-sample complexity tradeoff, since the only prior method compared to (PILCO) has unusually high computational cost compared to most model-based RL methods.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
