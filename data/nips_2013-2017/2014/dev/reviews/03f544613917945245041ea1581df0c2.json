{"title": "QUIC &amp; DIRTY: A Quadratic Approximation Approach for Dirty Statistical Models", "abstract": "In this paper, we develop a family of algorithms for optimizing superposition-structured\u201d or \u201cdirty\u201d statistical estimators for high-dimensional problems involving the minimization of the sum of a smooth loss function with a hybrid regularization. Most of the current approaches are first-order methods, including proximal gradient or Alternating Direction Method of Multipliers (ADMM). We propose a new family of second-order methods where we approximate the loss function using quadratic approximation. The superposition structured regularizer then leads to a subproblem that can be efficiently solved by alternating minimization. We propose a general active subspace selection approach to speed up the solver by utilizing the low-dimensional structure given by the regularizers, and provide convergence guarantees for our algorithm. Empirically, we show that our approach is more than 10 times faster than state-of-the-art first-order approaches for the latent variable graphical model selection problems and multi-task learning problems when there is more than one regularizer. For these problems, our approach appears to be the first algorithm that can extend active subspace ideas to multiple regularizers.\"", "id": "03f544613917945245041ea1581df0c2", "authors": ["Cho-Jui Hsieh", "Inderjit S. Dhillon", "Pradeep K. Ravikumar", "Stephen Becker", "Peder A. Olsen"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Summary: The authors propose a new Newton-like method to optimize the sum of a smooth (convex) cost function and multiple decomposable norms. Their contributions are (1) an active subspace selection procedure that allows to speed up the solution of the quadratic approximation problem (2) a proof that solving the quadratic approximation problem over the (changing) active subspace still leads to convergence. The authors also provide numerical results showing that, for two important problems, their methods gives 10x speed up over state-of-the-art methods and, in the appendix, give numerical results that illustrate which fraction of the speed up is due to the quadratic approximation technique and which fraction of the speed up is due to the active subspace selection method.\n\nQuality: The amount of critical information in the appendix makes this paper more suited for a journal than a conference. \nAlthough I did not check all of the proofs, the ones I did check are correct and well written (apart from a few minor typos).\n\nClarity: The paper is very well written and organized. A few minor suggestions and questions follow. The references could be numbered in order. Including a curly-braket inside the $\\min$ in eq. (1) would make it more clear. In line 114, should not the subscript of $\\|x\\|$ be $(1,\\alpha)$ similar to what is written in line 116 ? In Prop. 1, line 236, it would be helpful to clarify that the orthogonal subspace is a function of $\\theta$. In line 259 it seems like the over-bar on top of $Q$ is a notation that was not introduced. In line 266 is would be helpful to clarify that $D$ does not need to be diagonal. Line 299 makes reference to eq. 11 and in Th. 1, line 303, makes reference to eq. 8. The appendix also makes reference to eq. 8. This is confusing since in eq. 11 the optimization is constrained and in eq. 8 it is not. It would help clarify that it is because the problem is quadratic that optimizing over the \"free\" subspace is equivalent to optimizing over the whole space. In line 308, \"gives\" should be \"give\". In line 354, it could be useful to remind the reader again that $\\Delta_D$ does not need to be diagonal. In Fig. 1, the authors should explain what the percentages mean as well as what the times mean.\n\nOriginality: Although there are ideas in the literature related to this work (and these the authors clearly point out) their specific active selection method is new and so is their proof of convergence.\n\nSignificance: The 10x speed up is very encouraging and I am certain that the algorithm in this paper will be significant for several practitioners.  A well written paper with significant theoretical and practical contributions. Unfortunately, the amount of critical information in the appendix makes this paper more suited for a journal (so that reviewers can read all the proofs) than for a conference where there is an 8 page limit on the part of the papers that reviewers are supposed to review.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper addresses the question of estimating so called dirty models in which the parameter is a sum of \"simple parameters\", complexities of which are individually regularized. The paper proposes a subspace selection strategy that makes quadratic approximation feasible. The theoretic development builds on the notion of decomposable norms. \n\nI have to confess that I have not followed this line of work very closely, so my comments are necessarily those of a generally knowledgeable person in machine learning and model selection.\nIt appears that the work is a neat combination of ideas that have been published before, such \nas minimizing nuclear norm via active subspace selection (ICML 2014), and using quadratic approximation for sparse inverse covariance estimation (NIPS 2011), but the work is now carried out in a more general setting of decomposable norms. To me, this appears elegant, but some questions remain. \n\nThe algorithmic achievement of 10-fold speedup does not sound very impressive, such factors are usually easily achieved by optimizing the implementation, thus the question becomes how easy it is to optimize (say parallelize etc.) the most demanding parts of the algorithm. For very practically oriented person the achievement may not appear great.\n\nThe presentation of the theory is in general rather clear even if somewhat compact possibly due to page limit. The writing is not impeccable - some of the language errors are distracting to the level that hampers comprehension. The use of citations as a part of sentence is generally not encouraged and it creates new grammatical problems: should we write \"[7, 5] consider the estimation ...\" or \"[7, 5] considers the estimation ...\"? Sometimes poor punctuation creates problems like in \"[14] in turn use a superposition ...\" or \"we consider instead a proximal ...\". \nAt worst the sentences are ungrammatical to the level of being incomprehensible like the sentence: \"Overall, our algorithmic ....\" on page 2. \n\nWriting formulas inside the text creates a set of problems of its own and sometimes the result is awkward - too often do we see a line ending in equal-sign (say 0 = \\newline). On page 7, the formula breaches the margin as does the figure 1 and the table 1 on page 8.\n\nIn general, it is customary to have some kind of conclusion or discussion in the paper. Now there is none. For example, discussion about the future directions would be welcome, since it is not clear to me where to go from here.\n\nIn the algorithm 1, line 7, the superscript (t) should probably be (r), and updating the sum on line 8 looks strange (one should update variables rather than expressions).\n\n The theoretical development for using quadratic approximation for dirty models with decomposable regularizers appears interesting, but it is not clear to me how big a step this is in practice. To some extent, the work appears to me as an end of a branch of development rather than an interesting new opening. The editing is not quite good enough.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors extended the results in [11] and [12] to more general settings. The proposed computational framework works for\n(a) Strongly convex and twice differentiable Loss functions.\n(b) Decomposable regularization functions.\nThe theoretical analysis sounds reasonable, and the numerical results are also convincing. (b) is natural to sparse learning problems, but (a) seems a little restrictive, since many sparse learning problems are formulated as nonstrongly convex problems.\nHere are my two concerns:\n(1) Eq.(16) seems strong. Could the authors provide some examples?\n(2) Block coordinate descent algorithms are also competitive for solving the optimization problem in Section 5.2. Could the authors provide some detailed comparison?\n\nResponse to the rebuttal: If we also consider the statistical error, then we usually do not need a \"too accurate\" solution. To more comprehensively comparing the proposed algorithm with the first-order block coordinate descent algorithms, the authors may need to conduct more experiments, e.g., timing v.s. classification error in multi-task learning. My conjecture is that the local quadratic convergence might not be so competitive in term of the statistical error. But I agreed that it would not hurt to get a \"very accurate\" solution if the computational resource is affordable.\n Overall, I am positive for the quality of this paper. I would like to see this paper appearing in NIPS.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
