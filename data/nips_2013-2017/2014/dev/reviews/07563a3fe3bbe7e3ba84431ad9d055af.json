{"title": "Discriminative Unsupervised Feature Learning with Convolutional Neural Networks", "abstract": "Current methods for training convolutional neural networks depend on large amounts of labeled samples for supervised training. In this paper we present an approach for training a convolutional neural network using only unlabeled data. We train the network to discriminate between a set of surrogate classes. Each surrogate class is formed by applying a variety of transformations to a randomly sampled 'seed' image patch. We find that this simple feature learning algorithm is surprisingly successful when applied to visual object recognition. The feature representation learned by our algorithm achieves classification results matching or outperforming the current state-of-the-art for unsupervised learning on several popular datasets (STL-10, CIFAR-10, Caltech-101).", "id": "07563a3fe3bbe7e3ba84431ad9d055af", "authors": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Martin Riedmiller", "Thomas Brox"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper described a novel method to train convolutional neural networks (CNNs) in an unsupervised fashion such that the model still learns to be invariant to common transformations.\n\nThe method proposed by the authors is simple and, some extent, elegant. The idea is to simply train the CNN to distinguish image patches and their transformations from other image patches and their transformations.\n\nThis approach allows the model to learn to be invariant to common transformation. However, as the authors mention, the method is vulnerable to \"collisions\" where distinct image patches -- that the model will try to distinguish -- share the same content. \n\nThe method is novel and, while simple, isn't necessarily obvious (at least not to me). It's a clever idea and I think, with some minor changes (see below), it deserves to be published. \n\nThe author also present theoretical result that the authors claim support the argument that the model learning invariant features. I did not find this analysis particularly compelling as it seems to basically just state that the if the model were to find a perfect invariant feature representation, then it would achieve the global optimum of the objective function. This seems obvious from the definition of the objective function. Perhaps I've missed something more subtle.\n\nThe empirical exploration of the proposed approach is relatively thorough and insightful, including experiments carefully exploring the potential vulnerability described above as the number of surrogate \"classes\" (or base training patches) increases. The experimental results also include an evaluation of the invariance properties of the learned representation. On\nthe empirical side, this work is relatively solid and mature.\n\nMy only quibble is that in presenting the state-of-the-art results for STL-10, CIFAR-10 and Caltech-101, the authors neglect to provide a comparison with the actual state-of-the-art (particularly for CIFAR-10). They state that they are not comparing to standard discriminatively-trained CNN models -- the motivation for this line in the sand seems arbitrary and frankly a bit self-serving. The problem is that the presentation in the paper clearly leaves the reader with the impression that this method approaches the state-of-the-art for all these datasets. This is clearly not the case and it isn't acceptable to leave that impression with the reader. I ask simply that the authors provide the true state-of-the-art for each of these datasets, with and without dataset transformations.\n A good paper about a simple, but clever idea well explored empirically.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a new approach to discriminative unsupervised feature learning. Given an unlabelled set of images, 'seed' patches are extracted which act as surrogate classes for training. For each of these seed images, a variety of transformations is applied to generate training examples within the seed classes. A convolutional network is trained on top of the patches to predict the seed classes. After training, feature are extracted by forward propagating a new image, extracting the feature maps at each layer and pooling over them. Experimentation is performed on several standard image classification benchmarks, with additional experimentation to determine the effectiveness of transformations and invariance properties of the learned features.\n\nThe proposed approach is simple and demonstrates strong performance on each of the classification tasks. The additional analysis, particularly the effects of removing transformations, was most welcome. The paper is clearly written for the most part with enough information given for the reader to reproduce the results.\n\nDetailed comments / questions:\n\n- Can you go into more detail about patch extraction? Are you just sampling a large number of patches, computing the gradients for each one, and choosing the top-N?\n\n- You should mention that the STL-10 dataset uses 96x96 images earlier in the paper, in order to give context for the choice of 32x32 patch size.\n\n- Was the 32x32 patch size arbitrary chosen? Do you have any intuition on how to select patch sizes as a function of the image sizes?\n\n- section 4.1: A picture or two, perhaps one illustrating the convnet during surrogate training and another illustrating feature extraction would be really helpful. The description is fairly clear for a reader familiar with this line of work but may be challenging to understand for those that are not.\n\n- From a practical point of view, given a dataset with a small amount of labels, one of the simplest things to try is feeding the images through a pre-trained convolutional net on ImageNet and training a linear classifer on top of these features. I think it would beneficial to include these results in table 1, even through they fall under a different category of algorithms (you could include this in a separate block of the table). It would be interesting to see how your results compare (at least on Caltech 101, your results are on par to DeCAF)\n\n- How much does validation on the surrogate task act as a proxy for validation on the classification task? Does the network with the best validation performance on the surrogate task also have the best validation performance on the classification task? Figure 3 hints that this might be true. Table 1 in the supplementary material presents classification accuracies with several networks. I would highly recommended including (maybe in a separate table) both the surrogate validation scores as well as the classification validation scores for each of these tasks, to see how well they correspond.\n\n- Supplementary, table 2: How come the diagonal is different than the results in table 1 with the same network?\n\n- To my best knowledge, the best published result on STL-10 is 70.1% +- 0.6% from \"Multi-Task Bayesian Optimization\" (Swersky et al, NIPS 2013). They achieved their results by first extracting k-means feature maps and training a convolutional network on top of these. By fixing the k-means features, the network is much less prone to overfitting. A strong baseline that I'd recommend the authors try, if time permits, is to use the same approach they do but include each of the transformations you use in the paper. This give a relatively straightforward way of running convnets on small labelled datasets which I suspect would be very competitive with your approach. In summary, this is a nice paper with a simple algorithm that gets very good results on standard benchmarks. I suspect the generality of this method should lead to several interesting research directions. For these reasons, I recommend acceptance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In this paper, the authors propose an interesting idea for learning invariances, in the unsupervised learning setting for image classification problems. The key idea is to create surrogate classes consisting of groups of transformations of randomly selected patches. The transformations are 2D functions such as rotations, scaling, translations and color shifts. The formal analysis is interesting and sheds further light on why this approach could work.\n\nThe paper is well written and easy to follow. The experiments cover a number of aspects of the problem, including number of transformations and size of network. \n\nOne drawback of the approach in the paper seems to be that two randomly selected patches which are very similar in content will be \"forced apart\" since they will be considered different surrogate classes. The authors implicitly sidestep this by using only a maximum of 32000 patches from large datasets. However, using very few surrogate patches may impede generalization in case of larger datasets. Do the authors have any comments on this? \n\nAnother experiment which would be nice is to have used the same set of surrogate classes for unsupervised learning and then training dataset-specific classifiers. For example, since STL-10 and CIFAR-10 are closely related, why did the authors need to retrain on each dataset? \n\nFinally, experiments on varying patch size would also have been interesting. \n\n\n\n\n In this paper, the authors propose an interesting idea for learning invariances, in the unsupervised learning setting for image classification problems. The key idea is to create surrogate classes consisting of groups of transformations of randomly selected patches. The paper's idea is novel and interesting for unsupervised learning. It is well written and easy to follow.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
