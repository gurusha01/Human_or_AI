{"title": "On a Theory of Nonparametric Pairwise Similarity for Clustering: Connecting Clustering to Classification", "abstract": "Pairwise clustering methods partition the data space into clusters by the pairwise similarity between data points. The success of pairwise clustering largely depends on the pairwise similarity function defined over the data points, where kernel similarity is broadly used. In this paper, we present a novel pairwise clustering framework by bridging the gap between clustering and multi-class classification. This pairwise clustering framework learns an unsupervised nonparametric classifier from each data partition, and search for the optimal partition of the data by minimizing the generalization error of the learned classifiers associated with the data partitions. We consider two nonparametric classifiers in this framework, i.e. the nearest neighbor classifier and the plug-in classifier. Modeling the underlying data distribution by nonparametric kernel density estimation, the generalization error bounds for both unsupervised nonparametric classifiers are the sum of nonparametric pairwise similarity terms between the data points for the purpose of clustering. Under uniform distribution, the nonparametric similarity terms induced by both unsupervised classifiers exhibit a well known form of kernel similarity. We also prove that the generalization error bound for the unsupervised plug-in classifier is asymptotically equal to the weighted volume of cluster boundary for Low Density Separation, a widely used criteria for semi-supervised learning and clustering. Based on the derived nonparametric pairwise similarity using the plug-in classifier, we propose a new nonparametric exemplar-based clustering method with enhanced discriminative capability, whose superiority is evidenced by the experimental results.", "id": "069059b7ef840f0c74a814ec9237b6ec", "authors": ["Yingzhen Yang", "Feng Liang", "Shuicheng Yan", "Zhangyang Wang", "Thomas S. Huang"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper proposes a new pairwise clustering framework where nonparametric pairwise similarity is derived by minimizing the generalization error unsupervised nonparametric classifier. The proposed framework bridges the gap between clustering and multi-class classification, and explains the widely used kernel similarity for clustering. The authors also prove that the generalization error bound for the unsupervised plug-in classifier is asymptotically equal to the weighted volume of cluster boundary for low density separation. Based on the derived nonparametric pairwise similarity using the plug-in classifier, the authors propose a new nonparametric exemplar-based clustering method with enhanced discriminative capability compared to the exiting exemplar-based clustering methods.\n\nHere are few comments.\n\n1. In equation (6) which which provides and estimate for the regression function, shouldn\u2019t there be a \\pi^{(i)} in the numerator as was in line 123?\n \n2. In lemma 2, kernel bandwidth h should be h_n\n\n3. typo: line 63, remove double the.\n\n3. The theoretical results are interesting and nice, especially the connection to the low density separation is interesting.\n\n4. While the theoretical results are nice, its application towards developing new nonparametric examplar based clustering algorithm looks a little bit complex.\n\nOverall, this is an interesting paper.\n Overall, this is an interesting paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents clustering framework using pairwise similarity function model.\nDifferent data partitions are used to construct different nonparametric classifiers.\nThe optimal clustering is obtained by minimizing the generalization error of the \nlearned classifiers related to data partitions.\nThe authors consider two nonparametric classifiers in their framework:\nthe plug-in and the nearest-neighbor classifier. \nThe authors prove also a result interesting in itself, namely that the generalization\nerror bound for the unsupervised plug-in clustering is asymptotically equal to the\nweighted volume of cluster boundary for Low Density Separation. \n\nThis paper tights together two important machine learning subjects: multi-class \nclassification problem and clustering. Furthermore, it has strong theoretical background.\nThe paper is however very poorely written.\nThe introduction is well-organized however in the next sections the authors\ngo into several technical details and do not summarize their results in the \ncompact understandable form. Two technical lemmas given in Section 2 only deepen\nthe confusion. Thus, it is extremely difficult to measure the real impact of this paper.\nSection 2 contains too many technical details that are given before giving the reader any sort of intuition. \nBesides the presented lemma statements are completely unclear. \nFor instance:\n\nLemma 1:\n\n - is it possible to say how n_{0} depends on \\sigma_{0} and VC characteristics of K ?\n - the statements seems not to be put in the grammatical form, the authors should review it\n\nLemma 2:\n\n - again the authors introduce many parameters that mutually depend on each other,\n it is almost impossible to guess from all the asusmptions required by the lemma what it \n says in fact\n\nLemma 3:\n\n - similar problems to those mentioned above\n\n\n\nLemma statements are unreadable and some of them are put at the very beginning of the paper.\nThe authors should spend much more time to work on the presentation of their results.\nMaybe some summary of the obtained theoretical results should be conducted before giving exact statements.\nThe authors should also give some intuition regarding all lemmas presented in the main body of the paper.\n\nThe authors present how their techniques may be applied to the exemplar-based clustering but the comparison\nwith existing state-of-the-art methods is missing. The experimental section is very tiny and does not shed \nany light of advantages of the technique developed by the authors over other clustering algorithms.\nThe difficulty of the clustering heavily depends on the considered setting: whether data is truly high-dimensional,\nor maybe lies on the small-dimensional manifolds, whether data points are sparse or dense, \nwhether the groundtruth clusters are convex or there are no\nassumptions about their shape, what is the objective function the authors aim to minimize or the groundtruth\nclustering they want to approximate well. Without detailed analysis of the performance of the algorithm in the specific\nsetting defined by these parameters it is almost impossible to say whether the presented method can be applied in \npractice and solves a nontrivial problem.\n\nWhat is the computational complexity of the algorithms that use presented technique ? This issue is not addressed at all.\nIs it possible to extend the analysis for other classifiers (only nearest-neighbor and plug-in classifiers were analyzed) ? \nIs it possible to release some source code files (with the implementation of the presented method) that were used by \nthe authors to test their approach ? (The last remark is not crucial though.)\n To sum it up, this paper focuses on the very important machine learning problem and is somehow innovative (presentsnew ideas that according to what authors say: \"bridge the gap between clustering and the multilabel classification\"; tosome extend I do agree with aurhors' statement). However it seems to me that the authors do not spend much time onpreparing a readable version of the paper. The presentation is chaotic, it is very hard to understand what are the maintheoretical results of the paper. The experimental section is incomplete. No comparison with currently existing methods is conducted.I would suggetst the authors to spend much more time on completely reorganizing the paper according to the points I mentioned above.In particular, sections: 2 and 3 should be completely rewritten.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In this paper, the authors take a different look at discriminative clustering. Their central idea is the following: they imagine a classifier that separates the different (cluster) classes, and derive a bound for the generalization error induced by this classifier. This is the bound that they then attempt to optimize using a clustering algorithm (in this case, a belief-propagation based method). \n\nThe main feature that distinguishes this work from prior work is that they reformulate the cost function as a new kind of similarity measure that makes use of a kernel density estimate, so as to avoid a parameter search problem. This is not to say that they don't have parameters in their formulation: but these are either balance paramters or a variance parameter for the kernel density function they estimate. \n\nOverall, I think this is a reasonable idea, and does have some merit in terms of reducing the number of parameters for discriminative clustering. It also provides a better story about what exactly the clustering algorithm is attempting to optimize. \n\nIn terms of whether the idea has merit in practice, the paper is a little thinner. The main comparison is to a different exemplar-based clustering approach. This to me seems a little odd, since one of the claimed selling points of the paper is that it requires fewer parameters than other discriminative clustering methods. Would it have been more useful to compare to MM clustering or even the information-theoretic approaches as well ? The space spent proving consistency of the KDE could very well have been utilized for such a comparison, because the consistency results follow fairly directly from known results. \n\nI didn't find the paper very easy to read, mainly because of the excess of notation used to explain the main ideas. It would have benefited (somewhere) from a final \"here is our algorithm\" explanation that puts everything together. While I understand how the pieces fit together, I spent some time trying to track down various parameters (including the variance h) and how they were being used in the overall algorithm. \n A reasonable idea that deserves consideration. But could do with a more thorough experimental evaluation.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
