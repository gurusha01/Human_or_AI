{"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "abstract": "A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.", "id": "17e23e50bedc63b4095e3d8204ce063b", "authors": ["Yann N. Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "---------------------\nBasically, I think it's really important to show that |H| is better than the Fisher/Natural Gradient matrix in an \"apples to apples\" comparison. Same setup, same learning rate heuristics, only different curvature. It's important because the Fisher matrix is known to be much better for nonlinear optimization than the Hessian. The experiment that you mention show that |H| is also much better than the Hessian. So the big question is, which is better: |H| or F, where both matrices are used in the same setup.\n\n---------------------\nOptimization methods assume that they are given convex approximations to the objective function. However, this assumption is clearly false due to the presence of the large number of saddle points in high dimensional spaces. The idea of explicitly modelling and dealing with saddle points is very interesting, and the results are intriguing: working with the absolute value of the eigenvalues really does seem to result in better performance on the tasks considered.\n\nHowever, I feel that the experiments do not quite prove the point of the paper as well as it could have. Specifically, the experiments do not show convincingly that working with the absolute value of the Hessian is better than the Natural Gradient, or the Gauss Newton. To be precise, I noticed that Saddle Free Newton was initialized by SGD. In contrast, previous work on second order methods, including Martens (2010), did not initialize from SGD. And while you report better results, it is possible that most of the improvement was caused by the use of SGD as a \u201cpretraining\u201d stage of the second order. Thus the paper would be much more convincing if alongside SFN it included the same experiment (with the same optimizer, Krylov Subspace Descent), with the Gauss-Newton matrix. At present, there is only a comparison with damped newton. \n\nThe second issue, which is small yet significant, is that too much space and emphasis was placed on unnecessary formal justification of the method, introducing a new trust region method (which isn\u2019t new --- the classical derivation of natural gradients minimizes the linear gradient over a quadratic constraint, which is precisely equivalent to an elliptical trust region method). Likewise, the |H| matrix has positive eigenvalues, and therefore dividing by this positive definite matrix is a proper optimization procedure. \n\n I strongly recommend the authors to replace the Hessian with the Gauss-Newton / Natural Gradient matrix while keeping everything else fixed (and spending the appropriate amount of time tuning the new setting), in order to see whether it is the Hessian that contributes to the better performance, or whether it is the SGD pre-training.\n\n\n The paper proposes an interesting and a new way of optimizing high dimensional objectives by explicitly handling saddle points and negative curvature. The experiments are interesting, and the results are good. However, the experiments do not prove the main point of the paper as well as it should.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary:\n\nThis paper argues that saddle points, and not local minima, are the primary difficulty in gradient- or Netwon\u2019s method-based optimization of high dimensional non convex functions such as deep neural networks. The paper reviews related theoretical results, empirically demonstrates the problem in deep neural networks doing visual object recognition, and proposes a solution which is shown to be effective for training deep neural networks.\n\nMain comments:\n\nThis is an original paper that could significantly reshape intuitions about learning in deep networks (and about other non convex optimization problems). The review of relevant theoretical work focuses on results that are not widely known in the deep learning community, and a useful contribution of the paper is the empirical verification on MNIST and CIFAR of the relationship between training error and index, showing that the result was not dependent on the simplifying assumptions used in the theory.\n\nIs there a justification for only using the k biggest eigenvectors of the Hessian? It seems like the Krylov method is going to ignore directions with low or negative curvature that could be seen near saddle points. In particular, it seems possible that the Hessian could look fully positive definite in the krylov subspace when in fact there are other descent directions with negative eigenvalues. Consider the classical saddle from Fig 2a. My understanding is, if the Krylov subspace had dimension 1 it would point in the high positive curvature parabola even though the negative curvature direction exists. Hence the specific approximate SFN method proposed here seems like it could also be trapped by saddle points for which there are more positive, large eigenvalues than the Krylov subspace dimension. \n\nIt could be interesting to visualize the weights at the end of the MSGD epoch compared to a few iterations after SFN takes over. Is there a clear pattern to the changes, such as an unused unit with very small norm weights that becomes active? There may not be but if there is, it would be interesting to gain intuition: In practice is this mainly overcoming scaling symmetries across layers? Permutation symmetries among hidden units? Etc.\n\nThe paper does not particularly emphasize its result of lower error for the deep auto encoder compared to Hessian free optimization, but it should be noted that the Hessian free result did not use carefully tuned MSGD to initialize its behavior. This could have impacted the comparison.\n\nThe paper is clear and well written.\n\nMinor:\n\nIt would be helpful to put a vertical line on Fig 4b where the switch to SFN occurs.\n The paper convincingly argues that saddle points, not local minima, represent a key difficulty for current learning algorithms applied to non-convex models like deep neural networks. The paper highlights an algorithm able to more rapidly escape saddle points.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper argues about the existence of saddle points, which becomes the main burden that first order methods (such as SGD), and some second order methods (such as methods derived from Newton method) fail to exploit. \n\nIn particular, if a saddle point is encountered, an algorithm should exploit the direction of negative curvature to make progress: SGD methods will simply fail as they don't make much progress on directions of low curvature (as it has been argued in pervious art), while second order methods based on trust region will generally not exploit directions of (large) negative curvature, along which rapid progress could be achieved.\n\nThe authors propose a method that follows Vinyals and Povey's work, except that instead of using the Gauss Newton matrix, and regular Newton method on the Krylov subspace, they instead propose a new trust region method that generalizes to non-PSD Hessian matrices. They use the absolute value of the eigenvalues to set the trust region. This copes with saddle points in a much better way than SGD does (since it uses curvature), and much better than Newton based methods (since it can escape saddles naturally).\n\nSome comments to the authors:\n-At around line 315, perhaps clarify what you mean by k biggest eigenvectors - are they the biggest in eigenvalue absolute value? If not, how can the Krylov approximation capture negative curvature to be exploited by your algorithm?\n-The improvement with SFN seems dramatic in, e.g., PTB. Could the authors report perplexities or some more standard metric on the held out set, for comparison's sake (carefully tuned SGD does very well on the PTB).\n-Implementing these methods can be quite tricky. Are the authors planning on releasing code or a precise set of instructions with all the heuristics used?\n-From a practical stand-point, how do things like regularization (e.g. dropout) affect saddle points? \n-What was the network architecture, and how would the authors deal with ReLu nets? Can we say something about those in terms of second order behavior? Good paper, tackling an important and general problem in machine learning and deep neural nets - optimization. This paper argues about saddle points being a major cause for first order methods (e.g. SGD) to suffer for slowness and stuckness. Enough evidence and fairly in depth analysis is given on several machine learning tasks that supports the main argument of the paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
