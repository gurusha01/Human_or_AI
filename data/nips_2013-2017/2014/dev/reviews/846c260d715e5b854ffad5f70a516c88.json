{"title": "Spectral Methods for Indian Buffet Process Inference", "abstract": "The Indian Buffet Process is a versatile statistical tool for modeling distributions over binary matrices. We provide an efficient spectral algorithm as an alternative to costly Variational Bayes and sampling-based algorithms. We derive a novel tensorial characterization of the moments of the Indian Buffet Process proper and for two of its applications. We give a computationally efficient iterative inference algorithm, concentration of measure bounds, and reconstruction guarantees. Our algorithm provides superior accuracy and cheaper computation than comparable Variational Bayesian approach on a number of reference problems.", "id": "846c260d715e5b854ffad5f70a516c88", "authors": ["Hsiao-Yu Tung", "Alexander J. Smola"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Spectral methods are based on decomposing moment tensors. If data are generated from latent variable models, their empirical moment tensors have a kind of (approximate) low-rank decomposition (approximate due to both noisy observations and finite sample estimation error in the empirical moments). These decompositions can be computed from the moment tensors, e.g. using a kind of power iteration method on related (symmetrized) tensors. The basic ideas of these spectral methods are fairly well established and many examples have been explored, especially in [6].\n\nThis paper applies spectral fitting methods to data modeled as being generated from an IBP, including two common emission models (a linear gaussian model and a sparse factor analysis model, described in Section 2). The main ingredients are a calculation of the appropriate moment tensors and corresponding symmetrized versions (Section 3), an application of the standard tensor power decomposition method (Section 4), and concentration proofs to offer recovery guarantees when data are generated from the model (Section 5). There are also experiments (Section 6).\n\nThe paper applies existing methodology and 'turns the crank' in the context of the IBP. While having such methods available for the IBP is interesting to the community, their development would fit better as an example in a paper like [6] than a full NIPS paper. Therefore the technical contribution and originality seem low.\n\nThere could be contributions here that I'm not appreciating. For example, there could be substantial deviations from the standard machinery that are required, either for the algorithm or for the analysis in Section 5. In developing the algorithm, there is a need to use fourth-order moments (Section 3.1), though this doesn't seem like a significant leap. In the analysis of Section 5 the authors suggest (around line 281) that they provide a more general methodology for producing relevant concentration of measure bounds, but again it is not made clear whether this is a contribution of great originality or significance outside of the scope of this paper. If there are contributions in this paper that differentiate it from a 'turn the crank' application of standard machinery, the authors should highlight them in their response and clarify them in the text.\n\nThe paper is decently written overall, though some points that seem relevant are not discussed. For example, Section 5 seems cut short and seems a bit disconnected from the rest of the paper. It could benefit substantially from more explicit connections back to the IBP. In addition, it could be made clear how the notion of concentration of measure interacts with the nonparametric generative model; does the size of K grow with the number of samples m? (The references [6,7] do not mention the Dirichlet process explicitly as far as I can tell, so I'm not sure if the nonparametric aspect of the story is made clear in some existing work without a more direct reference.) Also, does the notation for the number of samples switch between m and n in Sections 5.1 and 5.2?\n\nThe experiments in Section 6 seem a bit weak. Both the first and second involve data that are essentially generated from the model (the first being synthetic data and the second being the IBP classic of objects-in-images) and therefore they don't shed much light on whether the method can perform well when data are not extremely close to following the model's assumptions, a question which seems especially relevant for spectral methods. In the short discussion of the third experiment, there isn't much to reveal the advantages and disadvantages of the proposed spectral algorithm over other algorithms. If the proposed method and analysis are essentially an application of existing machinery to the IBP, then experiments could provide a real contribution in shedding light on when spectral methods may be appropriate and when they may fail. But this experiments section does not provide such insight.\n\nHere are some miscellaneous points:\n- Since [6,7] don't mention the Dirichlet process explicitly (based on searching the text), the reference around line 38 seems strange.\n- Line 113 says all proofs are deferred to the appendix, but one proof remains in the paper.\n- The {0;1} on line 134 should probably be {0,1}, though that sentence could be revised for clarity.\n- The comment on line 142 that \"[t]his is undesirable for the power method\" could be clarified.\n- The plots in Figure 1 look like they were saved to a raster format, which makes them hard to read. The text and line sizes should probably be adjusted.\n- In Figure 3, was the MCMC algorithm stopped short at 1.72 seconds, or was there some reason to believe it was stuck in such a suboptimal mode? While the topic is interesting, the paper explains an essentially standard application of spectral methods to IBP models and does not shed light on when to choose the spectral method over more traditional ones.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Spectral methods have recently been proposed as an alternative and efficient framework for inference based on characterizing moments in order to identify model parameters. In this paper the authors extend this framework to the Indian Buffet Process (IBP) as well as two models based on the IBP namely the linear Gaussian Latent Factor Model and the Infinite Sparse Factor Analysis model based on Laplace and Gaussian priors forming respectively a sparse PCA and ICA type of decomposition. Multilinearity of higher order moments are explored and non-identifiability issues in the third order moment is handled by also analyzing the structure of the fourth order moment. An algorithm is proposed for spectral inference and the framework demonstrated to be more efficient than Variational and MCMC inference. Extensive derivations of moments and proofs of bounds are provided in the accompanying appendix. \n\nThe paper builds on the existing ideas of spectral approaches (ref. 5-8) and presently extend these to the Indian Buffet Process. This extension including proofs of bounds is non-trivial and the very condensed paper provides a very technical and detailed mathematical analysis which may warrants its publication. From an application point of view the method appears both to be simple to implement and to work well in practice which may make the framework be adapted by other researchers giving the paper a substantial impact.\n\nWhile the experiments indicate that the method is more efficient than a variational approach it would improve the paper to analyze how the approach compares in accuracy to a full MCMC procedure. Comparison is provided to MCMC for one synthetic example and it is shown that in less computational time better reconstruction is achieved. However, how would the method compare to the MCMC procedure run for much longer? The results for the real gene expression data also appear to be meaningful but again how do these results compare to Variational and MCMC inference? \u2013 and do these existing alternative approaches also identify in the order of 10 (similar) components? Clearly, the spectral approach proposed seems to be accurate and very efficient but it would be interesting to see in practice how accurate the results are compared to more exhaustive MCMC procedures. \n\nIn order to determine the dimensionality K in algorithm 1 a truncation to eigenvalues larger than epsilon is applied in order to determine the rank of S_2. As determination of K is very central to non-parametric approaches it would improve the paper to further discuss the influence of truncation for determining the order K and how epsilon/the truncation was chosen. In general the spectral approaches admit to quantify model order by truncating eigenvalue decompositions as also remarked by the authors. Thus it would improve the paper to elaborate on how easily this truncation level can be defined in practice. It is mentioned that K can be determined by the largest slope of the eigenvalue spectrum \u2013 how reliable can this largest slope be identified in general?\n\nThe orthogonal tensor decomposition requires a number of random initializations to be reliably estimated. How many random initializations are needed in practice and how severely do local optima impact the results? (It is unclear in the paper how critical this issue is.)\n\nMinor comments:\nSynthetic dataset figures to the right: should 0101 not be 0110 and 1001 be 1010? Otherwise, please explain how the binary notation is used to reflect the symbols learned.\n\nThe notation is not always clear in terms of what are scalars and vectors. I.e., \\pi is a vector but when writing \\pi^2 I believe element wise exponentiation is used. Some places C_x appears as vectors but other places they are treated as scalars. Please clarify this notation to make the paper more assessable. Also diag is used both some places to form a diagonal matrix but also to form an order fourth diagonal tensor.\n\nMinor comments to the extensive appendix:\nLine 511: x^T A -> v^T A\n\nEquation 32 is top -> \\top\nEquation 36 please clarify how E_x becomes only E_z in line 36 and then in line 37 E_z, E_y. \nLine 669: In this section, we provides bounds for moments of linear gaussian latent feature model -> In this section, we provide bounds for moments of the linear gaussian latent feature model\n\nLine 968: Before starting the put everything together -> Before starting to put everything together\n\nLine 1126: in order to \u2013 do you here mean: in order for / in order to assure?\n\nPlease explain the Poly(.) notation.\n\nLine 1126: Lamma 11 -> Lemma 11\n The authors extend the existing spectral approaches to the Indian Buffet Process including the Linear Gaussian Latent Feature Model and Infinite Sparse Factor Analysis providing what appears to be a useful and very efficient inference framework. Considering non-parametric models it is unclear how well the model orders in general can be determined.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper developed spectral algorithms for the models of [18] and [24]\nby developing general theory for the IBP and applying it. Algorithm 1\nis a precise description of the approach. Very impressive.\n\nTheorem 3, a challenging result, doesn't really imply anything in\npractice, other that \"eventually it works\", so experimentation is\nreally needed. Can you say anything about the nature of the\npolynomial?\n\nThe experimental work looks at some toy data from [18]\nand then considers one gene expression data problem.\nThe experimental work is thus suggestive but not \nconvincing in any way. More comparisons are needed.\n\nNow it seems your experimental work is on par with the\nstandards of the original papers, such as [24]. Their experimental\nwork is \"illustrative\" since they have a new model.\nYour experimental work needs to be more thorough in showing the\ncomparison with the original. You work on a few toy\nexamples. The interesting result, Fig. 4 is not done for MCMC.\nWas it able to cope with the size? Perhaps not.\n\nAs an example, consider Reed and Ghahramani ICML 2013. \nThey give a variety of algorithms compared on\nrealistic looking data sets, including an analysis\nof computational complexity.\n\nRegarding introductory statement:\n\"the issue of spectral inference in Dirichlet Processes is largely settled [6,7]\"\nThis statement is completely unsupported by the references cited.\n\nEquations (2) and (3): you mean IBP(\\alpha) ?\n Impressive spectral algorithm with a convergence result looks good, but the experimentalcomparison is on toy data, whereas the standard for comparisons for new algorithmsis set by Reed and Ghahramani 2013 ICML. Better experimental work needed since thetheory is very vague on performance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper shows how to apply a method-of-moments technique to\ninference in the Indian Buffet Process (IBP). The IBP is an important\nnonparametric Bayesian model and has many applications.\nExisting inference methods based on sampling are computationally\nexpensive. The method-of-moments approach to inference developed in\nthis paper is based on the tensor decomposition approach from [6]. The\nmain contributions of the present paper are: the moment analysis for\nthe IBP that motivate the proposed method, computational and sample\ncomplexity analysis for the proposed inference method, and a\ndemonstration of the method on some synthetic and image recovery\nproblems, showing a favorable comparison to the \"infinite variational\napproach\".\n\nThe paper quite well-written and is well-motivated. One takeaway from\nthis paper is the view of nonparametric models as \"effectively\nparametric\" models, which permits a \"parameter estimation\" approach to\ninference. This is quite similar to approach taken in [7] for\ninference in the mixed-membership block model. I think this is a nice\nidea that is worth promoting, and the present paper does this well.\n\nAs mentioned above, the main technical contribution is the moment\nanalysis for the IBP. One interesting aspect of the IBP is that the\nthird and fourth order moments can be missing some components with\ncertain \"dish probabilities\", but together they contain all of the\ncomponents.\n\nThe concentration inequalities are fairly standard, although there is\na slight improvement over the loose analysis from [23]. The error\nanalysis essentially follows [23,8]. Much of the discussion in Section\n4 on using random projections and the \"projected\" tensor power method\nin an efficient way is actually from [6] (and should be properly\nattributed).\n\nLine 91: \"z ~ IBP(z)\" --- what does this mean?\nLine 223: \"(...)_epsilon\" --- what's epsilon?\nLine 283: \"average-median theorem\" --- where is this used?\nLine 582: \"convexity of expectations\" --- probably you just mean\nJensen's inequality and convexity of the Euclidean norm?\n\n[I've read the author rebuttal.] In summary, the paper is a well-written description of how to use amethod-of-moments based on tensor decompositions for inference in aparticular nonparametric Bayesian model (IBP). There is somealgorithmic and other technical novelty here, though not a whole lot;still, I think the paper is an interesting contribution and should beof interest to the machine learning community.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
