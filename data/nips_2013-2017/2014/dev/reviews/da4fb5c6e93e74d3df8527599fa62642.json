{"title": "Multivariate Regression with Calibration", "abstract": "", "id": "da4fb5c6e93e74d3df8527599fa62642", "authors": ["Han Liu", "Lie Wang", "Tuo Zhao"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The paper proposes a new regression method, namely calibrated multivariate regression (CMR), for high dimensional data analysis. Besides proposing the CMR formulation, the paper focuses on (1) using a smoothed proximal gradient method to compute CMR\u2019s optimal solutions; (2) analyzing CMR\u2019 statical properties. \n\nOne key contribution of the paper lies in the introduction of this CMR formulation; its loss term can be interpreted as calibrating each regression task\u2019s loss term with respect to its noise level. I am wondering whether there is any more intuitive interpretation behind the use of the noise level for calibration? The authors are encouraged to explain more on this point. \n\nThe results from Theorem 3.2 shows that CMR achieves the same rates of convergences as its non-calibrated counterpart OMR. Since OMR has a differentiable loss term, OMR seems to have computational advantages compared to CMR. The authors are encouraged to provide some guides on the selection between OMR and CMR. \n The paper proposes a new regression method, called CMR, for high-dimensional data analysis. The papers propose to employ the smoothed proximal method to compute its solution, and also theoretically analyze its statistical properties.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposed a calibrated multivariate regression method for fitting high dimensional data. Then they proposed a computational method for solving the optimization problem. The main comments on the details of the paper are as follows:\n1.\tThe main improvement of the model is substituting the Frobenious norm by L_{2,1} norm. The calibration is in fact substituting the variance as standard deviation. The authors used numerical experiments to show its effects. Can the authors give some theoretical explanation or intuitive explanation why the standard deviation works better than variance?\n2.\tFor the numerical experiments, matrix D_I is added to show the different variances of the noise. However, in the setting, the range of D_I is not large. What will the results be when the range of D_I is much larger? In Table 4.1, when lambda changes in its wide range, are there the similar results?\n3.\tSome typing errors such as: in (1.2), a lambda is lost.\n The manuscript is clearly written, but lack of novelty and enough experiments.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposed a new method called calibrated multivariate regression (CMR) for fitting high dimensional multivariate regression models. The assumption is that the noise matrix has an uncorrelated structure, and different regression tasks have different noise variances. Instead of using the same tuning parameter lambda for all the regression tasks, CMR calibrates different tasks by solving a penalized weighted least square problem weights define in (2.3) (i.e., some kind of estimate of noise standard deviation). The paper is well-structured and clearly-written. The appendix contains a lot of technical details. The idea of CMR formulation is technically sound. The proposed computational algorithms and demonstrated statistical properties make sense at a high level, although I did not check every step of the derivation and proof given the limited expertise in this field. The empirical study on simulated and real data yielded promising results.\n\nLine 129: Should \"weighted least square program\" read \"weighted least square problem\"?\n\nAnother straightforward approach to calibrate the parameter lambda is to run multiple regression on each response variable separately. How does this compare with the proposed CMR? It would be interesting to compare them on both simulated and real data. Sometimes simple models might yield better or more interpretable results on real data than more advanced ones. This paper proposed a new method called calibrated multivariate regression (CMR) for fitting high dimensional multivariate regression models. The idea is technically sound and the results are promising.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
