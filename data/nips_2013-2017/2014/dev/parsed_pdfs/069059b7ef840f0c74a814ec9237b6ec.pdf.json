{
  "name" : "069059b7ef840f0c74a814ec9237b6ec.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On a Theory of Nonparametric Pairwise Similarity for Clustering: Connecting Clustering to Classification",
    "authors" : [ "Yingzhen Yang", "Feng Liang", "Shuicheng Yan", "Zhangyang Wang", "Thomas S. Huang" ],
    "emails" : [ "yyang58@illinois.edu", "liangf@illinois.edu", "zwang119@illinois.edu", "t-huang1@illinois.edu", "eleyans@nus.edu.sg" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Pairwise clustering methods partition the data into a set of self-similar clusters based on the pairwise similarity between the data points. Representative clustering methods include K-means [2] which minimizes the within-cluster dissimilarities, spectral clustering [3] which identifies clusters of more complex shapes lying on low dimensional manifolds, and the pairwise clustering method [4] using message-passing algorithm to inference the cluster labels in a pairwise undirected graphical model. Utilizing pairwise similarity, these pairwise clustering methods often avoid estimating complex hidden variables or parameters, which is difficult for high dimensional data.\nHowever, most pairwise clustering methods assume that the pairwise similarity is given [2, 3], or they learn a more complicated similarity measure based on several given base similarities [4]. In this paper, we present a new framework for pairwise clustering where the pairwise similarity is derived as the generalization error bound for the unsupervised nonparametric classifier. The un-\nsupervised classifier is learned from unlabeled data and the hypothetical labeling. The quality of the hypothetical labeling is measured by the associated generalization error of the learned classifier, and the hypothetical labeling with minimum associated generalization error bound is preferred. We consider two nonparametric classifiers, i.e. the nearest neighbor classifier (NN) and the plug-in classifier (or the kernel density classifier). The generalization error bounds for both unsupervised classifiers are expressed as sum of pairwise terms between the data points, which can be interpreted as nonparametric pairwise similarity measure between the data points. Under uniform distribution, both nonparametric similarity measures exhibit a well known form of kernel similarity. We also prove that the generalization error bound for the unsupervised plug-in classifier is asymptotically equal to the weighted volume of cluster boundary [1] for Low Density Separation, a widely used criteria for semi-supervised learning and clustering.\nOur work is closely related to discriminative clustering methods by unsupervised classification, which search for the cluster boundaries with the help of unsupervised classifier. For example, [5] learns a max-margin two-class classifier to group unlabeled data in an unsupervised manner, known as unsupervised SVM whose theoretical property is further analyzed in [6]. Also, [7] learns the kernel logistic regression classifier, and uses the entropy of the posterior distribution of the class label by the classifier to measure the quality of the learned classifier. More recent work presented in [8] learns an unsupervised classifier by maximizing the mutual information between cluster labels and the data, and the Squared-Loss Mutual Information is employed to produce a convex optimization problem. Although such discriminative methods produce satisfactory empirical results, the optimization of complex parameters hampers their application in high-dimensional data. Following the same principle of unsupervised classification using nonparametric classifiers, we derive nonparametric pairwise similarity and eliminate the need of estimating complicated parameters of the unsupervised classifer. As an application, we develop a new nonparametric exemplar-based clustering method with the derived nonparametric pairwise similarity induced by the plug-in classifier, and our new method demonstrates better empirical clustering results than the existing exemplar-based clustering methods.\nIt should be emphasized that our generalization bounds are essentially different from the literature. As nonparametric classification methods, the generalization properties of the nearest neighbor classifier (NN) and the plug-in classifier are extensively studied. Previous research focuses on the average generalization error of the NN [9, 10], which is the average error of the NN over all the random training data sets, or the excess risk of the plug-in classifier [11, 12]. In [9], it is shown that the average generalization error of the NN is bounded by twice of the Bayes error. Assuming that the class of the regression functions has a smooth parameter β, [11] proves that the excess risk of the plug-in classifier converges to 0 of the order n −β 2β+d where d is the dimension of the data. [12] further shows that the plug-in classifier attains faster convergence rate of the excess risk, namely n− 1 2 , under some margin assumption on the data distribution. All these generalization error bounds depend on the unknown Bayes error. By virtue of kernel density estimation and generalized kernel density estimation [13], our generalization bounds are represented mostly in terms of the data, leading to the pairwise similarities for clustering."
    }, {
      "heading" : "2 Formulation of Pairwise Clustering by Unsupervised Nonparametric Classification",
      "text" : "The discriminative clustering literature [5, 7] has demonstrated the potential of multi-class classification for the clustering problem. Inspired by the natural connection between clustering and classification, we model the clustering problem as a multi-class classification problem: a classifier is learned from the training data built by a hypothetical labeling, which is a possible cluster labeling. The optimal hypothetical labeling is supposed to be the one such that its associated classifier has the minimum generalization error bound. To study the generalization bound for the classifier learned from the hypothetical labeling, we define the concept of classification model. Given unlabeled data {xl}nl=1, a classification model MY is constructed for any hypothetical labeling Y = {yl}nl=1 as below:\nDefinition 1. The classification model corresponding to the hypothetical labeling Y = {yl}nl=1 is defined as MY = ( S, PXY , {πi, fi}Qi=1, F ) . S = {xl,yl}nl=1 are the labeled data by the\nhypothetical labeling, and S are assumed to be i.i.d. samples drawn from the joint distribution PXY = PX|Y PY , where (X,Y ) is a random couple, X ∈ IRd represents the data and Y ∈ {1, 2, ..., Q} is the class label of X , Q is the number of classes determined by the hypothetical labeling. Furthermore, PXY is specified by {π(i), f (i)}Qi=1 as follows: π(i) is the class prior for class i, i.e. Pr [Y = i] = π(i); the conditional distribution PX|Y=i has probabilistic density function f (i), i = 1, . . . , Q. F is a classifier trained using the training data S. The generalization error of the classification model MY is defined as the generalization error of the classifier F in MY .\nIn this paper, we study two types of classification models with the nearest neighbor classifier and the plug-in classifier respectively, and derive their generalization error bounds as sum of pairwise similarity between the data. Given a specific type of classification model, the optimal hypothetical labeling corresponds to the classification model with minimum generalization error bound. The optimal hypothetical labeling also generates a data partition where the sum of pairwise similarity between the data from different clusters is minimized, which is a common criteria for discriminative clustering.\nIn the following text, we derive the generalization error bounds for the two types of classification models. Before that, we introduce more notations and assumptions for the classification model. Denote by PX the induced marginal distribution of X , and f is the probabilistic density function of\nPX which is a mixture of Q class-conditional densities: f = Q∑ i=1 π(i)f (i). η(i) (x) is the regression function of Y on X = x, i.e. η(i) (x) = Pr [Y = i |X = x ] = π (i)f(i)(x) f(x) . For the sake of the consistency of the kernel density estimators used in the sequel, there are further assumptions on the marginal density and class-conditional densities in the classification model for any hypothetical labeling:\n(A) f is bounded from below, i.e. f ≥ fmin > 0\n(B) {f (i)} is bounded from above, i.e. f (i) ≤ f (i)max, and f (i) ∈ Σγ,ci , 1 ≤ i ≤ Q. where Σγ,c is the class of Hölder-γ smooth functions with Hölder constant c:\nΣγ,c , {f : IRd → IR | ∀x, y, |f (x)− f (y)| ≤ c∥x− y∥γ}, γ > 0 It follows from assumption (B) that f ∈ Σγ,c where c = ∑ i π(i)ci. Assumption (A) and (B) are mild. The upper bound for the density functions is widely required for the consistency of kernel density estimators [14, 15]; Hölder-γ smoothness is required to bound the bias of such estimators, and it also appears in [12] for estimating the excess risk of the plug-in classifier. The lower bound for the marginal density is used to derive the consistency of the estimator of the regression function η(i) (Lemma 2) and the consistency of the generalized kernel density estimator (Lemma 3). We denote by PX the collection of marginal distributions that satisfy assumption (A), and denote by PX|Y the collection of class-conditional distributions that satisfy assumption (B). We then define the collection of joint distributions PXY that PXY belongs to, which requires the marginal density and class-conditional densities satisfy assumption (A)-(B):\nPXY , {PXY | PX ∈ PX , {PX|Y =i} ∈ PX|Y ,min i {π(i)} > 0} (1)\nGiven the joint distribution PXY , the generalization error of the classifier F learned from the training data S is:\nR (FS) , Pr [(X,Y ) : F (X) ̸= Y ] (2)\nNonparametric kernel density estimator (KDE) serves as the primary tool of estimating the underlying probabilistic density functions in our generalization analysis, and we introduce the KDE of f as below:\nf̂n,hn (x) = 1\nn n∑ l=1 Khn (x− xl) (3)\nwhere Kh (x) = 1hdK ( x h ) is the isotropic Gaussian kernel with bandwidth h and K (x) ,\n1 (2π)d/2\ne− ∥x∥2 2 . We have the following VC property of the Gaussian kernel K. Define the class\nof functions\nF , {K ( t− · h ) , t ∈ IRd, h ̸= 0} (4)\nThe VC property appears in [14, 15, 16, 17, 18], and it is proved that F is a bounded VC class of measurable functions with respect to the envelope function F such that |u| ≤ F for any u ∈ F (e.g. F ≡ (2π)− d2 ). It follows that there exist positive numbers A and v such that for every probability measure P on IRd for which ∫ F 2dP < ∞ and any 0 < τ < 1,\nN ( F , ∥·∥L2(P ) , τ ∥F∥L2(P ) ) ≤ ( A\nτ\n)v (5)\nwhere N ( T , d̂, ϵ ) is defined as the minimal number of open d̂-balls of radius ϵ required to cover\nT in the metric space ( T , d̂ ) . A and v are called the VC characteristics of F .\nThe VC property of K is required for the consistency of kernel density estimators shown in Lemma 2. Also, we adopt the kernel estimator of η(i) below\nη̂ (i) n,hn (x) =\nn∑ l=1 Khn (x− xl)1I{yl=i}\nnf̂n,hn (x) (6)\nBefore stating Lemma 2, we introduce several frequently used quantities throughout this paper. Let L,C > 0 be constants which only depend on the VC characteristics of the Gaussian kernel K. We define\nf0 , Q∑\ni=1\nπ(i)f (i)max σ 2 0 , ∥K∥22f0 (7)\nAlso, for all positive numbers λ ≥ C and σ > 0, we define\nEσ2 , log (1 + λ/4L)\nλLσ2 (8)\nBased on Corollary 2.2 in [14], Lemma 2 and Lemma 3 in the Appendix (more complete version in the supplementary) show the strong consistency (almost sure uniformly convergence) of several kernel density estimators, i.e. f̂n,hn , {η̂ (i) n,hn\n} and the generalized kernel density estimator, and they form the basis for the derivation of the generalization error bounds for the two types of classification models."
    }, {
      "heading" : "3 Generalization Bounds",
      "text" : "We derive the generalization error bounds for the two types of classification models with the nearest neighbor classifier and the plug-in classifier respectively. Substituting these kernel density estimators for the corresponding true density functions, Theorem 1 and Theorem 2 present the generalization error bounds for the classification models with the plug-in classifier and the nearest neighbor classifier. The dominant terms of both bounds are expressed as sum of pairwise similarity depending solely on the data, which facilitates the application of clustering. We also show the connection between the error bound for the plug-in classifier and Low Density Separation in this section. The detailed proofs are included in the supplementary."
    }, {
      "heading" : "3.1 Generalization Bound for the Classification Model with Plug-In Classifier",
      "text" : "The plug-in classifier resembles the Bayes classifier while it uses the kernel density estimator of the regression function η(i) instead of the true η(i). It has the form\nPI (X) = argmax 1≤i≤Q\nη̂ (i) n,hn (X) (9)\nwhere η̂(i)n,hn is the nonparametric kernel estimator of the regression function η (i) by (6). The generalization capability of the plug-in classifier has been studied by the literature[11, 12]. Let\nF ∗ be the Bayes classifier, it is proved that the excess risk of PIS , namely IESR (PIS) − R (F ∗), converges to 0 of the order n −β 2β+d under some complexity assumption on the class of the regression functions with smooth parameter β that {η(i)} belongs to [11, 12]. However, this result cannot be used to derive the generalization error bound for the plug-in classifier comprising of nonparametric pairwise similarities in our setting.\nWe show the upper bound for the generalization error of PIS in Lemma 1. Lemma 1. For any PXY ∈ PXY , there exists a n0 which depends on σ0 and VC characteristics of K, when n > n0, with probability greater than 1 − 2QLh E σ20\nn , the generalization error of the plug-in classifier satisfies\nR (PIS) ≤ RPIn +O (√ log h−1n\nnhdn + hγn\n) (10)\nRPIn = ∑\ni,j=1,...,Q,i ̸=j\nIEX [ η̂ (i) n,hn (X) η̂ (j) n,hn (X) ]\n(11)\nwhere Eσ2 is defined by (8), hn is chosen such that hn → 0, log h−1n nhdn → 0, η̂(i)n,hn is the kernel estimator of the regression function. Moreover, the equality in (10) holds when η̂(i)n,hn ≡ 1 Q for 1 ≤ i ≤ Q.\nBased on Lemma 1, we can bound the error of the plug-in classifier from above by RPIn . Theorem 1 then gives the bound for the error of the plug-in classifier in the corresponding classification model using the generalized kernel density estimator in Lemma 3. The bound has a form of sum of pairwise similarity between the data from different classes. Theorem 1. (Error of the Plug-In Classifier) Given the classification model MY =( S, PXY , {πi, fi}Qi=1,PI ) such that PXY ∈ PXY , there exists a n1 which depends on σ0, σ1 and\nthe VC characteristics of K, when n > n1, with probability greater than 1− 2QLh E σ20 n −QLh E σ21\nn , the generalization error of the plug-in classifier satisfies\nR (PIS) ≤ R̂n (PIS) +O (√ log h−1n\nnhdn + hγn\n) (12)\nwhere R̂n (PIS) = 1n2 ∑ l,m θlmGlm, √ 2hn , σ21 = ∥K∥22fmax fmin , θlm = 1I{yl ̸=ym} is a class indicator\nfunction and\nGlm,h = Gh (xl,xm) , Gh (x, y) = Kh (x− y)\nf̂ 1 2 n,h (x)f̂ 1 2 n,h (y)\n, (13)\nEσ2 is defined by (8), hn is chosen such that hn → 0, log h−1n nhdn\n→ 0, f̂n,hn is the kernel density estimator of f defined by (3).\nR̂n is the dominant term determined solely by the data and the excess error O (√\nlog h−1n nhdn + hγn ) goes to 0 with infinite n. In the following subsection, we show the close connection between the error bound for the plug-in classifier and the weighted volume of cluster boundary, and the latter is proposed by [1] for Low Density Separation."
    }, {
      "heading" : "3.1.1 Connection to Low Density Separation",
      "text" : "Low Density Separation [19], a well-known criteria for clustering, requires that the cluster boundary should pass through regions of low density. It has been extensively studied in unsupervised learning and semi-supervised learning [20, 21, 22]. Suppose the data {xl}nl=1 lies on a domain Ω ⊆ Rd. Let f be the probability density function on Ω, S be the cluster boundary which separates Ω into two parts S1 and S2. Following the Low Density Separation assumption, [1] suggests that the\ncluster boundary S with low weighted volume ∫ S f (s)ds should be preferable. [1] also proves that a particular type of cut function converges to the weighted volume of S. Based on their study, we obtain the following result relating the error of the plug-in classifier to the weighted volume of the cluster boundary. Corollary 1. Under the assumption of Theorem 1, for any kernel bandwidth sequence {hn}∞n=1 such that lim\nn→∞ hn = 0 and hn > n− 1 4d+4 , with probability 1,\nlim n→∞\n√ π 2hn R̂n (PIS) = ∫ S f (s)ds (14)"
    }, {
      "heading" : "3.2 Generalization Bound for the Classification Model with Nearest Neighbor Classifier",
      "text" : "Theorem 2 shows the generalization error bound for the classification model with nearest neighbor classifier (NN), which has a similar form as (12).\nTheorem 2. (Error of the NN) Given the classification model MY = ( S, PXY , {πi, fi}Qi=1,NN ) such that PXY ∈ PXY and the support of PX is bounded by [−M0,M0]d, there exists a n0 which depends on σ0 and VC characteristics of K, when n > n0, with probability greater than 1 − 2QLh E σ20 n − (2M0)dndd0e−n 1−dd0fmin , the generalization error of the NN satisfies:\nR (NNS) ≤ R̂n (NNS) + c0 (√ d )γ n−d0γ +O (√ log h−1n\nnhdn + hγn\n) (15)\nwhere R̂n (NN) = 1n ∑\n1≤l<m≤n Hlm,hnθlm,\nHlm,hn = Khn (xl − xm) (∫ Vl f̂n,hn (x) dx\nf̂n,hn (xl) +\n∫ Vm f̂n,hn (x) dx\nf̂n,hn (xm)\n) , (16)\nEσ2 is defined by (8), d0 is a constant such that dd0 < 1, f̂n,hn is the kernel density estimator of\nf defined by (3) with the kernel bandwidth hn satisfying hn → 0, log h −1 n\nnhdn → 0, Vl is the Voronoi\ncell associated with xl, c0 is a constant, θlm = 1I{yl ̸=ym} is a class indicator function such that θlm = 1 if xl and xm belongs to different classes, and 0 otherwise. Moreover, the equality in (15) holds when η(i) ≡ 1Q for 1 ≤ i ≤ Q.\nGlm, √ 2hn in (13) and Hlm,hn in (16) are the new pairwise similarity functions induced by the plugin classifier and the nearest neighbor classifier respectively. According to the proof of Theorem 1 and Theorem 2, the kernel density estimator f̂ can be replaced by the true density f in the denominators of (13) and (16), and the conclusions of Theorem 1 and 2 still hold. Therefore, both Glm,√2hn and Hlm,hn are equal to ordinary Gaussian kernels (up to a scale) with different kernel bandwidth under uniform distribution, which explains the broadly used kernel similarity in data clustering from an angle of supervised learning."
    }, {
      "heading" : "4 Application to Exemplar-Based Clustering",
      "text" : "We propose a nonparametric exemplar-based clustering algorithm using the derived nonparametric pairwise similarity by the plug-in classifier. In exemplar-based clustering, each xl is associated with a cluster indicator el (l ∈ {1, 2, ...n} , el ∈ {1, 2, ...n}), indicating that xl takes xel as the cluster exemplar. Data from the same cluster share the same cluster exemplar. We define e , {el}nl=1. Moreover, a configuration of the cluster indicators e is consistent iff el = l when em = l for any l,m ∈ 1..n, meaning that xl should take itself as its exemplar if any xm take xl as its exemplar. It is required that the cluster indicators e should always be consistent. Affinity Propagation (AP) [23], a representative of the exemplar-based clustering methods, solves the following optimization problem\nmin e n∑ l=1 Sl,el s.t. e is consistent (17)\nSl,el is the dissimilarity between xl and xel , and note that Sl,l is set to be nonzero to avoid the trivial minimizer of (17).\nNow we aim to improve the discriminative capability of the exemplar-based clustering (17) using the nonparametric pairwise similarity derived by the unsupervised plug-in classifier. As mentioned before, the quality of the hypothetical labeling ŷ is evaluated by the generalization error bound for the nonparametric plug-in classifier trained by Sŷ, and the hypothetical labeling ŷ with minimum associated error bound is preferred, i.e. argminŷ R̂n (PIS) = argminŷ ∑ l,m θlmGlm, √ 2hn where\nθlm = 1Iŷl ̸=ŷm and Glm,√2hn is defined in (13). By Lemma 3, minimizing ∑ l,m θlmGlm, √ 2hn also enforces minimization of the weighted volume of cluster boundary asymptotically. To avoid the trivial clustering where all the data are grouped into a single cluster, we use the sum of within-\ncluster dissimilarities term n∑\nl=1\nexp ( −Glel, √ 2hn ) to control the size of clusters. Therefore, the\nobjective function of our pairwise clustering method is below:\nΨ(e) = n∑ l=1 exp ( −Glel, √ 2hn ) + λ ∑ l,m ( θ̃lmGlm, √ 2hn + ρlm (el, em) )\n(18)\nwhere ρlm is a function to enforce the consistency of the cluster indicators:\nρlm (el, em) = { ∞ em = l, el ̸= l or el = m, em ̸= m 0 otherwise ,\nλ is a balancing parameter. Due to the form of (18), we construct a pairwise Markov Random Field (MRF) representing the unary term ul and the pairwise term θ̃lmGlm,√2hn + ρlm as the data likelihood and prior respectively. The variables e are modeled as nodes and the unary term and pairwise term in (18) are modeled as potential functions in the pairwise MRF. The minimization of the objective function is then converted to a MAP (Maximum a Posterior) problem in the pairwise MRF. (18) is minimized by Max-Product Belief Propagation (BP).\nThe computational complexity of our clustering algorithm is O(TEN), where E is the number of edges in the pairwise MRF, T is the number of iterations of message passing in the BP algorithm. We call our new algorithm Plug-In Exemplar Clustering (PIEC), and compare it to representative exemplar-based clustering methods, i.e. AP and Convex Clustering with Exemplar-Based Model (CEB) [24], for clustering on three real data sets from UCI repository, i.e. Iris, Vertebral Column (VC) and Breast Tissue (BT). We record the average clustering accuracy (AC) and the standard deviation of AC for all the exemplar-based clustering methods when they produce the correct number of clusters for each data set with different values of hn and λ, and the results are shown in Table 1. Although AP produces better clustering accuracy on the VC data set, PIEC generates the correct cluster numbers for much more times. The dash in Table 1 indicates that the corresponding clustering method cannot produce the correct cluster number. The default value for the kernel bandwidth hn is h∗n, which is set as the variance of the pairwise distance between data points { ∥xl − xm∥l<m } . The default value for the balancing parameter λ is 1. We let hn = αh∗n, λ varies between [0.2, 1] and α varies between [0.2, 1.9] with step 0.2 and 0.05 respectively, resulting in 170 different parameter settings. We also generate the same number of parameter settings for AP and CEB."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We propose a new pairwise clustering framework where nonparametric pairwise similarity is derived by minimizing the generalization error unsupervised nonparametric classifier. Our framework bridges the gap between clustering and multi-class classification, and explains the widely used kernel similarity for clustering. In addition, we prove that the generalization error bound for the unsupervised plug-in classifier is asymptotically equal to the weighted volume of cluster boundary for\nLow Density Separation. Based on the derived nonparametric pairwise similarity using the plug-in classifier, we propose a new nonparametric exemplar-based clustering method with enhanced discriminative capability compared to the exiting exemplar-based clustering methods.\nAppendix\nLemma 2. (Consistency of Kernel Density Estimator) Let the kernel bandwidth hn of the Gaussian kernel K be chosen such that hn → 0, log h −1 n\nnhdn → 0. For any PX ∈ PX , there exists a n0 which\ndepends on σ0 and VC characteristics of K, when n > n0, with probability greater than 1−Lh E σ20 n over the data {xl}, ∥∥∥f̂n,hn (x)− f (x)∥∥∥ ∞ = O (√ log h−1n nhdn + hγn ) (19)\nwhere f̂n,hn is the kernel density estimator of f . Furthermore, for any PXY ∈ PXY , when n > n0,\nthen with probability greater than 1− 2Lh E σ20 n over the data {xl},∥∥∥η̂(i)n,hn (x)− η(i) (x)∥∥∥∞ = O( √ log h−1n nhdn + hγn ) (20)\nfor each 1 ≤ i ≤ Q. Lemma 3. (Consistency of the Generalized Kernel Density Estimator) Suppose f is the probabilistic density function of PX ∈ PX . Let g be a bounded function defined on X and g ∈ Σγ,g0 , 0 < gmin ≤ g ≤ gmax, and e = fg . Define the generalized kernel density estimator of e as\nên,h , 1\nn n∑ l=1 Kh (x− xl) g (xl)\n(21)\nLet σ2g = ∥K∥22fmax\ng2min . There exists ng which depends on σg and the VC characteristics of K, When\nn > ng , with probability greater than 1− Lh Eσ2g n over the data {xl},\n∥ên,hn (x)− e (x)∥∞ = O (√ log h−1n\nnhdn + hγn\n) (22)\nwhere hn is chosen such that hn → 0, log h −1 n\nnhdn → 0.\nSketch of proof: For fixed h ̸= 0, we consider the class of functions\nFg , { K\n( t−· h ) g (·) , t ∈ IR d}\nIt can be verified that Fg is also a bounded VC class with the envelope function Fg = Fgmin , and\nN ( Fg, ∥·∥L2(P ) , τ ∥Fg∥L2(P ) ) ≤ ( A\nτ\n)v (23)\nThen (22) follows from similar argument in the proof of Lemma 2 and Corollary 2.2 in [14].\nThe generalized kernel density estimator (21) is also used in [13] to estimate the Laplacian PDF Distance between two probabilistic density functions, and the authors only provide the proof of pointwise weak consistency of this estimator in [13]. Under mild conditions, our Lemma 3 and Lemma 2 show the strong consistency of the generalized kernel density estimator and the traditional kernel density estimator under the same theoretical framework of the VC property of the kernel.\nAcknowledgements. This material is based upon work supported by the National Science Foundation under Grant No. 1318971."
    } ],
    "references" : [ {
      "title" : "On the relation between low density separation, spectral clustering and graph cuts",
      "author" : [ "Hariharan Narayanan", "Mikhail Belkin", "Partha Niyogi" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "A K-means clustering algorithm",
      "author" : [ "J.A. Hartigan", "M.A. Wong" ],
      "venue" : "Applied Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1979
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "Andrew Y. Ng", "Michael I. Jordan", "Yair Weiss" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "Pairwise clustering and graphical models",
      "author" : [ "Noam Shental", "Assaf Zomet", "Tomer Hertz", "Yair Weiss" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Maximum margin clustering",
      "author" : [ "Linli Xu", "James Neufeld", "Bryce Larson", "Dale Schuurmans" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Unsupervised svms: On the complexity of the furthest hyperplane problem",
      "author" : [ "Zohar Karnin", "Edo Liberty", "Shachar Lovett", "Roy Schwartz", "Omri Weinstein" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Discriminative clustering by regularized information maximization",
      "author" : [ "Ryan Gomes", "Andreas Krause", "Pietro Perona" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "On informationmaximization clustering: Tuning parameter selection and analytic solution",
      "author" : [ "Masashi Sugiyama", "Makoto Yamada", "Manabu Kimura", "Hirotaka Hachiya" ],
      "venue" : "In ICML,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Nearest neighbor pattern classification",
      "author" : [ "T. Cover", "P. Hart" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1967
    }, {
      "title" : "A probabilistic theory of pattern recognition, volume",
      "author" : [ "Luc Devroye" ],
      "venue" : "springer,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1996
    }, {
      "title" : "Minimax nonparametric classification - part i: Rates of convergence",
      "author" : [ "Yuhong Yang" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1999
    }, {
      "title" : "Fast learning rates for plug-in classifiers",
      "author" : [ "Jean-Yves Audibert", "Alexandre B. Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "The laplacian pdf distance: A cost function for clustering in a kernel feature space",
      "author" : [ "Robert Jenssen", "Deniz Erdogmus", "José Carlos Prı́ncipe", "Torbjørn Eltoft" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2004
    }, {
      "title" : "Rates of strong uniform consistency for multivariate kernel density estimators",
      "author" : [ "Evarist Giné", "Armelle Guillou" ],
      "venue" : "Ann. Inst. H. Poincaré Probab. Statist.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    }, {
      "title" : "Uniform in bandwidth consistency of kernel-type function estimators",
      "author" : [ "Uwe Einmahl", "David M. Mason" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2005
    }, {
      "title" : "Uniform Central Limit Theorems",
      "author" : [ "R.M. Dudley" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1999
    }, {
      "title" : "Weak Convergence and Empirical Processes",
      "author" : [ "A.W. van der Vaart", "J.A. Wellner" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1996
    }, {
      "title" : "U-Processes: Rates of convergence",
      "author" : [ "Deborah Nolan", "David Pollard" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1987
    }, {
      "title" : "Semi-Supervised Classification by Low Density Separation",
      "author" : [ "Olivier Chapelle", "Alexander Zien" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Influence of graph construction on graph-based clustering measures",
      "author" : [ "Markus Maier", "Ulrike von Luxburg", "Matthias Hein" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    }, {
      "title" : "Adaptive regularization for transductive support vector machine",
      "author" : [ "Zenglin Xu", "Rong Jin", "Jianke Zhu", "Irwin King", "Michael R. Lyu", "Zhirong Yang" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Semi-supervised learning with graphs",
      "author" : [ "Xiaojin Zhu", "John Lafferty", "Ronald Rosenfeld" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "Clustering by passing messages between data points",
      "author" : [ "Brendan J. Frey", "Delbert Dueck" ],
      "venue" : "Science, 315:972–977,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2007
    }, {
      "title" : "Convex clustering with exemplar-based models",
      "author" : [ "Danial Lashkari", "Polina Golland" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We also prove that the generalization error bound for the unsupervised plugin classifier is asymptotically equal to the weighted volume of cluster boundary [1] for Low Density Separation, a widely used criteria for semi-supervised learning and clustering.",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 1,
      "context" : "Representative clustering methods include K-means [2] which minimizes the within-cluster dissimilarities, spectral clustering [3] which identifies clusters of more complex shapes lying on low dimensional manifolds, and the pairwise clustering method [4] using message-passing algorithm to inference the cluster labels in a pairwise undirected graphical model.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "Representative clustering methods include K-means [2] which minimizes the within-cluster dissimilarities, spectral clustering [3] which identifies clusters of more complex shapes lying on low dimensional manifolds, and the pairwise clustering method [4] using message-passing algorithm to inference the cluster labels in a pairwise undirected graphical model.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "Representative clustering methods include K-means [2] which minimizes the within-cluster dissimilarities, spectral clustering [3] which identifies clusters of more complex shapes lying on low dimensional manifolds, and the pairwise clustering method [4] using message-passing algorithm to inference the cluster labels in a pairwise undirected graphical model.",
      "startOffset" : 250,
      "endOffset" : 253
    }, {
      "referenceID" : 1,
      "context" : "However, most pairwise clustering methods assume that the pairwise similarity is given [2, 3], or they learn a more complicated similarity measure based on several given base similarities [4].",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "However, most pairwise clustering methods assume that the pairwise similarity is given [2, 3], or they learn a more complicated similarity measure based on several given base similarities [4].",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "However, most pairwise clustering methods assume that the pairwise similarity is given [2, 3], or they learn a more complicated similarity measure based on several given base similarities [4].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 0,
      "context" : "We also prove that the generalization error bound for the unsupervised plug-in classifier is asymptotically equal to the weighted volume of cluster boundary [1] for Low Density Separation, a widely used criteria for semi-supervised learning and clustering.",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "For example, [5] learns a max-margin two-class classifier to group unlabeled data in an unsupervised manner, known as unsupervised SVM whose theoretical property is further analyzed in [6].",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 5,
      "context" : "For example, [5] learns a max-margin two-class classifier to group unlabeled data in an unsupervised manner, known as unsupervised SVM whose theoretical property is further analyzed in [6].",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : "Also, [7] learns the kernel logistic regression classifier, and uses the entropy of the posterior distribution of the class label by the classifier to measure the quality of the learned classifier.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "More recent work presented in [8] learns an unsupervised classifier by maximizing the mutual information between cluster labels and the data, and the Squared-Loss Mutual Information is employed to produce a convex optimization problem.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "Previous research focuses on the average generalization error of the NN [9, 10], which is the average error of the NN over all the random training data sets, or the excess risk of the plug-in classifier [11, 12].",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 9,
      "context" : "Previous research focuses on the average generalization error of the NN [9, 10], which is the average error of the NN over all the random training data sets, or the excess risk of the plug-in classifier [11, 12].",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "Previous research focuses on the average generalization error of the NN [9, 10], which is the average error of the NN over all the random training data sets, or the excess risk of the plug-in classifier [11, 12].",
      "startOffset" : 203,
      "endOffset" : 211
    }, {
      "referenceID" : 11,
      "context" : "Previous research focuses on the average generalization error of the NN [9, 10], which is the average error of the NN over all the random training data sets, or the excess risk of the plug-in classifier [11, 12].",
      "startOffset" : 203,
      "endOffset" : 211
    }, {
      "referenceID" : 8,
      "context" : "In [9], it is shown that the average generalization error of the NN is bounded by twice of the Bayes error.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 10,
      "context" : "Assuming that the class of the regression functions has a smooth parameter β, [11] proves that the excess risk of the plug-in classifier converges to 0 of the order n −β 2β+d where d is the dimension of the data.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "[12] further shows that the plug-in classifier attains faster convergence rate of the excess risk, namely n− 1 2 , under some margin assumption on the data distribution.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "By virtue of kernel density estimation and generalized kernel density estimation [13], our generalization bounds are represented mostly in terms of the data, leading to the pairwise similarities for clustering.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "The discriminative clustering literature [5, 7] has demonstrated the potential of multi-class classification for the clustering problem.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "The discriminative clustering literature [5, 7] has demonstrated the potential of multi-class classification for the clustering problem.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "The upper bound for the density functions is widely required for the consistency of kernel density estimators [14, 15]; Hölder-γ smoothness is required to bound the bias of such estimators, and it also appears in [12] for estimating the excess risk of the plug-in classifier.",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "The upper bound for the density functions is widely required for the consistency of kernel density estimators [14, 15]; Hölder-γ smoothness is required to bound the bias of such estimators, and it also appears in [12] for estimating the excess risk of the plug-in classifier.",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 11,
      "context" : "The upper bound for the density functions is widely required for the consistency of kernel density estimators [14, 15]; Hölder-γ smoothness is required to bound the bias of such estimators, and it also appears in [12] for estimating the excess risk of the plug-in classifier.",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 13,
      "context" : "The VC property appears in [14, 15, 16, 17, 18], and it is proved that F is a bounded VC class of measurable functions with respect to the envelope function F such that |u| ≤ F for any u ∈ F (e.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "The VC property appears in [14, 15, 16, 17, 18], and it is proved that F is a bounded VC class of measurable functions with respect to the envelope function F such that |u| ≤ F for any u ∈ F (e.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "The VC property appears in [14, 15, 16, 17, 18], and it is proved that F is a bounded VC class of measurable functions with respect to the envelope function F such that |u| ≤ F for any u ∈ F (e.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "The VC property appears in [14, 15, 16, 17, 18], and it is proved that F is a bounded VC class of measurable functions with respect to the envelope function F such that |u| ≤ F for any u ∈ F (e.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "The VC property appears in [14, 15, 16, 17, 18], and it is proved that F is a bounded VC class of measurable functions with respect to the envelope function F such that |u| ≤ F for any u ∈ F (e.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 13,
      "context" : "2 in [14], Lemma 2 and Lemma 3 in the Appendix (more complete version in the supplementary) show the strong consistency (almost sure uniformly convergence) of several kernel density estimators, i.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 10,
      "context" : "The generalization capability of the plug-in classifier has been studied by the literature[11, 12].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "The generalization capability of the plug-in classifier has been studied by the literature[11, 12].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "F ∗ be the Bayes classifier, it is proved that the excess risk of PIS , namely IESR (PIS) − R (F ∗), converges to 0 of the order n −β 2β+d under some complexity assumption on the class of the regression functions with smooth parameter β that {η} belongs to [11, 12].",
      "startOffset" : 257,
      "endOffset" : 265
    }, {
      "referenceID" : 11,
      "context" : "F ∗ be the Bayes classifier, it is proved that the excess risk of PIS , namely IESR (PIS) − R (F ∗), converges to 0 of the order n −β 2β+d under some complexity assumption on the class of the regression functions with smooth parameter β that {η} belongs to [11, 12].",
      "startOffset" : 257,
      "endOffset" : 265
    }, {
      "referenceID" : 0,
      "context" : "In the following subsection, we show the close connection between the error bound for the plug-in classifier and the weighted volume of cluster boundary, and the latter is proposed by [1] for Low Density Separation.",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 18,
      "context" : "Low Density Separation [19], a well-known criteria for clustering, requires that the cluster boundary should pass through regions of low density.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 19,
      "context" : "It has been extensively studied in unsupervised learning and semi-supervised learning [20, 21, 22].",
      "startOffset" : 86,
      "endOffset" : 98
    }, {
      "referenceID" : 20,
      "context" : "It has been extensively studied in unsupervised learning and semi-supervised learning [20, 21, 22].",
      "startOffset" : 86,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "It has been extensively studied in unsupervised learning and semi-supervised learning [20, 21, 22].",
      "startOffset" : 86,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "Following the Low Density Separation assumption, [1] suggests that the",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 22,
      "context" : "Affinity Propagation (AP) [23], a representative of the exemplar-based clustering methods, solves the following optimization problem",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 23,
      "context" : "AP and Convex Clustering with Exemplar-Based Model (CEB) [24], for clustering on three real data sets from UCI repository, i.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "The generalized kernel density estimator (21) is also used in [13] to estimate the Laplacian PDF Distance between two probabilistic density functions, and the authors only provide the proof of pointwise weak consistency of this estimator in [13].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "The generalized kernel density estimator (21) is also used in [13] to estimate the Laplacian PDF Distance between two probabilistic density functions, and the authors only provide the proof of pointwise weak consistency of this estimator in [13].",
      "startOffset" : 241,
      "endOffset" : 245
    } ],
    "year" : 2014,
    "abstractText" : "Pairwise clustering methods partition the data space into clusters by the pairwise similarity between data points. The success of pairwise clustering largely depends on the pairwise similarity function defined over the data points, where kernel similarity is broadly used. In this paper, we present a novel pairwise clustering framework by bridging the gap between clustering and multi-class classification. This pairwise clustering framework learns an unsupervised nonparametric classifier from each data partition, and search for the optimal partition of the data by minimizing the generalization error of the learned classifiers associated with the data partitions. We consider two nonparametric classifiers in this framework, i.e. the nearest neighbor classifier and the plug-in classifier. Modeling the underlying data distribution by nonparametric kernel density estimation, the generalization error bounds for both unsupervised nonparametric classifiers are the sum of nonparametric pairwise similarity terms between the data points for the purpose of clustering. Under uniform distribution, the nonparametric similarity terms induced by both unsupervised classifiers exhibit a well known form of kernel similarity. We also prove that the generalization error bound for the unsupervised plugin classifier is asymptotically equal to the weighted volume of cluster boundary [1] for Low Density Separation, a widely used criteria for semi-supervised learning and clustering. Based on the derived nonparametric pairwise similarity using the plug-in classifier, we propose a new nonparametric exemplar-based clustering method with enhanced discriminative capability, whose superiority is evidenced by the experimental results.",
    "creator" : null
  }
}