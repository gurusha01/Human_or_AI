{
  "name" : "140f6969d5213fd0ece03148e62e461e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multi-View Perceptron: a Deep Model for Learning Face Identity and View Representations",
    "authors" : [ "Zhenyao Zhu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang" ],
    "emails" : [ "zz012@ie.cuhk.edu.hk", "lp011@ie.cuhk.edu.hk", "xgwang@ee.cuhk.edu.hk", "xtang@ie.cuhk.edu.hk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The performance of face recognition systems depends heavily on facial representation, which is naturally coupled with many types of face variations, such as view, illumination, and expression. As face images are often observed in different views, a major challenge is to untangle the face identity and view representations. Substantial efforts have been dedicated to extract identity features by hand, such as LBP [1], Gabor [14], and SIFT [15]. The best practise of face recognition extracts the above features on the landmarks of face images with multiple scales and concatenates them into high dimensional feature vectors [4, 21]. Deep learning methods, such as Boltzmann machine [9], sum product network [17], and deep neural net [16, 25, 22, 23, 24, 26] have been applied to face recognition. For instance, Sun et al. [25, 22] employed deep neural net to learn identity features from raw pixels by predicting 10, 000 identities.\nDeep neural net is inspired by the understanding of hierarchical cortex in the primate brain and mimicking some aspects of its activities. Recent studies [5, 19] discovered that macaque monkeys have a face-processing network that was made of six interconnected face-selective regions, where neurons in some of these regions were view-specific, while some others were tuned to identity across views, making face recognition in brain of primate robust to view variation. This intriguing function of primate brain inspires us to develop a novel deep neural net, called multi-view perceptron (MVP), which can disentangle identity and view representations, and also reconstruct images under multiple views. Specifically, given a single face image of an identity under an arbitrary view, it can generate a sequence of output face images of the same identity, one at a time, under a full spectrum of viewpoints. Examples of the input images and the generated multi-view outputs of two identities are illustrated in Fig. 1. The images in the last two rows are from the same person. The extracted features of MVP with respect to identity and view are plotted correspondingly in blue and orange.\nWe can observe that the identity features of the same identity are similar, even though the inputs are captured in very different views, whilst the view features of images in the same view are similar, although they are across different identities.\nUnlike other deep networks that produce a deterministic output from an input, MVP employs the deterministic hidden neurons to learn the identity features, whilst using the random hidden neurons to capture the view representation. By sampling distinct values of the random neurons, output images in distinct views are generated. Moreover, to yield images of different viewpoints, we add regularization that images under similar viewpoints should have similar view representations on the random neurons. The two types of neurons are modeled in a probabilistic way. In the training stage, the parameters of MVP are updated by back-propagation, where the gradient is calculated by maximizing a variational lower bound of the complete data log-likelihood. With our proposed learning algorithm, the EM updates on the probabilistic model are converted to forward and backward propagation. In the testing stage, given an input image, MVP can extract its identity and view features. In addition, if an order of viewpoints is also provided, MVP can sequentially reconstruct multiple views of the input image by following this order.\nThis paper has several key contributions. (i) We propose a multi-view perceptron (MVP) and its learning algorithm to factorize the identity and view representations with different sets of neurons, making the learned features more discriminative and robust. (ii) MVP can reconstruct a full spectrum of views given a single 2D image. The full spectrum of views can better distinguish identities, since different identities may look similar in a particular view but differently in others as illustrated in Fig. 1. (iii) MVP can interpolate and predict images under viewpoints that are unobserved in the training data, in some sense imitating the reasoning ability of human.\nRelated Works. In the literature of computer vision, existing methods that deal with view (pose) variation can be divided into 2D- and 3D-based methods. For example, the 2D methods, such as [6], infer the deformation (e.g. thin plate splines) between 2D images across poses. The 3D methods, such as [2, 12], capture 3D face models in different parametric forms. The above methods have their inherent shortages. Extra cost and resources are necessitated to capture and process 3D data. Because of lacking one degree of freedom, inferring 3D deformation from 2D transformation is often ill-posed. More importantly, none of the existing approaches simulates how the primate brain encodes view representations. In our approach, instead of employing any geometric models, view information is encoded with a small number of neurons, which can recover the full spectrum of views together with identity neurons. This representation of encoding identity and view information into different neurons is closer to the face-processing system in the primate brain and new to the deep learning literature. Our previous work [28] learned identity features by using CNN to recover a single frontal view face image, which is a special case of MVP after removing the random neurons. [28] did not learn the view representation as we do. Experimental results show that our approach not only provides rich multi-view representation but also learns better identity features compared with\n[28]. Fig. 1 shows examples that different persons may look similar in the front view, but are better distinguished in other views. Thus it improves the performance of face recognition significantly. More recently, Reed et al. [20] untangled factors of image variation by using a high-order Boltzmann machine, where all the neurons are stochastic and it is solved by gibbs sampling. MVP contains both stochastic and deterministic neurons and thus can be efficiently solved by back-propagation.\n2 Multi-View Perceptron\nThe training data is a set of image pairs, I = {xij , (yik,vik)}N,M,Mi=1,j=1,k=1, where xij is the input image of the ith identity under the j-th view, yik denotes the output image of the same identity in the k-th view, and vik is the view label of the output. vik is a M dimensional binary vector, with the k-th element as 1 and the remaining zeros. MVP is learned from the training data such that given an input x, it can output images y of the same identity in different views and their view labels v. Then, the output v and y are generated as1,\nv = F (y,hv; Θ), y = F (x,hid,hv,hr; Θ) + , (1)\nwhere F is a non-linear function and Θ is a set of weights and biases to be learned. There are three types of hidden neurons, hid, hv , and hr, which respectively extract identity features, view features, and the features to reconstruct the output face image. signifies a noise variable.\nFig. 2 shows the architecture2 of MVP, which is a directed graphical model with six layers, where the nodes with and without filling represent the observed and hidden variables, and the nodes in green and blue indicate the deterministic and random neurons, respectively. The generation process of y and v starts from x, flows through the neurons that extract identity feature hid, which combines with the hidden view representation hv to yield the feature hr for face recovery. Then, hr generates y.\nMeanwhile, both hv and y are united to generate v. hid and hr are the deterministic binary hidden neurons, while hv are random binary hidden neurons sampled from a distribution q(hv). Different sampled hv generates different y, making the perception of multi-view possible. hv usually has a low dimensionality, approximately ten, as ten binary neurons can ideally model 210 distinct views.\nFor clarity of derivation, we take an example of MVP that contains only one hidden layer of hid and hv . More layers can be added and derived in a similar fashion. We consider a joint distribution, which marginalizes out the random hidden neurons,\np(y,v |hid; Θ) = ∑ hv p(y,v,hv|hid; Θ) = ∑ hv p(v |y,hv; Θ)p(y|hid,hv; Θ)p(hv), (2)\nwhere Θ = {U0,U1,V1,U2,V2}, the identity feature is extracted from the input image, hid = f(U0x), and f is the sigmoid activation function, f(x) = 1/(1 + exp(−x)). Other activation functions, such as rectified linear function [18] and tangent [11], can be used as well. To model continuous values of the output, we assume y follows a conditional diagonal Gaussian distribution, p(y|hid,hv; Θ) = N (y|U1hid + V1hv,σ2y). The probability of y belonging to the j-th view is modeled with the softmax function, p(vj = 1|y,hv; Θ) = exp(U2j∗y+V 2 j∗h\nv)∑K k=1 exp(U 2 k∗y+V 2 k∗h v) , where Uj∗\nindicates the j-th row of the matrix.\n1The subscripts i, j, k are omitted for clearness. 2For clarity, the biases are omitted."
    }, {
      "heading" : "2.1 Learning Procedure",
      "text" : "The weights and biases of MVP are learned by maximizing the data log-likelihood. The lower bound of the log-likelihood can be written as,\nlog p(y,v |hid; Θ) = log ∑ hv p(y,v,hv|hid; Θ) ≥ ∑ hv q(hv) log p(y,v,hv|hid; Θ) q(hv) . (3)\nEq.(3) is attained by decomposing the log-likelihood into two terms, log p(y,v |hid; Θ) = − ∑\nhv q(h v) log p(h v|y,v;Θ) q(hv) + ∑ hv q(h v) log p(y,v,h v|hid;Θ)\nq(hv) , which can be easily verified by substituting the product, p(y,v,hv|hid) = p(y,v |hid)p(hv|y,v), into the right hand side of the decomposition. In particular, the first term is the KL-divergence [10] between the true posterior and the distribution q(hv). As KL-divergence is non-negative, the second term is regarded as the variational lower bound on the log-likelihood.\nThe above lower bound can be maximized by using the Monte Carlo Expectation Maximization (MCEM) algorithm recently introduced by [27], which approximates the true posterior by using the importance sampling with the conditional prior as the proposal distribution. With the Bayes’ rule, the true posterior of MVP is p(hv|y,v) = p(y,v |h\nv)p(hv) p(y,v) , where p(y,v |h\nv) represents the multi-view perception error, p(hv) is the prior distribution over hv , and p(y,v) is a normalization constant. Since we do not assume any prior information on the view distribution, p(hv) is chosen as a uniform distribution between zero and one. To estimate the true posterior, we let q(hv) = p(hv|y,v; Θold). It is approximated by sampling hv from the uniform distribution, i.e. hv ∼ U(0, 1), weighted by the importance weight p(y,v |hv; Θold). With the EM algorithm, the lower bound of the log-likelihood turns into\nL(Θ,Θold) = ∑ hv p(hv|y,v; Θold) log p(y,v,hv|hid; Θ) ' 1 S S∑ s=1 ws log p(y,v,h v s |hid; Θ),\n(4) where ws = p(y,v |hv; Θold) is the importance weight. The E-step samples the random hidden neurons, i.e. hvs ∼ U(0, 1), while the M-step calculates the gradient,\n∂L ∂Θ ' 1 S S∑ s=1 ∂L(Θ,Θold) ∂Θ = 1 S S∑ s=1 ws ∂ ∂Θ {log p(v |y,hvs) + log p(y|hid,hvs)}, (5)\nwhere the gradient is computed by averaging over all the gradients with respect to the importance samples.\nThe two steps have to be iterated. When more samples are needed to estimate the posterior, the space complexity will increase significantly, because we need to store a batch of data, the proposed samples, and their corresponding outputs at each layer of the deep network. When implementing the algorithm with GPU, one needs to make a tradeoff between the size of the data and the accurateness of the approximation, if the GPU memory is not sufficient for large scale training data. Our empirical study (Sec. 3.1) shows that the M-step of MVP can be computed by using only one sample, because the uniform prior typically leads to sparse weights during training. Therefore, the EM process develops into the conventional back-propagation.\nIn the forward pass, we sample a number of hvs based on the current parameters Θ, such that only the sample with the largest weight need to be stored. We demonstrate in the experiment (Sec. 3.1) that a small number of times (e.g. < 20) are sufficient to find good proposal. In the backward pass, we seek to update the parameters by the gradient,\n∂L(Θ) ∂Θ ' ∂ ∂Θ\n{ ws ( log p(v |y,hvs) + log p(y|hid,hvs) )} , (6)\nwhere hvs is the sample that has the largest weight ws. We need to optimize the following two terms, log p(y|hid,hvs) = − logσy − ‖ŷ−(U1hid+V1hvs )‖ 2 2\n2σ2y and log p(v |y,hvs) =∑\nj v̂j log( exp(U2j∗y+V 2 j∗h v s )∑K\nk=1 exp(U 2 k∗y+V 2 k∗h v s )\n), where ŷ and v̂ are the ground truth.\n• Continuous View In the previous discussion, v is assumed to be a binary vector. Note that v can also be modeled as a continuous variable with a Gaussian distribution,\np(v |y,hv) = N (v |U2y + V2hv,σv), (7)\nwhere v is a scalar corresponding to different views from −90◦ to +90◦. In this case, we can generate views not presented in the training data by interpolating v, as shown in Fig. 6.\n• Difference with multi-task learning Our model, which only has a single task, is also different from multi-task learning (MTL), where reconstruction of each view could be treated as a different task, although MTL has not been used for multi-view reconstruction in literature to the best of our knowledge. In MTL, the number of views to be reconstructed is predefined, equivalent to the number of tasks, and it encounters problems when the training data of different views are unbalanced; while our approach can sample views continuously and generate views not presented in the training data by interpolating v as described above. Moreover, the model complexity of MTL increases as the number of views and its training is more difficult since different tasks may have difference convergence rates."
    }, {
      "heading" : "2.2 Testing Procedure",
      "text" : "Given the view label v, and the input x, we generate the face image y under the viewpoint of v in the testing stage. A set of hv are first sampled, {hvs}Ss=1 ∼ U(0, 1), which corresponds to a set of outputs {ys}Ss=1. For example, in a simple network with only one hidden layer, ys = U1hid+V1hvs and hid = f(U0x). Then, the desired face image in view v is the output ys that produces the largest probability of p(v |ys,hvs). A full spectrum of multi-view images are reconstructed for all the possible view labels v."
    }, {
      "heading" : "2.3 View Estimation",
      "text" : "Our model can also be used to estimate viewpoint of the input image x. First, given all possible values of viewpoint v, we can generate a set of corresponding output images {yz}, where z indicates the index of the values of view we generated (or interpolated). Then, to estimate viewpoint, we assign the view label of the z-th output yz to x, such that yz is the most similar image to x. The above procedure is formulated as below. If v is discrete, the problem is, arg minj,z ‖ p(vj = 1|x,hvz) − p(vj = 1|yz,hvz) ‖22 = arg minj,z ‖ exp(U2j∗x+V 2 j∗h v z)∑K\nk=1 exp(U 2 k∗x+V 2 k∗h v z)\n− exp(U 2 j∗yz+V 2 j∗h v z)∑K\nk=1 exp(U 2 k∗yz+V 2 k∗h v z) ‖22. If v is continuous, the problem is defined as, arg minz ‖ (U2x +\nV2h v z)− (U2yz + V2hvz) ‖22 = arg minz ‖ x− yz ‖22."
    }, {
      "heading" : "3 Experiments",
      "text" : "Several experiments are designed for evaluation and comparison3. In Sec. 3.1, MVP is evaluated on a large face recognition dataset to demonstrate the effectiveness of the identity representation. Sec. 3.2 presents a quantitative evaluation, showing that the reconstructed face images are in good quality and the multi-view spectrum has retained discriminative information for face recognition. Sec. 3.3 shows that MVP can be used for view estimation and achieves comparable result as the discriminative methods specially designed for this task. An interesting experiment in Sec. 3.4 shows that by modeling the view as a continuous variable, MVP can analyze and reconstruct views not seen in the training data."
    }, {
      "heading" : "3.1 Multi-View Face Recognition",
      "text" : "MVP on multi-view face recognition is evaluated on the MultiPIE dataset [7], which contains 754, 204 images of 337 identities. Each identity was captured under 15 viewpoints from −90◦ to +90◦ and 20 different illuminations. It is the largest and most challenging dataset for evaluating face recognition under view and lighting variations. We conduct the following three experiments to demonstrate the effectiveness of MVP.\n3http://mmlab.ie.cuhk.edu.hk/projects/MVP.htm. For more technical details of this work, please contact the corresponding author Ping Luo (pluo.lhi@gmail.com).\n• Face recognition across views This setting follows the existing methods, e.g. [2, 12, 28], which employs the same subset of MultiPIE that covers images from −45◦ to +45◦ and with neutral illumination. The first 200 identities are used for training and the remaining 137 identities for test. In the testing stage, the gallery is constructed by choosing one canonical view image (0◦) from each testing identity. The remaining images of the testing identities from −45◦ to +45◦ are selected as probes. The number of neurons in MVP can be expressed as 32× 32− 512− 512(10)− 512(10)− 1024−32×32[7], where the input and output images have the size of 32×32, [7] denotes the length of the view label vector (v), and (10) represents that the third and forth layers have ten random neurons.\nWe examine the performance of using the identity features, i.e. hid2 (denoted as MVPhid2 ), and compare it with seven state-of-the-art methods in Table 1. The first three methods are based on 3D face models and the remaining ones are 2D feature extraction methods, including deep models, such as FIP [28] and RL [28], which employed the traditional convolutional network to recover the frontal view face image. As the existing methods did, LDA is applied to all the 2D methods to reduce the features’ dimension. The first and the second best results are highlighted for each viewpoint, as shown in Table 1. The two deep models (MVP and RL) outperform all the existing methods, including the 3D face models. RL achieves the best results on three viewpoints, whilst MVP is the best on four viewpoints. The extracted feature dimensions of MVP and RL are 512 and 9216, respectively. In summary, MVP obtains comparable averaged accuracy as RL under this setting, while the learned feature representation is more compact.\n• Face recognition across views and illuminations To examine the robustness of different feature representations under more challenging conditions, we extend the first setting by employing a larger subset of MultiPIE, which contains images from −60◦ to +60◦ and 20 illuminations. Other experimental settings are the same as the above. In Table 2, feature representations of different layers in MVP are compared with seven existing features, including raw pixels, LBP [1] on image grid, LBP on facial landmarks [4], CNN features, FIP [28], RL [28], and MTL+RL. LDA is applied to all the feature representations. Note that the last four methods are built on the convolutional neural networks. The only distinction is that they adopted different objective functions to learn features. Specifically, CNN uses cross-entropy loss to classify face identity as in [26]. FIP and RL utilized least-square loss to recover the frontal view image. MTL+RL is an extension of RL. It employs multiple tasks, each of which is formulated as a least square loss, to recover multi-view images, and all the tasks share feature layers. To achieve fair comparisons, CNN, FIP, and MTL+RL adopt the same convolutional structure as RL [28], since RL achieves competitive results in our first experiment.\nThe first and second best results are emphasized in bold in Table 2. The identity feature hid2 of MVP outperforms all the other methods on all the views with large margins. MTL+RL achieves the second best results except on ±60◦. These results demonstrate the superior of modeling multiview perception. For the features at different layers of MVP, the performance can be summarized as hid2 > h r 3 > h id 1 > h r 4, which conforms our expectation. h id 2 performs the best because it is the highest level of identity features. hid2 performs better than h id 1 because pose factors coupled in the input image x have be further removed, after one more forward mapping from hid1 to h id 2 . h id 2 also outperforms hr3 and h r 4, because some randomly generated view factors (h v 2 and h v 3) have been incorporated into these two layers during the construction of the full view spectrum. Please refer to Fig. 2 for a better understanding.\n• Effectiveness of the BP Procedure\nFig. 3 (a) compares the convergence rates during training, when using different number of samples to estimate the true posterior. We observe that a few number of samples, such as twenty, can lead to reasonably good convergence. Fig. 3 (b) empirically shows that uniform prior leads to sparse weights during training. In other words,\nif we seek to calculate the gradient of BP using only one sample, as did in Eq.(6). Fig. 3 (b) demonstrates that 20 samples are sufficient, since only 6 percent of the samples’ weights approximate one (all the others are zeros). Furthermore, as shown in Fig. 3 (c), the convergence rates of the one-sample gradient and the weighted summation are comparable.\n3.2 Reconstruction Quality\nAnother experiment is designed to quantitatively evaluate the multiview reconstruction result. The setting is the same as the first experiment in Sec. 3.1. The gallery images are all in the frontal view (0◦). Differently, LDA is applied to the raw pixels of the original images (OI) and the reconstructed images (RI) under the same view, respectively. Fig. 4 plots the accuracies of face recognition with respect to distinct viewpoints. Not surprisingly, under the viewpoints of +30◦ and −45◦ the accuracies of RI are decreased compared to OI. Nevertheless, this decrease is comparatively small (< 5%). It implies that the reconstructed images are in reasonably good quality. We notice that the reconstructed images in Fig. 1 lose some detailed textures, while well preserving the shapes of profile and the facial components.\n3.3 Viewpoint Estimation\nThis experiment is conducted to evaluate the performance of viewpoint estimation. MVP is compared to Linear Regression (LR) and Support Vector Regression (SVR), both of which have been used in viewpoint estimation, e.g. [8, 13]. Similarly, we employ the first setting as introduced in Sec. 3.1, implying that we train the models using images of a set of identities, and then estimate poses of the images of the remaining identities. For\ntraining LR and SVR, the features are obtained by applying PCA on the raw image pixels. Fig. 5 reports the view estimation errors, which are measured by the differences between the pose degrees\nof ground truth and the predicted degrees. The averaged errors of MVP, LR, and SVR are 5.03◦, 9.79◦, and 5.45◦, respectively. MVP achieves slightly better results compared to the discriminative model, i.e. SVR, demonstrating that it is also capable for view estimation, even though it is not designated for this task."
    }, {
      "heading" : "3.4 Viewpoint Interpolation",
      "text" : "When the viewpoint is modeled as a continuous variable as described in Sec. 2.1, MVP implicitly captures a 3D face model, such that it can analyze and reconstruct images under viewpoints that have not been seen before, while this cannot be achieved with MTL. In order to verify such capability, we conduct two tests. First, we adopt the images from MultiPIE in 0◦, 30◦, and 60◦ for training, and test whether MVP can generate images under 15◦ and 45◦. For each testing identity, the result is obtained by using the image in 0◦ as input and reconstructing images in 15◦ and 45◦. Several synthesized images (left) compared with the ground truth (right) are visualized in Fig. 6 (a). Although the interpolated images have noise and blurring effect, they have similar views as the ground truth and more importantly, the identity information is preserved. Second, under the same training setting as above, we further examine, when the images of the testing identities in 15◦ and 45◦ are employed as inputs, whether MVP can still generate a full spectrum of multi-view images and preserve identity information in the meanwhile. The results are illustrated in Fig. 6 (b), where the first image is the input and the remaining are the reconstructed images in 0◦, 30◦, and 60◦.\nThese two experiments show that MVP essentially models a continuous space of multi-view images such that first, it can predict images in unobserved views, and second, given an image under an unseen viewpoint, it can correctly extract identity information and then produce a full spectrum of multi-view images. In some sense, it performs multi-view reasoning, which is an intriguing function of human brain."
    }, {
      "heading" : "4 Conclusions",
      "text" : "In this paper, we have presented a generative deep network, called Multi-View Perceptron (MVP), to mimic the ability of multi-view perception in primate brain. MVP can disentangle the identity and view representations from an input image, and also can generate a full spectrum of views of the input image. Experiments demonstrated that the identity features of MVP achieve better performance on face recognition compared to state-of-the-art methods. We also showed that modeling the view factor as a continuous variable enables MVP to interpolate and predict images under the viewpoints, which are not observed in training data, imitating the reasoning capacity of human.\nAcknowledgement This work is partly supported by Natural Science Foundation of China (91320101, 61472410), Shenzhen Basic Research Program (JCYJ20120903092050890, JCYJ20120617114614438, JCYJ20130402113127496), Guangdong Innovative Research Team Program (201001D0104648280)."
    } ],
    "references" : [ {
      "title" : "Face description with local binary patterns: Application to face recognition",
      "author" : [ "T. Ahonen", "A. Hadid", "M. Pietikainen" ],
      "venue" : "TPAMI, 28:2037–2041,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fully automatic poseinvariant face recognition via 3d pose normalization",
      "author" : [ "A. Asthana", "T.K. Marks", "M.J. Jones", "K.H. Tieu", "M. Rohith" ],
      "venue" : "ICCV,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Face recognition with learning-based descriptor",
      "author" : [ "Z. Cao", "Q. Yin", "X. Tang", "J. Sun" ],
      "venue" : "CVPR,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification",
      "author" : [ "D. Chen", "X. Cao", "F. Wen", "J. Sun" ],
      "venue" : "CVPR,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Functional compartmentalization and viewpoint generalization within the macaque face-processing system",
      "author" : [ "W.A. Freiwald", "D.Y. Tsao" ],
      "venue" : "Science, 330(6005):845–851,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Toward pose-invariant 2-d face recognition through point distribution models and facial symmetry",
      "author" : [ "D. González-Jiménez", "J.L. Alba-Castro" ],
      "venue" : "IEEE Transactions on Information Forensics and Security, 2:413–429,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Multi-pie",
      "author" : [ "R. Gross", "I. Matthews", "J.F. Cohn", "T. Kanade", "S. Baker" ],
      "venue" : "Image and Vision Computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Estimating face pose by facial asymmetry and geometry",
      "author" : [ "Y. Hu", "L. Chen", "Y. Zhou", "H. Zhang" ],
      "venue" : "AFGR,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning hierarchical representations for face verification with convolutional deep belief networks",
      "author" : [ "G.B. Huang", "H. Lee", "E. Learned-Miller" ],
      "venue" : "CVPR,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On information and sufficiency",
      "author" : [ "S. Kullback", "R.A. Leibler" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1951
    }, {
      "title" : "Gradient based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of IEEE, 86(11):2278–2324,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Morphable displacement field based image matching for face recognition across pose",
      "author" : [ "S. Li", "X. Liu", "X. Chai", "H. Zhang", "S. Lao", "S. Shan" ],
      "venue" : "ECCV,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Support vector regression and classification based multi-view face detection and recognition",
      "author" : [ "Y. Li", "S. Gong", "H. Liddell" ],
      "venue" : "AFGR,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Gabor feature based classification using the enhanced fisher linear discriminant model for face recognition",
      "author" : [ "C. Liu", "H. Wechsler" ],
      "venue" : "TIP, 11:467–476,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Distinctive image features from scale-invariant keypoints",
      "author" : [ "D.G. Lowe" ],
      "venue" : "IJCV, 60:91–110,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Hierarchical face parsing via deep learning",
      "author" : [ "P. Luo", "X. Wang", "X. Tang" ],
      "venue" : "CVPR,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A deep sum-product architecture for robust facial attributes analysis",
      "author" : [ "P. Luo", "X. Wang", "X. Tang" ],
      "venue" : "ICCV,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "ICML,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "What makes a cell face selective? the importance of contrast",
      "author" : [ "S. Ohayon", "W.A. Freiwald", "D.Y. Tsao" ],
      "venue" : "Neuron, 74:567–581,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning to disentangle factors of variation with manifold interaction",
      "author" : [ "S. Reed", "K. Sohn", "Y. Zhang", "H. Lee" ],
      "venue" : "ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fisher vector faces in the wild",
      "author" : [ "K. Simonyan", "O.M. Parkhi", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "BMVC,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Hybrid deep learning for face verification",
      "author" : [ "Y. Sun", "X. Wang", "X. Tang" ],
      "venue" : "ICCV,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep convolutional network cascade for facial point detection",
      "author" : [ "Y. Sun", "X. Wang", "X. Tang" ],
      "venue" : "CVPR,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep learning face representation by joint identification-verification",
      "author" : [ "Y. Sun", "Y. Chen", "X. Wang", "X. Tang" ],
      "venue" : "NIPS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep learning face representation from predicting 10,000 classes",
      "author" : [ "Y. Sun", "X. Wang", "X. Tang" ],
      "venue" : "CVPR,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deepface: Closing the gap to human-level performance in face verification",
      "author" : [ "Y. Taigman", "M. Yang", "M.A. Ranzato", "L. Wolf" ],
      "venue" : "CVPR,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning stochastic feedforward neural networks",
      "author" : [ "Y. Tang", "R. Salakhutdinov" ],
      "venue" : "NIPS,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Deep learning identity preserving face space",
      "author" : [ "Z. Zhu", "P. Luo", "X. Wang", "X. Tang" ],
      "venue" : "ICCV,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Recent studies [5, 19] discovered that primate brain has a face-processing network, where view and identity are processed by different neurons.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 18,
      "context" : "Recent studies [5, 19] discovered that primate brain has a face-processing network, where view and identity are processed by different neurons.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Substantial efforts have been dedicated to extract identity features by hand, such as LBP [1], Gabor [14], and SIFT [15].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "Substantial efforts have been dedicated to extract identity features by hand, such as LBP [1], Gabor [14], and SIFT [15].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "Substantial efforts have been dedicated to extract identity features by hand, such as LBP [1], Gabor [14], and SIFT [15].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "The best practise of face recognition extracts the above features on the landmarks of face images with multiple scales and concatenates them into high dimensional feature vectors [4, 21].",
      "startOffset" : 179,
      "endOffset" : 186
    }, {
      "referenceID" : 20,
      "context" : "The best practise of face recognition extracts the above features on the landmarks of face images with multiple scales and concatenates them into high dimensional feature vectors [4, 21].",
      "startOffset" : 179,
      "endOffset" : 186
    }, {
      "referenceID" : 8,
      "context" : "Deep learning methods, such as Boltzmann machine [9], sum product network [17], and deep neural net [16, 25, 22, 23, 24, 26] have been applied to face recognition.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : "Deep learning methods, such as Boltzmann machine [9], sum product network [17], and deep neural net [16, 25, 22, 23, 24, 26] have been applied to face recognition.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "Deep learning methods, such as Boltzmann machine [9], sum product network [17], and deep neural net [16, 25, 22, 23, 24, 26] have been applied to face recognition.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 24,
      "context" : "Deep learning methods, such as Boltzmann machine [9], sum product network [17], and deep neural net [16, 25, 22, 23, 24, 26] have been applied to face recognition.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 21,
      "context" : "Deep learning methods, such as Boltzmann machine [9], sum product network [17], and deep neural net [16, 25, 22, 23, 24, 26] have been applied to face recognition.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : "Deep learning methods, such as Boltzmann machine [9], sum product network [17], and deep neural net [16, 25, 22, 23, 24, 26] have been applied to face recognition.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 23,
      "context" : "Deep learning methods, such as Boltzmann machine [9], sum product network [17], and deep neural net [16, 25, 22, 23, 24, 26] have been applied to face recognition.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 25,
      "context" : "Deep learning methods, such as Boltzmann machine [9], sum product network [17], and deep neural net [16, 25, 22, 23, 24, 26] have been applied to face recognition.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 24,
      "context" : "[25, 22] employed deep neural net to learn identity features from raw pixels by predicting 10, 000 identities.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 21,
      "context" : "[25, 22] employed deep neural net to learn identity features from raw pixels by predicting 10, 000 identities.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 4,
      "context" : "Recent studies [5, 19] discovered that macaque monkeys have a face-processing network that was made of six interconnected face-selective regions, where neurons in some of these regions were view-specific, while some others were tuned to identity across views, making face recognition in brain of primate robust to view variation.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 18,
      "context" : "Recent studies [5, 19] discovered that macaque monkeys have a face-processing network that was made of six interconnected face-selective regions, where neurons in some of these regions were view-specific, while some others were tuned to identity across views, making face recognition in brain of primate robust to view variation.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "For example, the 2D methods, such as [6], infer the deformation (e.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "The 3D methods, such as [2, 12], capture 3D face models in different parametric forms.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "The 3D methods, such as [2, 12], capture 3D face models in different parametric forms.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 27,
      "context" : "Our previous work [28] learned identity features by using CNN to recover a single frontal view face image, which is a special case of MVP after removing the random neurons.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 27,
      "context" : "[28] did not learn the view representation as we do.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] untangled factors of image variation by using a high-order Boltzmann machine, where all the neurons are stochastic and it is solved by gibbs sampling.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "Other activation functions, such as rectified linear function [18] and tangent [11], can be used as well.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "Other activation functions, such as rectified linear function [18] and tangent [11], can be used as well.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "In particular, the first term is the KL-divergence [10] between the true posterior and the distribution q(h).",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "The above lower bound can be maximized by using the Monte Carlo Expectation Maximization (MCEM) algorithm recently introduced by [27], which approximates the true posterior by using the importance sampling with the conditional prior as the proposal distribution.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "MVP on multi-view face recognition is evaluated on the MultiPIE dataset [7], which contains 754, 204 images of 337 identities.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "[2, 12, 28], which employs the same subset of MultiPIE that covers images from −45◦ to +45◦ and with neutral illumination.",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "[2, 12, 28], which employs the same subset of MultiPIE that covers images from −45◦ to +45◦ and with neutral illumination.",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 27,
      "context" : "[2, 12, 28], which employs the same subset of MultiPIE that covers images from −45◦ to +45◦ and with neutral illumination.",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 6,
      "context" : "The number of neurons in MVP can be expressed as 32× 32− 512− 512(10)− 512(10)− 1024−32×32[7], where the input and output images have the size of 32×32, [7] denotes the length of the view label vector (v), and (10) represents that the third and forth layers have ten random neurons.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "The number of neurons in MVP can be expressed as 32× 32− 512− 512(10)− 512(10)− 1024−32×32[7], where the input and output images have the size of 32×32, [7] denotes the length of the view label vector (v), and (10) represents that the third and forth layers have ten random neurons.",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 27,
      "context" : "The first three methods are based on 3D face models and the remaining ones are 2D feature extraction methods, including deep models, such as FIP [28] and RL [28], which employed the traditional convolutional network to recover the frontal view face image.",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 27,
      "context" : "The first three methods are based on 3D face models and the remaining ones are 2D feature extraction methods, including deep models, such as FIP [28] and RL [28], which employed the traditional convolutional network to recover the frontal view face image.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "In Table 2, feature representations of different layers in MVP are compared with seven existing features, including raw pixels, LBP [1] on image grid, LBP on facial landmarks [4], CNN features, FIP [28], RL [28], and MTL+RL.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "In Table 2, feature representations of different layers in MVP are compared with seven existing features, including raw pixels, LBP [1] on image grid, LBP on facial landmarks [4], CNN features, FIP [28], RL [28], and MTL+RL.",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 27,
      "context" : "In Table 2, feature representations of different layers in MVP are compared with seven existing features, including raw pixels, LBP [1] on image grid, LBP on facial landmarks [4], CNN features, FIP [28], RL [28], and MTL+RL.",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 27,
      "context" : "In Table 2, feature representations of different layers in MVP are compared with seven existing features, including raw pixels, LBP [1] on image grid, LBP on facial landmarks [4], CNN features, FIP [28], RL [28], and MTL+RL.",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 25,
      "context" : "Specifically, CNN uses cross-entropy loss to classify face identity as in [26].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 27,
      "context" : "To achieve fair comparisons, CNN, FIP, and MTL+RL adopt the same convolutional structure as RL [28], since RL achieves competitive results in our first experiment.",
      "startOffset" : 95,
      "endOffset" : 99
    } ],
    "year" : 2014,
    "abstractText" : "Various factors, such as identity, view, and illumination, are coupled in face images. Disentangling the identity and view representations is a major challenge in face recognition. Existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy. This is different from the behavior of primate brain. Recent studies [5, 19] discovered that primate brain has a face-processing network, where view and identity are processed by different neurons. Taking into account this instinct, this paper proposes a novel deep neural net, named multi-view perceptron (MVP), which can untangle the identity and view features, and in the meanwhile infer a full spectrum of multi-view images, given a single 2D face image. The identity features of MVP achieve superior performance on the MultiPIE dataset. MVP is also capable to interpolate and predict images under viewpoints that are unobserved in the training data.",
    "creator" : null
  }
}