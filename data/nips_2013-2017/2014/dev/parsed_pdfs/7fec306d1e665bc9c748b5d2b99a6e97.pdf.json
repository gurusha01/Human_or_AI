{
  "name" : "7fec306d1e665bc9c748b5d2b99a6e97.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Probabilistic Differential Dynamic Programming",
    "authors" : [ "Yunpeng Pan", "Evangelos A. Theodorou", "Daniel Guggenheim" ],
    "emails" : [ "ypan37@gatech.edu,", "evangelos.theodorou@ae.gatech.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Differential Dynamic Programming (DDP) is a powerful trajectory optimization approach. Originally introduced in [1], DDP generates locally optimal feedforward and feedback control policies along with an optimal state trajectory. Compared with global optimal control approaches, the local optimal DDP shows superior computational efficiency and scalability to high-dimensional problems. In the last decade, variations of DDP have been proposed in both control and machine learning communities [2][3][4][5][6]. Recently, DDP was applied for high-dimensional policy search which achieved promising results in challenging control tasks [7].\nDDP is derived based on linear approximations of the nonlinear dynamics along state and control trajectories, therefore it relies on accurate and explicit dynamics models. However, modeling a dynamical system is in general a challenging task and model uncertainty is one of the principal limitations of model-based methods. Various parametric and semi-parametric approaches have been developed to address these issues, such as minimax DDP using Receptive Field Weighted Regression (RFWR) by Morimoto and Atkeson [8], and DDP using expert-demonstrated trajectories by Abbeel et al. [9]. Motivated by the complexity of the relationships between states, controls and observations in autonomous systems, in this work we take a Bayesian non-parametric approach using Gaussian Processes (GPs).\nOver last few years, GP-based control and Reinforcement Learning (RL) algorithms have increasingly drawn more attention in control theory and machine learning communities. For instance, the works by Rasmussen et al.[10], Nguyen-Tuong et al.[11], Deisenroth et al.[12][13][14] and Hemakumara et al.[15] have demonstrated the remarkable applicability of GP-based control and RL methods in robotics. In particular, a recently proposed GP-based policy search framework called PILCO, developed by Deisenroth and Rasmussen [13] (an improved version has been developed by Deisenroth, Fox and Rasmussen [14]) has achieved unprecedented performances in terms of data-\nefficiency and policy learning speed. PILCO as well as most gradient-based policy search algorithms require iterative methods (e.g.,CG or BFGS) for solving non-convex optimization to obtain optimal policies.\nThe proposed approach does not require a policy parameterization. Instead PDDP finds a linear, time varying control policy based on Bayesian non-parametric representation of the dynamics and outperforms PILCO in terms of control learning speed while maintaining a comparable data-efficiency."
    }, {
      "heading" : "2 Proposed Approach",
      "text" : "The proposed PDDP framework consists of 1) a Bayesian non-parametric representation of the unknown dynamics; 2) local approximations of the dynamics and value functions; 3) locally optimal controller learning."
    }, {
      "heading" : "2.1 Problem formulation",
      "text" : "We consider a general unknown stochastic system described by the following differential equation\ndx = f(x,u)dt+ C(x,u)dω, x(t0) = x0, dω ∼ N (0,Σω), (1) where x ∈ Rn is the state, u ∈ Rm is the control, t is time and ω ∈ Rp is standard Brownian motion noise. The trajectory optimization problem is defined as finding a sequence of state and controls that minimize the expected cost\nJπ(x(t0)) = E [ h ( x(T ) ) + ∫ T t0 L ( x(t), π(x(t)), t ) dt ] , (2)\nwhere h(x(T )) is the terminal cost, L(x(t), π(x(t)), t) is the instantaneous cost rate, u(t) = π(x(t)) is the control policy. The cost Jπ(x(t0)) is defined as the expectation of the total cost accumulated from t0 to T . For the rest of our analysis, we denote xk = x(tk) in discrete-time where k = 0, 1, ...,H is the time step, we use this subscript rule for other variables as well."
    }, {
      "heading" : "2.2 Probabilistic dynamics model learning",
      "text" : "The continuous functional mapping from state-control pair x̃ = (x,u) ∈ Rn+m to state transition dx can be viewed as an inference with the goal of inferring dx given x̃. We view this inference as a nonlinear regression problem. In this subsection, we introduce the Gaussian processes (GP) approach to learning the dynamics model in (1). A GP is defined as a collection of random variables, any finite number subset of which have a joint Gaussian distribution. Given a sequence of state-control pairs X̃ = {(x0,u0), . . . (xH ,uH)}, and the corresponding state transition dX = {dx0, . . . ,dxH}, a GP is completely defined by a mean function and a covariance function. The joint distribution of the observed output and the output corresponding to a given test statecontrol pair x̃∗ = (x∗,u∗) can be written as p ( dX dx∗ ) ∼ N ( 0, [ K(X̃, X̃) + σnI K(X̃, x̃ ∗) K(x̃∗, X̃) K(x̃∗, x̃∗) ] ) . The covariance of this multivariate Gaussian distribution is defined via a kernel matrix K(xi,xj). In particular, in this paper we consider the Gaussian kernel K(xi,xj) = σ2s exp(− 12 (xi−xj)\nTW(xi− xj))+σ 2 n,with σs, σn,W the hyper-parameters. The kernel function can be interpreted as a similarity measure of random variables. More specifically, if the training pairs X̃i and X̃j are close to each other in the kernel space, their outputs dxi and dxj are highly correlated. The posterior distribution, which is also a Gaussian, can be obtained by constraining the joint distribution to contain the output dx∗ that is consistent with the observations. Assuming independent outputs (no correlation between each output dimension) and given a test input x̃k = (xk,uk) at time step k, the one-step predictive mean and variance of the state transition are specified as\nEf [dxk] = K(x̃k, X̃)(K(X̃, X̃) + σnI)−1dX, (3) VARf [dxk] = K(x̃k, x̃k)−K(x̃k, X̃)(K(X̃, X̃) + σnI)−1K(X̃, x̃k).\nThe state distribution at k = 1 is p(x1) ∼ N (µ1,Σ1) where the state mean and variance are µ1 = x0 +Ef [dx0],Σ1 = VARf [dx0]. When propagating the GP-based dynamics over a trajectory of time horizon H , the input state-control pair x̃k becomes uncertain with a Gaussian distribution\n(initially x̃0 is deterministic). Here we define the joint distribution over state-control pair at k as p(x̃k) = p(xk,uk) ∼ N (µ̃k, Σ̃k). Thus the distribution over state transition becomes p(dxk) =∫\np(f(x̃k)|x̃k)p(x̃k)dx̃k. Generally, this predictive distribution cannot be computed analytically because the nonlinear mapping of an input Gaussian distribution lead to a non-Gaussian predictive distribution. However, the predictive distribution can be approximated by a Gaussian p(dxk) ∼ N (dµk,dΣk) [16]. Thus the state distribution at k + 1 is also a Gaussian N (µk+1,Σk+1) [14]\nµk+1 = µk + dµk, Σk+1 = Σk + dΣk + COVf ,x̃k [xk,dxk] + COVf ,x̃k [dxk,xk]. (4) Given an input joint distribution N (µ̃k, Σ̃k), we employ the moment matching approach [16][14] to compute the posterior GP. The predictive mean dµk is evaluated as\ndµk = Ex̃k [ Ef [dxk] ] = ∫ Ef [dxk]N ( µ̃k, Σ̃k ) dx̃k.\nNext, we compute the predictive covariance matrix\ndΣk = [ VARf,x̃k [dxk1 ] . . . COVf,x̃k [dxkn , dxk1 ] . . . . . . . . .\nCOVf,x̃k [dxk1 , dxkn ] . . . VARf,x̃k [dxkn ]\n] ,\nwhere the variance term on the diagonal for output dimension i is obtained as VARf ,x̃k [dxki ] = Ex̃k [ VARf [dxki ] ] + Ex̃k [ Ef [dxki ]2 ] − Ex̃k [ Ef [dxki ] ]2 , (5) and the off-diagonal covariance term for output dimension i, j is given by the expression COVf ,x̃k [dxki ,dxkj ] = Ex̃k [ Ef [dxki ]Ef [dxkj ] ] − Ex̃k [Ef [dxki ]]Ex̃k [Ef [dxkj ]]. (6) The input-output cross-covariance is formulated as COVf ,x̃k [x̃k,dxk] = Ex̃k [ x̃kEf [dxk]T ] − Ex̃k [x̃k]Ef ,x̃k [dxk]T. (7) COVf ,x̃k [xk,dxk] can be easily obtained as a sub-matrix of (7). The kernel or hyper-parameters Θ = (σn, σs,W) can be learned by maximizing the log-likelihood of the training outputs given the inputs\nΘ∗ = argmax Θ\n{ log ( p ( dX|X̃,Θ ))} . (8)\nThis optimization problem can be solved using numerical methods such as conjugate gradient [17]."
    }, {
      "heading" : "2.3 Local dynamics model",
      "text" : "In DDP-related algorithms, a local model along a nominal trajectory (x̄k, ūk), is created based on: i) a first or second-order linear approximation of the dynamics model; ii) a second-order local approximation of the value function. In our proposed PDDP framework, we will create a local model along a trajectory of state distribution-control pair (p(x̄k), ūk). In order to incorporate uncertainty explicitly in the local model, we introduce the Gaussian belief augmented state vector zxk = [µk vec(Σk)]\nT ∈ Rn+n×n where vec(Σk) is the vectorization of Σk. Now we create a local linear model of the dynamics. Based on eq.(4), the dynamics model with the augmented state is\nzxk+1 = F(zxk,uk). (9) Define the control and state variations δzxk = z x k− z̄xk and δuk = uk− ūk. In this work we consider the first-order expansion of the dynamics. More precisely we have δzxk+1 = Fxk δzxk + Fuk δuk, (10) where the Jacobian matrices Fxk and Fuk are specified as\nFxk = ∇xkF =  ∂µk+1∂µk ∂µk+1∂Σk ∂Σk+1 ∂µk Σk+1 ∂Σk  ∈ R(n+n2)×(n+n2), Fuk = ∇ukF = [ ∂µk+1 ∂uk\n∂Σk+1 ∂uk\n] ∈ R(n+n 2)×m.\n(11)\nThe partial derivatives ∂µk+1 ∂µk , ∂µk+1 ∂Σk , ∂Σk+1∂µk , ∂Σk+1 ∂Σk , ∂µk+1 ∂uk , ∂Σk+1∂uk can be computed analytically. Their forms are provided in the supplementary document of this work. For numerical implementation, the dimension of the augmented state can be reduced by eliminating the redundancy of Σk and the principle square root of Σk may be used for numerical robustness [6]."
    }, {
      "heading" : "2.4 Cost function",
      "text" : "In the classical DDP and many optimal control problems, the following quadratic cost function is used\nL(xk,uk) = (xk − xgoalk ) TQ(xk − xgoalk ) + u T kRuk, (12)\nwhere xgoalk is the target state. Given the distribution p(xk) ∼ N (µk,Σk), the expectation of original quadratic cost function is formulated as\nExk [ L(xk,uk) ] = tr(QΣk) + (µk − x goal k ) TQ(µk − x goal k ) + u T kRuk. (13)\nIn PDDP, we use the cost function L(zxk,uk) = Exk [L(xk,uk)]. The analytic expressions of partial derivatives ∂∂zxkL(z x k,uk) and ∂ ∂uk L(zxk,uk) can be easily obtained. The cost function (13) scales linearly with the state covariance, therefore the exploration strategy of PDDP is balanced between the distance from the target and the variance of the state. This strategy fits well with DDP-related frameworks that rely on local approximations of the dynamics. A locally optimal controller obtained from high-risk explorations in uncertain regions might be highly undesirable."
    }, {
      "heading" : "2.5 Control policy",
      "text" : "The Bellman equation for the value function in discrete-time is specified as follows\nV (zxk, k) = min uk E\n[ L(zxk,uk) + V ( F(zxk,uk), k + 1 ) ︸ ︷︷ ︸\nQ(zxk,uk)\n|xk ] . (14)\nWe create a quadratic local model of the value function by expanding the Q-function up to the second order\nQk(z x k+δz x k,uk+δuk) ≈ Q0k+Qxkδzxk+Qukδuk+\n1\n2 [ δzxk δuk ]T [ Qxxk Q xu k Quxk Q uu k ] [ δzxk δuk ] , (15)\nwhere the superscripts of the Q-function indicate derivatives. For instance, Qxk = ∇xQk(zxk,uk). For the rest of the paper, we will use this superscript rule for L and V as well. To find the optimal control policy, we compute the local variations in control δûk that maximize the Q-function\nδûk = arg max uk\n[ Qk(z x k + δz x k,uk + δuk) ] = −(Quuk )−1Quk︸ ︷︷ ︸\nIk\n−(Quuk )−1Quxk︸ ︷︷ ︸ Lk δzxk = Ik + Lkδz x k.\n(16) The optimal control can be found as ûk = ūk + δûk. The quadratic expansion of the value function is backward propagated based on the equations that follow\nQxk = Lxk + V xk Fxk , Quk = Luk + V xk Fuk , Qxxk = Lxxk + (Fxk )TV xxk Fxk , Quxk = Luxk + (Fuk )TV xxk Fxk , Quuk = Luuk + (Fuk )TV xxk Fuk , Vk−1 = Vk + Q u kIk, V x k−1 = Q x k + Q u kLk, V xx k−1 = Q xx k + Q xu k Lk. (17)\nThe second-order local approximation of the value function is propagated backward in time iteratively. We use the learned controller to generate a locally optimal trajectory by propagating the dynamics forward in time. The control policy is a linear function of the augmented state zxk , therefore the controller is deterministic. The state propagations have been discussed in Sec. 2.2."
    }, {
      "heading" : "2.6 Summary of algorithm",
      "text" : "The proposed algorithm can be summarized in Algorithm 1. The algorithm consists of 8 modules. In Model learning (Step 1-2) we sample trajectories from the original physical system in order to collect training data and learn a probabilistic model. In Local approximation (Step 4) we obtain a local linear approximation (10) of the learned probabilistic model along a nominal trajectory by computing Jacobian matrices (11). In Controller learning (Step 5) we compute a local optimal control sequence (16) by backward-propagation of the value function (17). To ensure convergence, we\nemploy the line search strategy as in [2]. We compute the control law as δûk = αIk + Lkδzxk . Initially α = 1, then decrease it until the expected cost is smaller than the previous one. In Forward propagation (Step 6), we apply the control sequence from last step and obtain a new nominal trajectory for the next iteration. In Convergence condition (Step 7), we set a threshold on the accumulated cost J∗ such that when Jπ < J∗, the algorithm is terminated with the optimized state and control trajectory. In Interaction condition (Step 8), when the state covariance Σk exceeds a threshold Σtol, we sample new trajectories from the physical system using the control obtained in step 5, and go back to step 2 to learn a more accurate model. The old GP training data points are removed from the training set to keep its size fixed. Finally in Nominal trajectory update (step 9), the trajectory obtained in Step 6 or 8 becomes the new nominal trajectory for the next iteration. An simple illustration of the algorithm is shown in Fig. 3a. Intuitively, PDDP requires interactions with the physical systems only if the GP model no longer represents the true dynamics around the nominal trajectory.\nGiven: A system with unknown dynamics, target states Goal : An optimized trajectory of state and control\n1 Generate N state trajectories by applying random control sequences to the physical system (1); 2 Obtain state and control training pairs from sampled trajectories and optimize the\nhyper-parameters of GP (8); 3 for i = 1 to Imax do 4 Compute a linear approximation of the dynamics along (z̄xk, ūk) (10); 5 Backpropagate in time to get the locally optimal control ûk = ūk + δûk and value function V (zxk, k) according to (16) (17); 6 Forward propagate the dynamics (9) by applying the optimal control ûk, obtain a new trajectory (zxk,uk); 7 if Converge then Break the for loop; 8 if Σk > Σtol then apply the optimal control to the original physical system to generate a\nnew nominal trajectory (zxk,uk) and N − 1 additional trajectories by applying small variations of the learned controller, update the GP training set and go back to step 2;\n9 Set z̄xk = z x k , ūk = uk and i = i+ 1, go back to step 4;\n10 end 11 Apply the optimized controller to the physical system, obtain the optimized trajectory.\nAlgorithm 1: PDDP algorithm"
    }, {
      "heading" : "2.7 Computational complexity",
      "text" : "Dynamics propagation: The major computational effort is devoted to GP inferences. In particular, the complexity of one-step moment matching (2.2) isO ( (N)2n2(n+m) ) [14], which is fixed during the iterative process of PDDP. We found a small number of sampled trajectories (N ≤ 5) are able to provide good performances for a system of moderate size (6-12 state dimensions). However, for higher dimensional problems, sparse or local approximation of GP (e.g. [11][18][19], etc) may be used to reduce the computational cost of GP dynamics propagation.\nController learning: According to (16), learning policy parameters Ik and Lk requires computing the inverse of Quuk , which has the computational complexity of O(m3), where m is the dimension of control input. As a local trajectory optimization method, PDDP offers comparable scalability to the classical DDP."
    }, {
      "heading" : "2.8 Relation to existing works",
      "text" : "Here we summarize the novel features of PDDP in comparison with some notable DDP-related frameworks for stochastic systems (see also Table 1). First, PDDP shares some similarities with the belief space iLQG [6] framework, which approximates the belief dynamics using an extended Kalman filter. Belief space iLQG assumes a dynamics model is given and the stochasticity comes from the process noises. PDDP, however, is a data-driven approach that learns the dynamics models and controls from sampled data, and it takes into account model uncertainties by using GPs. Second, PDDP is also comparable with iLQG-LD [5], which applies Locally Weighted Projection Regression (LWPR) to represent the dynamics. iLQG-LD does not incorporate model uncertainty therefore requires a large amount of data to learn an accurate model. Third, PDDP does not suffer from the\nhigh computational cost of finite differences used to numerically compute the first-order expansions [2][6] and second-order expansions [4] of the underlying stochastic dynamics. PDDP computes Jacobian matrices analytically (11)."
    }, {
      "heading" : "3 Experimental Evaluation",
      "text" : "We evaluate the PDDP framework using two nontrivial simulated examples: i) cart-double inverted pendulum swing-up; ii) six-link robotic arm reaching. We also compare the learning efficiency of PDDP with the classical DDP [1] and PILCO [13][14]. All experiments were performed in MATLAB."
    }, {
      "heading" : "3.1 Cart-double inverted pendulum swing-up",
      "text" : "Cart-Double Inverted Pendulum (CDIP) swing-up is a challenging control problem because the system is highly underactuated with 3 degrees of freedom and only 1 control input. The system has 6 state-dimensions (cart position/velocity, link 1,2 angles and angular velocities). The swing-up problem is to find a sequence of control input to force both pendulums from initial position (π,π) to the inverted position (2π,2π). The balancing task requires the velocity of the cart, angular velocities of both pendulums to be zero. We sample 4 initial trajectories with time horizon H = 50. The CDIP swing-up problem has been solved by two controllers for swing-up and balancing, respectively [20]. PILCO [14] is one of the few RL methods that is able to complete this task without knowing the dynamics. The results are shown in Fig.1."
    }, {
      "heading" : "3.2 Six-link robotic arm",
      "text" : "The six-link robotic arm model consist of six links of equal length and mass, connected in an open chain with revolute joints. The system has 6 degrees of freedom, and 12 state dimensions (angle and angular velocity for each joint). The goal for the first 3 joints is to move to the target angle π4 and for the rest 3 joints to −π4 . The desired velocities for all 6 joints are zeros. We sample 2 initial trajectories with time horizon H = 50. The results are shown in Fig. 2."
    }, {
      "heading" : "3.3 Comparative analysis",
      "text" : "DDP: Originally introduced in the 70’s, the classical DDP [1] is still one of the most effective and efficient trajectory optimization approaches. The major differences between DDP and PDDP can\nbe summarized as follow: firstly, DDP relies on a given accurate dynamics model, while PDDP is a data-driven framework that learns a locally accurate model by forward sampling; secondly, DDP does not deal with model uncertainty, PDDP takes into account model uncertainty using GPs and perform local dynamic programming in Gaussian belief spaces; thirdly, generally in applications of DDP linearizations are performed using finite differences while in PDDP Jacobian matrices are computed analytically (11).\nPILCO: The recently proposed PILCO [14] framework has demonstrated state-of-the-art learning efficiency compared with other methods such as [21][22]. The proposed PDDP is different from PILCO in several ways. Firstly, based on local linear approximation of dynamics and quadratic approximation of the value function, PDDP finds linear, time-varying feedforward and feedback policy, PILCO requires an a priori policy parameterization and an extra optimization solver. Secondly, PDDP keeps a fixed size of training data for GP inferences, while PILCO adds new data to the training set after each trial (recently, the authors applied sparse GP approximation [19] in an improved version of PILCO when the data size reached a threshold). Thirdly, by using the Gaussian belief augmented state and cost function (13), PDDP’s exploration scheme is balanced between the distance from the target and the variance of the state. PILCO employs a saturating cost function which leads to automatic explorations in the high-variance regions in the early stages of learning.\nIn both tasks, PDDP, DDP and PILCO bring the system to the desired states. The resulting trajectories for PDDP are shown in Fig.1a and 2a. The reason for low variances of some optimized trajectories is that during final stage of learning, interactions with the physical systems (forward samplings using the locally optimal controller) would reduce the variances significantly. The costs are shown in Fig. 1b and 2b. For both tasks, PDDP and DDP performs similarly and slightly different from PILCO in terms of cost reduction. The major reasons for this difference are: i) different cost functions used by these methods; ii) we did not impose any convergence condition for the optimized trajectories on PILCO. We now compare PDDP with DDP and PILCO in terms of data-efficiency and controller learning speed.\nData-efficiency: As shown in Fig.4a, in both tasks, PDDP performs slightly worse than PILCO in terms of data-efficiency based on the number of interactions required with the physical systems. For the systems used for testing, PDDP requires around 15% − 25% more interactions than PILCO. The number of interactions indicates the amount of sampled trajectories required from the physical system. At each trial we sample N trajectories from the physical systems (algorithm 1). Possible reasons for the slightly worse performances are: i) PDDP’s policy is linear which is restrictive, while PILCO yields nonlinear policy parameterizations; ii) PDDP’s exploration scheme is more conservative than PILCO in the early stages of learning. We believe PILCO is the most data-efficient framework for these tasks. However, PDDP is able to offer close performances thanks to the probabilistic representation of the dynamics as well as the use of Gaussian belief augmented state.\nLearning speed: In terms of total computational time required to obtain the final controller, PDDP outperforms PILCO significantly as shown in Fig.4b. For the 6 and 12 dimensional systems used for testing, PILCO requires an iterative method (e.g.,CG or BFGS) to solve high dimensional optimization problems (depending on the policy parameterization), while PDDP computes local optimal controls (16) without an extra optimizer. In terms of computational time per iteration, as shown in\nFig.3b, PDDP is slower than the classical DDP due to the high computational cost of GP dynamics propagations. However, for DDP, the time dedicated to linearizing the dynamics model is around 70% − 90% of the total time per iteration for the two tasks considered in this work. PDDP avoids the high computational cost of finite differences by evaluating all Jacobian matrices analytically, the time dedicated to linearization is less than 10% of the total time per iteration."
    }, {
      "heading" : "4 Conclusions",
      "text" : "In this work we have introduced a probabilistic model-based control and trajectory optimization method for systems with unknown dynamics based on Differential Dynamic Programming (DDP) and Gaussian processes (GPs), called Probabilistic Differential Dynamic Programming (PDDP). PDDP takes model uncertainty into account explicitly by representing the dynamics using GPs and performing local Dynamic Programming in Gaussian belief spaces. Based on the quadratic approximation of the value function, PDDP yields a linear, locally optimal control policy and features a more efficient control improvement scheme compared with typical gradient-based policy search methods. Thanks to the probabilistic representation of the dynamics, PDDP offers reasonable data-efficiency comparable to a state of the art GP-based policy search method [14]. In general, local trajectory optimization is a powerful approach to challenging control and RL problems. Due to its model-based nature, model inaccuracy has always been the major obstacle for advanced applications. Grounded on the solid developments of classical trajectory optimization and Bayesian machine learning, the proposed PDDP has demonstrated encouraging performance and potential for many applications."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by a National Science Foundation grant NRI-1426945."
    } ],
    "references" : [ {
      "title" : "Differential dynamic programming",
      "author" : [ "D. Jacobson", "D. Mayne" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1970
    }, {
      "title" : "A generalized iterative lqg method for locally-optimal feedback control of constrained nonlinear stochastic systems",
      "author" : [ "E. Todorov", "W. Li" ],
      "venue" : "In American Control Conference,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Stochastic differential dynamic programming",
      "author" : [ "E. Theodorou", "Y. Tassa", "E. Todorov" ],
      "venue" : "In American Control Conference,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Adaptive optimal feedback control with learned internal dynamics models",
      "author" : [ "D. Mitrovic", "S. Klanke", "S. Vijayakumar" ],
      "venue" : "From Motor Learning to Interaction Learning in Robots, pages 65–84. Springer",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Motion planning under uncertainty using iterative local optimization in belief space",
      "author" : [ "J. Van Den Berg", "S. Patil", "R. Alterovitz" ],
      "venue" : "The International Journal of Robotics Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Variational policy search via trajectory optimization",
      "author" : [ "S. Levine", "V. Koltun" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Minimax differential dynamic programming: An application to robust biped walking",
      "author" : [ "J. Morimoto", "C.G. Atkeson" ],
      "venue" : "NIPS, pages 1539–1546",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "An application of reinforcement learning to aerobatic helicopter flight",
      "author" : [ "P. Abbeel", "A. Coates", "M. Quigley", "A.Y. Ng" ],
      "venue" : "NIPS, pages 1–8",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Gaussian processes in reinforcement learning",
      "author" : [ "C.E. Rasmussen", "M. Kuss" ],
      "venue" : "NIPS, pages 751–759",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Local gaussian process regression for real time online model learning",
      "author" : [ "D. Nguyen-Tuong", "J. Peters", "M. Seeger" ],
      "venue" : "NIPS, pages 1193–1200",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Gaussian process dynamic programming",
      "author" : [ "M.P. Deisenroth", "C.E. Rasmussen", "J. Peters" ],
      "venue" : "Neurocomputing, 72(7):1508–1524",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Pilco: A model-based and data-efficient approach to policy search",
      "author" : [ "M.P. Deisenroth", "C.E. Rasmussen" ],
      "venue" : "ICML, pages 465–472",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Gaussian processes for data-efficient learning in robotics and control",
      "author" : [ "M.P. Deisenroth", "D. Fox", "C.E. Rasmussen" ],
      "venue" : "IEEE Transsactions on Pattern Analysis and Machine Intelligence, 27:75–90",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning uav stability and control derivatives using gaussian processes",
      "author" : [ "P. Hemakumara", "S. Sukkarieh" ],
      "venue" : "IEEE Transactions on Robotics, 29:813–824",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Propagation of uncertainty in bayesian kernel models-application to multiple-step ahead forecasting",
      "author" : [ "J. Quinonero Candela", "A. Girard", "J. Larsen", "C.E. Rasmussen" ],
      "venue" : "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Gaussian processes for machine learning",
      "author" : [ "C.K.I Williams", "C.E. Rasmussen" ],
      "venue" : "MIT Press",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Sparse on-line gaussian processes",
      "author" : [ "L. Csató", "M. Opper" ],
      "venue" : "Neural Computation, 14(3):641– 668",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Sparse gaussian processes using pseudo-inputs",
      "author" : [ "E. Snelson", "Z. Ghahramani" ],
      "venue" : "NIPS, pages 1257–1264",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Energy and passivity based control of the double inverted pendulum on a cart",
      "author" : [ "W. Zhong", "H. Rock" ],
      "venue" : "In International Conference on Control Applications,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2001
    }, {
      "title" : "Variational bayesian learning of nonlinear hidden state-space models for model predictive control",
      "author" : [ "T. Raiko", "M. Tornio" ],
      "venue" : "Neurocomputing, 72(16):3704–3712",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Insights in reinforcement learning",
      "author" : [ "H. van Hasselt" ],
      "venue" : "Hado van Hasselt,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Originally introduced in [1], DDP generates locally optimal feedforward and feedback control policies along with an optimal state trajectory.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "In the last decade, variations of DDP have been proposed in both control and machine learning communities [2][3][4][5][6].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "In the last decade, variations of DDP have been proposed in both control and machine learning communities [2][3][4][5][6].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "In the last decade, variations of DDP have been proposed in both control and machine learning communities [2][3][4][5][6].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "In the last decade, variations of DDP have been proposed in both control and machine learning communities [2][3][4][5][6].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "Recently, DDP was applied for high-dimensional policy search which achieved promising results in challenging control tasks [7].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : "Various parametric and semi-parametric approaches have been developed to address these issues, such as minimax DDP using Receptive Field Weighted Regression (RFWR) by Morimoto and Atkeson [8], and DDP using expert-demonstrated trajectories by Abbeel et al.",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "For instance, the works by Rasmussen et al.[10], Nguyen-Tuong et al.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "[10], Nguyen-Tuong et al.[11], Deisenroth et al.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 10,
      "context" : "[11], Deisenroth et al.[12][13][14] and Hemakumara et al.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "[12][13][14] and Hemakumara et al.[15] have demonstrated the remarkable applicability of GP-based control and RL methods in robotics.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "In particular, a recently proposed GP-based policy search framework called PILCO, developed by Deisenroth and Rasmussen [13] (an improved version has been developed by Deisenroth, Fox and Rasmussen [14]) has achieved unprecedented performances in terms of data-",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "In particular, a recently proposed GP-based policy search framework called PILCO, developed by Deisenroth and Rasmussen [13] (an improved version has been developed by Deisenroth, Fox and Rasmussen [14]) has achieved unprecedented performances in terms of data-",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 14,
      "context" : "However, the predictive distribution can be approximated by a Gaussian p(dxk) ∼ N (dμk,dΣk) [16].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "Thus the state distribution at k + 1 is also a Gaussian N (μk+1,Σk+1) [14] μk+1 = μk + dμk, Σk+1 = Σk + dΣk + COVf ,x̃k [xk,dxk] + COVf ,x̃k [dxk,xk].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "(4) Given an input joint distribution N (μ̃k, Σ̃k), we employ the moment matching approach [16][14] to compute the posterior GP.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "(4) Given an input joint distribution N (μ̃k, Σ̃k), we employ the moment matching approach [16][14] to compute the posterior GP.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "This optimization problem can be solved using numerical methods such as conjugate gradient [17].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "For numerical implementation, the dimension of the augmented state can be reduced by eliminating the redundancy of Σk and the principle square root of Σk may be used for numerical robustness [6].",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 1,
      "context" : "employ the line search strategy as in [2].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 12,
      "context" : "2) isO ( (N)(2)n(2)(n+m) ) [14], which is fixed during the iterative process of PDDP.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "[11][18][19], etc) may be used to reduce the computational cost of GP dynamics propagation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[11][18][19], etc) may be used to reduce the computational cost of GP dynamics propagation.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "[11][18][19], etc) may be used to reduce the computational cost of GP dynamics propagation.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 4,
      "context" : "First, PDDP shares some similarities with the belief space iLQG [6] framework, which approximates the belief dynamics using an extended Kalman filter.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "Second, PDDP is also comparable with iLQG-LD [5], which applies Locally Weighted Projection Regression (LWPR) to represent the dynamics.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "high computational cost of finite differences used to numerically compute the first-order expansions [2][6] and second-order expansions [4] of the underlying stochastic dynamics.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "high computational cost of finite differences used to numerically compute the first-order expansions [2][6] and second-order expansions [4] of the underlying stochastic dynamics.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "high computational cost of finite differences used to numerically compute the first-order expansions [2][6] and second-order expansions [4] of the underlying stochastic dynamics.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "PDDP Belief space iLQG[6] iLQG-LD[5] iLQG[2]/sDDP[4] State μk,Σk μk,Σk xk xk Dynamics model Unknown Known Unknown Known Linearization Analytic Jacobian Finite differences Analytic Jacobian Finite differences",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "PDDP Belief space iLQG[6] iLQG-LD[5] iLQG[2]/sDDP[4] State μk,Σk μk,Σk xk xk Dynamics model Unknown Known Unknown Known Linearization Analytic Jacobian Finite differences Analytic Jacobian Finite differences",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "PDDP Belief space iLQG[6] iLQG-LD[5] iLQG[2]/sDDP[4] State μk,Σk μk,Σk xk xk Dynamics model Unknown Known Unknown Known Linearization Analytic Jacobian Finite differences Analytic Jacobian Finite differences",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "PDDP Belief space iLQG[6] iLQG-LD[5] iLQG[2]/sDDP[4] State μk,Σk μk,Σk xk xk Dynamics model Unknown Known Unknown Known Linearization Analytic Jacobian Finite differences Analytic Jacobian Finite differences",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "We also compare the learning efficiency of PDDP with the classical DDP [1] and PILCO [13][14].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "We also compare the learning efficiency of PDDP with the classical DDP [1] and PILCO [13][14].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "We also compare the learning efficiency of PDDP with the classical DDP [1] and PILCO [13][14].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 18,
      "context" : "The CDIP swing-up problem has been solved by two controllers for swing-up and balancing, respectively [20].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "PILCO [14] is one of the few RL methods that is able to complete this task without knowing the dynamics.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "DDP: Originally introduced in the 70’s, the classical DDP [1] is still one of the most effective and efficient trajectory optimization approaches.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "PILCO: The recently proposed PILCO [14] framework has demonstrated state-of-the-art learning efficiency compared with other methods such as [21][22].",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : "PILCO: The recently proposed PILCO [14] framework has demonstrated state-of-the-art learning efficiency compared with other methods such as [21][22].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 20,
      "context" : "PILCO: The recently proposed PILCO [14] framework has demonstrated state-of-the-art learning efficiency compared with other methods such as [21][22].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 17,
      "context" : "Secondly, PDDP keeps a fixed size of training data for GP inferences, while PILCO adds new data to the training set after each trial (recently, the authors applied sparse GP approximation [19] in an improved version of PILCO when the data size reached a threshold).",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 12,
      "context" : "Thanks to the probabilistic representation of the dynamics, PDDP offers reasonable data-efficiency comparable to a state of the art GP-based policy search method [14].",
      "startOffset" : 162,
      "endOffset" : 166
    } ],
    "year" : 2014,
    "abstractText" : "We present a data-driven, probabilistic trajectory optimization framework for systems with unknown dynamics, called Probabilistic Differential Dynamic Programming (PDDP). PDDP takes into account uncertainty explicitly for dynamics models using Gaussian processes (GPs). Based on the second-order local approximation of the value function, PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces. Different from typical gradientbased policy search methods, PDDP does not require a policy parameterization and learns a locally optimal, time-varying control policy. We demonstrate the effectiveness and efficiency of the proposed algorithm using two nontrivial tasks. Compared with the classical DDP and a state-of-the-art GP-based policy search method, PDDP offers a superior combination of data-efficiency, learning speed, and applicability.",
    "creator" : null
  }
}