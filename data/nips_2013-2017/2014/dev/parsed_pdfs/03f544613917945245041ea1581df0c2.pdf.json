{
  "name" : "03f544613917945245041ea1581df0c2.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "QUIC & DIRTY: A Quadratic Approximation Approach for Dirty Statistical Models",
    "authors" : [ "Cho-Jui Hsieh", "Inderjit S. Dhillon", "Pradeep Ravikumar", "Peder A. Olsen" ],
    "emails" : [ "cjhsieh@cs.utexas.edu", "inderjit@cs.utexas.edu", "pradeepr@cs.utexas.edu", "stephen.becker@colorado.edu", "pederao@us.ibm.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "From the considerable amount of recent research on high-dimensional statistical estimation, it has now become well understood that it is vital to impose structural constraints upon the statistical model parameters for their statistically consistent estimation. These structural constraints take the form of sparsity, group-sparsity, and low-rank structure, among others; see [18] for unified statistical views of such structural constraints. In recent years, such “clean” structural constraints are frequently proving insufficient, and accordingly there has been a line of work on “superposition-structured” or “dirty model” constraints, where the model parameter is expressed as the sum of a number of parameter components, each of which have their own structure. For instance, [4, 6] consider the estimation of a matrix that is neither low-rank nor sparse, but which can be decomposed into the sum of a lowrank matrix and a sparse outlier matrix (this corresponds to robust PCA when the matrix-structured parameter corresponds to a covariance matrix). [5] use such matrix decomposition to estimate the structure of latent-variable Gaussian graphical models. [15] in turn use a superposition of sparse and group-sparse structure for multi-task learning. For other recent work on such superpositionstructured models, see [1, 7, 14]. For a unified statistical view of such superposition-structured models, and the resulting classes of M -estimators, please see [27].\nConsider a general superposition-structured parameter θ̄ := ∑k r=1 θ\n(r), where {θ(r)}kr=1 are the parameter-components, each with their own structure. Let {R(r)(·)}kr=1 be regularization functions suited to the respective parameter components, and let L(·) be a (typically non-linear) loss function\nthat measures the goodness of fit of the superposition-structured parameter θ̄ to the data. We now have the notation to consider a popular class of M -estimators studied in the papers above for these superposition-structured models:\nmin {θ(r)}kr=1\n{ L (∑\nr\nθ(r) ) + ∑ r λrR(r)(θ(r)) } := F (θ), (1)\nwhere {λr}kr=1 are regularization penalties. In (1), the overall regularization contribution is separable in the individual parameter components, but the loss function term itself is not, and depends on the sum θ̄ := ∑k r=1 θ\n(r). Throughout the paper, we use θ̄ to denote the overall superpositionstructured parameter, and θ=[θ(1),. . . ,θ(k)] to denote the concatenation of all the parameters.\nDue to the wide applicability of this class of M -estimators in (1), there has been a line of work on developing efficient optimization methods for solving special instances of this class ofM -estimators [14, 26], in addition to the papers listed above. In particular, due to the superposition-structure in (1) and the high-dimensionality of the problem, this class seems naturally amenable to a proximal gradient descent approach or the ADMM method [2, 17]; note that these are first-order methods and are thus very scalable.\nIn this paper, we consider instead a proximal Newton framework to minimize the M -estimation objective in (1). Specifically, we use iterative quadratic approximations, and for each of the quadratic subproblems, we use an alternating minimization approach to individually update each of the parameter components comprising the superposition-structure. Note that the Hessian of the loss might be structured, as for instance with the logdet loss for inverse covariance estimation and the logistic loss, which allows us to develop very efficient second-order methods. Even given this structure, solving the regularized quadratic problem in order to obtain the proximal Newton direction is too expensive due to the high dimensional setting. The key algorithmic contribution of this paper is in developing a general active subspace selection framework for general decomposable norms, which allows us to solve the proximal Newton steps over a significantly reduced search space. We are able to do so by leveraging the structural properties of decomposable regularization functions in the M -estimator in (1).\nOur other key contribution is theoretical. While recent works [16, 21] have analyzed the convergence of proximal Newton methods, the superposition-structure here poses a key caveat: since the loss function term only depends on the sum of the individual parameter components, the Hessian is not positive-definite, as is required in previous analyses of proximal Newton methods. The theoretical analysis [9] relaxes this assumption by instead assuming the loss is self-concordant but again allows at most one regularizer. Another key theoretical difficulty is our use of active subspace selection, where we do not solve for the vanilla proximal Newton direction, but solve the proximal Newton step subproblem only over a restricted subspace, which moreover varies with each step. We deal with these issues and show super-linear convergence of the algorithm when the sub-problems are solved exactly. We apply our algorithm to two real world applications: latent Gaussian Markov random field (GMRF) structure learning (with low-rank + sparse structure), and multitask learning (with sparse + group sparse structure), and demonstrate that our algorithm is more than ten times faster than state-of-the-art methods.\nOverall, our algorithmic and theoretical developments open up the state of the art but forbidding class of M -estimators in (1) to very large-scale problems.\nOutline of the paper. We begin by introducing some background in Section 2. In Section 3, we propose our quadratic approximation framework with active subspace selection for general dirty statistical models. We derive the convergence guarantees of our algorithm in Section 4. Finally, in Section 5, we apply our model to solve two real applications, and show experimental comparisons with other state-of-the-art methods."
    }, {
      "heading" : "2 Background and Applications",
      "text" : "Decomposable norms. We consider the case where all the regularizers {R(r)}kr=1 are decomposable norms ‖ · ‖Ar . A norm ‖ · ‖ is decomposable at x if there is a subspace T and a vector e ∈ T such that the sub differential at x has the following form:\n∂‖x‖r = {ρ ∈ Rn | ΠT (ρ) = e and ‖ΠT ⊥(ρ)‖∗Ar ≤ 1}, (2)\nwhere ΠT (·) is the orthogonal projection onto T , and ‖x‖∗ := sup‖a‖≤1〈x,a〉 is the dual norm of ‖ · ‖. The decomposable norm was defined in [3, 18], and many interesting regularizers belong to this category, including:\n• Sparse vectors: for the `1 regularizer, T is the span of all points with the same support as x. • Group sparse vectors: suppose that the index set can be partitioned into a set of NG disjoint groups, say G = {G1, . . . , GNG}, and define the (1,α)-group norm by ‖x‖1,α := ∑NG t=1 ‖xGt‖α. If SG denotes the subset of groups where xGt 6= 0, then the subgradient has the following form:\n∂‖x‖1,α := {ρ | ρ = ∑ t∈SG xGt/‖xGt‖∗α + ∑ t/∈SG mt},\nwhere ‖mt‖∗α ≤ 1 for all t /∈ SG. Therefore, the group sparse norm is also decomposable with T := {x | xGt = 0 for all t /∈ SG}. (3)\n• Low-rank matrices: for the nuclear norm regularizer ‖ · ‖∗, which is defined to be the sum of singular values, the subgradient can be written as\n∂‖X‖∗ = {UV T +W | UTW = 0,WV = 0, ‖W‖2 ≤ 1}, where ‖ · ‖2 is the matrix 2 norm and U, V are the left/right singular vectors of X corresponding to non-zero singular values. The above subgradient can also be written in the decomposable form (2), where T is defined to be span({uivTj }ki,j=1) where {ui}ki=1, {vi}ki=1 are the columns of U and V .\nApplications. Next we discuss some widely used applications of superposition-structured models, and the corresponding instances of the class of M -estimators in (1).\n• Gaussian graphical model with latent variables: let Θ denote the precision matrix with corresponding covariance matrix Σ = Θ−1. [5] showed that the precision matrix will have a low rank + sparse structure when some random variables are hidden, thus Θ = S − L can be estimated by solving the following regularized MLE problem:\nmin S,L:L 0,S−L 0 − log det(S − L) + 〈S − L,Σ〉+ λS‖S‖1 + λL trace(L). (4)\nWhile proximal Newton methods have recently become a dominant technique for solving the `1- regularized log-determinant problems [12, 10, 13, 19], our development is the first to apply proximal Newton methods to solve log-determinant problems with sparse and low rank regularizers.\n• Multi-task learning: given k tasks, each with sample matrix X(r) ∈ Rnr×d (nr samples in the r-th task) and labels y(r), [15] proposes minimizing the following objective:\nk∑ r=1 `(y(r), X(r)(S(r) +B(r))) + λS‖S‖1 + λB‖B‖1,∞, (5)\nwhere `(·) is the loss function and S(r) is the r-th column of S. • Noisy PCA: to recover a covariance matrix corrupted with sparse noise, a widely used technique is to solve the matrix decomposition problem [6]. In contrast to the squared loss above, an exponential PCA problem [8] would use a Bregman divergence for the loss function."
    }, {
      "heading" : "3 Our proposed framework",
      "text" : "To perform a Newton-like step, we iteratively form quadratic approximations of the smooth loss function. Generally the quadratic subproblem will have a large number of variables and will be hard to solve. Therefore we propose a general active subspace selection technique to reduce the problem size by exploiting the structure of the regularizersR1, . . . ,Rk."
    }, {
      "heading" : "3.1 Quadratic Approximation",
      "text" : "Given k sets of variables θ = [θ(1), . . . ,θ(k)], and each θ(r) ∈ Rn, let ∆(r) denote perturbation of θ(r), and ∆ = [∆(1), . . . ,∆(k)]. We define g(θ) := L( ∑k r=1 θ\n(r)) = L(θ̄) to be the loss function, and h(θ) := ∑k r=1R(r)(θ\n(r)) to be the regularization. Given the current estimate θ, we form the quadratic approximation of the smooth loss function:\nḡ(θ + ∆) = g(θ) + k∑ r=1 〈∆(r), G〉+ 1 2 ∆TH∆, (6)\nwhere G = ∇L(θ̄) is the gradient of L and H is the Hessian matrix of g(θ). Note that ∇θ̄L(θ̄) = ∇θ(r)L(θ̄) for all r so we simply write∇ and refer to the gradient at θ̄ as G (and similarly for∇2). By the chain rule, we can show that\nLemma 1. The Hessian matrix of g(θ) is\nH := ∇2g(θ) = H · · · H... . . . ... H · · · H  , H := ∇2L(θ̄). (7) In this paper we focus on the case where H is positive definite. When it is not, we add a small constant to the diagonal of H to ensure that each block is positive definite.\nNote that the full Hessian,H, will in general, not be positive definite (in fact rank(H) = rank(H)). However, based on its special structure, we can still give convergence guarantees (along with rate of convergence) for our algorithm. The Newton direction d is defined to be:\n[d(1), . . . ,d(k)] = argmin ∆(1),...,∆(k) ḡ(θ + ∆) + k∑ r=1 λr‖θ(r) + ∆(r)‖Ar := QH(∆;θ). (8)\nThe quadratic subproblem (8) cannot be directly separated into k parts because the Hessian matrix (7) is not a block-diagonal matrix. Also, each set of parameters has its own regularizer, so it is hard to solve them all together. Therefore, to solve (8), we propose a block coordinate descent method. At each iteration, we pick a variable set ∆(r) where r ∈ {1, 2, . . . , k} by a cyclic (or random) order, and update the parameter set ∆(r) while keeping other parameters fixed. Assume ∆ is the current solution (for all the variable sets), then the subproblem with respect to ∆(r) can be written as\n∆(r) ← argmin d∈Rn 1 2 dTHd+ 〈d, G+ ∑ t:r 6=t H∆(t)〉+ λr‖θ(r) + d‖Ar . (9)\nThe subproblem (9) is just a typical quadratic problem with a specific regularizer, so there already exist efficient algorithms for solving it for different choices of ‖ · ‖A. For the `1 norm regularizer, coordinate descent methods can be applied to solve (9) efficiently as used in [12, 21]; (accelerated) proximal gradient descent or projected Newton’s method can also be used, as shown in [19]. For a general atomic norm where there might be infinitely many atoms (coordinates), a greedy coordinate descent approach can be applied, as shown in [22].\nTo iterate between different groups of parameters, we have to maintain the term ∑k r=1H∆\n(r) during the Newton iteration. Directly computingH∆(r) requiresO(n2) flops; however, the Hessian matrix often has a special structure so that H∆(r) can be computed efficiently. For example, in the inverse covariance estimation problem H = Θ−1 ⊗ Θ−1 where Θ−1 is the current estimate of covariance, and in the empirical risk minimization problem H = XDXT where X is the data matrix and D is diagonal.\nAfter solving the subproblem (8), we have to search for a suitable stepsize. We apply an Armijo rule for line search [24], where we test the step size α = 20, 2−1, ... until the following sufficient decrease condition is satisfied for a pre-specified σ ∈ (0, 1) (typically σ = 10−4):\nF (θ + α∆) ≤ F (θ) + ασδ, δ = 〈G,∆〉+ k∑ r=1 λr‖Θr + α∆(r)‖Ar − k∑ r=1 λr‖θ(r)‖Ar . (10)"
    }, {
      "heading" : "3.2 Active Subspace Selection",
      "text" : "Since the quadratic subproblem (8) contains a large number of variables, directly applying the above quadratic approximation framework is not efficient. In this subsection, we provide a general active subspace selection technique, which dramatically reduces the size of variables by exploiting the structure of regularizers. A similar method has been discussed in [12] for the `1 norm and in [11] for the nuclear norm, but it has not been generalized to all decomposable norms. Furthermore, a key point to note is that in this paper our active subspace selection is not only a heuristic, but comes with strong convergence guarantees that we derive in Section 4.\nGiven the current θ, our subspace selection approach partitions each θ(r) into S(r)fixed and S (r) free = (S(r)fixed)⊥ and then restricts the search space of the Newton direction in (8) within S (r) free, which yields the following quadratic approximation problem:\n[d(1), . . . ,d(k)] = argmin ∆(1)∈S(1)free ,...,∆(k)∈S (k) free ḡ(θ + ∆) + k∑ r=1 λr‖θ(r) + ∆(r)‖Ar . (11)\nEach group of parameter has its own fixed/free subspace, so we now focus on a single parameter component θ(r). An ideal subspace selection procedure would satisfy:\nProperty (I). Given the current iterate θ, any updates along directions in the fixed set, for instance as θ(r) ← θ(r) + a, a ∈ S(r)fixed, does not improve the objective function value. Property (II). The subspace Sfree converges to the support of the final solution in a finite number of iterations.\nSuppose given the current iterate, we first do updates along directions in the fixed set, and then do updates along directions in the free set. Property (I) ensures that this is equivalent to ignoring updates along directions in the fixed set in this current iteration, and focusing on updates along the free set. As we will show in the next section, this property would suffice to ensure global convergence of our procedure. Property (II) will be used to derive the asymptotic quadratic convergence rate.\nWe will now discuss our active subspace selection strategy which will satisfy both properties above. Consider the parameter component θ(r), and its corresponding regularizer ‖ · ‖Ar . Based on the definition of decomposable norm in (2), there exists a subspace Tr where ΠTr (ρ) is a fixed vector for any subgradient of ‖ · ‖Ar . The following proposition explores some properties of the subdifferential of the overall objective F (θ) in (1). Proposition 1. Consider any unit-norm vector a, with ‖a‖Ar = 1, such that a ∈ T ⊥r .\n(a) The inner-product of the sub-differential ∂θ(r)F (θ) with a satisfies: 〈a, ∂θ(r)F (θ)〉 ∈ [〈a, G〉 − λr, 〈a, G〉+ λr]. (12)\n(b) Suppose |〈a, G〉| ≤ λr. Then, 0 ∈ argminσ F (θ + σa).\nSee Appendix 7.8 for the proof. Note that G = ∇L(θ̄) denotes the gradient of L. The proposition thus implies that if |〈a, G〉| ≤ λr and S(r)fixed ⊂ T ⊥r then Property (I) immediately follows. The difficulty is that the set {a | |〈a, G〉| ≤ λr} is possibly hard to characterize, and even if we could characterize this set, it may not be amenable enough for the optimization solvers to leverage in order to provide a speedup. Therefore, we propose an alternative characterization of the fixed subspace:\nDefinition 1. Let θ(r) be the current iterate, prox(r)λ be the proximal operator defined by\nprox(r)λ (x) = argmin y 1 2 ‖y − x‖2 + λ‖y‖Ar ,\nand Tr(x) be the subspace for the decomposable norm (2) ‖ · ‖Ar at point x. We can define the fixed/free subset at θ(r) as:\nS(r)fixed := [T (θ (r))]⊥ ∩ [T (prox(r)λr (G))] ⊥, S(r)free = S (r) fixed\n⊥ . (13)\nIt can be shown that from the definition of the proximal operator, and Definition 1, it holds that |〈a, G〉| < λr, so that we would have local optimality in the direction a as before. We have the following proposition:\nProposition 2. Let S(r)fixed be the fixed subspace defined in Definition 1. We then have:\n0 = argmin ∆(r)∈S(r)fixed QH([0, . . . ,0,∆(r),0, . . . ,0];θ).\nWe will prove that Sfree as defined above converges to the final support in Section 4, as required in Property (II) above. We will now detail some examples of the fixed/free subsets defined above.\n• For `1 regularization: Sfixed = span{ei | θi = 0 and |∇iL(θ̄)| ≤ λ} where ei is the ith canonical vector. • For nuclear norm regularization: the selection scheme can be written as\nSfree = {UAMV TA |M ∈ Rk×k}, (14) where UA = span(U,Ug), VA = span(V, Vg), with Θ = UΣV T is the thin SVD of Θ and Ug, Vg are the left and right singular vectors of proxλ(Θ−∇L(Θ)). The proximal operator proxλ(·) in this case corresponds to singular-value soft-thresholding, and can be computed by randomized SVD or the Lanczos algorithm.\n• For group sparse regularization: in the (1, 2)-group norm case, let SG be the nonzero groups, then the fixed groups FG can be defined by FG := {i | i /∈ SG and ‖∇LGi(θ̄)‖ ≤ λ}, and the free subspace will be Sfree = {θ | θi = 0 ∀i ∈ FG}. (15) In Figure 3 (in the appendix) that the active subspace selection can significantly improve the speed for the block coordinate descent algorithm [20]. Algorithm 1: QUIC & DIRTY: Quadratic Approximation Framework for Dirty Statistical Models\nInput : Loss function L(·), regularizers λr‖ · ‖Ar for r = 1, . . . , k, and initial iterate θ0. Output: Sequence {θt} such that {θ̄t} converges to θ̄\n?."
    }, {
      "heading" : "1 for t = 0, 1, . . . do",
      "text" : "2 Compute θ̄t ← ∑k r=1 θ (r) t . 3 Compute∇L(θ̄t). 4 Compute Sfree by (13). 5 for sweep = 1, . . . , Touter do 6 for r = 1, . . . , k do 7 Solve the subproblem (9) within S(r)free. 8 Update ∑k r=1∇2L(θ̄t)∆ (r).\n9 Find the step size α by (10). 10 θ(r) ← θ(r) + α∆(r) for all r = 1, . . . , k."
    }, {
      "heading" : "4 Convergence",
      "text" : "The recently developed theoretical analysis of proximal Newton methods [16, 21] cannot be directly applied because (1) we have the active subspace selection step, and (2) the Hessian matrix for each quadratic subproblem is not positive definite. We first prove the global convergence of our algorithm when the quadratic approximation subproblem (11) is solved exactly. Interestingly, in our proof we show that the active subspace selection can be modeled within the framework of the Block Coordinate Gradient Descent algorithm [24] with a carefully designed Hessian approximation, and by making this connection we are able to prove global convergence. Theorem 1. Suppose L(·) is convex (may not be strongly convex), and the quadratic subproblem (8) at each iteration is solved exactly, Algorithm 1 converges to the optimal solution.\nThe proof is in Appendix 7.1. Next we consider the case that L(θ̄) is strongly convex. Note that even when L(θ̄) is strongly convex with respect to θ̄, L( ∑k r=1 θ\n(r)) will not be strongly convex in θ (if k > 1) and there may exist more than one optimal solution. However, we show that all solutions give the same θ̄ := ∑k r=1 θ (r).\nLemma 2. Assume L(·) is strongly convex, and {x(r)}kr=1, {y(r)}kr=1 are two optimal solutions of (1), then ∑k r=1 x (r) = ∑k r=1 y (r).\nThe proof is in Appendix 7.2. Next, we show that S(r)free (from Definition 1) will converge to the final support T̄ (r) for each parameter set r = 1, . . . , k. Let θ̄? be the global minimizer (which is unique as shown in Lemma 2), and assume that we have\n‖Π(T̄ (r))⊥ ( ∇L(θ̄?) ) ‖∗Ar < λr ∀r = 1, . . . , k. (16)\nThis is the generalization of the assumption used in earlier literature [12] where only `1 regularization was considered. The condition is similar to strict complementary in linear programming. Theorem 2. If L(·) is strongly convex and assumption (16) holds, then there exists a finite T > 0 such that S(r)free = T̄ (r) ∀r = 1, . . . , k after t > T iterations.\nThe proof is in Appendix 7.3. Next we show that our algorithm has an asymptotic quadratic convergence rate (the proof is in Appendix 7.4). Theorem 3. Assume that∇2L(·) is Lipschitz continuous, and assumption (16) holds. If at each iteration the quadratic subproblem (8) is solved exactly, and L(·) is strongly convex, then our algorithm converges with asymptotic quadratic convergence rate."
    }, {
      "heading" : "5 Applications",
      "text" : "We demonstrate that our algorithm is extremely efficient for two applications: Gaussian Markov Random Fields (GMRF) with latent variables (with sparse + low rank structure) and multi-task learning problems (with sparse + group sparse structure)."
    }, {
      "heading" : "5.1 GMRF with Latent Variables",
      "text" : "We first apply our algorithm to solve the latent feature GMRF structure learning problem in eq (4), where S ∈ Rp×p is the sparse part, L ∈ Rp×p is the low-rank part, and we require L = LT 0, S = ST and Y = S − L 0 (i.e. θ(2) = −L). In this case, L(Y ) = − log det(Y ) + 〈Σ, Y 〉, hence\n∇2L(Y ) = Y −1 ⊗ Y −1, and ∇L(Y ) = Σ− Y −1. (17)\nActive Subspace. For the sparse part, the free subspace is a subset of indices {(i, j) | Sij 6= 0 or |∇ijL(Y )| ≥ λ}. For the low-rank part, the free subspace can be presented as {UAMV TA | M ∈ Rk×k} where UA and VA are defined in (14). Updating ∆L. To solve the quadratic subproblem (11), first we discuss how to update ∆L using subspace selection. The subproblem is\nmin ∆L=U∆DUT :L+∆L 0 1 2 trace(∆LY −1∆LY −1)+trace((Y −1−Σ−Y −1∆SY −1)∆L)+λL‖L+∆L‖∗,\nand since ∆L is constrained to be a perturbation of L = UAMUTA so that we can write ∆L = UA∆MUTA , and the subproblem becomes\nmin ∆M :M+∆M 0 1 2 trace(Ȳ∆M Ȳ∆M ) + trace(Σ̄∆M ) + λL trace(M + ∆M ) := q(∆M ), (18)\nwhere Ȳ := UTAY −1UA and Σ̄ := UTA (Y −1 − Σ − Y −1∆SY −1)UA. Therefore the subproblem (18) becomes a k × k dimensional problem where k p. To solve (18), we first check if the closed form solution exists. Note that ∇q(∆M ) = Ȳ∆M Ȳ + Σ̄ + λLI , thus the minimizer is ∆M = −Ȳ −1(Σ̄ + λLI)Ȳ −1 if M + ∆M 0. If not, we solve the subproblem by the projected gradient descent method, where each step only requires O(k2) time.\nUpdating ∆S . The subproblem with respect to ∆S can be written as\nmin ∆S 1 2 vec(∆S)T (Y −1⊗Y −1) vec(∆S)+trace((Σ−Y −1−Y −1(∆L)Y −1)∆S)+λS‖S+∆S‖1,\nIn our implementation we apply the same coordinate descent procedure proposed in QUIC [12] to solve this subproblem.\nResults. We compare our algorithm with two state-of-the-art software packages. The LogdetPPA algorithm was proposed in [26] and used in [5] to solve (4). The PGALM algorithm was proposed in [17]. We run our algorithm on three gene expression datasets: the ER dataset (p = 692), the Leukemia dataset (p = 1255), and a subset of the Rosetta dataset (p = 2000)1 For the parameters, we use λS = 0.5, λL = 50 for the ER and Leukemia datasets, which give us low-rank and sparse results. For the Rosetta dataset, we use the parameters suggested in LogdetPPA, with λS = 0.0313, λL = 0.1565. The results in Figure 1 shows that our algorithm is more than 10 times faster than other algorithms. Note that in the beginning PGALM tends to produce infeasible solutions (L or S −L is not positive definite), which is not plotted in the figures.\nOur proximal Newton framework has two algorithmic components: the quadratic approximation, and our active subspace selection. From Figure 1 we can observe that although our algorithm is a Newton-like method, the time cost for each iteration is similar or even cheaper than other first order methods. The reason is (1) we take advantage from active selection, and (2) the problem has a special structure of the Hessian (17), where computing it is no more expensive than the gradient. To delineate the contribution of the quadratic approximation to the gain in speed of convergence, we further compare our algorithm to an alternating minimization approach for solving (4), together with our active subspace selection. Such an alternating minimization approach would iteratively fix one of S,L, and update the other; we defer detailed algorithmic and implementation details to Appendix 7.6 for reasons of space. The results show that by using the quadratic approximation, we get a much faster convergence rate (see Figure 2 in Appendix 7.6).\n1The full dataset has p = 6316 but the other methods cannot solve this size problem."
    }, {
      "heading" : "5.2 Multiple-task learning with superposition-structured regularizers",
      "text" : "Next we solve the multi-task learning problem (5) where the parameter is a sparse matrix S ∈ Rd×k and a group sparse matrix B ∈ Rd×k. Instead of using the square loss (as in [15]), we consider the logistic loss `logistic(y, a) = log(1 + e−ya), which gives better performance as seen by comparing Table 1 to results in [15]. Here the Hessian matrix has a special structure again: H = XDXT where X is the data matrix and D is the diagonal matrix, and in Appendix 7.7 we have a detail description of how to applying our algorithm to solve this problem.\nResults. We follow [15] and transform multi-class problems into multi-task problems. For a multiclass dataset with k classes and n samples, for each r = 1, . . . , k, we generate yr ∈ {0, 1}n\nto be the vector such that y(k)i = 1 if and only if the i-th sample is in class r. Our first dataset is the USPS dataset which was first collected in [25] and subsequently widely used in multi-task papers. On this dataset, the use of several regularizers is crucial for good performance. For example, [15] demonstrates that on USPS, using lasso and group lasso regularizations together outperforms models with a single regularizer. However, they only consider the squared loss in their paper, whereas we consider a logistic loss which leads to better performance. For example, we get 7.47% error rate using 100 samples in USPS dataset, while using the squared loss the error rate is 10.8% [15]. Our second dataset is a larger document dataset RCV1 downloaded from LIBSVM Data, which has 53 classes and 47,236 features. We show that our algorithm is much faster than other algorithms on both datasets, especially on RCV1 where we are more than 20 times faster than proximal gradient descent. Here our subspace selection techniques works well because we expect that the active subspace at the true solution is small."
    }, {
      "heading" : "6 Acknowledgements",
      "text" : "This research was supported by NSF grants CCF-1320746 and CCF-1117055. C.-J.H also acknowledges support from an IBM PhD fellowship. P.R. acknowledges the support of ARO via W911NF12-1-0390 and NSF via IIS-1149803, IIS-1447574, and DMS-1264033. S.R.B. was supported by an IBM Research Goldstine Postdoctoral Fellowship while the work was performed."
    } ],
    "references" : [ {
      "title" : "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions",
      "author" : [ "A. Agarwal", "S. Negahban", "M.J. Wainwright" ],
      "venue" : "Annals if Statistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Simple bounds for recovering low-complexity models",
      "author" : [ "E. Candes", "B. Recht" ],
      "venue" : "Mathemetical Programming,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "E.J. Candes", "X. Li", "Y. Ma", "J. Wright" ],
      "venue" : "J. Assoc. Comput. Mach.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Latent variable graphical model selection via convex optimization",
      "author" : [ "V. Chandrasekaran", "P.A. Parrilo", "A.S. Willsky" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Rank-sparsity incoherence for matrix decomposition",
      "author" : [ "V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky" ],
      "venue" : "Siam J. Optim,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Low-rank matrix recovery from errors and erasures",
      "author" : [ "Y. Chen", "A. Jalali", "S. Sanghavi", "C. Caramanis" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "A generalization of principal component analysis to the exponential family",
      "author" : [ "M. Collins", "S. Dasgupta", "R.E. Schapire" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "An inexact proximal path-following algorithm for constrained convex minimization",
      "author" : [ "Q.T. Dinh", "A. Kyrillidis", "V. Cevher" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "A divide-and-conquer method for sparse inverse covariance estimation",
      "author" : [ "C.-J. Hsieh", "I.S. Dhillon", "P. Ravikumar", "A. Banerjee" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Nuclear norm minimization via active subspace selection",
      "author" : [ "C.-J. Hsieh", "P.A. Olsen" ],
      "venue" : "In ICML,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Sparse inverse covariance matrix estimation using quadratic approximation",
      "author" : [ "C.-J. Hsieh", "M.A. Sustik", "I.S. Dhillon", "P. Ravikumar" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "BIG & QUIC: Sparse inverse covariance estimation for a million variables",
      "author" : [ "C.-J. Hsieh", "M.A. Sustik", "I.S. Dhillon", "P. Ravikumar", "R.A. Poldrack" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Robust matrix decomposition with sparse corruptions",
      "author" : [ "D. Hsu", "S.M. Kakade", "T. Zhang" ],
      "venue" : "IEEE Trans. Inform. Theory,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "A dirty model for multi-task learning",
      "author" : [ "A. Jalali", "P. Ravikumar", "S. Sanghavi", "C. Ruan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Proximal Newton-type methods for convex optimization",
      "author" : [ "J.D. Lee", "Y. Sun", "M.A. Saunders" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Alternating direction methods for latent variable Gaussian graphical model selection",
      "author" : [ "S. Ma", "L. Xue", "H. Zou" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "S.N. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Newton-like methods for sparse inverse covariance estimation",
      "author" : [ "P. Olsen", "F. Oztoprak", "J. Nocedal", "S. Rennie" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Efficient block-coordinate descent algorithm for the group lasso",
      "author" : [ "Z. Qin", "K. Scheinberg", "D. Goldfarb" ],
      "venue" : "Mathematical Programming Computation,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Practical inexact proximal quasi-newton method with global complexity analysis",
      "author" : [ "K. Scheinberg", "X. Tang" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Greedy algorithms for structurally constrained high dimensional problems",
      "author" : [ "A. Tewari", "P. Ravikumar", "I. Dhillon" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "A block coordinate gradient descent method for regularized convex separable optimization and covariance selection",
      "author" : [ "K.-C. Toh", "P. Tseng", "S. Yun" ],
      "venue" : "Mathemetical Programming,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "A coordinate gradient descent method for nonsmooth separable minimization",
      "author" : [ "P. Tseng", "S. Yun" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2007
    }, {
      "title" : "Handwritten digit recognition by combined",
      "author" : [ "M. van Breukelen", "R.P.W. Duin", "D.M.J. Tax", "J.E. den Hartog" ],
      "venue" : "classifiers. Kybernetika,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1998
    }, {
      "title" : "Solving log-determinant optimization problems by a Newton-CG primal proximal point algorithm",
      "author" : [ "C. Wang", "D. Sun", "K.-C. Toh" ],
      "venue" : "SIAM J. Optimization,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "Dirty statistical models",
      "author" : [ "E. Yang", "P. Ravikumar" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Constant nullspace strong convexity and fast convergence of proximal methods under high-dimensional settings",
      "author" : [ "E.-H. Yen", "C.-J. Hsieh", "P. Ravikumar", "I.S. Dhillon" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "An improved GLMNET for L1-regularized logistic regression",
      "author" : [ "G.-X. Yuan", "C.-H. Ho", "C.-J. Lin" ],
      "venue" : "JMLR, 13:1999–2030,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "These structural constraints take the form of sparsity, group-sparsity, and low-rank structure, among others; see [18] for unified statistical views of such structural constraints.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "For instance, [4, 6] consider the estimation of a matrix that is neither low-rank nor sparse, but which can be decomposed into the sum of a lowrank matrix and a sparse outlier matrix (this corresponds to robust PCA when the matrix-structured parameter corresponds to a covariance matrix).",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "For instance, [4, 6] consider the estimation of a matrix that is neither low-rank nor sparse, but which can be decomposed into the sum of a lowrank matrix and a sparse outlier matrix (this corresponds to robust PCA when the matrix-structured parameter corresponds to a covariance matrix).",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "[5] use such matrix decomposition to estimate the structure of latent-variable Gaussian graphical models.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "[15] in turn use a superposition of sparse and group-sparse structure for multi-task learning.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "For other recent work on such superpositionstructured models, see [1, 7, 14].",
      "startOffset" : 66,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "For other recent work on such superpositionstructured models, see [1, 7, 14].",
      "startOffset" : 66,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "For other recent work on such superpositionstructured models, see [1, 7, 14].",
      "startOffset" : 66,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "For a unified statistical view of such superposition-structured models, and the resulting classes of M -estimators, please see [27].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 13,
      "context" : "Due to the wide applicability of this class of M -estimators in (1), there has been a line of work on developing efficient optimization methods for solving special instances of this class ofM -estimators [14, 26], in addition to the papers listed above.",
      "startOffset" : 204,
      "endOffset" : 212
    }, {
      "referenceID" : 25,
      "context" : "Due to the wide applicability of this class of M -estimators in (1), there has been a line of work on developing efficient optimization methods for solving special instances of this class ofM -estimators [14, 26], in addition to the papers listed above.",
      "startOffset" : 204,
      "endOffset" : 212
    }, {
      "referenceID" : 1,
      "context" : "In particular, due to the superposition-structure in (1) and the high-dimensionality of the problem, this class seems naturally amenable to a proximal gradient descent approach or the ADMM method [2, 17]; note that these are first-order methods and are thus very scalable.",
      "startOffset" : 196,
      "endOffset" : 203
    }, {
      "referenceID" : 16,
      "context" : "In particular, due to the superposition-structure in (1) and the high-dimensionality of the problem, this class seems naturally amenable to a proximal gradient descent approach or the ADMM method [2, 17]; note that these are first-order methods and are thus very scalable.",
      "startOffset" : 196,
      "endOffset" : 203
    }, {
      "referenceID" : 15,
      "context" : "While recent works [16, 21] have analyzed the convergence of proximal Newton methods, the superposition-structure here poses a key caveat: since the loss function term only depends on the sum of the individual parameter components, the Hessian is not positive-definite, as is required in previous analyses of proximal Newton methods.",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 20,
      "context" : "While recent works [16, 21] have analyzed the convergence of proximal Newton methods, the superposition-structure here poses a key caveat: since the loss function term only depends on the sum of the individual parameter components, the Hessian is not positive-definite, as is required in previous analyses of proximal Newton methods.",
      "startOffset" : 19,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "The theoretical analysis [9] relaxes this assumption by instead assuming the loss is self-concordant but again allows at most one regularizer.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "The decomposable norm was defined in [3, 18], and many interesting regularizers belong to this category, including:",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "The decomposable norm was defined in [3, 18], and many interesting regularizers belong to this category, including:",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "[5] showed that the precision matrix will have a low rank + sparse structure when some random variables are hidden, thus Θ = S − L can be estimated by solving the following regularized MLE problem: min S,L:L 0,S−L 0 − log det(S − L) + 〈S − L,Σ〉+ λS‖S‖1 + λL trace(L).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "While proximal Newton methods have recently become a dominant technique for solving the `1regularized log-determinant problems [12, 10, 13, 19], our development is the first to apply proximal Newton methods to solve log-determinant problems with sparse and low rank regularizers.",
      "startOffset" : 127,
      "endOffset" : 143
    }, {
      "referenceID" : 9,
      "context" : "While proximal Newton methods have recently become a dominant technique for solving the `1regularized log-determinant problems [12, 10, 13, 19], our development is the first to apply proximal Newton methods to solve log-determinant problems with sparse and low rank regularizers.",
      "startOffset" : 127,
      "endOffset" : 143
    }, {
      "referenceID" : 12,
      "context" : "While proximal Newton methods have recently become a dominant technique for solving the `1regularized log-determinant problems [12, 10, 13, 19], our development is the first to apply proximal Newton methods to solve log-determinant problems with sparse and low rank regularizers.",
      "startOffset" : 127,
      "endOffset" : 143
    }, {
      "referenceID" : 18,
      "context" : "While proximal Newton methods have recently become a dominant technique for solving the `1regularized log-determinant problems [12, 10, 13, 19], our development is the first to apply proximal Newton methods to solve log-determinant problems with sparse and low rank regularizers.",
      "startOffset" : 127,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "• Multi-task learning: given k tasks, each with sample matrix X ∈ Rnr×d (nr samples in the r-th task) and labels y, [15] proposes minimizing the following objective:",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "• Noisy PCA: to recover a covariance matrix corrupted with sparse noise, a widely used technique is to solve the matrix decomposition problem [6].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 7,
      "context" : "In contrast to the squared loss above, an exponential PCA problem [8] would use a Bregman divergence for the loss function.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "For the `1 norm regularizer, coordinate descent methods can be applied to solve (9) efficiently as used in [12, 21]; (accelerated) proximal gradient descent or projected Newton’s method can also be used, as shown in [19].",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "For the `1 norm regularizer, coordinate descent methods can be applied to solve (9) efficiently as used in [12, 21]; (accelerated) proximal gradient descent or projected Newton’s method can also be used, as shown in [19].",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 18,
      "context" : "For the `1 norm regularizer, coordinate descent methods can be applied to solve (9) efficiently as used in [12, 21]; (accelerated) proximal gradient descent or projected Newton’s method can also be used, as shown in [19].",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 21,
      "context" : "For a general atomic norm where there might be infinitely many atoms (coordinates), a greedy coordinate descent approach can be applied, as shown in [22].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 23,
      "context" : "We apply an Armijo rule for line search [24], where we test the step size α = 2, 2−1, .",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 11,
      "context" : "A similar method has been discussed in [12] for the `1 norm and in [11] for the nuclear norm, but it has not been generalized to all decomposable norms.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "A similar method has been discussed in [12] for the `1 norm and in [11] for the nuclear norm, but it has not been generalized to all decomposable norms.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : "(15) In Figure 3 (in the appendix) that the active subspace selection can significantly improve the speed for the block coordinate descent algorithm [20].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 15,
      "context" : "The recently developed theoretical analysis of proximal Newton methods [16, 21] cannot be directly applied because (1) we have the active subspace selection step, and (2) the Hessian matrix for each quadratic subproblem is not positive definite.",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 20,
      "context" : "The recently developed theoretical analysis of proximal Newton methods [16, 21] cannot be directly applied because (1) we have the active subspace selection step, and (2) the Hessian matrix for each quadratic subproblem is not positive definite.",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "Interestingly, in our proof we show that the active subspace selection can be modeled within the framework of the Block Coordinate Gradient Descent algorithm [24] with a carefully designed Hessian approximation, and by making this connection we are able to prove global convergence.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "This is the generalization of the assumption used in earlier literature [12] where only `1 regularization was considered.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "min ∆S 1 2 vec(∆S) (Y −1⊗Y −1) vec(∆S)+trace((Σ−Y −1−Y (∆L)Y )∆S)+λS‖S+∆S‖1, In our implementation we apply the same coordinate descent procedure proposed in QUIC [12] to solve this subproblem.",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 25,
      "context" : "The LogdetPPA algorithm was proposed in [26] and used in [5] to solve (4).",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "The LogdetPPA algorithm was proposed in [26] and used in [5] to solve (4).",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : "The PGALM algorithm was proposed in [17].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "Instead of using the square loss (as in [15]), we consider the logistic loss `logistic(y, a) = log(1 + e−ya), which gives better performance as seen by comparing Table 1 to results in [15].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 14,
      "context" : "Instead of using the square loss (as in [15]), we consider the logistic loss `logistic(y, a) = log(1 + e−ya), which gives better performance as seen by comparing Table 1 to results in [15].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 14,
      "context" : "We follow [15] and transform multi-class problems into multi-task problems.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "Our first dataset is the USPS dataset which was first collected in [25] and subsequently widely used in multi-task papers.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 14,
      "context" : "For example, [15] demonstrates that on USPS, using lasso and group lasso regularizations together outperforms models with a single regularizer.",
      "startOffset" : 13,
      "endOffset" : 17
    } ],
    "year" : 2014,
    "abstractText" : "In this paper, we develop a family of algorithms for optimizing “superpositionstructured” or “dirty” statistical estimators for high-dimensional problems involving the minimization of the sum of a smooth loss function with a hybrid regularization. Most of the current approaches are first-order methods, including proximal gradient or Alternating Direction Method of Multipliers (ADMM). We propose a new family of second-order methods where we approximate the loss function using quadratic approximation. The superposition structured regularizer then leads to a subproblem that can be efficiently solved by alternating minimization. We propose a general active subspace selection approach to speed up the solver by utilizing the low-dimensional structure given by the regularizers, and provide convergence guarantees for our algorithm. Empirically, we show that our approach is more than 10 times faster than state-of-the-art first-order approaches for the latent variable graphical model selection problems and multi-task learning problems when there is more than one regularizer. For these problems, our approach appears to be the first algorithm that can extend active subspace ideas to multiple regularizers.",
    "creator" : null
  }
}