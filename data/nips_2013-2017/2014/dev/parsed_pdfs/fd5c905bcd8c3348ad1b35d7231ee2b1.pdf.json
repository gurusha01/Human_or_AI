{
  "name" : "fd5c905bcd8c3348ad1b35d7231ee2b1.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Analysis of Brain States from Multi-Region LFP Time-Series",
    "authors" : [ "Kyle Ulrich", "David E. Carlson", "Wenzhao Lian", "Jana Schaich Borg", "Kafui Dzirasa", "Lawrence Carin" ],
    "emails" : [ "lcarin}@duke.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The local field potential (LFP) is a source of information about the broad patterns of brain activity, and the frequencies present in these time-series measurements are often highly correlated between regions. It is believed that these regions may jointly constitute a “brain state,” relating to cognition and behavior. An infinite hidden Markov model (iHMM) is proposed to model the evolution of brain states, based on electrophysiological LFP data measured at multiple brain regions. A brain state influences the spectral content of each region in the measured LFP. A new state-dependent tensor factorization is employed across brain regions, and the spectral properties of the LFPs are characterized in terms of Gaussian processes (GPs). The LFPs are modeled as a mixture of GPs, with state- and regiondependent mixture weights, and with the spectral content of the data encoded in GP spectral mixture covariance kernels. The model is able to estimate the number of brain states and the number of mixture components in the mixture of GPs. A new variational Bayesian split-merge algorithm is employed for inference. The model infers state changes as a function of external covariates in two novel electrophysiological datasets, using LFP data recorded simultaneously from multiple brain regions in mice; the results are validated and interpreted by subject-matter experts."
    }, {
      "heading" : "1 Introduction",
      "text" : "Neuroscience has made significant progress in learning how activity in specific neurons or brain areas correlates with behavior. One of the remaining mysteries is how to best represent and understand the way whole-brain activity relates to cognition: in other words, how to describe brain states [1]. Although different brain regions have different functions, neural activity across brain regions is often highly correlated. It has been proposed that the specific way brain regions are correlated at any given time may represent a “state” designed specifically to optimize neural computations relevant to the behavioral context an organism is in [2]. Unfortunately, although there is great interest in the concept of global brain states, little progress has been made towards developing methods to identify or characterize them.\nThe study of arousal is an important area of research relating to brain states. Arousal is a hotly debated topic that generally refers to the way the brain dynamically responds to varying levels of stimulation [3]. One continuum of arousal used in the neuroscience literature is sleep (low arousal) to wakefulness (higher arousal). Another is calm (low arousal) to excited or stressed (high arousal) [4]. A common electrophysiological measurement used to determine arousal levels is local field potentials (LFPs), or low-frequency (< 200 Hz) extracellular neural oscillations that represent coordinated\n0.5 1 1.5 2\n−2\n−1\n0\n1\n2\nN o rm\na li ze d P o te n ti a l\nSeconds\nExample LFP Data\ny1 y2\nF⇐⇒ Classify yw\n4.2 8.3 12.5 16.7 0\n0.02\n0.04\n0.06\n0.08\nFrequency, Hz\nSpectral Density\nREM\nSWS WK\nneural activity across distributed spatial and temporal scales. LFPs are useful for describing overall brain states since they reflect activity across many neural networks. We examine brain states under different levels of arousal by recording LFPs simultaneously in multiple regions of the mouse brain, first, as mice pass through different stages of sleep, and second, as mice are moved from a familiar environment to a novel environment to induce interest and exploration.\nIn neuroscience, the analysis of electrophysiological time-series data is largely centered around dynamic causal modeling (DCM) [5], where continuous state-space models are formulated based on differential equations that are specifically crafted around knowledge of underlying neurobiological processes. However, DCM is not suitable for exploratory analysis of data, such as inferring unknown arousal levels, for two reasons: because the differential equations are driven by inputs of experimental conditions, and the analysis is dependent on a priori hypotheses about which neuronal populations and interactions are important. This work focuses on methods suitable for exploratory analysis.\nPreviously published neuroscience studies distinguished between slow-wave sleep (SWS), rapideye-movement (REM), and wake (WK) using proportions of high-frequency (33-55 Hz) gamma oscillations and lower frequency theta (4-9 Hz) oscillations in a brain area called the Hippocampus [6, 7]. As an alternative approach, recent statistical methods for tensor factorization [8] can be applied to short time Fourier transform (STFT) coefficients by factorizing a 3–way LFP tensor, with dimensions of brain region, frequency band and time. Distinct sleep states may then be revealed by clustering the inferred sequence of time-varying score vectors.\nAlthough good first steps, the above two methods have several shortcomings: 1) They do not consider the time dependency of brain activity, and therefore cannot capture state-transition properties. 2) They cannot directly work on raw data, but require preprocessing that only considers spectral content in predefined frequency bins, thus leading to information loss. 3) They do not allow for individual brain regions to take on their own set of sub-state characteristics within a given global brain state. 4) Finally, they cannot leverage the shared information of LFP data across multiple animals.\nIn this paper we overcome the shortcomings of previously published brain-state methods by defining a sequence of brain states over a sliding window of raw, filtered LFP data, where we impose an infinite hidden Markov model (iHMM) [9] on these state assignments. Conditioned on this brain state, each brain region is assigned to a cluster in a mixture model. Each cluster is associated with a specific spectral content (or density) pattern, manifested through a spectral mixture kernel [10] of a Gaussian process. Each window of LFP data is generated as a draw from this mixture of Gaussian processes. Thus, all animals share an underlying brain state space, of which, all brain regions share the underlying components of the mixture model."
    }, {
      "heading" : "2 Model",
      "text" : "For each animal a ∈ {1, . . . A}, we have time-series of the LFP in R different regions, measured simultaneously. These time-series are split into sequential, sliding windows, y(ar)w ∈ RN for w ∈ {1, . . . ,W}, such that windows are common across regions. These windows are chosen to be overlapping, thereby sharing data points between consecutive windows; nonoverlapping win-\ndows may also be used. Each window is considered as a single observation vector, and we wish to model the generative process of these observations, {y(ar)w }. The proposed model aims to describe the spectral content in each of these LFP signals, as a function of brain region and time. This is done by first assigning a joint “brain state” to each time window, {s(a)1 , . . . , s (a) W }, shared across all brain regions {1, . . . , R}. The brain state is assumed to evolve in time as a latent Markov process. The LFP data from a particular brain region is assumed drawn from a mixture of Gaussian processes. The characteristics of each mixture component are shared across brain states and brain regions, with mixture weights that are dependent on these two entities."
    }, {
      "heading" : "2.1 Brain state assignment",
      "text" : "Within the generative process, each animal has a latent brain state for every time window, w. This brain state is represented through a categorical latent variable s(a)w , and an infinite hidden Markov model (iHMM) is placed on the state dynamics [9, 12]. This process is formulated as\ns(a)w ∼ Categorical(λ (a)\ns (a) w−1\n), λ(a)g ∼ DP(α0β), β ∼ GEM(γ0), (1)\nwhere GEM is the stick-breaking process βh = β′h ∏h−1 i=1 (1 − β′i) with β′h ∼ Beta(1, γ0). Here, {βh}Hh=1 represents global transition probabilities to each state in a potentially infinite state space. For the stick-breaking process, H → ∞, but in a finite collection of data only a finite number of state transitions will be used and H can be efficiently truncated. Since the state space is shared across animals, we cannot predefine initial state assignments, s(a)1 . To remedy this, we allow s (a) 1 ∼ Categorical(ψ(a)) and place a discrete uniform prior on ψ(a) over the truncated state space.\nEach animal is given a transition matrix Λ(a), where each row of this matrix is a transition probability vector λ(a)g such that the transition from state g to state h for animal a is λ (a) gh , each centered around the global transition vector β. Because each animal’s brain can be structured differently (e.g., as an extreme case, consider a central nervous system disorder), we allow Λ(a) to vary from animal to animal."
    }, {
      "heading" : "2.2 Assigning brain regions to clusters",
      "text" : "For each brain state, mixture weights are drawn to define the distribution over clusters independently for each region r, centered around a global mixture η using a hierarchical Dirichlet Process [12]:\nφ (r) h ∼ DP(α1η), η ∼ GEM(γ1), (2)\nwhere φ(r)h` is the probability of assigning region r of a window with brain state h to cluster `. This cluster assignment can be written as\nz(ar)w |s(a)w ∼ Categorical(φ (r)\ns (a) w\n). (3)\nFor each cluster ` there is a set of parameters, θ`, describing a Gaussian process (GP), detailed in Section 2.3. One could consider the joint probability over cluster assignments for all brain regions as an extension of a latent nonnegative PARAFAC tensor decomposition [11, 13]. We refer to the Supplemental Material for details. Our clustering model differs from the infinite tensor factorization (ITF) model of [11] in three significant ways: we place Markovian dynamics on state assignments for each animal, we model separate draws from the prior jointly for each animal, and we share cluster atoms across all regions through use of an HDP."
    }, {
      "heading" : "2.3 Infinite mixture of Gaussian processes",
      "text" : ""
    }, {
      "heading" : "2.3.1 Gaussian processes and the spectral mixture kernel",
      "text" : "For a single window of data, y(ar)w ∈ RN , we wish to model the data in the limit of a continuoustime function (allowingN →∞), motivating a GP formulation, and we are interested in the spectral properties of the LFP signal in this window. Previous research has established a link between the kernel function of a GP and its spectral properties [10]. We write a distribution over the time-series:\ny(t) ∼ GP(m(t), k(t, t′)), (4)\nwhere m(t) is known as the mean function, and k(t, t′) is the covariance function [14]. This framework provides a flexible, structured method to model time-series data. The structure of observations in the output space, y, is defined through a careful choice of the covariance function. Since this work aims to model the spectral content of the LFP signal, we set the mean function to 0, and use a recently proposed spectral mixture (SM) kernel [10]. This kernel is defined through a spectral domain representation, S(s), of the stationary kernel, represented by a mixture of Q Gaussian components:\nφ(s) = ∑Q\nq=1 ωqN (s;µq, νq), S(s) =\n1 2 [φ(s) + φ(−s)], (5)\nwhere φ(s) is reflected about the origin to obtain a valid spectral density, and µq , νq , and ωq respectively define the mean, variance, and relative weight of the q-th Gaussian component in the spectral domain. Priors may be placed on these parameters; for example, we use the uninformative priors µq ∼ Uniform(µmin, µmax), νq ∼ Uniform(0, νmax) and ωq ∼ Gamma(e0, f0). A bandpass filter is applied to the LFP signal from µmin to µmax Hz as a preprocessing step, so this prior knowledge is justified. Also, νmax is set to prevent overfitting, and e0 and f0 are set to manifest a broad prior.\nWe assume that only a noisy version of the true function is observed, so the kernel is defined as the Fourier transform of the spectral density S(s) plus white Gaussian noise:\nf(τ ;θ) = ∑Q\nq=1 ωq exp{−2π2τ2νq} cos(2πτµq), k(τ ;θ, γ) = f(τ ;θ) + γ−1δτ , (6)\nwhere the set of parameters θ = {ω,µ,ν} and γ define the covariance kernel, τ = |t− t′|, and δτ is the Kronecker delta function which equals one if τ = 0. We set the prior γ ∼ Gamma(e1, f1) where the hyperparemeters e1 and f1 are chosen to manifest a broad prior. The formulation of (6) results in an interpretable kernel in the spectral domain, where the weights ωq correspond to the relative contribution of each component, the means µq represent spectral peaks, and the variances νq play a role similar to an inverse length-scale.\nThrough a realization of this Gaussian process, an analytical representation is obtained for the marginal likelihood of the observed data y given the parameters {θ, γ}, and the observation locations t, p(y|θ, γ, t). The optimal set of kernel parameters {θ, γ} can then be chosen as the set that maximizes the marginal likelihood. Further discussions on the inference for the Gaussian process parameters is presented in Section 3."
    }, {
      "heading" : "2.3.2 Generating observed data",
      "text" : "To combine the clustering model with our SM kernel, each cluster ` is associated with a distinct set of kernel parameters θ`. To generate the observations {y(ar)w }, where each y(ar)w ∈ RN has observation times t = {t1, . . . , tN} such that |ti − tj | = |i− j|τ for all i and j, we consider a draw from the multivariate normal distribution:\ny(ar)w ∼ N (0,Σz(ar)w ), (Σ`)ij = k(|ti − tj |;θ`, γ), (7)\nwhere each observation is generated from the cluster indicated by z(ar)w (described in Section 2.2), and each cluster is represented uniquely by a covariance matrix, Σ`, whose elements are defined through the covariance kernel k(τ ;θ`, γ). Therefore, the parameters θz(ar)w describe the autocorrelation content associated with each y(ar)w .\nWe address two concerns with this formulation. First, this observation model ignores complex cross-covariance functions between regions. Although LFP measurements exhibit coherence patterns across regions, the generative model in (7) only weakly couples the spectral densities of each region through the brain state. In principle, the generative model could be extended to incorporate this coherence information. Second, (7) does not model the time-series itself as a stochastic process, but rather the preprocessed, ‘independent’ observation vectors. This shortcoming is not ideal, but the windowing process allows for efficient computation via the mixture of Gaussian processes."
    }, {
      "heading" : "3 Inference",
      "text" : "In the following, latent model variables are represented by Ω = {Z,S,Φ,η,Λ,β,Ψ}, the kernel parameters to be optimized are Θ = {{θ`}L1 , γ}, and H and L are upper limit truncations on the number of brain states and clusters, respectively. As described throughout this section, the proposed algorithm adaptively adjusts the truncation levels on the number of brain states, H , and clusters, L,\nthrough a series of split-merge moves. The joint probability of the proposed model is p(Y ,Ω,Θ) = p(Y |Z,Θ)p(Z,S|Φ,Λ,Ψ)p(Φ|η)p(η)p(Λ|β)p(β)p(Ψ)p(Θ)\n= [∏\na,r,w p(y(ar)w |z(ar)w ,Θ)p(z(ar)w |s(a)w ,Φ)\n][ p(η|γ1) ∏ r,h p(φ (r) h |η, α1) ] [∏\na p(s\n(a) 1 |ψ(a))p(ψ(a)) ∏W w=2 p(s(a)w |s (a) w−1,Λ (a)) ][ p(β|γ0) ∏ a,g p(λ(a)g |β, α0) ]\n[ p(γ|e1, f1) ∏Q q=1 p(ωq|e0, f0)p(µq|µmin, µmax)p(νq|νmax) ] . (8)\nA variational inference scheme is developed to update Ω and Θ."
    }, {
      "heading" : "3.1 Variational inference",
      "text" : "With variational inference, an approximate variational posterior distribution is sought that is similar to the true posterior distribution, q(Ω,Θ) ≈ p(Ω,Θ|Y ). This variational posterior is assumed to have a factorization into simpler distributions, where q(Ω,Θ) = q(Z)q(S)q(Φ)q(η)q(Λ)q(β)q(Ψ)q(Θ), with further factorization\nq(Z) = ∏\na,r,w Cat(z(ar)w ; ζ (ar) w ), q(Φ) = ∏ h,r Dir(φ(r)h ;ν (r) h ), q(η) = δη∗(η),\nq(S) = ∏\na q({s(a)w }Ww=1), q(Λ) = ∏ g,a Dir(λ(a)g ;κ (a) g ), q(β) = δβ∗(β),\nq(Ψ) = ∏\na δψ(a)∗(ψ\n(a)), q(Θ) = ∏\nj δΘ∗j (Θj), (9)\nwhere only necessary sufficient statistics of the latent factors q({s(a)w }Ww=1) are required, and the approximate posteriors of η, β, {ψ(a)} and {Θj} are represented by point estimates at η∗, β∗, {ψ(a)∗} and {Θ∗j}, respectively.\nThe degenerate distributions δη∗(η) and δβ∗(β) are described in previous work on variational inference for HDPs [15, 16]. The idea is that the point estimates of the stick-breaking processes simplify the derivation of the variational posterior, and the authors of [16] show that obtaining a full posterior distribution on the stick-breaking weights has little impact on model fitting since the variational lower bound is not heavily influenced by the terms dependent on η and β. Furthermore, the Dirichlet process is truncated for both the number of states and the number of clusters such that q(z(ar)w = `) = 0 for ` > L and q(s (a) w = h) = 0 for h > H . This truncation method (see [17] for details) is notably different than other common truncation methods of the DP (e.g., [18] and [19]), and is primarily important for facilitating the split-merge inference techniques described in Section 3.2.\nIn mean-field variational inference, the variational distribution q(Ω,Θ) is chosen such that the Kullback-Leibler divergence of p(Ω,Θ|Y ) from q(Ω,Θ), DKL(q(Ω,Θ)||p(Ω,Θ|Y )), is minimized. This is equivalent to maximizing the evidence lower bound (also known as the variational free energy in the DCM literature), L(q) = Eq[log p(Y ,Ω,Θ)] − Eq[log q(Ω,Θ)], where both expectations are taken with respect to the variational distribution. The resulting lower bound is L(q) = E[ln p(Y |Z,Θ)] + E[ln p(Z,S|Φ,Λ,Ψ)] + E[ln p(Φ|η)] + E[ln p(η)] + E[ln p(Λ|β)] + E[ln p(β)] + E[ln p(Ψ)] + E[ln p(Θ)] +H[q(Z)] +H[q(S)] +H[q(Φ)] +H[q(Λ)], (10)\nwhere all expectations are with respect to the variational distribution, the hyperparameters are excluded for notational simplicity, and we define H[q(·)] as the sum over the entropies of the individual factors of q(·). Due to the degenerate approximations for q(η), q(β), q(Ψ) and q(Θ), these full posterior distributions are not obtained, and, therefore, the terms H[q(η)], H[q(β)], H[q(Ψ)] and H[q(Θ)] are set to zero in the lower bound.\nThe updates for ζ(ar)w and ν (r) h are standard. Variational inference for the HDP-HMM is detailed in other work (e.g., see [20, 21]); using these methods, updates for κ(a)g , ψ(a) and the necessary expected sufficient statistics of the factors of q(S) are realized. Finally, updates for β∗, η∗ and {Θj} are non-conjugate, so a gradient-ascent method is performed to optimize these values. We use a simple resilient back-propagation (Rprop), though most line-search methods should suffice. Details on all updates and taking the gradient of L(q) with respect to β, η and {Θj} are found in the Supplemental Material."
    }, {
      "heading" : "3.2 Split-merge moves",
      "text" : "During inference, a series of split and merge operations are used to help the algorithm jump out of local optima [22]. This work takes the viewpoint that two clusters (or states) should merge only if the variational lower bound increases, and, when a split is proposed for a cluster (or state), it should always be accepted, whether or not the split increases the variational lower bound. If the split is not appropriate, a future merge step is expected to undo this operation. In this way, the opportunity is provided for cluster and state assignments to jump out of local optima, allowing the inference algorithm to readjust assignments as desired.\nMerge states: To merge states h′ and h′′ into a new state h, new parameters are initialized as: ρ (a) wh = ρ (a) wh′ + ρ (a) wh′′ , κ (a) gh = κ (a) gh′ + κ (a) gh′′ , β ∗ h = β ∗ h′ + β ∗ h′′ , and v (a) h = v (a) h′ + v (a) h′′ , such that the model now has a truncation at Hnew = H − 1 states. In order to account for problems with merging two states in an HMM, a single restricted iteration is allowed, where only the statedependent variational parameters in Ωnew are updated, producing a new distribution q(Ωnew). The merge is accepted (i.e., Ω = Ωnew) if L(q(Ωnew)) > L(q(Ω)). Since these computations are not excessive, all possible state merges are computed and a small number of merges are accepted per iteration.\nMerge clusters: To merge clusters `′ and `′′ into a new cluster `, new parameters are initialized as: ζ (ar) w` = ζ (ar) w`′ + ζ (ar) w`′′ , ν (r) h` = ν (r) h`′ + ν (r) h`′′ , η ∗ ` = η ∗ `′ + η ∗ `′′ , and θ new ` = θ\n∗, such that there is a truncation at Lnew = L− 1 clusters. We set θ∗ = θ`′ for simplicity, and allow a restricted iteration of updates to Ωnew and θnew` . The merge is accepted (i.e., Ω = Ω\nnew and Θ = Θnew) if the lower bound is improved, L(q(Ωnew,Θnew)) > L(q(Ω,Θ)). Since the restricted iteration for θnew` is expensive, only a few cluster merges may be proposed at a time. Therefore, merges are proposed for clusters with the smallest earth mover’s distance [23] between their spectral densities.\nSplit step: When splitting states and clusters, the opposite process to the initialization of the merging procedures described above is performed. For clusters, data points within a cluster ` are randomly chosen to stay in cluster ` or split to a new cluster `′. For splitting state h, the cluster assignment vector φ(r)h is replicated and windows within state h are randomly chosen to stay in state h or split to a new cluster h′. Regardless of how this effects the lower bound, a split step is always accepted.\nFor implementation details, we allow the model to accept 3 state merges every third iteration, propose 5 cluster merges every third iteration, and split one state and one cluster every third iteration. Therefore, every iteration may affect the truncation level of either the number of states or clusters. A ‘burn-in’ period is allowed before starting the proposing of splits/merges, and a ‘burn-out’ period is employed in which split proposals cease. In this way, the algorithm has guarantees of improving the lower bound only during iterations when a split is not proposed, and convergence tests are only considered during the burn-out period."
    }, {
      "heading" : "4 Datasets",
      "text" : "Three datasets are considered in this work, as follows:\nToy data: Data is generated for a single animal according to the proposed model in Section 2. The purpose of this dataset is to ensure the inference scheme can recover known ground truth, since ground truth information is not known for the real datasets. We set L = 5 and H = 3. For each cluster, a spectral density was generated with Q = 4, µq ∼ Unif(4, 50), νq ∼ Unif(1, 50) and ω ∼ Dir(1, . . . , 1). The cluster usage probability vector was drawn φ(r)h ∼ Dir( 1 10 , . . . , 1 10 ). State transition probabilities were drawn according to λgh ∼ Unif(0, 1)+10δ(g=h). States were assigned to W = 1000 windows according to an HMM with transition matrix Λ, and cluster assignments were drawn conditioned on this state. Data with N = 200 was drawn for each window.\nSleep data: Twelve hours of LFP data from sixteen different brain regions were recorded from three mice naturally transitioning through different levels of sleep arousal. Due to the high number of brain regions, we present only three hours of sleep data from a single mouse for simplicity. The multi-animal analysis is reserved for the novel environment dataset.\nNovel environment data: Thirty minutes of LFP data from five brain regions was recorded from five mice who were moved from their home cage to a novel environment approximately nine minutes into the recording. Placing animals into novel environments has been shown to increase arousal, and\nshould therefore result in (at least one) network state change [3]. Data acquisition methods for the latter two datasets are discussed in [24]."
    }, {
      "heading" : "5 Results",
      "text" : "For all results, we set Q = 10, H = 15, L = 25, stop the ‘burn-in’ period after iteration 6, and start the subsequent computation period after iteration 25. Hyperparameters were set to γ0 = γ1 = .01, α0 = α1 = 1, µmin = 0, µmax = 50, νmax = 10, and e0 = f0 = 10−6. In all results, the model was seen to converge to a local optima after 30 iterations, and each iteration took on the order of 20 seconds using Matlab code on a PC with a 2.30GHz quad-core CPU and 8GB RAM.\nFigure 2 shows results on the toy data. The model correctly recovers exactly 3 states and 5 clusters, and, as seen in the figure, the state assignments and spectral densities of each cluster component are recovered almost perfectly. The model was implemented for different values of the noise variance, γ−1, and, though not shown, in all cases the noise variance was recovered accurately during inference, implying the spectral mixture kernels are not overfitting the noise. In this way, we confirm that the inference scheme recovers a ground truth. For further model verification, ten-fold cross-validation was used to compute predictive probabilities for held-out data (reported in Table 1), where we compare to two simpler versions of our model: 1) the HDP-HMM on brain states in (1) is replaced with an HDP, and 2) a single brain state. For the HDP-HMM, the hold-out data was considered as ‘missing data’ in the training data and the window index was used to assign time-dependent probabilities over clusters, whereas in the HDP and Single State models it was simply withheld from the training data. We see large predictive performance gains when considering multiple brain states, and even more improvement on average (though modest) when considering an HDP-HMM.\nThe sleep and novel environment results are presented in Figures 3 and 4, respectively. With the sleep dataset, our results are compared with the two methods discussed in the Introduction: that of [6, 7], and the tensor method of [8]. We refer to the Supplemental Material for exact specifications of the tensor method.\nFor each of these datasets, we infer the intended arousal states. In the novel environment data, we observe broad arousal changes at 9–minutes for all animals, as expected. In the sleep data, we successfully uncover at least as many states as the simple approach of [6, 7], to include SWS, REM and WK states. Thus far neuroscientists have focused primarily on 2 stages of sleep (NREM and REM), but as many as 5 have been discussed (4 different stages of NREM sleep, and 1 stage of REM). Different stages of sleep affect memory and behavior in different ways (e.g., see [25]), as does the number of times animals transition between these states [26]. Our results suggest that there may be even more levels of sleep that should be considered (e.g., transition states and sub states). This is very interesting and important for neuroscientists to know, because it is possible that each of our newly observed states could affect memory and behavior in different ways. There is no other published method that has provided evidence of these other states.\nIn addition to brain states, we infer spectral information for each brain region through cluster assignments. Though not the primary focus of this work, it is interesting that groups of brain regions tend to share similar attributes. In Figure 3, we have sorted brain regions into groups based on cluster assignment similarity, essentially recovering a ‘network’ of the brain. This underscores the power of the proposed method: not only do we develop unsupervised methods to classify whole-brain activity into states, we infer the cross-region/animal relationships within these states."
    }, {
      "heading" : "6 Conclusion",
      "text" : "The contributions of this paper are three-fold. First, we design an extension of the infinite tensor mixture model, incorporating time dependency. Second, we develop variational inference for the proposed generative model, including an efficient inference scheme using split-merge moves for two general models: the ITM and iHMM. To the authors’ knowledge, neither of these inference schemes have been developed previously. Finally, with respect to neuroscience application, we model brain states given multi-channel LFP data in a principled manner, showing significant advantages over other potential approaches to modeling brain states. Using the proposed framework, we discover distinct brain states directly from the raw, filtered data, defined by their spectral content and network properties, and we can infer relationships between and share statistical strength across data from multiple animals."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The research reported here was funded in part by ARO, DARPA, DOE, NGA and ONR."
    } ],
    "references" : [ {
      "title" : "Brain States: Top-down Influences in Sensory Processing",
      "author" : [ "C D Gilbert", "M Sigman" ],
      "venue" : "Neuron, vol. 54, no. 5, pp. 677–96, June 2007.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Correlations and Brain States: from Electrophysiology to Functional Imaging",
      "author" : [ "A Kohn", "A Zandvakili", "M A Smith" ],
      "venue" : "Curr. Opin. Neurobiol., vol. 19, no. 4, Aug. 2009.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Concepts and Mechanisms of Generalized Central Nervous System Arousal",
      "author" : [ "D Pfaff", "A Ribeiro", "J Matthews", "L Kow" ],
      "venue" : "ANYAS, Jan. 2008.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Emotion and the Motivational Brain",
      "author" : [ "P J Lang", "M M Bradley" ],
      "venue" : "Biol. Psychol., vol. 84, no. 3, pp. 437–50, July 2010.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Dynamic Causal Modelling",
      "author" : [ "K J Friston", "L Harrison", "W Penny" ],
      "venue" : "NeuroImage, vol. 19, no. 4, pp. 1273–1302, 2003.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Dopaminergic Control of Sleep–Wake States",
      "author" : [ "K Dzirasa", "S Ribeiro", "R Costa", "L M Santos", "S C Lin", "A Grosmark", "T D Sotnikova", "R R Gainetdinov", "M G Caron", "M A L Nicolelis" ],
      "venue" : "J. Neurosci., vol. 26, no. 41, pp. 10577–10589, 2006.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Global Forebrain Dynamics Predict Rat Behavioral States and their Transitions",
      "author" : [ "D Gervasoni", "S C Lin", "S Ribeiro", "E S Soares", "J Pantoja", "M A L Nicolelis" ],
      "venue" : "J. Neurosci., vol. 24, no. 49, pp. 11137–11147, 2004.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Scalable Bayesian Low-Rank Decomposition of Incomplete Multiway Tensors",
      "author" : [ "P Rai", "Y Wang", "S Guo", "G Chen", "D Dunson", "L Carin" ],
      "venue" : "ICML, 2014.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The Infinite Hidden Markov Model",
      "author" : [ "M J Beal", "Z Ghahramani", "C E Rasmussen" ],
      "venue" : "NIPS, 2002.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Gaussian Process Kernels for Pattern Discovery and Extrapolation",
      "author" : [ "A G Wilson", "R P Adams" ],
      "venue" : "ICML, 2013.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Bayesian learning of joint distributions of objects",
      "author" : [ "J Murray", "D B Dunson" ],
      "venue" : "AISTATS, 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sharing Clusters Among Related Groups : Hierarchical Dirichlet Processes",
      "author" : [ "Y W Teh", "M I Jordan", "M J Beal", "D M Blei" ],
      "venue" : "NIPS, 2005.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Foundations of the Parafac Procedure",
      "author" : [ "R A Harshman" ],
      "venue" : "Work. Pap. Phonetics, 1970.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Truly nonparametric online variational inference for hierarchical Dirichlet processes",
      "author" : [ "M Bryant", "E B Sudderth" ],
      "venue" : "NIPS, pp. 1–9, 2012.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The Infinite PCFG using Hierarchical Dirichlet Processes",
      "author" : [ "P Liang", "S Petrov", "M I Jordan", "D Klein" ],
      "venue" : "Conf. Empir. Methods Nat. Lang. Process. Comput. Nat. Lang. Learn., pp. 688–697, 2007.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Collapsed Variational Inference for HDP",
      "author" : [ "Y W Teh", "K Kurihara", "M Welling" ],
      "venue" : "NIPS, 2007.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Variational Inference for Dirichlet Process Mixtures",
      "author" : [ "D M Blei", "M I Jordan" ],
      "venue" : "Bayesian Anal, 2004.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Accelerated Variational Dirichlet Process Mixtures",
      "author" : [ "K Kurihara", "M Welling", "N Vlassis" ],
      "venue" : "NIPS, 2007.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Variational Algorithms for Approximate Bayesian Inference",
      "author" : [ "M J Beal" ],
      "venue" : "Diss. Univ. London, 2003.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Hidden Markov Models With Stick-Breaking Priors",
      "author" : [ "J Paisley", "L Carin" ],
      "venue" : "IEEE Trans. Signal Process., vol. 57, no. 10, pp. 3905–3917, 2009.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Splitting and merging components of a nonconjugate Dirichlet process mixture model",
      "author" : [ "S Jain", "R M Neal" ],
      "venue" : "Bayesian Anal, Sept. 2007.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The Earth Movers Distance as a Metric for Image Retrieval",
      "author" : [ "Y Rubner", "C Tomasi", "L J Guibas" ],
      "venue" : "Int. J. Comput. Vis., vol. 40, no. 2, pp. 99–121, 2000.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Chronic in Vivo Multi-Circuit Neurophysiological Recordings in Mice",
      "author" : [ "K Dzirasa", "R Fuentes", "S Kumar", "J M Potes", "M A L Nicolelis" ],
      "venue" : "J. Neurosci. Methods, vol. 195, no. 1, pp. 36–46, Jan. 2011.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A Daytime Nap Containing Solely Non-REM Sleep Enhances Declarative but not Procedural Memory",
      "author" : [ "M A Tucker", "Y Hirota", "E J Wamsley", "H Lau", "A Chaklader", "W Fishbein" ],
      "venue" : "Neurobiol. Learn. Mem., vol. 86, no. 2, pp. 241–7, Sept. 2006.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Optogenetic Disruption of Sleep Continuity Impairs Memory Consolidation",
      "author" : [ "A Rolls", "D Colas", "A Adamantidis", "M Carter", "T Lanre-Amos", "H C Heller", "L de Lecea" ],
      "venue" : "PNAS, vol. 108, no. 32, pp. 13305–10, Aug. 2011. 9",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "One of the remaining mysteries is how to best represent and understand the way whole-brain activity relates to cognition: in other words, how to describe brain states [1].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 1,
      "context" : "It has been proposed that the specific way brain regions are correlated at any given time may represent a “state” designed specifically to optimize neural computations relevant to the behavioral context an organism is in [2].",
      "startOffset" : 221,
      "endOffset" : 224
    }, {
      "referenceID" : 2,
      "context" : "Arousal is a hotly debated topic that generally refers to the way the brain dynamically responds to varying levels of stimulation [3].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 3,
      "context" : "Another is calm (low arousal) to excited or stressed (high arousal) [4].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "In neuroscience, the analysis of electrophysiological time-series data is largely centered around dynamic causal modeling (DCM) [5], where continuous state-space models are formulated based on differential equations that are specifically crafted around knowledge of underlying neurobiological processes.",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "Previously published neuroscience studies distinguished between slow-wave sleep (SWS), rapideye-movement (REM), and wake (WK) using proportions of high-frequency (33-55 Hz) gamma oscillations and lower frequency theta (4-9 Hz) oscillations in a brain area called the Hippocampus [6, 7].",
      "startOffset" : 279,
      "endOffset" : 285
    }, {
      "referenceID" : 6,
      "context" : "Previously published neuroscience studies distinguished between slow-wave sleep (SWS), rapideye-movement (REM), and wake (WK) using proportions of high-frequency (33-55 Hz) gamma oscillations and lower frequency theta (4-9 Hz) oscillations in a brain area called the Hippocampus [6, 7].",
      "startOffset" : 279,
      "endOffset" : 285
    }, {
      "referenceID" : 7,
      "context" : "As an alternative approach, recent statistical methods for tensor factorization [8] can be applied to short time Fourier transform (STFT) coefficients by factorizing a 3–way LFP tensor, with dimensions of brain region, frequency band and time.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "In this paper we overcome the shortcomings of previously published brain-state methods by defining a sequence of brain states over a sliding window of raw, filtered LFP data, where we impose an infinite hidden Markov model (iHMM) [9] on these state assignments.",
      "startOffset" : 230,
      "endOffset" : 233
    }, {
      "referenceID" : 9,
      "context" : "Each cluster is associated with a specific spectral content (or density) pattern, manifested through a spectral mixture kernel [10] of a Gaussian process.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 8,
      "context" : "This brain state is represented through a categorical latent variable s w , and an infinite hidden Markov model (iHMM) is placed on the state dynamics [9, 12].",
      "startOffset" : 151,
      "endOffset" : 158
    }, {
      "referenceID" : 11,
      "context" : "This brain state is represented through a categorical latent variable s w , and an infinite hidden Markov model (iHMM) is placed on the state dynamics [9, 12].",
      "startOffset" : 151,
      "endOffset" : 158
    }, {
      "referenceID" : 11,
      "context" : "2 Assigning brain regions to clusters For each brain state, mixture weights are drawn to define the distribution over clusters independently for each region r, centered around a global mixture η using a hierarchical Dirichlet Process [12]:",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 10,
      "context" : "One could consider the joint probability over cluster assignments for all brain regions as an extension of a latent nonnegative PARAFAC tensor decomposition [11, 13].",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 12,
      "context" : "One could consider the joint probability over cluster assignments for all brain regions as an extension of a latent nonnegative PARAFAC tensor decomposition [11, 13].",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 10,
      "context" : "Our clustering model differs from the infinite tensor factorization (ITF) model of [11] in three significant ways: we place Markovian dynamics on state assignments for each animal, we model separate draws from the prior jointly for each animal, and we share cluster atoms across all regions through use of an HDP.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "Previous research has established a link between the kernel function of a GP and its spectral properties [10].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "Since this work aims to model the spectral content of the LFP signal, we set the mean function to 0, and use a recently proposed spectral mixture (SM) kernel [10].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 13,
      "context" : "The degenerate distributions δη∗(η) and δβ∗(β) are described in previous work on variational inference for HDPs [15, 16].",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : "The degenerate distributions δη∗(η) and δβ∗(β) are described in previous work on variational inference for HDPs [15, 16].",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : "The idea is that the point estimates of the stick-breaking processes simplify the derivation of the variational posterior, and the authors of [16] show that obtaining a full posterior distribution on the stick-breaking weights has little impact on model fitting since the variational lower bound is not heavily influenced by the terms dependent on η and β.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 15,
      "context" : "This truncation method (see [17] for details) is notably different than other common truncation methods of the DP (e.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 16,
      "context" : ", [18] and [19]), and is primarily important for facilitating the split-merge inference techniques described in Section 3.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 17,
      "context" : ", [18] and [19]), and is primarily important for facilitating the split-merge inference techniques described in Section 3.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 18,
      "context" : ", see [20, 21]); using these methods, updates for κ g , ψ and the necessary expected sufficient statistics of the factors of q(S) are realized.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : ", see [20, 21]); using these methods, updates for κ g , ψ and the necessary expected sufficient statistics of the factors of q(S) are realized.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 20,
      "context" : "2 Split-merge moves During inference, a series of split and merge operations are used to help the algorithm jump out of local optima [22].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 21,
      "context" : "Therefore, merges are proposed for clusters with the smallest earth mover’s distance [23] between their spectral densities.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "should therefore result in (at least one) network state change [3].",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "Data acquisition methods for the latter two datasets are discussed in [24].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "Right: State assignments of our method and the tensor method conditioned on the method of [6].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "With the sleep dataset, our results are compared with the two methods discussed in the Introduction: that of [6, 7], and the tensor method of [8].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "With the sleep dataset, our results are compared with the two methods discussed in the Introduction: that of [6, 7], and the tensor method of [8].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "With the sleep dataset, our results are compared with the two methods discussed in the Introduction: that of [6, 7], and the tensor method of [8].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "In the sleep data, we successfully uncover at least as many states as the simple approach of [6, 7], to include SWS, REM and WK states.",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "In the sleep data, we successfully uncover at least as many states as the simple approach of [6, 7], to include SWS, REM and WK states.",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : ", see [25]), as does the number of times animals transition between these states [26].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 24,
      "context" : ", see [25]), as does the number of times animals transition between these states [26].",
      "startOffset" : 81,
      "endOffset" : 85
    } ],
    "year" : 2014,
    "abstractText" : "The local field potential (LFP) is a source of information about the broad patterns of brain activity, and the frequencies present in these time-series measurements are often highly correlated between regions. It is believed that these regions may jointly constitute a “brain state,” relating to cognition and behavior. An infinite hidden Markov model (iHMM) is proposed to model the evolution of brain states, based on electrophysiological LFP data measured at multiple brain regions. A brain state influences the spectral content of each region in the measured LFP. A new state-dependent tensor factorization is employed across brain regions, and the spectral properties of the LFPs are characterized in terms of Gaussian processes (GPs). The LFPs are modeled as a mixture of GPs, with stateand regiondependent mixture weights, and with the spectral content of the data encoded in GP spectral mixture covariance kernels. The model is able to estimate the number of brain states and the number of mixture components in the mixture of GPs. A new variational Bayesian split-merge algorithm is employed for inference. The model infers state changes as a function of external covariates in two novel electrophysiological datasets, using LFP data recorded simultaneously from multiple brain regions in mice; the results are validated and interpreted by subject-matter experts.",
    "creator" : null
  }
}