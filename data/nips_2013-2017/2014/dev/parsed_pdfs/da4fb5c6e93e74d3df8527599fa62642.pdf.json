{
  "name" : "da4fb5c6e93e74d3df8527599fa62642.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multivariate Regression with Calibration",
    "authors" : [ "Han Liu", "Tuo Zhao" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Given a design matrix X 2 Rn⇥d and a response matrix Y 2 Rn⇥m, we consider a multivariate linear model Y = XB0 + Z, where B0 2 Rd⇥m is an unknown regression coefficient matrix and Z 2 Rn⇥m is a noise matrix [1]. For a matrix A = [Ajk] 2 Rd⇥m, we denote Aj⇤ = (Aj1, ..., Ajm) 2 Rm and A⇤k = (A1k, ...,Adk)T 2 Rd to be its jth row and kth column respectively. We assume that all Zi⇤’s are independently sampled from an m-dimensional Gaussian distribution with mean 0 and covariance matrix ⌃ 2 Rm⇥m. We can represent the multivariate linear model as an ensemble of univariate linear regression models: Y⇤k = XB0⇤k+Z⇤k, k = 1, ...,m. Then we get a multi-task learning problem [3, 2, 26]. Multi-task learning exploits shared common structure across tasks to obtain improved estimation performance. In the past decade, significant progress has been made towards designing a variety of modeling assumptions for multivariate regression.\nA popular assumption is that all the regression tasks share a common sparsity pattern, i.e., many B0j⇤’s are zero vectors. Such a joint sparsity assumption is a natural extension of that for univariate linear regressions. Similar to the L\n1 -regularization used in Lasso [23], we can adopt group regularization to obtain a good estimator of B0 [25, 24, 19, 13]. Besides the aforementioned approaches, there are other methods that aim to exploit the covariance structure of the noise matrix Z [7, 22]. For\n⇤The authors are listed in alphabetical order. This work is partially supported by the grants NSF IIS1408910, NSF IIS1332109, NSF Grant DMS-1005539, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841.\n†Tuo Zhao is also affiliated with Department of Operations Research and Financial Engineering at Princeton University.\ninstance, [22] assume that all Zi⇤’s follow a multivariate Gaussian distribution with a sparse inverse covariance matrix ⌦ = ⌃ 1. They propose an iterative algorithm to estimate sparse B0 and ⌦ by maximizing the penalized Gaussian log-likelihood. Such an iterative procedure is effective in many applications, but the theoretical analysis is difficult due to its nonconvex formulation.\nIn this paper, we assume an uncorrelated structure for the noise matrix Z, i.e., ⌃ = diag( 2\n1 , 2 2 , . . . , 2m 1, 2 m). Under this setting, we can efficiently solve the resulting estimation problem with a convex program as follows\nbB = argmin B\n1\np n ||Y XB||2 F + ||B|| 1,p, (1.1)\nwhere > 0 is a tuning parameter, and ||A|| F =\nq\nP\nj,k A 2 jk is the Frobenius norm of a ma-\ntrix A. Popular choices of p include p = 2 and p = 1: ||B|| 1,2 = Pd j=1\nq\nPm k=1 B 2 jk and\n||B|| 1,1 = Pd j=1 max1km |Bjk|. Computationally, the optimization problem in (1.1) can be efficiently solved by some first order algorithms [11, 12, 4].\nThe problem with the uncorrelated noise structure is amenable to statistical analysis. Under suitable conditions on the noise and design matrices, let\nmax = maxk k, if we choose = 2c ·\nmax\np\nlog d+m1 1/p , for some c > 1, then the estimator bB in (1.1) achieves the optimal rates of convergence1 [13], i.e., there exists some universal constant C such that with high probability, we have\n1\np\nm ||\nbB B0|| F  C · max\nr\ns log d\nnm +\nr\nsm1 2/p\nn\n!\n,\nwhere s is the number of rows with non-zero entries in B0. However, the estimator in (1.1) has two drawbacks: (1) All the tasks are regularized by the same tuning parameter , even though different tasks may have different k’s. Thus more estimation bias is introduced to the tasks with smaller k’s to compensate the tasks with larger k’s. In another word, these tasks are not calibrated. (2) The tuning parameter selection involves the unknown parameter\nmax . This requires tuning the regularization parameter over a wide range of potential values to get a good finite-sample performance.\nTo overcome the above two drawbacks , we formulate a new convex program named calibrated multivariate regression (CMR). The CMR estimator is defined to be the solution of the following convex program:\nbB = argmin B ||Y XB|| 2,1 + ||B||1,p, (1.2)\nwhere ||A|| 2,1 =\nP\nk\nq\nP\nj A 2 jk is the nonsmooth L2,1 norm of a matrix A = [Ajk] 2 Rd⇥m. This is a multivariate extension of the square-root Lasso [5]. Similar to the square-root Lasso, the tuning parameter selection of CMR does not involve\nmax . Moreover, the L 2,1 loss function can\nbe viewed as a special example of the weighted least square loss, which calibrates each regression task (See more details in §2). Thus CMR adapts to different k’s and achieves better finite-sample performance than the ordinary multivariate regression estimator (OMR) defined in (1.1).\nSince both the loss and penalty functions in (1.2) are nonsmooth, CMR is computationally more challenging than OMR. To efficiently solve CMR, we propose a smoothed proximal gradient (SPG) algorithm with an iteration complexity O(1/✏), where ✏ is the pre-specified accuracy of the objective value [18, 4]. Theoretically, we provide sufficient conditions under which CMR achieves the optimal rates of convergence in parameter estimation. Numerical experiments on both synthetic and real data show that CMR universally outperforms existing multivariate regression methods. For a brain activity prediction task, prediction based on the features selected by CMR significantly outperforms that based on the features selected by OMR, and is even competitive with that based on the handcrafted features selected by human experts.\nNotations: Given a vector v = (v 1 , . . . , vd)T 2 Rd, for 1  p  1, we define the Lp-vector\nnorm of v as ||v||p = ⇣ Pd j=1 |vj | p ⌘ 1/p if 1  p < 1 and ||v||p = max1jd |vj | if p = 1.\n1The rate of convergence is optimal when p = 2, i.e., the regularization function is ||B|| 1,p\nGiven two matrices A = [Ajk] and C = [Cjk] 2 Rd⇥m, we define the inner product of A and C as hA,Ci =\nPd j=1 Pm k=1 AjkCjk = tr(A TC), where tr(A) is the trace of a matrix A. We use A⇤k = (A1k, ...,Adk)T and Aj⇤ = (Aj1, ...,Ajm) to denote the kth column and jth row of A. Let S be some subspace of Rd⇥m, we use AS to denote the projection of A onto S: AS = argminC2S ||C A|| 2\nF . Moreover, we define the Frobenius and spectral norms of A as ||A||\nF\n=\np\nhA,Ai and ||A|| 2 = 1 (A), 1 (A) is the largest singular value of A. In addition, we define the matrix block norms as ||A|| 2,1 = Pm\nk=1 ||A⇤k||2, ||A||2,1 = max1km ||A⇤k||2, ||A|| 1,p = Pd\nj=1 ||Aj⇤||p, and ||A||1,q = max1jd ||Aj⇤||q , where 1  p  1 and 1  q  1. It is easy to verify that ||A||\n2,1 is the dual norm of ||A||2,1. Let 1/1 = 0, then if 1/p+ 1/q = 1, ||A||1,q and ||A||1,p are also dual norms of each other."
    }, {
      "heading" : "2 Method",
      "text" : "We solve the multivariate regression problem by the following convex program, bB = argmin\nB ||Y XB|| 2,1 + ||B||1,p. (2.1)\nThe only difference between (2.1) and (1.1) is that we replace the L 2 -loss function by the nonsmooth L 2,1-loss function. The L2,1-loss function can be viewed as a special example of the weighted square loss function. More specifically, we consider the following optimization problem,\nbB = argmin B\nm X\nk=1\n1\nk p n ||Y⇤k XB⇤k|| 2 2 + ||B|| 1,p, (2.2)\nwhere 1 k p n is a weight assigned to calibrate the kth regression task. Without prior knowledge on k’s, we use the following replacement of k’s,\ne k = 1 p n ||Y⇤k XB⇤k||2, k = 1, ...,m. (2.3)\nBy plugging (2.3) into the objective function in (2.2), we get (2.1). In another word, CMR calibrates different tasks by solving a penalized weighted least square program with weights defined in (2.3).\nThe optimization problem in (2.1) can be solved by the alternating direction method of multipliers (ADMM) with a global convergence guarantee [20]. However, ADMM does not take full advantage of the problem structure in (2.1). For example, even though the L\n2,1 norm is nonsmooth, it is nondifferentiable only when a task achieves exact zero residual, which is unlikely in applications. In this paper, we apply the dual smoothing technique proposed by [18] to obtain a smooth surrogate function so that we can avoid directly evaluating the subgradient of the L\n2,1 loss function. Thus we gain computational efficiency like other smooth loss functions.\nWe consider the Fenchel’s dual representation of the L 2,1 loss:\n||Y XB|| 2,1 = max ||U||2,11 hU,Y XBi. (2.4)\nLet µ > 0 be a smoothing parameter. The smooth approximation of the L 2,1 loss can be obtained by solving the following optimization problem\n||Y XB||µ = max ||U||2,11\nhU,Y XBi µ\n2\n||U||2 F , (2.5)\nwhere ||U||2 F is the proximity function. Due to the fact that ||U||2 F  m||U||2 2,1, we obtain the following uniform bound by combing (2.4) and (2.5),\n||Y XB|| 2,1\nmµ\n2\n ||Y XB||µ  ||Y XB||2,1. (2.6)\nFrom (2.6), we see that the approximation error introduced by the smoothing procedure can be controlled by a suitable µ. Figure 2.1 shows several two-dimensional examples of the L\n2 norm smoothed by different µ’s. The optimization problem in (2.5) has a closed form solution bUB with bUB⇤k = (Y⇤k XB⇤k)/max {||Y⇤k XB⇤k||2, µ}.\nThe next lemma shows that ||Y XB||µ is smooth in B with a simple form of gradient.\nLemma 2.1. For any µ > 0, ||Y XB||µ is a convex and continuously differentiable function in B. In addition, Gµ(B)—the gradient of ||Y XB||µ w.r.t. B—has the form\nGµ(B) = @ ⇣ h bUB,Y XBi+ µ||bUB||2 F /2 ⌘\n@B = XT bUB. (2.7)\nMoreover, let = ||X||2 2 , then we have that Gµ(B) is Lipschitz continuous in B with the Lipschitz constant /µ, i.e., for any B0, B00 2 Rd⇥m,\n||Gµ(B0) Gµ(B00)|| F = ||hX, bUB 0 bUB 00 i|| F  1 µ ||XTX(B0 B00)|| F  µ ||B0 B00|| F .\nLemma 2.1 is a direct result of Theorem 1 in [18] and implies that ||Y XB||µ has good computational structure. Therefore we apply the smooth proximal gradient algorithm to solve the smoothed version of the optimization problem as follows,\neB = argmin B ||Y XB||µ + ||B||1,p. (2.8)\nWe then adopt the fast proximal gradient algorithm to solve (2.8) [4]. To derive the algorithm, we first define three sequences of auxiliary variables {A(t)}, {V(t)}, and {H(t)} with A(0) = H(0) = V(0) = B(0), a sequence of weights {✓t = 2/(t + 1)}, and a nonincreasing sequence of step-sizes {⌘t > 0}. For simplicity, we can set ⌘t = µ/ . In practice, we use the backtracking line search to dynamically adjust ⌘t to boost the performance. At the tth iteration, we first take V(t) = (1 ✓t)B(t 1) + ✓tA(t 1). We then consider a quadratic approximation of ||Y XH||µ as\nQ ⇣ H,V(t), ⌘t ⌘ = ||Y XV(t)||µ + hG µ (V(t)),H V(t)i+ 1\n2⌘t ||H V(t)||2 F .\nConsequently, let eH(t) = V(t) ⌘tGµ(V(t)), we take\nH(t) = argmin H\nQ ⇣ H,V(t), ⌘t ⌘ + ||H|| 1,p = argmin\nH\n1 2⌘t ||H eH(t)||2 F + ||H|| 1,p. (2.9)\nWhen p = 2, (2.9) has a closed form solution H(t)j⇤ = eHj⇤ · max n 1 ⌘t /|| eHj⇤||2, 0 o\n. More details about other choices of p in the L\n1,p norm can be found in [11] and [12]. To ensure that the objective value is nonincreasing, we choose\nB(t) = argmin B2{H(t), B(t 1)} ||Y XB||µ + ||B||1,p. (2.10)\nAt last, we take A(t) = B(t 1)+ 1✓t (H (t) B(t 1)). The algorithm stops when ||H(t) V(t)|| F  \", where \" is the stopping precision.\nThe numerical rate of convergence of the proposed algorithm with respect to the original optimization problem (2.1) is presented in the following theorem. Theorem 2.2. Given a pre-specified accuracy ✏ and let µ = ✏/m, after t = 2pm ||B(0) bB||\nF\n/✏\n1 = O (1/✏) iterations, we have ||Y XB(t)|| 2,1 + ||B(t)||1,p  ||Y XbB||2,1 + ||bB||1,p + ✏.\nThe proof of Theorem 2.2 is provided in Appendix A.1. This result achieves the minimax optimal rate of convergence over all first order algorithms [18]."
    }, {
      "heading" : "3 Statistical Properties",
      "text" : "For notational simplicity, we define a re-scaled noise matrix W = [Wik] 2 Rn⇥m with Wik = Zik/ k, where EZ2ik = 2k. Thus W is a random matrix with all entries having mean 0 and variance 1. We define G0 to be the gradient of ||Y XB|| 2,1 at B = B0. It is easy to see that\nG0⇤k = XTZ⇤k ||Z⇤k||2 = XTW⇤k k ||W⇤k k||2 = XTW⇤k ||W⇤k||2\ndoes not depend on the unknown quantities k for all k = 1, ...,m. G0⇤k works as an important pivotal in our analysis. Moreover, our analysis exploits the decomposability of the L\n1,p norm [17]. More specifically, we assume that B0 has s rows with all zero entries and define\nS = C 2 Rd⇥m | Cj⇤ = 0 for all j such that B0j⇤ = 0 , (3.1)\nN = C 2 Rd⇥m | Cj⇤ = 0 for all j such that B0j⇤ 6= 0 . (3.2)\nNote that we have B0 2 S and the L 1,p norm is decomposable with respect to the pair (S,N ), i.e.,\n||A|| 1,p = ||AS ||1,p + ||AN ||1,p.\nThe next lemma shows that when is suitably chosen, the solution to the optimization problem in (2.1) lies in a restricted set. Lemma 3.1. Let B0 2 S and bB be the optimum to (2.1), and 1/p + 1/q = 1. We denote the estimation error as\nb = bB B0. If c||G0||1,q for some c > 1, we have\nb 2 Mc :=\n⇢\n2 Rd⇥m | || N ||1,p  c+ 1\nc 1 || S ||1,p\n. (3.3)\nThe proof of Lemma 3.1 is provided in Appendix B.1. To prove the main result, we also need to assume that the design matrix X satisfies the following condition. Assumption 3.1. Let B0 2 S , then there exist positive constants  and c > 1 such that\n  min 2Mc\\{0}\n||X || F\np\nn|| || F\n.\nAssumption 3.1 is the generalization of the restricted eigenvalue conditions for analyzing univariate sparse linear models [17, 15, 6], Many common examples of random design satisfy this assumption [13, 21].\nNote that Lemma 3.1 is a deterministic result of the CMR estimator for a fixed . Since G is essentially a random matrix, we need to show that cR⇤(G0) holds with high probability to deliver a concrete rate of convergence for the CMR estimator in the next theorem. Theorem 3.2. We assume that each column of X is normalized as m1/2 1/pkX⇤jk2 = p\nn for all j = 1, ..., d. Then for some universal constant c\n0\nand large enough n, taking\n= 2c(m1 1/p +\np\nlog d) p\n1 c 0\n, (3.4)\nwith probability at least 1 2 exp( 2 log d) 2 exp nc2 0 /8 + logm , we have\n1\np\nm ||\nbB B0|| F \n16c max\n2(c 1)\nr\n1 + c 0\n1 c 0\nr\nsm1 2/p\nn +\nr\ns log d\nnm\n!\n.\nThe proof of Theorem 3.2 is provided in Appendix B.2. Note that when we choose p = 2, the column normalization condition is reduced to kX⇤jk2 = p\nn. Meanwhile, the corresponding error bound is further reduced to\n1\np\nm ||\nbB B0|| F = OP\nr\ns\nn +\nr\ns log d\nnm\n!\n,\nwhich achieves the minimax optimal rate of convergence presented in [13]. See Theorem 6.1 in [13] for more technical details. From Theorem 3.2, we see that CMR achieves the same rates of convergence as the noncalibrated counterpart, but the tuning parameter in (3.4) does not involve k’s. Therefore CMR not only calibrates all the regression tasks, but also makes the tuning parameter selection insensitive to\nmax\n."
    }, {
      "heading" : "4 Numerical Simulations",
      "text" : "To compare the finite-sample performance between the calibrated multivariate regression (CMR) and ordinary multivariate regression (OMR), we generate a training dataset of 200 samples. More specifically, we use the following data generation scheme: (1) Generate each row of the design matrix Xi⇤, i = 1, ..., 200, independently from a 800-dimensional normal distribution N(0,⌃) where ⌃jj = 1 and ⌃j` = 0.5 for all ` 6= j.(2) Let k = 1, . . . , 13, set the regression coefficient matrix B0 2 R800⇥13 as B0\n1k = 3, B 0 2k = 2, B 0 4k = 1.5, and B 0 jk = 0 for all j 6= 1, 2, 4. (3) Generate the random noise matrix Z = WD, where W 2 R200⇥13 with all entries of W are independently generated from N(0, 1), and D is either of the following matrices\nDI = max · diag ⇣ 2 0/4, 2 1/4, · · · , 2 11/4, 2 12/4 ⌘ 2 R13⇥13\nDH = max · I 2 R13⇥13. We generate a validation set of 200 samples for the regularization parameter selection and a testing set of 10,000 samples to evaluate the prediction accuracy.\nIn numerical experiments, we set max = 1, 2, and 4 to illustrate the tuning insensitivity of CMR. The regularization parameter of both CMR and OMR is chosen over a grid ⇤ =\n2 40/4 0, 239/4 0, · · · , 2 17/4 0, 2 18/4 0 , where 0 = p log d + p m. The optimal regularization parameter b is determined by the prediction error as b = argmin 2⇤ || eY eXbB ||2\nF , where bB denotes the obtained estimate using the regularization parameter , and eX and eY denote the design and response matrices of the validation set.\nSince the noise level k’s are different in regression tasks, we adopt the following three criteria to evaluate the empirical performance: Pre. Err. = 1\n10000\n||Y XbB|| F , Adj. Pre. Err. = 1\n10000m ||(Y X bB)D 1||2 F , and Est. Err. = 1m ||bB B 0 || 2 F , where X and Y denotes the design and response matrices of the testing set.\nAll simulations are implemented by MATLAB using a PC with Intel Core i5 3.3GHz CPU and 16GB memory. CMR is solved by the proposed smoothing proximal gradient algorithm, where we set the stopping precision \" = 10 4, the smoothing parameter µ = 10 4. OMR is solved by the monotone fast proximal gradient algorithm, where we set the stopping precision \" = 10 4. We set p = 2, but the extension to arbitrary p > 2 is straightforward.\nWe first compare the smoothed proximal gradient (SPG) algorithm with the ADMM algorithm (the detailed derivation of ADMM can be found in Appendix A.2). We adopt the backtracking line search to accelerate both algorithms with a shrinkage parameter ↵ = 0.8. We set\nmax = 2 for the adopted multivariate linear models. We conduct 200 simulations. The results are presented in Table 4.1. The SPG and ADMM algorithms attain similar objective values, but SPG is up to 4 times faster than ADMM. Both algorithms also achieve similar estimation errors.\nWe then compare the statistical performance between CMR and OMR. Tables 4.2 and 4.3 summarize the results averaged over 200 replicates. In addition, we also present the results of the oracle estimator, which is obtained by solving (2.2), since we know the true values of k’s. Note that the oracle estimator is only for comparison purpose, and it is not a practical estimator. Since CMR calibrates the regularization for each task with respect to k, CMR universally outperforms OMR, and achieves almost the same performance as the oracle estimator when we adopt the scale matrix DI to generate the random noise. Meanwhile, when we adopt the scale matrix DH , where all k’s are the same, CMR and OMR achieve similar performance. This further implies that CMR can be a safe replacement of OMR for multivariate regressions.\nIn addition, we also examine the optimal regularization parameters for CMR and OMR over all replicates. We visualize the distribution of all 200 selected b ’s using the kernel density estimator. In particular, we adopt the Gaussian kernel, and the kernel bandwidth is selected based on the 10- fold cross validation. Figure 4.1 illustrates the estimated density functions. The horizontal axis corresponds to the rescaled regularization parameter as log ⇣\nb p log d+ p m\n⌘\n. We see that the optimal regularization parameters of OMR significantly vary with different\nmax . In contrast, the optimal regularization parameters of CMR are more concentrated. This is inconsistent with our claimed tuning insensitivity."
    }, {
      "heading" : "5 Real Data Experiment",
      "text" : "We apply CMR on a brain activity prediction problem which aims to build a parsimonious model to predict a person’s neural activity when seeing a stimulus word. As is illustrated in Figure 5.1, for a given stimulus word, we first encode it into an intermediate semantic feature vector using some corpus statistics. We then model the brain’s neural activity pattern using CMR. Creating such a predictive model not only enables us to explore new analytical tools for the fMRI data, but also helps us to gain deeper understanding on how human brain represents knowledge [16].\n;5'%'<3'.)/'+=2%0.'%*-+&:*='&%)+%>\")=*5'44%'=%0?%8*)'+*'%@AB\nOur experiments involves 9 participants, and Table 5.1 summarizes the prediction performance of different methods on these participants. We see that the prediction based on the features selected by CMR significantly outperforms that based on the features selected by OMR, and is as competitive as that based on the handcrafted features selected by human experts. But due to the space limit, we present the details of the real data experiment in the technical report version."
    }, {
      "heading" : "6 Discussions",
      "text" : "A related method is the square-root sparse multivariate regression [8]. They solve the convex program with the Frobenius loss function and L\n1,p regularization function bB = argmin\nB ||Y XB|| F + ||B|| 1,p. (6.1)\nThe Frobenius loss function in (6.1) makes the regularization parameter selection independent of max , but it does not calibrate different regression tasks. Note that we can rewrite (6.1) as\n( bB, b ) = argmin B,\n1\np nm ||Y XB||2 F + ||B|| 1,p s. t. =\n1\np nm ||Y XB|| F . (6.2)\nSince in (6.2) is not specific to any individual task, it cannot calibrate the regularization. Thus it is fundamentally different from CMR."
    } ],
    "references" : [ {
      "title" : "An introduction to multivariate statistical analysis",
      "author" : [ "T.W Anderson" ],
      "venue" : "Wiley New York",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1958
    }, {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "Rie Kubota Ando", "Tong Zhang" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "A model of inductive bias learning",
      "author" : [ "J Baxter" ],
      "venue" : "Journal of Artificial Intelligence Research, 12:149–198",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems",
      "author" : [ "A. Beck", "M Teboulle" ],
      "venue" : "IEEE Transactions on Image Processing, 18(11):2419–2434",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Square-root lasso: pivotal recovery of sparse signals via conic programming",
      "author" : [ "A. Belloni", "V. Chernozhukov", "L Wang" ],
      "venue" : "Biometrika, 98(4):791–806",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Simultaneous analysis of lasso and dantzig selector",
      "author" : [ "Peter J Bickel", "Yaacov Ritov", "Alexandre B Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Predicting multivariate responses in multiple linear regression",
      "author" : [ "L. Breiman", "J.H Friedman" ],
      "venue" : "Journal of the Royal Statistical Society: Series B, 59(1):3–54",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The group square-root lasso: Theoretical properties and fast algorithms",
      "author" : [ "Florentina Bunea", "Johannes Lederer", "Yiyuan She" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Chi-square oracle inequalities",
      "author" : [ "Iain M Johnstone" ],
      "venue" : "Lecture Notes-Monograph Series, pages 399–418,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    }, {
      "title" : "Probability in Banach Spaces: isoperimetry and processes",
      "author" : [ "Michel Ledoux", "Michel Talagrand" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Blockwise coordinate descent procedures for the multi-task lasso",
      "author" : [ "H. Liu", "M. Palatucci", "J Zhang" ],
      "venue" : "with applications to neural semantic basis discovery. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 649–656. ACM",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Efficient `  1 /`q norm regularization",
      "author" : [ "J. Liu", "J Ye" ],
      "venue" : "Technical report, Arizona State University",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "S",
      "author" : [ "K. Lounici", "M. Pontil" ],
      "venue" : "Van De Geer, and A.B Tsybakov. Oracle inequalities and optimal inference under group sparsity. The Annals of Statistics, 39(4):2164–2204",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Stability selection",
      "author" : [ "N. Meinshausen", "P. Bühlmann" ],
      "venue" : "Journal of the Royal Statistical Society: Series B, 72(4):417–473",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Lasso-type recovery of sparse representations for high-dimensional data",
      "author" : [ "Nicolai Meinshausen", "Bin Yu" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Predicting human brain activity associated with the meanings of nouns",
      "author" : [ "T.M. Mitchell", "S.V. Shinkareva", "A. Carlson", "K.M. Chang", "V.L. Malave", "R.A. Mason", "M.A Just" ],
      "venue" : "Science, 320(5880):1191–1195",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "Sahand N. Negahban", "Pradeep Ravikumar", "Martin J. Wainwright", "Bin Yu" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Mathematical Programming, 103(1):127– 152",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Support union recovery in high-dimensional multivariate regression",
      "author" : [ "G. Obozinski", "M.J. Wainwright", "M.I Jordan" ],
      "venue" : "The Annals of Statistics, 39(1):1–47",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Stochastic alternating direction method of multipliers",
      "author" : [ "Hua Ouyang", "Niao He", "Long Tran", "Alexander Gray" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Restricted eigenvalue properties for correlated gaussian designs",
      "author" : [ "Garvesh Raskutti", "Martin J Wainwright", "Bin Yu" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Sparse multivariate regression with covariance estimation",
      "author" : [ "A.J. Rothman", "E. Levina", "J Zhu" ],
      "venue" : "Journal of Computational and Graphical Statistics, 19(4):947–962",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B, 58(1):267–288",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Simultaneous variable selection",
      "author" : [ "B.A. Turlach", "W.N. Venables", "S.J Wright" ],
      "venue" : "Technometrics, 47(3):349–363",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "M. Yuan", "Y Lin" ],
      "venue" : "Journal of the Royal Statistical Society: Series B, 68(1):49–67",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A probabilistic framework for multi-task learning",
      "author" : [ "Jian Zhang" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction Given a design matrix X 2 Rn⇥d and a response matrix Y 2 Rn⇥m, we consider a multivariate linear model Y = XB0 + Z, where B0 2 Rd⇥m is an unknown regression coefficient matrix and Z 2 Rn⇥m is a noise matrix [1].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 2,
      "context" : "Then we get a multi-task learning problem [3, 2, 26].",
      "startOffset" : 42,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "Then we get a multi-task learning problem [3, 2, 26].",
      "startOffset" : 42,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : "Then we get a multi-task learning problem [3, 2, 26].",
      "startOffset" : 42,
      "endOffset" : 52
    }, {
      "referenceID" : 22,
      "context" : "1 -regularization used in Lasso [23], we can adopt group regularization to obtain a good estimator of B0 [25, 24, 19, 13].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : "1 -regularization used in Lasso [23], we can adopt group regularization to obtain a good estimator of B0 [25, 24, 19, 13].",
      "startOffset" : 105,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "1 -regularization used in Lasso [23], we can adopt group regularization to obtain a good estimator of B0 [25, 24, 19, 13].",
      "startOffset" : 105,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "1 -regularization used in Lasso [23], we can adopt group regularization to obtain a good estimator of B0 [25, 24, 19, 13].",
      "startOffset" : 105,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "1 -regularization used in Lasso [23], we can adopt group regularization to obtain a good estimator of B0 [25, 24, 19, 13].",
      "startOffset" : 105,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "Besides the aforementioned approaches, there are other methods that aim to exploit the covariance structure of the noise matrix Z [7, 22].",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 21,
      "context" : "Besides the aforementioned approaches, there are other methods that aim to exploit the covariance structure of the noise matrix Z [7, 22].",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 21,
      "context" : "instance, [22] assume that all Zi⇤’s follow a multivariate Gaussian distribution with a sparse inverse covariance matrix ⌦ = ⌃ (1).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : "1) can be efficiently solved by some first order algorithms [11, 12, 4].",
      "startOffset" : 60,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "1) can be efficiently solved by some first order algorithms [11, 12, 4].",
      "startOffset" : 60,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "1) can be efficiently solved by some first order algorithms [11, 12, 4].",
      "startOffset" : 60,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "1) achieves the optimal rates of convergence1 [13], i.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "This is a multivariate extension of the square-root Lasso [5].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "To efficiently solve CMR, we propose a smoothed proximal gradient (SPG) algorithm with an iteration complexity O(1/✏), where ✏ is the pre-specified accuracy of the objective value [18, 4].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 3,
      "context" : "To efficiently solve CMR, we propose a smoothed proximal gradient (SPG) algorithm with an iteration complexity O(1/✏), where ✏ is the pre-specified accuracy of the objective value [18, 4].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 19,
      "context" : "1) can be solved by the alternating direction method of multipliers (ADMM) with a global convergence guarantee [20].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "In this paper, we apply the dual smoothing technique proposed by [18] to obtain a smooth surrogate function so that we can avoid directly evaluating the subgradient of the L 2,1 loss function.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "1 is a direct result of Theorem 1 in [18] and implies that ||Y XB||μ has good computational structure.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "More details about other choices of p in the L 1,p norm can be found in [11] and [12].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "More details about other choices of p in the L 1,p norm can be found in [11] and [12].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "This result achieves the minimax optimal rate of convergence over all first order algorithms [18].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 16,
      "context" : "Moreover, our analysis exploits the decomposability of the L 1,p norm [17].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "1 is the generalization of the restricted eigenvalue conditions for analyzing univariate sparse linear models [17, 15, 6], Many common examples of random design satisfy this assumption [13, 21].",
      "startOffset" : 110,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "1 is the generalization of the restricted eigenvalue conditions for analyzing univariate sparse linear models [17, 15, 6], Many common examples of random design satisfy this assumption [13, 21].",
      "startOffset" : 110,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "1 is the generalization of the restricted eigenvalue conditions for analyzing univariate sparse linear models [17, 15, 6], Many common examples of random design satisfy this assumption [13, 21].",
      "startOffset" : 110,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "1 is the generalization of the restricted eigenvalue conditions for analyzing univariate sparse linear models [17, 15, 6], Many common examples of random design satisfy this assumption [13, 21].",
      "startOffset" : 185,
      "endOffset" : 193
    }, {
      "referenceID" : 20,
      "context" : "1 is the generalization of the restricted eigenvalue conditions for analyzing univariate sparse linear models [17, 15, 6], Many common examples of random design satisfy this assumption [13, 21].",
      "startOffset" : 185,
      "endOffset" : 193
    }, {
      "referenceID" : 12,
      "context" : ", which achieves the minimax optimal rate of convergence presented in [13].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "Creating such a predictive model not only enables us to explore new analytical tools for the fMRI data, but also helps us to gain deeper understanding on how human brain represents knowledge [16].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 15,
      "context" : "1: An illustration of the fMRI brain activity prediction problem [16].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "819 6 Discussions A related method is the square-root sparse multivariate regression [8].",
      "startOffset" : 85,
      "endOffset" : 88
    } ],
    "year" : 2014,
    "abstractText" : "We propose a new method named calibrated multivariate regression (CMR) for fitting high dimensional multivariate regression models. Compared to existing methods, CMR calibrates the regularization for each regression task with respect to its noise level so that it is simultaneously tuning insensitive and achieves an improved finite-sample performance. Computationally, we develop an efficient smoothed proximal gradient algorithm which has a worst-case iteration complexity O(1/✏), where ✏ is a pre-specified numerical accuracy. Theoretically, we prove that CMR achieves the optimal rate of convergence in parameter estimation. We illustrate the usefulness of CMR by thorough numerical simulations and show that CMR consistently outperforms other high dimensional multivariate regression methods. We also apply CMR on a brain activity prediction problem and find that CMR is as competitive as the handcrafted model created by human experts.",
    "creator" : null
  }
}