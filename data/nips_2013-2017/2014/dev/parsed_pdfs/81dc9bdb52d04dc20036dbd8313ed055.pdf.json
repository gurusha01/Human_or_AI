{
  "name" : "81dc9bdb52d04dc20036dbd8313ed055.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Chordal Markov Networks by Dynamic Programming",
    "authors" : [ "Kustaa Kangas", "Teppo Niinimäki", "Mikko Koivisto" ],
    "emails" : [ "jwkangas@cs.helsinki.fi", "tzniinim@cs.helsinki.fi", "mkhkoivi@cs.helsinki.fi" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Structure learning in Markov networks, also known as undirected graphical models or Markov random fields, has attracted considerable interest in computational statistics, machine learning, and artificial intelligence. Natural score-and-search formulations of the task have, however, proved to be computationally very challenging. For example, Srebro [1] showed that finding a maximum-likelihood chordal (or triangulated or decomposable) Markov network is NP-hard even for networks of treewidth at most 2, in sharp contrast to the treewidth-1 case [2]. Consequently, various approximative approaches and local search heuristics have been proposed [3, 1, 4, 5, 6, 7, 8, 9, 10, 11].\nOnly very recently, Corander et al. [12] published the first non-trivial algorithm that is guaranteed to find a globally optimal chordal Markov network. It is based on expressing the search space in terms of logical constraints and employing the state-of-the-art solver technology equipped with optimization capabilities. To this end, they adopt the usual clique tree, or junction tree, representation of chordal graphs, and work with a particular characterization of clique trees, namely, that for any vertex of the graph the cliques containing that vertex induce a connected subtree in the clique tree. The key idea is to rephrase this property as what they call a balancing condition: for any vertex, the number of cliques that contain it is one larger than the number of edges (the intersection of the adjacent cliques) that contain it. They show that with appropriate, efficient encodings of the constraints, an eight-vertex instance can be solved to the optimum in a few days of computing, which could have been impossible by a brute-force search. However, while the constraint satisfaction approach enables exploiting the powerful technology, it is currently not clear, whether it scales to larger instances.\nHere, we investigate an alternative approach to find an optimal chordal Markov network. Like the work of Corander at al. [12], our algorithm stems from a particular characterization of clique trees of chordal graphs. However, our characterization is quite different, being recursive in nature. It concords the structure of common scoring functions and so yields a natural dynamic programming algorithm that grows an optimal clique tree by selecting its cliques one by one. In its basic form, the algorithm\nis very inefficient. Fortunately, the fine structure of the scoring function enables us to further factorize the main dynamic programming step and so bring the time requirement down to O(4n) for instances with n vertices. We also show that by setting the maximum clique size, equivalently the treewidth (plus one), to w ≤ n/4, the time requirement can be improved to O ( 3n−w ( n w ) w ) .\nWhile our recursive characterization of clique trees and the resulting dynamic programming algorithm are new, they are similar in spirit to a recent work by Korhonen and Parviainen [13]. Their algorithm finds a bounded-treewidth Bayesian network structure that maximizes a decomposable score, running in 3nnw+O(1) time, where w is the treewidth bound. For large w it thus is superexponentially slower than our algorithm. The problems solved by the two algorithms are, of course, different: the class of treewidth-w Bayesian networks properly extends the class of treewidth-w chordal Markov networks. There is also more recent work for finding bounded-treewidth Bayesian networks by employing constraint solvers: Berg et al. [14] solve the problem by casting into maximum satisfiability, while Parviainen et al. [15] cast into integer linear programming. For unbounded-treewidth Bayesian networks, O(2nn2)-time algorithms based on dynamic programming are available [16, 17, 18]. However, none of these dynamic programming algorithms, nor their A* search based variant [19], enables adding the constraints of chordality or bounded width.\nBut the integer linear programming approach to finding optimal Bayesian networks, especially the recent implementation by Bartlett and Cussens [20], also enables adding the further constraints.1 We are not aware of any reasonable worst-case bounds for the algorithm’s time complexity, nor any previous applications of the algorithm to the problem of learning chordal Markov networks. As a second contribution of this paper, we report on an experimental study of the algorithm’s performance, using both synthetic data and some frequently used machine learning benchmark datasets.\nThe remainder of this article begins by formulating the learning task as an optimization problem. Next we present our recursive characterization of clique trees and a derivation of the dynamic programming algorithm, with a rigorous complexity analysis. The experimental setting and results are reported in a dedicated section. We end with a brief discussion."
    }, {
      "heading" : "2 The problem of learning chordal Markov networks",
      "text" : "We adopt the hypergraph treatment of chordal Markov networks. For a gentler presentation and proofs, see Lauritzen and Spiegelhalter [21, Sections 6 and 7], Lauritzen [22], and references therein.\nLet p be a positive probability function over a product of n state spaces. Let G be an undirected graph on the vertex set V = {1, . . . , n}, and call any maximal set of pairwise adjacent vertices of G a clique. Together, G and p form a Markov network if p(x1, . . . , xn) = ∏ C ψC(xC), where C runs through the cliques of G and each ψC is a mapping to positive reals. Here xC denotes (xv : v ∈ C). The factors ψC take a particularly simple form when the graph G is chordal, that is, when every cycle of G of length greater than three has a chord, which is an edge of G joining two nonconsecutive vertices of the cycle. The chordality requirement can be expressed in terms of hypergraphs. Consider first an arbitrary hypergraph on V , identified with a collection C of subsets of V such that each element of V belongs to some set in C. We call C reduced if no set in C is a proper subset of another set in C, and acyclic if, in addition, the sets in C admit an ordering C1, . . . , Cm that has the running intersection property: for each 2 ≤ j ≤ m, the intersection Sj = Cj ∩ (C1 ∪ · · · ∪Cj−1) is a subset of some Ci with i < j. We call the sets Sj the separators. The multiset of separators, denoted by S, does not depend on the ordering and is thus unique for an acyclic hypergraph. Now, letting C be the set of cliques of the chordal graph G, it is known that the hypergraph C is acyclic and that each factor ψCj (xCj ) can be specified as the ratio p(xCj )/p(xSj ) of marginal probabilities (where we define p(xS1) = 1). Also the converse holds: by connecting all pairs of vertices within each set of an acyclic hypergraph we obtain a chordal graph.\nGiven multiple observations over the product state space, the data, we associate with each hypergraph C on V a score s(C) = ∏ C∈C p(C) /∏ S∈S p(S), where the local score p(A) measures the probability (density) of the data projected on A ⊆ V , possibly extended by some structure prior or penalization term. The structure learning problem is to find an acyclic hypergraph C on V that\n1We thank an anonymous reviewer of an earlier version of this work for noticing this fact, which apparently was not well known in the community, including the authors and reviewers of Corander’s et al. work [12].\nmaximizes the score s(C). This formulation covers a Bayesian approach, in which each p(A) is the marginal likelihood for the data on A under a Dirichlet–multinomial model [23, 7, 12], but also the maximum-likelihood formulation, in which each p(A) is the empirical probability of the data on A [23, 1]. Motivated by these instantiations, we will assume that for any given A the value p(A) can be efficiently computed, and we treat the values as the problem input.\nOur approach to the problem exploits the fact [22, Prop. 2.27] that a reduced hypergraph C is acyclic if and only if there is a junction tree T for C, that is, an undirected tree on the node set C that has the junction property (JP): for any two nodes A and B in C and any C on the unique path in T between A and B we have A ∩ B ⊆ C. Furthermore, by labeling each edge of T by the intersection of its endpoints, the edge labels amount to the multiset of separators of the hypergraph C. Thus a junction tree gives the separators explicitly, which motivates us to write s(T ) for the respective score s(C) and solve the structure learning problem by finding a junction tree T over V that maximizes s(T ). Here and henceforth, we say that a tree is over a set if the union of the tree’s nodes equals the set.\nAs our problem formulation does not explicitly refer to the underlying chordal graph and cliques, we will speak of junction trees instead of equivalent but semantically more loaded clique trees. From here on, a junction tree refers specifically to a junction tree whose node set is a reduced hypergraph."
    }, {
      "heading" : "3 Recursive characterization and dynamic programming",
      "text" : "The score of a junction tree obeys a recursive factorization along subtrees (by rooting the tree at any node), given in Section 3.2 below. While this is the essential structural property of the score for our dynamic programming algorithm, it does not readily yield the needed recurrence for the optimal score. Indeed, we need a characterization of, not a fixed junction tree, but the entire search space of junction trees that concords the factorization of the score. We next give such a characterization before we proceed to the derivation and analysis of the dynamic programming algorithm."
    }, {
      "heading" : "3.1 Recursive partition trees",
      "text" : "We characterize the set of junction trees by expressing the ways in which they can partition V . The idea is that when any tree of interest is rooted at some node, the subtrees amount to a partition of not only the remaining nodes in the tree (which holds trivially) but also the remaining vertices (contained in the nodes); and the subtrees also satisfy this property. See Figure 1 for an illustration.\nIf T is a tree over a set S, we write C(T ) for its node set and V (T ) for the union of its nodes, S. For a familyR of subsets of a set S, we say thatR is a partition of S and denoteR @ S if the members ofR are non-empty and pairwise disjoint, and their union is S. Definition 1 (Recursive partition tree, RPT). Let T be a tree over a finite set V , rooted at C ∈ C(T ). Denote byC1, . . . , Ck the children ofC, by Ti the subtree rooted atCi, and letRi = V (Ti)\\C. We say that T is a recursive partition tree (RPT) if it satisfies the following three conditions: (R1) each Ti is a RPT over Ci ∪Ri, (R2) {R1, . . . , Rk} @ V \\ C, and (R3) C ∩ Ci is a proper subset of both C and Ci. We denote by RPT(V,C) the set of all RPTs over V rooted at C.\nWe now present the following theorems to establish that, when edge directions are ignored, the definitions of junction trees and recursive partition trees are equivalent. Theorem 1. A junction tree T is a RPT when rooted at any C ∈ C(T ). Theorem 2. A RPT is a junction tree (when considered undirected).\nOur proofs of these results will use the following two observations: Observation 3. A subtree of a junction tree is also a junction tree. Observation 4. If T is a RPT, so is its every subtree rooted at any C ∈ C(T ).\nProof of Theorem 1. Let T be a junction tree over V and consider an arbitrary C ∈ C(T ). We show by induction over the number of nodes that T is a RPT when rooted at C. Let Ci, Ti, and Ri be defined as in Definition 1 and consider the three RPT conditions. If C is the only node in T , the conditions hold trivially. Assume they hold up to n− 1 nodes and consider the case |C(T )| = n. We show that each condition holds.\n(R1) By Observation 3 each Ti is a junction tree and thus, by the induction assumption, a RPT. It remains to show that V (Ti) = Ci ∪ Ri. By definition both Ci ⊆ V (Ti) and Ri ⊆ V (Ti). Thus Ci ∪ Ri ⊆ V (Ti). Assume then that x ∈ V (Ti), i.e. x ∈ C ′ for some C ′ ∈ C(Ti). If x /∈ Ri, then by definition x ∈ C. Since Ci is on the path between C and C ′, by JP x ∈ Ci. Therefore V (Ti) ⊆ Ci ∪Ri. (R2) We show that the sets Ri partition V \\ C. First, each Ri is non-empty since by definition of reduced hypergraph Ci is non-empty and not contained in C. Second, ⋃ iRi = ⋃ i(V (Ti) \\ C) =\n(C ∪ ⋃ i V (Ti)) \\C = ⋃ C(T ) \\C = V \\C. Finally, to see that Ri are pairwise disjoint, assume to the contrary that x ∈ Ri ∩Rj for distinct Ri and Rj . This implies x ∈ A ∩B for some A ∈ C(Ti) and B ∈ C(Tj). Now, by JP x ∈ C, which contradicts the definition of Ri. (R3) Follows by the definition of reduced hypergraph.\nProof of Theorem 2. Assume now that T is a RPT over V . We show that T is a junction tree. To see that T has JP, consider arbitrary A,B ∈ C(T ). We show that A ∩B is a subset of every C ∈ C(T ) on the path between A and B.\nConsider first the case that A is an ancestor of B and let B = C1, . . . , Cm = A be the path that connects them. We show by induction over m that C1 ∩ Cm ⊆ Ci for every i = 1, . . . ,m. The base case m = 1 is trivial. Assume m > 1 and the claim holds up to m− 1. If i = m, the claim is trivial. Let i < m. Denote by Tm−1 the subtree rooted at Cm−1 and let Rm−1 = V (Tm−1) \\ Cm. Since C1 ⊆ V (Tm−1) we have that C1 ∩ Cm = (C1 ∩ V (Tm−1)) ∩ Cm = C1 ∩ (Cm ∩ V (Tm−1)). By Observation 4 Tm−1 is a RPT. Therefore, from (R1) it follows that V (Tm−1) = Cm−1 ∪Rm−1 and thus Cm ∩ V (Tm−1) = (Cm ∩ Cm−1) ∪ (Cm ∩ Rm−1) = Cm ∩ Cm−1. Plugging this above and using the induction assumption we get C1 ∩ Cm = C1 ∩ (Cm ∩ Cm−1) ⊆ C1 ∩ Cm−1 ⊆ Ci. Consider now the case that A and B have a least common ancestor C. By Observation 4, the subtree rooted at C is a RPT. Thus, by (R1) and (R2) there are disjoint R and R′ such that A ⊆ C ∪R and B ⊆ C ∪R′. Thus, A ∩B ⊆ C, and consequently A ∩B ⊆ A ∩ C. As we proved above, A ∩ C is a subset of every node on the path between A and C, and therefore A ∩B is also a subset of every such node. Similarly, A ∩ B is a subset of every node on the path between B and C. Combining these results, we have that A ∩B is a subset of every node on the path between A and B. Finally, to see that C(T ) is reduced, assume the opposite, that A ⊆ B for distinct A,B ∈ C(T ). Let C be the node next to A on the path from A to B. By the initial assumption and JP A ⊆ A ∩B ⊆ C. As either A or C is a child of the other, this contradicts (R3) in the subtree rooted at the parent."
    }, {
      "heading" : "3.2 The main recurrence",
      "text" : "We want to find a junction tree T over V that maximizes the score s(T ). By Theorems 1 and 2 this is equivalent to finding a RPT T that maximizes s(T ). Let T be a RPT rooted at C and denote by C1, . . . , Ck the children of C and by Ti the subtree rooted at Ci. Then, the score factorizes as follows\ns(T ) = p(C) k∏\ni=1\ns(Ti) p(C ∩ Ci) . (1)\nTo see this, observe that each term of s(T ) is associated with a particular node or edge (separator) of T . Thus the product of the s(Ti) consists of exactly the terms of s(T ), except for the ones associated with the root C of T and the edges between C and each Ci.\nTo make use of the above factorization, we introduce suitable constraints under which an optimal tree can be constructed from subtrees that are, in turn, optimal with respect to analogous constraints (cf. Bellman’s principle of optimality). Specifically, we define a function f that gives the score of an optimal subtree over any subset of nodes as follows: Definition 2. For S ⊂ V and ∅ 6= R ⊆ V \\ S, let f(S,R) be the score of an optimal RPT over S ∪R rooted at a proper superset of S. That is\nf(S,R) = max S ⊂ C ⊆ S ∪ R T ∈RPT(S∪R,C)\ns(T ) .\nCorollary 5. The score of an optimal RPT over V is given by f(∅, V ).\nWe now show that f admits the following recurrence, which shall be used as the basis of our dynamic programming algorithm. Lemma 6. Let S ⊂ V and ∅ 6= R ⊆ V \\ S. Then\nf(S,R) = max S ⊂ C ⊆ S ∪ R\n{R1, . . . , Rk} @ R \\ C S1, . . . , Sk ⊂ C\np(C) k∏ i=1 f(Si, Ri) p(Si) .\nProof. We first show inductively that the recurrence is well defined. Assume that the conditions S ⊂ V and ∅ 6= R ⊆ V \\ S hold. Observe that R is non-empty, every set has a partition, and C is selected to be non-empty. Therefore, all three maximizations are over non-empty ranges and it remains to show that the product over i = 1, . . . , k is well defined. If |R| = 1, then R \\ C = ∅ and the product equals 1 by convention. Assume now that f(S,R) is defined when |R| < m and consider the case |R| = m. By construction Si ⊂ V , ∅ 6= Ri ⊆ V \\Si and |Ri| < |R| for every i = 1, . . . , k. Thus, by the induction assumption each f(Si, Ri) is defined and therefore the product is defined.\nWe now show that the recurrence indeed holds. Let the rootC in Definition 2 be fixed and consider the maximization over the trees T . By Definition 1, choosing a tree T ∈ RPT(S ∪R,C) is equivalent to choosing sets R1, . . . , Rk, sets C1, . . . , Ck, and trees T1, . . . , Tk such that (R0) Ri = V (Ti) \\ C, (R1) Ti is a RPT over Ci ∪Ri rooted at Ci, (R2) {R1, . . . , Rk} @ (S ∪R) \\C, and (R3) C ∩Ci is a proper subset of C and Ci.\nObserve first that (S ∪ R) \\ C = R \\ C and therefore (R2) is equivalent to choosing sets Ri such that {R1, . . . , Rk} @ R \\ C. Denote by Si the intersection C ∩ Ci. We show that together (R0) and (R1) are equivalent to saying that Ti is a RPT over Si ∪ Ri rooted at Ci. Assume first that the conditions are true. By (R1) it’s sufficient to show that Ci ∪ Ri = Si ∪ Ri. From (R1) it follows that Ci ⊆ V (Ti) and therefore Ci \\ C ⊆ V (Ti) \\ C, which by (R0) implies Ci \\ C ⊆ Ri. This in turn implies Ci ∪Ri = (Ci ∩C)∪ (Ci \\C)∪Ri = Si ∪Ri. Assume then that Ti is a RPT over Si ∪Ri rooted at Ci. Condition (R0) holds since V (Ti) \\C = (Si ∪Ri) \\C = (Si \\C) ∪ (Ri \\C) = ∅ ∪Ri = Ri. Condition (R1) holds since Si ⊆ Ci ⊆ V (Ti) = Si ∪Ri and thus Si ∪Ri = Ci ∪Ri. Finally observe that (R3) is equivalent to first choosing Si ⊂ C and then Ci ⊃ Si. By (R1) it must also be that Ci ⊆ V (Ti) = Si ∪Ri. Based on these observations, we can now write\nf(S,R) = max S ⊂ C ⊆ S ∪ R\n{R1, . . . , Rk} @ R \\ C S1,...,Sk⊂C\n∀i:Si⊂Ci⊆Ri∪Si ∀i:Ti is a RPT over Si ∪ Ri rooted at Ci\ns(T ) .\nNext we factorize s(T ) using the factorization (1) of the score. In addition, once a root C, a partition {R1, . . . , Rk}, and separators {S1, . . . , Sk} have been fixed, then each pair (Ci, Ti) can be chosen independently for different i. Thus, the above maximization can be written as\nmax S ⊂ C ⊆ S ∪ R\n{R1, . . . , Rk} @ R \\ C S1,...,Sk⊂C\np(C) k∏ i=1  1 p(Si) · max Si⊂Ci⊆Ri∪Si\nTi∈RPT(Si∪Ri,Ci)\ns(Ti)  . By applying Definition 2 to the inner maximization the claim follows."
    }, {
      "heading" : "3.3 Fast evaluation",
      "text" : "The direct evaluation of the recurrence in Lemma 6 would be very inefficient, especially since it involves maximization over all partitions of the vertex set. In order to evaluate it more efficiently, we decompose it into multiple recurrences, each of which can take advantage of dynamic programming.\nObserve first that we can rewrite the recurrence as\nf(S,R) = max S ⊂ C ⊆ S ∪ R\n{R1, . . . , Rk} @ R \\ C\np(C) k∏ i=1 h(C,Ri) , (2)\nwhere h(C,R) = max\nS⊂C f(S,R)\n/ p(S) . (3)\nWe have simply moved the maximization over Si ⊂ C inside the product and written each factor using a new function h. Due to how the sets C and Ri are selected, the arguments to h are always non-empty and disjoint subsets of V . In a similar fashion, we can further rewrite recurrence 2 as\nf(S,R) = max S⊂C⊆S∪R\np(C)g(C,R \\ C) , (4)\nwhere we define\ng(C,U) = max {R1,...,Rk}@U k∏ i=1 h(C,Ri) .\nAgain, note that C and U are disjoint and C is non-empty. If U = ∅, then g(C,U) = 1. Otherwise\ng(C,U) = max ∅6=R⊆U h(C,R) max {R2,...,Rk}@U\\R k∏ i=2 h(C,Ri) = max ∅ 6=R⊆U h(C,R)g(C,U \\R) . (5)\nThus, we have split the original recurrence into three simpler recurrences (4,5,3). We now obtain a straightforward dynamic programming algorithm that evaluates f , g and h using these recurrences with memoization, and then outputs the score f(∅, V ) of an optimal RPT."
    }, {
      "heading" : "3.4 Time and space requirements",
      "text" : "We measure the time requirement by the number of basic operations, namely comparisons and arithmetic operations, executed for pairs of real numbers. Likewise, we measure the space requirement by the maximum number of real values stored at any point during the execution of the algorithm. We consider both time and space in the more general setting where the width w ≤ n of the optimal network is restricted by selecting every node (clique) C in recurrence (4) with the constraint |C| ≤ w. We prove the following bounds by counting, for each of the three functions, the associated subset triplets that meet the applicable disjointness, inclusion, and cardinality constraints:\nTheorem 7. Let V be a set of size n and w ≤ n. Given the local scores of the subsets of V of size at most w as input, a maximum-score junction tree over V of width at most w can be found using 6 ∑w\ni=0 ( n i ) 3n−i basic operations and having a storage for 3 ∑w i=0 ( n i ) 2n−i real numbers.\nProof. To bound the number of basic operations needed, we consider the evaluation of each the functions f , g, and h using the recurrences (4,5,3). Consider first f . Due to memoization, the algorithm executes at most two basic operations (one comparison and one multiplication) per triplet (S,R,C), with S and R disjoint, S ⊂ C ⊆ S ∪R, and |C| ≤ w. Subject to these constraints, a set C of size i can be chosen in ( n i ) ways, the set S ⊂ C in at most 2i ways, and the set R \\C in 2n−i ways.\nThus, the number of basic operations needed is at most Nf = 2 ∑w\ni=0 ( n i ) 2n−i2i = 2n+1 ∑w i=0 ( n i ) .\nSimilarly, for h the algorithm executes at most two basic operations per triplet (C,R, S), with now C and R disjoint, |C| ≤ w, and S ⊂ C. A calculation gives the same bound as for f . Finally consider g. Now the algorithm executes at most two basic operations per triplet (C,U,R), with C and U disjoint, |C| ≤ w, and ∅ 6= R ⊆ U . A set C of size i can be chosen in ( n i ) ways, and the remaining n − i elements can be assigned into U and its subset R in 3n−i ways. Thus, the number of basic operations\nneeded is at most Ng = 2 ∑w\ni=0 ( n i ) 3n−i. Finally, it is sufficient to observe that there is a j such that(\nn i\n) 3n−i is larger than ( n i ) 2n when i ≤ j, and smaller when i > j. Now because both terms sum up\nto the same value 4n when i = 0, . . . , n, the bound Ng is always greater or equal to Nf .\nWe bound the storage requirement in a similar manner. For each function, the size of the first argument is at most w and the second argument is disjoint from the first, yielding the claimed bound.\nRemark 1. For w = n, the bounds for the number of basic operations and storage requirement in Theorem 7 become 6 · 4n and 3 · 3n, respectively. When w ≤ n/4, the former bound can be replaced by 6w ( n w ) 3n−w, since ( n i ) 3n−i ≤ ( n i+1 ) 3n−i−1 if and only if i ≤ (n− 3)/4.\nRemark 2. Memoization requires indexing with pairs of disjoint sets. Representing sets as integers allows efficient lookups to a two-dimensional array, using O(4n) space. We can achieve O(3n) space by mapping a pair of sets (A,B) to ∑n a=1 3\na−1Ia(A,B) where Ia(A,B) is 1 if a ∈ A, 2 if a ∈ B, and 0 otherwise. Each pair gets a unique index from 0 to 3n − 1 to a compact array. A naı̈ve evaluation of the index adds an O(n) factor to the running time. This can be improved to constant amortized time by updating the index incrementally while iterating over sets."
    }, {
      "heading" : "4 Experimental results",
      "text" : "We have implemented the presented algorithm in a C++ program Junctor (Junction Trees Optimally Recursively).2 In the experiments reported below, we compared the performance of Junctor and the integer linear programming based solver GOBNILP by Bartlett and Cussens [20]. While GOBNILP has been tailored for finding an optimal Bayesian network, it enables forbidding the so-called v-structures in the network and, thereby, finding an optimal chordal Markov network, provided that we use the BDeu score, as we have done, or some other special scoring function [23, 24]. We note that when forbidding v-structures, the standard score pruning rules [20, 25] are no longer valid.\nWe first investigated the performance on synthetic data generated from Bayesian networks of varying size and density. We generated 15 datasets for each combination of the number of vertices n from 8 to 18, maximum indegree k = 4 (sparse) or k = 8 (dense), and the number of samples m equaling 100, 1000, or 10,000, as follows: Along a random vertex ordering, we first drew for each vertex the number of its parents from the uniform distribution between 0 and k and then the actual parents uniformly at random from its predecessors in the vertex ordering. Next, we assigned each vertex two possible states and drew the parameters of the conditional distributions from the uniform distribution. Finally, from the obtained joint distribution, we drew m independent samples. The input for Junctor and\n2Junctor is publicly available at www.cs.helsinki.fi/u/jwkangas/junctor/.\nGOBNILP was produced using the BDeu score with equivalent sample size 1. For both programs, we varied the maximum width parameter w from 3 to 6 and, in addition, examined the case of unbounded width (w = ∞). Because the performance of Junctor only depends on n and w, we ran it only once for each combination of the two. In contrast, the performance of GOBNILP is very sensitive to various characteristics of the data, and therefore we ran it for all the combinations. All runs were allowed 4 CPU hours and 32 GB of memory. The results (Figure 2) show that for large widths Junctor scales better than GOBNILP (with respect to n), and even for low widths Junctor is superior to GOBNILP for smaller n. We found GOBNILP to exhibit moderate variance: 93% of all running times (excluding timeouts) were within a factor of 5 of the respective medians shown in Figure 2, while 73% were within a factor of 2. We observe that the running time of GOBNILP may behave “discontinuously” (e.g., small datasets around 15 vertices with width 4).\nWe also evaluated both programs on several benchmark instances taken from the UCI repository [26]. The datasets are summarized in Table 1. Figure 3 shows the results on the instances with at most 19 attributes, for which the runs were, again, allowed 4 CPU hours and 32 GB of memory. The results are qualitatively in well agreement with the results obtained with synthetic data. For example, solving the Bridges dataset on 12 attributes with width 5, takes less than one second by Junctor but around 7 minutes by GOBNILP. For the two 22-attribute datasets we allowed both programs one week of CPU time and 128 GB of memory. Junctor was able to solve each within 33 hours for w = 3 and within 74 hours for w = 4. GOBNILP was able to solve Hypothyroid up to w = 6 (in 24 hours, or less for small widths), but Mushroom only up to w = 3. For higher widths GOBNILP ran out of time."
    }, {
      "heading" : "5 Concluding remarks",
      "text" : "We have investigated the structure learning problem in chordal Markov networks. We showed that the commonly used scoring functions factorize in a way that enables a relatively efficient dynamic programming treatment. Our algorithm is the first that is guaranteed to solve moderate-size instances to the optimum within reasonable time. For example, whereas Corander et al. [12] report their algorithm took more than 3 days on an eight-variable instance, our Junctor program solves any eight-variable instance within 20 milliseconds. We also reported on the first evaluation of GOBNILP [20] for solving the problem, which highlighted the advantages of the dynamic programming approach."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the Academy of Finland, grant 276864. The authors thank Matti Järvisalo for useful discussions on constraint programming approaches to learning Markov networks."
    } ],
    "references" : [ {
      "title" : "Maximum likelihood bounded tree-width Markov networks",
      "author" : [ "N. Srebro" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Approximating discrete probability distributions with dependence trees",
      "author" : [ "C.K. Chow", "C.N. Liu" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1968
    }, {
      "title" : "Inducing features of random fields",
      "author" : [ "S. Della Pietra", "V.J. Della Pietra", "J.D. Lafferty" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1997
    }, {
      "title" : "PAC-learning bounded tree-width graphical models",
      "author" : [ "M. Narasimhan", "J.A. Bilmes" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2004
    }, {
      "title" : "Learning factor graphs in polynomial time and sample complexity",
      "author" : [ "P. Abbeel", "D. Koller", "A.Y. Ng" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Efficient principled learning of thin junction trees",
      "author" : [ "A. Chechetka", "C. Guestrin" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Parallell interacting MCMC for learning of topologies of graphical models",
      "author" : [ "J. Corander", "M. Ekdahl", "T. Koski" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "Learning bounded treewidth Bayesian networks",
      "author" : [ "G. Elidan", "S. Gould" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Efficient Markov network structure discovery using independence tests",
      "author" : [ "F. Bromberg", "D. Margaritis", "V. Honavar" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Bottom-up learning of Markov network structure",
      "author" : [ "J. Davis", "P. Domingos" ],
      "venue" : "In J. Fürnkranz and T. Joachims, editors,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Markov network structure learning: A randomized feature generation approach",
      "author" : [ "J. Van Haaren", "J. Davis" ],
      "venue" : "In J. Hoffmann and B. Selman, editors,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Learning chordal Markov networks by constraint satisfaction",
      "author" : [ "J. Corander", "T. Janhunen", "J. Rintanen", "H.J. Nyman", "J. Pensar" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Exact learning of bounded tree-width Bayesian networks",
      "author" : [ "J. Korhonen", "P. Parviainen" ],
      "venue" : "AISTATS, volume 31 of JMLR Proceedings,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Learning optimal bounded treewidth Bayesian networks via maximum satisfiability",
      "author" : [ "J. Berg", "M. Järvisalo", "B. Malone" ],
      "venue" : "JMLR.org,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Learning bounded tree-width Bayesian networks using integer linear programming",
      "author" : [ "P. Parviainen", "H.S. Farahani", "J. Lagergren" ],
      "venue" : "JMLR.org,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Finding optimal models for small gene networks",
      "author" : [ "S. Ott", "S. Imoto", "S. Miyano" ],
      "venue" : "World Scientific,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "Exact Bayesian structure discovery in Bayesian networks",
      "author" : [ "M. Koivisto", "K. Sood" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "A simple approach for finding the globally optimal Bayesian network structure",
      "author" : [ "T. Silander", "P. Myllymäki" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Learning optimal Bayesian networks: A shortest path perspective",
      "author" : [ "C. Yuan", "B. Malone" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Advances in Bayesian network learning using integer programming. In UAI, pages 182–191",
      "author" : [ "M. Bartlett", "J. Cussens" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Local computations with probabilities on graphical structures and their application to expert systems",
      "author" : [ "S.L. Lauritzen", "D.J. Spiegelhalter" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1988
    }, {
      "title" : "Graphical Models",
      "author" : [ "S.L. Lauritzen" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1996
    }, {
      "title" : "Hyper Markov laws in the statistical analysis of decomposable graphical models",
      "author" : [ "A.P. Dawid", "S.L. Lauritzen" ],
      "venue" : "The Annals of Statistics, 21(3):1272–1317,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1993
    }, {
      "title" : "Learning Bayesian networks: The combination of knowledge and statistical data",
      "author" : [ "D. Heckerman", "D. Geiger", "D.M. Chickering" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1995
    }, {
      "title" : "Efficient structure learning of Bayesian networks using constraints",
      "author" : [ "C.P. de Campos", "Q. Ji" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For example, Srebro [1] showed that finding a maximum-likelihood chordal (or triangulated or decomposable) Markov network is NP-hard even for networks of treewidth at most 2, in sharp contrast to the treewidth-1 case [2].",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "For example, Srebro [1] showed that finding a maximum-likelihood chordal (or triangulated or decomposable) Markov network is NP-hard even for networks of treewidth at most 2, in sharp contrast to the treewidth-1 case [2].",
      "startOffset" : 217,
      "endOffset" : 220
    }, {
      "referenceID" : 2,
      "context" : "Consequently, various approximative approaches and local search heuristics have been proposed [3, 1, 4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 94,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "Consequently, various approximative approaches and local search heuristics have been proposed [3, 1, 4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 94,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "Consequently, various approximative approaches and local search heuristics have been proposed [3, 1, 4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 94,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "Consequently, various approximative approaches and local search heuristics have been proposed [3, 1, 4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 94,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "Consequently, various approximative approaches and local search heuristics have been proposed [3, 1, 4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 94,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : "Consequently, various approximative approaches and local search heuristics have been proposed [3, 1, 4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 94,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "Consequently, various approximative approaches and local search heuristics have been proposed [3, 1, 4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 94,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "Consequently, various approximative approaches and local search heuristics have been proposed [3, 1, 4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 94,
      "endOffset" : 126
    }, {
      "referenceID" : 9,
      "context" : "Consequently, various approximative approaches and local search heuristics have been proposed [3, 1, 4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 94,
      "endOffset" : 126
    }, {
      "referenceID" : 10,
      "context" : "Consequently, various approximative approaches and local search heuristics have been proposed [3, 1, 4, 5, 6, 7, 8, 9, 10, 11].",
      "startOffset" : 94,
      "endOffset" : 126
    }, {
      "referenceID" : 11,
      "context" : "[12] published the first non-trivial algorithm that is guaranteed to find a globally optimal chordal Markov network.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12], our algorithm stems from a particular characterization of clique trees of chordal graphs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "While our recursive characterization of clique trees and the resulting dynamic programming algorithm are new, they are similar in spirit to a recent work by Korhonen and Parviainen [13].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : "[14] solve the problem by casting into maximum satisfiability, while Parviainen et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] cast into integer linear programming.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "For unbounded-treewidth Bayesian networks, O(2n(2))-time algorithms based on dynamic programming are available [16, 17, 18].",
      "startOffset" : 111,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "For unbounded-treewidth Bayesian networks, O(2n(2))-time algorithms based on dynamic programming are available [16, 17, 18].",
      "startOffset" : 111,
      "endOffset" : 123
    }, {
      "referenceID" : 17,
      "context" : "For unbounded-treewidth Bayesian networks, O(2n(2))-time algorithms based on dynamic programming are available [16, 17, 18].",
      "startOffset" : 111,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "However, none of these dynamic programming algorithms, nor their A* search based variant [19], enables adding the constraints of chordality or bounded width.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 19,
      "context" : "But the integer linear programming approach to finding optimal Bayesian networks, especially the recent implementation by Bartlett and Cussens [20], also enables adding the further constraints.",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 21,
      "context" : "For a gentler presentation and proofs, see Lauritzen and Spiegelhalter [21, Sections 6 and 7], Lauritzen [22], and references therein.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 22,
      "context" : "This formulation covers a Bayesian approach, in which each p(A) is the marginal likelihood for the data on A under a Dirichlet–multinomial model [23, 7, 12], but also the maximum-likelihood formulation, in which each p(A) is the empirical probability of the data on A [23, 1].",
      "startOffset" : 145,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : "This formulation covers a Bayesian approach, in which each p(A) is the marginal likelihood for the data on A under a Dirichlet–multinomial model [23, 7, 12], but also the maximum-likelihood formulation, in which each p(A) is the empirical probability of the data on A [23, 1].",
      "startOffset" : 145,
      "endOffset" : 156
    }, {
      "referenceID" : 11,
      "context" : "This formulation covers a Bayesian approach, in which each p(A) is the marginal likelihood for the data on A under a Dirichlet–multinomial model [23, 7, 12], but also the maximum-likelihood formulation, in which each p(A) is the empirical probability of the data on A [23, 1].",
      "startOffset" : 145,
      "endOffset" : 156
    }, {
      "referenceID" : 22,
      "context" : "This formulation covers a Bayesian approach, in which each p(A) is the marginal likelihood for the data on A under a Dirichlet–multinomial model [23, 7, 12], but also the maximum-likelihood formulation, in which each p(A) is the empirical probability of the data on A [23, 1].",
      "startOffset" : 268,
      "endOffset" : 275
    }, {
      "referenceID" : 0,
      "context" : "This formulation covers a Bayesian approach, in which each p(A) is the marginal likelihood for the data on A under a Dirichlet–multinomial model [23, 7, 12], but also the maximum-likelihood formulation, in which each p(A) is the empirical probability of the data on A [23, 1].",
      "startOffset" : 268,
      "endOffset" : 275
    }, {
      "referenceID" : 19,
      "context" : "2 In the experiments reported below, we compared the performance of Junctor and the integer linear programming based solver GOBNILP by Bartlett and Cussens [20].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 22,
      "context" : "While GOBNILP has been tailored for finding an optimal Bayesian network, it enables forbidding the so-called v-structures in the network and, thereby, finding an optimal chordal Markov network, provided that we use the BDeu score, as we have done, or some other special scoring function [23, 24].",
      "startOffset" : 287,
      "endOffset" : 295
    }, {
      "referenceID" : 23,
      "context" : "While GOBNILP has been tailored for finding an optimal Bayesian network, it enables forbidding the so-called v-structures in the network and, thereby, finding an optimal chordal Markov network, provided that we use the BDeu score, as we have done, or some other special scoring function [23, 24].",
      "startOffset" : 287,
      "endOffset" : 295
    }, {
      "referenceID" : 19,
      "context" : "We note that when forbidding v-structures, the standard score pruning rules [20, 25] are no longer valid.",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 24,
      "context" : "We note that when forbidding v-structures, the standard score pruning rules [20, 25] are no longer valid.",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "[12] report their algorithm took more than 3 days on an eight-variable instance, our Junctor program solves any eight-variable instance within 20 milliseconds.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "We also reported on the first evaluation of GOBNILP [20] for solving the problem, which highlighted the advantages of the dynamic programming approach.",
      "startOffset" : 52,
      "endOffset" : 56
    } ],
    "year" : 2014,
    "abstractText" : "We present an algorithm for finding a chordal Markov network that maximizes any given decomposable scoring function. The algorithm is based on a recursive characterization of clique trees, and it runs in O(4) time for n vertices. On an eight-vertex benchmark instance, our implementation turns out to be about ten million times faster than a recently proposed, constraint satisfaction based algorithm (Corander et al., NIPS 2013). Within a few hours, it is able to solve instances up to 18 vertices, and beyond if we restrict the maximum clique size. We also study the performance of a recent integer linear programming algorithm (Bartlett and Cussens, UAI 2013). Our results suggest that, unless we bound the clique sizes, currently only the dynamic programming algorithm is guaranteed to solve instances with around 15 or more vertices.",
    "creator" : null
  }
}