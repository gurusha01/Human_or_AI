{
  "name" : "c0c7c76d30bd3dcaefc96f40275bdc0a.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Zeta Hull Pursuits: Learning Nonconvex Data Hulls",
    "authors" : [ "Yuanjun Xiong", "Wei Liu", "Deli Zhao", "Xiaoou Tang" ],
    "emails" : [ "yjxiong@ie.cuhk.edu.hk", "xtang@ie.cuhk.edu.hk", "weiliu@us.ibm.com", "zhao@htc.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In the era of big data, a natural idea is to select a small subset of m samples Ce = {xe1 , . . . ,xem} from a whole set of n data points X = {x1, . . . ,xn} such that the selected points Ce can capture the underlying properties or structures of X . Then machine learning and data mining algorithms can be carried out with Ce instead of X , thereby leading to significant reductions in computational and space complexities. Let us write the matrix forms of Ce and X as C = [xe1 , . . . ,xem ] ∈ Rd×m and X = [x1, . . . ,xn] ∈ Rd×n, respectively. Here d is the dimensions of input data points. In other words, C is a column subset selection of X. The task of selecting C from X is also called by column sampling in the literature, and maintains importance in a variety of fields besides machine learning, such as signal processing, geoscience and remote sensing, and applied mathematics. This paper concentrates on solving the column sampling problem by means of graph-theoretic methods.\nExisting methods in column sampling fall into two main categories according to their objectives: 1) approximate the data matrix X, and 2) discover the underlying data structures. For machine learning methods using kernel or similar “N-Body” techniques, the Nyström matrix approximation is usually applied to approximate large matrices. Such circumstances include fast training of nonlinear kernel support vector machines (SVM) in the dual form [30], spectral clustering [8], manifold learning [25], etc. Minimizing a relative approximation error is typically harnessed as the objective of column sampling, by which the most intuitive solution is to perform uniform sampling [30]. Other non-uniform sampling schemes choose columns via various criteria, such as probabilistic samplings according to diagonal elements of a kernel matrix [7], reconstruction errors [15], determinant measurements [1], cluster centroids [33], and statistical leverage scores [21]. On the other hand, column sampling\nmay be cast into a combinatorial optimization problem, which can be tackled by using greedy strategies in polynomial time [4] and boosted by using advanced sampling strategies to further reduce the relative approximation error [14].\nFrom another perspective, we are aware that data points may form some interesting structures. Understanding these structures has been proven beneficial to approximate or represent data inputs [11]. One of the most famous algorithms for dimensionality reduction, Non-negative Matrix Factorization (NMF) [16], learns a low-dimensional convex hull from data points through a convex relaxation [3]. This idea was extended to signal separation by pursuing a convex hull with a maximized volume [27] to enclose input data points. Assuming that vertices are equally distant, the problem of fitting a simplex with a maximized volume to data reduces to a simple greedy column selection procedure [26]. The simplex fitting approach demonstrated its success in face recognition tasks [32]. Parallel research in geoscience and remote sensing is also active, where the vertices of a convex hull are coined as endmembers or extreme points, leading to a classic “N-Finder” algorithm [31]. The above approaches tried to learn data structures that are usually characterized by convexity. Hence, they may fail to reveal the intrinsic data structures when the distributions of data points are diverse, e.g., data being on manifolds or concave structures. Probabilistic models like Determinantal Point Process (DPP) [13] measure data densities, so they are likely to overcome the convexity issue. However, few previous work accessed structural information of possibly nonconvex data for column sampling/subset selection tasks.\nThis paper aims to address the issue of learning nonconvex structures of data in the case where the data distributions can be arbitrary. More specifically, we learn a nonconvex hull to encapsulate the data structure. The on-hull points tightly enclose the dataset but do not need to form a convex set. Thus, nonconvex hulls can be more adaptive to capture practically complex data structures. Akin to convex hull learning, our proposed approach also extracts extreme points from an input dataset. To complete this task, we start with exploring the property of graph cycles in a neighborhood graph built over the input data points. Using cycle-based measures to characterize data structures has been proven successful in clustering data of multiple types of distributions [34]. To induce a measure of structural complexities stemming from graph cycles, we introduce the Zeta Function which applies the integration of graph cycles to model the linkage properties of the neighborhood graph. The key advantage of the Zeta function is uniting both global and local connection properties of the graph. As such, we are able to learn a hull which encompasses almost all input data points but is not necessary to be convex. With structural complexities captured in the form of the Zeta function, we present a leave-one-out strategy to find the extreme points. The basic idea is that removing the on-hull points only has weak impact on structural complexities of the graph. The decision of removal will be based on extremeness of a data point. Our model, dubbed Zeta Hulls, is derived by computing and analyzing the extremeness of data points. The greedy pursuit of the Zeta Hull model requires the computation of the inversion of a matrix obtained from the graph affinity matrix, which is computationally prohibitive for massive-scale data. To accelerate such a matrix manipulation, we employ the Anchor Graph [18] technique in the sense that the original graph can be approximated with respect to the anchors originating from a randomly sampled data subset. Our model is testified through extensive experiments on toy data and real-world text and image datasets. Experimental results show that in terms of unsupervised data representation learning, the Zeta Hull based methods outperform the state-of-the-art methods used in convex hull learning, clustering, matrix factorization, and dictionary learning."
    }, {
      "heading" : "2 Nonconvex Hull Learning",
      "text" : "To elaborate on our approach, we first introduce and define the point extremeness. It measures the degree of a data point being prone to lie on or near a nonconvex hull by virtue of a neighborhood graph drawn from an input dataset. As an intuitive criterion, the data point with strong connections in the graph should have the low point extremeness. To obtain the extremeness measure, we need to explore the underlying structure of the graph, where graph cycles are employed."
    }, {
      "heading" : "2.1 Zeta Function and Structural Complexity",
      "text" : "We model graph cycles by means of a sum-product rule and then integrate them using a Zeta function. There are many variants of original Riemann Zeta Function, one of which is specialized in\nweighted adjacency graphs. Applying the theoretical results of Zeta functions provides us a powerful tool for characterizing structural complexities of graphs. The numerical description of graph structures will play a critical role in column sampling/subset selection tasks.\nFormally, given a graph G(X , E) with n nodes being data points in X = {xi}ni=1, let the n × n matrix W denote the weighted adjacency (or affinity) matrix of the graph G built over the dataset X . Usually the graph affinities are calculated with a proper distance metric. To be generic, we assume that G is directed. Then an edge leaving from xi to xj is denoted as eij . A path of length from xi to xj is defined as P (i, j, ) = {ehktk} k=1 with h1 = i and t = j. Note that the nodes in this path can be duplicate. A graph cycle, as a special case of paths of length , is also defined as γ = P (i, i, ) (i = 1, . . . , n). The sum-product path affinity ν for all -length cycles can then be computed by ν = ∑ γ ∈κ νγ = ∑ γ ∈κ wt −1h1 ∏ −1 k=1 whktk , where κ denotes the set of all possible cycles of length and whktk denotes the (hk, tk)-entry of W, i.e., the affinity from node xhk to node xtk . The edge et −1h1 is the last edge that closes the cycle. The computed compound affinity ν provides a measure for all cyclic connections of length . Then we integrate such affinities for the cycles of lengths being from one to infinity to derive the graph Zeta function as follows,\nζz(G) = exp ( ∞∑ =1 ν z ) , (1)\nwhere z is a constant. We only consider the situation where z is real-valued. The Zeta function in Eq. (1) has been proven to enjoy a closed form. Its convergence is also guaranteed when z < 1/ρ(W), where ρ(W) is the spectral radius of W. These lead to Theorem 1 [23]. Theorem 1. Let I be the identity matrix and ρ(W) be the spectral radius of the matrix W, respectively. If 0 < z < 1/ρ(W), then ζz(G) = 1/ det(I− zW). Note that W can be asymmetric, implying that λi can be complex. In this case, Theorem 1 still holds. Theorem 1 indicates that the graph Zeta function we formulate in Eq. (1) provides a closedform expression for describing the structural complexity of a graph. The next subsection will give the definition of the point extremeness by analyzing the structural complexity."
    }, {
      "heading" : "2.2 Zeta Hull Pursuits",
      "text" : "From now on, for simplicity we use G = ζz(G) to represent the structural complexity of the original graph G. To measure the point extremeness numerically, we perform a leave-one-out strategy in the sense that each point in C is successively left out and the variation of G is investigated. This is a natural way to pursue extreme points, because if a point xj lies on the hull it has few communications with the other points. After removing this point and its corresponding edges, the reductive structural complexity of the remaining graph G/xj , which we denote as G/xj , will still be close to G. Hence, the point extremeness εxj is modeled as the relative change of the structural complexity G, that is εxj =\nG G/xj . Now we have the following theorem.\nTheorem 2. Given G and G/xj as in Theorem 1, the point extremeness measure εxj of point xj satisfies εxj = (I − zW)−1(jj), i.e., the point extremeness measure of point xj is equal to the j-th diagonal entry of the matrix (I− zW)−1.\nAlgorithm 1 Zeta Hull Pursuits Input: A dataset X , the number m of data points to be selected, and free parameters z, λ and k. Output: The hull of sampled columns Ce := Cm+1. Initialize: construct W, C1 ← ∅, X1 = X , c1 = 0, and W1 = W for i = 1 to m do\nεxj := (I− zWi)−1(jj), for xj ∈ Xi xei := argminxj∈Xi(εxj + λ i e j Wci) Ci+1 := Ci ∪ xei ci+1 := ci + eeiXi+1 := Ci/xei Wi+1 := Wi with the ei-th row and column removed\nend for\nAccording to previous analysis, the data point with a small εxj tends to be on the hull and therefore has a strong extremeness. To seek the on-hull points, we need to select a subset of m points Ce = {xe1 , . . . ,xem} from X such that they have the strongest point extremenesses. We formulate this goal into the following optimization problem:\nCe = argmin C⊂X g(C) + λc Wc, (2)\nwhere c is a selection vector with m nonzero elements cei = 1 (i = 1, . . . ,m), and g(C) is the function which measures the impact on the structural complexity after removing the extracted points. In our case, g(C) = ∑mi=1 εxci . The second term in Eq. (2) is a regularization term enforcing that the selected data points do not intersect with each other. It will enable the selection process to have a better representative capability. The parameter λ controls the extent of the regularization.\nNaively solving the combinatorial optimization problem in Eq. (2) requires exponential time. By adopting a greedy strategy, we can solve this optimization problem in an iterative manner and with a feasible time complexity. Specifically, in each iteration we extract one point from the current data set and add it to the subset of the selected points. Sticking to this greedy strategy, we will attain the desired m on-hull points after m iterations. In the i-th iteration, we extract the point xei according to the criterion\nxei = argmin xj∈Xi−1\nεxj + λ\ni e j Wci−1, (3)\nwhere ej is the j-th standard basis vector, and ci−1 is the selection vector according to i−1 selected points before the i-th iteration.\nWe name our algorithm Zeta Hull Pursuits in order to emphasize that we use the Zeta function to pursue the nonconvex data hull. Algorithm 1 summarizes the Zeta Hull Pursuits algorithm."
    }, {
      "heading" : "3 Zeta Hull Pursuits via Anchors",
      "text" : "Algorithm 1 is applicable to small to medium-scale data X due to its cubical time complexity and quadratic space complexity with respect to the data size |X |. Here we propose a scalable algorithm facilitated by a reasonable prior to tackle the nonconvex hull learning problem efficiently. The idea is to build a low-rank approximation to the graph adjacency matrix W with a small number of sampled data points, namely anchor points. We resort to the Anchor Graph technique [18], which has been successfully applied to handle large-scale hashing[20] and semi-supervised learning problems."
    }, {
      "heading" : "3.1 Anchor Graphs",
      "text" : "The anchor graph framework is an elegant way to approximate neighborhood graphs. It first chooses a subset of l anchor points U = {uj}lj=1 from X . Then for each data point in X , its s nearest anchors in U are sought, thereby forming an s-nearest anchor graph. The anchor graph theory assumes that the original graph affinity matrix W can be reconstructed from the anchor graph with a small number of anchors (l n). Anchor points can be selected by random sampling or a rough clustering process. Many algorithms are available to embed a data point to its s nearest anchor points, as suggested in [18]. Here we adopt the simplest approach to build the anchor embedding matrix Ĥ;\nsay, ĥij = { exp (−d2ij/σ2) , uj ∈ {s nearest anchors of xi} 0, otherwise , where dij is the distance from data\nAlgorithm 2 Anchor-based Zeta Hull Pursuits Input: A dataset X , the number m of data points to be sampled, the number l of anchors, the number s of nearest anchors, and a free parameter z. Output: The hull of sampled columns Ce := Cm+1. Initialize: construct H, X1 = X , C1 = ∅, and H1 = H for i = 1 to m do\nperform SVD to obtain Hi := UΣVT εxj := z ∑l k=1 λ2j 1−zλ2k (Ujk) 2, for xj ∈ Xi\nxei := argminxj∈Xi(εxj + ∑\nxt∈Ci λ i hjht )\nCi+1 := Ci ∪ xeiXi+1 := Xi/xei Hi+1 := Hi with the ei-th row removed\nend for\npoint xi to anchor uj , and σ is a parameter controlling the bandwidth of the exponential function. The matrix Ĥ is then normalized so that its every row sums to one. In doing so, we can approximate the affinity matrix of the original graph as Ŵ = ĤΛ−1Ĥ , where Λ is a diagonal matrix whose i-th diagonal element is equal to the sum of the i-th column of Ĥ. As a result, all matrix manipulations upon the original graph affinity matrix W can be approximated by substituting the anchor graph affinity matrix Ŵ for W."
    }, {
      "heading" : "3.2 Extremeness Computation via Anchors",
      "text" : "Note that the computation of the point extremeness for εxj depends on the diagonal elements of (I − zW)−1. Using the anchor graph technique, we can write (I − zW)−1 = (I − zHH )−1, where H = ĤΛ− 1 2 . Thus we have the following theorem that enables an efficient computation of εxj . The proof is detailed in the supplementary material.\nTheorem 3. Let the singular vector decomposition of H be H = UΣV , where Σ = diag(λ1, . . . , λl). If H H is not singular, then ε−1xj = 1 + z ∑l k=1 λ2k 1−zλ2k (Ujk) 2, where U = HVΣ−1 and Ujk denotes the (i, j)-th entry of U.\nTheorem 3 reveals that the major computation of εxj will reduce to the eigendecomposition of a much smaller matrix H H, which results in a direct acceleration of the Zeta hull pursuit process. At the same time, the second term of Eq. (3) encountered in the i-th iteration can be estimated by e j Wci = 1 i ∑ xt∈Ci hjht\n, where hj denotes the j-th row of H and ci−1 is the selection vector of the extracted point set before the i-th iteration. These lead to the Anchor-based Zeta Hull Pursuits algorithm shown in Algorithm 2."
    }, {
      "heading" : "3.3 Downdating SVD",
      "text" : "In Algorithm 2, the singular value decomposition dominates the total time cost. We notice that reusing information in previous iterations can save the computation time. The removal of one row from H is equivalent to a rank-one modification to the original matrix. Downdating SVD [10] was proposed to handle this operation. Given the diagonal singular value matrix Σi and the point xei chosen in the i-th iteration, the singular value matrix Σi+1 for the next iteration can be calculated by the eigendecomposition of an l× l matrix D derived from Σi, where D = (I− 11+μheih ei)Σi, and μ2 + ‖hei‖22 = 1. The decomposition of D can be efficiently performed in O(l2) time [10]. Then the computation of Ui+1 is achieved by a multiplication of Ui with an l × l matrix produced by the decomposition operation on D, which permits a natural parallelism. Consequently, we can further accelerate Algorithm 2 by using a parallel computing scheme."
    }, {
      "heading" : "3.4 Complexity Analysis",
      "text" : "We now analyze the complexities of Algorithms 1 and 2. For Algorithm 1, the most time-consuming step is to solve the matrix inverse of n × n size, which costs a time complexity of O(n3). The overall time complexity is thus O(mn3) for extracting m points. In the implementation we can use\nthe sparse matrix computation to reduce the constant factor [5]. For Algorithm 2, the most timeconsuming step is to perform SVD over H, so the overall time complexity is O(mnl2). Leveraging downdating SVD, we only need to calculate the full SVD of H once in O(nl2) time and iteratively update the decomposition in O(l2) time per iteration. The matrix multiplication operation then dominates the total time cost. Also, it can be parallelized using a multi-core CPU or a modern GPU, resulting in a very small constant factor in the time complexity. Since l is usually less than 10% of n, Algorithm 2 is orders of magnitude faster than Algorithm 1. For cases where l needs to be relatively large (20% of n for example), the computational cost will not show a considerable increase since H is usually a very sparse matrix."
    }, {
      "heading" : "4 Experiments",
      "text" : "The Zeta Hull model aims at learning the structures of dataset. We evaluate how well our model achieves this goal by performing classification experiments. For simplicity, we abbreviate our algorithms as follows: the original Zeta Hull Pursuit algorithm (Algorithm 1), ZHP and its anchor version (Algorithm 2), A-ZHP. To compare with the state-of-the-art, we choose some renowned methods: K-medoids, CUR matrix factorization (CUR) [29], simplex volume maximization (Simplex) [26], sparse dictionary learning (DictLearn) [22] and convex non-negative matrix factorization (C-NMF) [6]. Basically, we use the extracted data points to learn a representation for each data point in an unsupervised manner. Classification is done by feeding the representation into a classifier. The representation will be built in two ways: 1) the sparse coding [22] and 2) the locality simplex coding [26]. To differentiate our algorithms from the original anchor graph framework, we conduct a set of experiments using the left singular vectors of the anchor embedding matrix H as the representation. In these experiments, anchors used in the anchor graph technique are randomly selected from the training set. To compare with existing low-dimension embedding approaches, we run the Large-Scale Manifold method [24] using the same number of landmarks as that of extracted points."
    }, {
      "heading" : "4.1 Toy Dataset",
      "text" : "First we illustrate our algorithms on a toy dataset. The dataset, commonly known as ”the two moons”, consists of 2000 data points on the 2D plane which are manifold-structured and comprise nonconvex distributions. This experiment on the two moons provides illustrative results of our algorithms in the presence of nonconvexity. We select different numbers of column subsets m = {20, 40, 80, 200} and compare with various other methods. A visualization of the results is shown in Figure 2. We can see that our algorithms can extract the nonconvex hull of the data cloud more accurately."
    }, {
      "heading" : "4.2 Text and Image Datasets",
      "text" : "For the classification experiments in this section, we derive the two types of data representations (the sparse coding and the local simplex coding) from the points/columns extracted by compared meth-\nods. By measuring the performance of applying these representations to solving the classification tasks, we can evaluate the representative power of the compared point/column selection methods.\nThe sparse coding is widely used for obtaining the representation for classification. Here a standard 1-regularized projection algorithm (LASSO) [22] is adopted to learn the sparse representation from the extracted data points. LASSO will deliver a sparse coefficient vector, which is applied as the representation of the data point. We use “SC” to indicate the related results in Table 1 and Table 2.\nThe local simplex coding reconstructs one data point as a convex combination of a set of nearest exemplar points, which form local simplexes [26]. Imposing this convex reconstruction constraint leads to non-negative combination coefficients. The sparse coefficients vector will be used as data representation. “LSC” indicates the related results in Table 1 and Table 2.\nThe classification pipeline is as follows. After extracting m points/columns from the training set, all data points will be represented with these selected points using the two approaches above. Then we feed the representations into a linear SVM for the training and testing. The better classification accuracy will reveal the stronger representative power of the column selection algorithm. In all experiments, the parameter z is fixed at 0.05 to guarantee the convergence of the Zeta function. We find that final results are robust to z once the convergence is guaranteed. For the A-ZHP algorithm, the parameter s is fixed at 10 and the number of anchor points l is set as 10% of the training set size. The bandwidth parameter σ of the exponential function is tuned on the training set to obtain a reasonable anchor embedding.\nThe classification of text contents relies on the informative representation of the plain words or sentences. Two text datasets are adopted for classification, i.e. the TDT2 dataset and the Newsgroups dataset [2]. In experiments, a subset of TDT2 is used (TDT2-30). It has 9394 samples from 30 classes. Each feature vector is of 36771 dimensions and normalized into unit length. The training set contains 6000 samples randomly selected from the dataset and rest of the samples are used for\ntesting. The parameter m is set to be 500 and 1000 on this dataset. The Newsgroups dataset contains 18846 samples from 20 classes. The training set contains 11314, while the testing set has 7532. The two sets are separated in advance [2] and ordered in time sequence to be more challenging for classifiers. The parameter m is set to be 500 and 1000 on this dataset. The classification results are reported in Table 1.\nFor object and face recognition tasks we conduct experiments under three classic scenarios, the hand-written digits classification, the image recognition, and the human face recognition. Related experimental results are reported in Table 1 and Table 2. The MNIST dataset serves as a standard benchmark for machine learning algorithms. It contains 10 classes of images corresponding to hand-written numbers from 0 to 9. The training set has 60000 images and the testing set has 10000 images. Each sample is a 784-dimensional vector. The Caltech101 dataset [17] is a widely used benchmark for object recognition systems. It consists of images from 102 classes of objects (101 object classes and one background class). We randomly select 30 labeled images from every class for training the classifier and 3000 images for testing. The recognition rates averaged over all classes are reported. Every image is processed into a feature vector of 21504 dimensions by the method in [28]. We also conduct experiment on a feature subset of the top 5000 dimensions (Caltech101-5k). In this experiment, m is set to be 500 and 1000. On-hull points are extracted on the training set. The MultiPIE human face dataset is a widely applied benchmark for face recognition [9]. We follow a standard gallery-probe protocol of face recognition. The testing set is divided into the gallery set and the probe set. The identity predication of a probe image comes from its nearest neighbor of Euclidean distance in the gallery. We randomly select 30, 000 images of 200 subjects as the training set for learning the data representation. Then we pick out 3000 images of the other 100 subjects (L = 30) to form the gallery set and 6000 images as the probes. The head poses of all these faces are between ±15 degrees. Each face image is processed into a vector of 5000 dimensions using the local binary pattern descriptor and PCA. We vary the parameter m from 500 to 2000 to evaluate the influence of number of sampled points. Discussion. For the experiments on these high-dimensional datasets, the methods based on the Zeta Hull model outperform most compared methods and also show promising performance improvements over raw data representation. When the number of extracted points grows, the resulting classification accuracy increases. This corroborates that the Zeta Hull model can effectively capture intrinsic structures of given datasets. More importantly, the discriminative information is preserved through learning these Zeta hulls. The representation yielded by the Zeta Hull model is sparse and of manageable dimensionality (500-2000), which substantially eases the workload of classifier training. This property is also favorable for tackling other large-scale learning problems. Due to the graph-theoretic measure that unifies the local and global connection properties of a graph, the Zeta Hull model leads to better data representation compared against existing graph-based embedding and manifold learning methods. For the comparison with the Large-Scale Manifold method [24] on the MultiPIE dataset, we find that even using 10K landmarks, its accuracy is still inferior to our methods relying on the Zeta Hull model. We also notice that noise may also affect the quality of Zeta hulls. This difficulty can be circumvented by running a number of well-established outlier removal methods such as [19]."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we proposed a geometric model, dubbed Zeta Hulls, for column sampling through learning nonconvex hulls of input data. The Zeta Hull model was built upon a novel graph-theoretic measure which quantifies the point extremeness to unify local and global connection properties of individual data point in an adjacency graph. By means of the Zeta function defined on the graph, the point extremeness measure amounts to the diagonal elements of a matrix related to the graph adjacency matrix. We also reduced the time and space complexities for computing a Zeta hull by incorporating an efficient anchor graph technique. A synthetic experiment first showed that the Zeta Hull model can detect appropriate hulls for non-convexly distributed data. The extensive real-world experiments conducted on benchmark text and image datasets further demonstrated the superiority of the Zeta Hull model over competing methods including convex hull learning, clustering, matrix factorization, and dictionary learning. Acknowledgement This research is partially supported by project #MMT-8115038 of the Shun Hing Institute of Advanced Engineering, The Chinese University of Hong Kong."
    } ],
    "references" : [ {
      "title" : "Spectral methods in machine learning and new strategies for very large datasets",
      "author" : [ "M.-A. Belabbas", "P.J. Wolfe" ],
      "venue" : "PNAS, 106(2):369–374,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Probabilistic dyadic data analysis with local and global consistency",
      "author" : [ "D. Cai", "X. Wang", "X. He" ],
      "venue" : "Proc. ICML,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Low dimensional polytope approximation and its application to nonnegative matrix factorization",
      "author" : [ "M. Chu", "M. Lin" ],
      "venue" : "SIAM Journal of Computing, pages 1131–1155,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Submodular meets spectral: greedy algorithms for subset selection, sparse approximation and dictionary selection",
      "author" : [ "A. Das", "D. Kempe" ],
      "venue" : "Proc. ICML,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "SPARSEINV: a MATLAB toolbox for computing the sparse inverse subset using the Takahashi equations",
      "author" : [ "T. Davis" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Convex and semi-nonnegative matrix factorizations",
      "author" : [ "C. Ding", "T. Li", "M. Jordan" ],
      "venue" : "TPAMI, 32(1):45–55,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On the Nyström method for approximating a gram matrix for improved kernel-based learning",
      "author" : [ "P. Drineas", "M. Mahoney" ],
      "venue" : "JMLR, 6:2153–2175,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Spectral grouping using the Nyström method",
      "author" : [ "C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik" ],
      "venue" : "TPAMI, 26:214–225,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multi-pie",
      "author" : [ "R. Gross", "I. Matthews", "J. Cohn", "T. Kanade", "S. Baker" ],
      "venue" : "Proc. Automatic Face Gesture Recognition, pages 1–8, Sept",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Downdating the singular value decomposition",
      "author" : [ "M. Gu", "S.C. Eisenstat" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications, 16(3):793–810,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "The elements of statistical learning, volume 1",
      "author" : [ "T. Hastie", "R. Tibshirani", "J.J.H. Friedman" ],
      "venue" : "Springer New York,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Finding groups in data: an introduction to cluster analysis, volume 344",
      "author" : [ "L. Kaufman", "P.J. Rousseeuw" ],
      "venue" : "John Wiley & Sons,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Determinantal point processes for machine learning",
      "author" : [ "A. Kulesza", "B. Taskar" ],
      "venue" : "Foundations and Trends in Machine Learning, 5(2–3),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Ensemble Nyström method",
      "author" : [ "S. Kumar", "M. Mohri", "A. Talwalkar" ],
      "venue" : "NIPS 23,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Sampling methods for the Nyström method",
      "author" : [ "S. Kumar", "M. Mohri", "A. Talwalkar" ],
      "venue" : "JMLR, 13(1):981– 1006,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning the parts of objects by non-negative matrix factorization",
      "author" : [ "D.D. Lee", "H.S. Seung" ],
      "venue" : "Nature, 401(6755):788–791,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories",
      "author" : [ "F. Li", "B. Fergus", "P. Perona" ],
      "venue" : "CVIU, 106(1):59–70,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Large graph construction for scalable semi-supervised learning",
      "author" : [ "W. Liu", "J. He", "S.-F. Chang" ],
      "venue" : "Proc. ICML,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Unsupervised one-class learning for automatic outlier removal",
      "author" : [ "W. Liu", "G. Hua", "J. Smith" ],
      "venue" : "Proc. CVPR,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Hashing with graphs",
      "author" : [ "W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang" ],
      "venue" : "Proc. ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Cur matrix decompositions for improved data analysis",
      "author" : [ "M.W. Mahoney", "P. Drineas" ],
      "venue" : "PNAS, 106(3):697–702,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Online learning for matrix factorization and sparse coding",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "JMLR, 11:19–60,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The Zeta-function and gibbs measures",
      "author" : [ "S. Savchenko" ],
      "venue" : "Russian Mathematical Surveys, 48(1):189–190,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Large-scale SVD and manifold learning",
      "author" : [ "A. Talwalkar", "S. Kumar", "M. Mohri", "H. Rowley" ],
      "venue" : "JMLR, 14(1):3129–3152,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Large-scale manifold learning",
      "author" : [ "A. Talwalkar", "S. Kumar", "H. Rowley" ],
      "venue" : "Proc. CVPR,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Yes we can: simplex volume maximization for descriptive web-scale matrix factorization",
      "author" : [ "C. Thurau", "K. Kersting", "C. Bauckhage" ],
      "venue" : "Proc. CIKM,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Nonnegative least correlated component analysis for separation of dependent sources by volume maximization",
      "author" : [ "F. Wang", "C. Chi", "T. Chan", "Y. Wang" ],
      "venue" : "TPAMI, 32:875–888,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Locality-constrained linear coding for image classification",
      "author" : [ "J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong" ],
      "venue" : "Proc. CVPR,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A scalable cur matrix decomposition algorithm: lower time complexity and tighter bound",
      "author" : [ "S. Wang", "Z. Zhang" ],
      "venue" : "NIPS 26,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Using the Nyström method to speed up kernel machines",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "NIPS 14,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "N-finder: an algorithm for fast autonomous spectral end-member determination in hyperspectral data",
      "author" : [ "M.E. Winter" ],
      "venue" : "SPIE’s International Symposium on Optical Science, Engineering, and Instrumentation. International Society for Optics and Photonics,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Face recognition via archetype hull ranking",
      "author" : [ "Y. Xiong", "W. Liu", "D. Zhao", "X. Tang" ],
      "venue" : "Proc. ICCV,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Density weighted Nyström method for computing large kernel eigensystems",
      "author" : [ "K. Zhang", "J. Kwok" ],
      "venue" : "Neural Computation, 21:121–146,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Cyclizing clusters via Zeta function of a graph",
      "author" : [ "D. Zhao", "X. Tang" ],
      "venue" : "NIPS 22,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Such circumstances include fast training of nonlinear kernel support vector machines (SVM) in the dual form [30], spectral clustering [8], manifold learning [25], etc.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "Such circumstances include fast training of nonlinear kernel support vector machines (SVM) in the dual form [30], spectral clustering [8], manifold learning [25], etc.",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 24,
      "context" : "Such circumstances include fast training of nonlinear kernel support vector machines (SVM) in the dual form [30], spectral clustering [8], manifold learning [25], etc.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 29,
      "context" : "Minimizing a relative approximation error is typically harnessed as the objective of column sampling, by which the most intuitive solution is to perform uniform sampling [30].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "Other non-uniform sampling schemes choose columns via various criteria, such as probabilistic samplings according to diagonal elements of a kernel matrix [7], reconstruction errors [15], determinant measurements [1], cluster centroids [33], and statistical leverage scores [21].",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 14,
      "context" : "Other non-uniform sampling schemes choose columns via various criteria, such as probabilistic samplings according to diagonal elements of a kernel matrix [7], reconstruction errors [15], determinant measurements [1], cluster centroids [33], and statistical leverage scores [21].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 0,
      "context" : "Other non-uniform sampling schemes choose columns via various criteria, such as probabilistic samplings according to diagonal elements of a kernel matrix [7], reconstruction errors [15], determinant measurements [1], cluster centroids [33], and statistical leverage scores [21].",
      "startOffset" : 212,
      "endOffset" : 215
    }, {
      "referenceID" : 32,
      "context" : "Other non-uniform sampling schemes choose columns via various criteria, such as probabilistic samplings according to diagonal elements of a kernel matrix [7], reconstruction errors [15], determinant measurements [1], cluster centroids [33], and statistical leverage scores [21].",
      "startOffset" : 235,
      "endOffset" : 239
    }, {
      "referenceID" : 20,
      "context" : "Other non-uniform sampling schemes choose columns via various criteria, such as probabilistic samplings according to diagonal elements of a kernel matrix [7], reconstruction errors [15], determinant measurements [1], cluster centroids [33], and statistical leverage scores [21].",
      "startOffset" : 273,
      "endOffset" : 277
    }, {
      "referenceID" : 3,
      "context" : "may be cast into a combinatorial optimization problem, which can be tackled by using greedy strategies in polynomial time [4] and boosted by using advanced sampling strategies to further reduce the relative approximation error [14].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "may be cast into a combinatorial optimization problem, which can be tackled by using greedy strategies in polynomial time [4] and boosted by using advanced sampling strategies to further reduce the relative approximation error [14].",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 10,
      "context" : "Understanding these structures has been proven beneficial to approximate or represent data inputs [11].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 15,
      "context" : "One of the most famous algorithms for dimensionality reduction, Non-negative Matrix Factorization (NMF) [16], learns a low-dimensional convex hull from data points through a convex relaxation [3].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "One of the most famous algorithms for dimensionality reduction, Non-negative Matrix Factorization (NMF) [16], learns a low-dimensional convex hull from data points through a convex relaxation [3].",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 26,
      "context" : "This idea was extended to signal separation by pursuing a convex hull with a maximized volume [27] to enclose input data points.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 25,
      "context" : "Assuming that vertices are equally distant, the problem of fitting a simplex with a maximized volume to data reduces to a simple greedy column selection procedure [26].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 31,
      "context" : "The simplex fitting approach demonstrated its success in face recognition tasks [32].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 30,
      "context" : "Parallel research in geoscience and remote sensing is also active, where the vertices of a convex hull are coined as endmembers or extreme points, leading to a classic “N-Finder” algorithm [31].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 12,
      "context" : "Probabilistic models like Determinantal Point Process (DPP) [13] measure data densities, so they are likely to overcome the convexity issue.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 33,
      "context" : "Using cycle-based measures to characterize data structures has been proven successful in clustering data of multiple types of distributions [34].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 17,
      "context" : "To accelerate such a matrix manipulation, we employ the Anchor Graph [18] technique in the sense that the original graph can be approximated with respect to the anchors originating from a randomly sampled data subset.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "We resort to the Anchor Graph technique [18], which has been successfully applied to handle large-scale hashing[20] and semi-supervised learning problems.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "We resort to the Anchor Graph technique [18], which has been successfully applied to handle large-scale hashing[20] and semi-supervised learning problems.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 9,
      "context" : "Downdating SVD [10] was proposed to handle this operation.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "The decomposition of D can be efficiently performed in O(l(2)) time [10].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 20,
      "context" : "For the leverage score approach, we follow the steps in [21].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "the sparse matrix computation to reduce the constant factor [5].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 28,
      "context" : "To compare with the state-of-the-art, we choose some renowned methods: K-medoids, CUR matrix factorization (CUR) [29], simplex volume maximization (Simplex) [26], sparse dictionary learning (DictLearn) [22] and convex non-negative matrix factorization (C-NMF) [6].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 25,
      "context" : "To compare with the state-of-the-art, we choose some renowned methods: K-medoids, CUR matrix factorization (CUR) [29], simplex volume maximization (Simplex) [26], sparse dictionary learning (DictLearn) [22] and convex non-negative matrix factorization (C-NMF) [6].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 21,
      "context" : "To compare with the state-of-the-art, we choose some renowned methods: K-medoids, CUR matrix factorization (CUR) [29], simplex volume maximization (Simplex) [26], sparse dictionary learning (DictLearn) [22] and convex non-negative matrix factorization (C-NMF) [6].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : "To compare with the state-of-the-art, we choose some renowned methods: K-medoids, CUR matrix factorization (CUR) [29], simplex volume maximization (Simplex) [26], sparse dictionary learning (DictLearn) [22] and convex non-negative matrix factorization (C-NMF) [6].",
      "startOffset" : 260,
      "endOffset" : 263
    }, {
      "referenceID" : 21,
      "context" : "The representation will be built in two ways: 1) the sparse coding [22] and 2) the locality simplex coding [26].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "The representation will be built in two ways: 1) the sparse coding [22] and 2) the locality simplex coding [26].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 23,
      "context" : "To compare with existing low-dimension embedding approaches, we run the Large-Scale Manifold method [24] using the same number of landmarks as that of extracted points.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "The “Anchor Graph” refers to the additional experiments using the original anchor graph framework [18].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 21,
      "context" : "Here a standard 1-regularized projection algorithm (LASSO) [22] is adopted to learn the sparse representation from the extracted data points.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 25,
      "context" : "The local simplex coding reconstructs one data point as a convex combination of a set of nearest exemplar points, which form local simplexes [26].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "the TDT2 dataset and the Newsgroups dataset [2].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "The two sets are separated in advance [2] and ordered in time sequence to be more challenging for classifiers.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "The Caltech101 dataset [17] is a widely used benchmark for object recognition systems.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 27,
      "context" : "Every image is processed into a feature vector of 21504 dimensions by the method in [28].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "The MultiPIE human face dataset is a widely applied benchmark for face recognition [9].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : "For the comparison with the Large-Scale Manifold method [24] on the MultiPIE dataset, we find that even using 10K landmarks, its accuracy is still inferior to our methods relying on the Zeta Hull model.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 18,
      "context" : "This difficulty can be circumvented by running a number of well-established outlier removal methods such as [19].",
      "startOffset" : 108,
      "endOffset" : 112
    } ],
    "year" : 2014,
    "abstractText" : "Selecting a small informative subset from a given dataset, also called column sampling, has drawn much attention in machine learning. For incorporating structured data information into column sampling, research efforts were devoted to the cases where data points are fitted with clusters, simplices, or general convex hulls. This paper aims to study nonconvex hull learning which has rarely been investigated in the literature. In order to learn data-adaptive nonconvex hulls, a novel approach is proposed based on a graph-theoretic measure that leverages graph cycles to characterize the structural complexities of input data points. Employing this measure, we present a greedy algorithmic framework, dubbed Zeta Hulls, to perform structured column sampling. The process of pursuing a Zeta hull involves the computation of matrix inverse. To accelerate the matrix inversion computation and reduce its space complexity as well, we exploit a low-rank approximation to the graph adjacency matrix by using an efficient anchor graph technique. Extensive experimental results show that data representation learned by Zeta Hulls can achieve state-of-the-art accuracy in text and image classification tasks.",
    "creator" : null
  }
}