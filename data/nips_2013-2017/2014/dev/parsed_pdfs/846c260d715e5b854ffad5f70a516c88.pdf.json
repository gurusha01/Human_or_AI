{
  "name" : "846c260d715e5b854ffad5f70a516c88.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Spectral Methods for Indian Buffet Process Inference",
    "authors" : [ "Hsiao-Yu Fish Tung", "Alexander J. Smola" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The Indian Buffet Process is a versatile statistical tool for modeling distributions over binary matrices. We provide an efficient spectral algorithm as an alternative to costly Variational Bayes and sampling-based algorithms. We derive a novel tensorial characterization of the moments of the Indian Buffet Process proper and for two of its applications. We give a computationally efficient iterative inference algorithm, concentration of measure bounds, and reconstruction guarantees. Our algorithm provides superior accuracy and cheaper computation than comparable Variational Bayesian approach on a number of reference problems."
    }, {
      "heading" : "1 Introduction",
      "text" : "Inferring the distributions of latent variables is a key tool in statistical modeling. It has a rich history dating back over a century to mixture models for identifying crabs [27] and has served as a key tool for describing diverse sets of distributions ranging from text [10] to images [1] and user behavior [4]. In recent years spectral methods have become a credible alternative to sampling [19] and variational methods [9, 13] for the inference of such structures. In particular, the work of [6, 5, 11, 21, 29] demonstrates that it is possible to infer latent variable structure accurately, despite the problem being nonconvex, thus exhibiting many local minima. A particularly attractive aspect of spectral methods is that they allow for efficient means of inferring the model complexity in the same way as the remaining parameters, simply by thresholding eigenvalue decomposition appropriately. This makes them suitable for nonparametric Bayesian approaches.\nWhile the issue of spectral inference in Dirichlet Distribution is largely settled [6, 7], the domain of nonparametric tools is much richer and it is therefore desirable to see whether the methods can be extended to other models such as the Indian Buffet Process (IBP). This is the main topic of our paper. We provide a full analysis of the tensors arising from the IBP and how spectral algorithms need to be modified, since a degeneracy in the third order tensor requires fourth order terms. To recover the parameters and latent factors, we use Excess Correlation Analysis (ECA) [8] to whiten the higher order tensors and to reduce their dimensionality. Subsequently we employ the power method to obtain symmetric factorization of the higher-order terms. The method provided in this work is simple to implement and has high efficiency in recovering the latent factors and related parameters. We demonstrate how this approach can be used in inferring an IBP structure in the models discussed in [18] and [24]. Moreover, we show that empirically the spectral algorithm provides higher accuracy and lower runtime than variational methods [14]. Statistical guarantees for recovery and stability of the estimates conclude the paper.\nOutline: Section 2 gives a brief primer on the IBP. Section 3 contains the lower-order moments of IBP and its application on different model. Section 5 discusses concentration of measure of moments. Section 4 applies Excess Correlation Analysis to the moments and it provides the basic structure of this Algorithm. Section 6 shows the empirical performance of our algorithm. Due to space constraints we relegate most derivations and proofs to the appendix."
    }, {
      "heading" : "2 The Indian Buffet Process",
      "text" : "The Indian Buffet Process defines a distribution over equivalence classes of binary matrices Z with a finite number of rows and a (potentially) infinite number of columns [17, 18]. The idea is that this allows for automatic adjustment of the number of binary entries, corresponding to the number of independent sources, underlying causes, etc. This is a very useful strategy and it has led to many applications including structuring Markov transition matrices [15], learning hidden causes with a bipartite graph [30] and finding latent features in link prediction [26]. n ∈ N the number of rows of Z, i.e. the number of customers sampling dishes from the “ Indian Buffet”, let mk be the number of customers who have sampled dish k, let K+ be the total number of dishes sampled, and denote by Kh the number of dishes with a particular selection history h ∈ {0; 1}n. That is, Kh > 1 only if there are two or more dishes that have been selected by exactly the same set of customers. Then the probability of generating a particular matrix Z is given by [18]\np(Z) = αK+∏ hKh! exp\n[ −α\nn∑ j=1 1 j ] K+∏ k=1 (n−mk)!(mk − 1)! n!\n(1)\nHere α is a parameter determining the expected number of nonzero columns in Z. Due to the conjugacy of the prior an alternative way of viewing p(Z) is that each column (aka dish) contains nonzero entries Zij that are drawn from the binomial distribution Zij ∼ Bin(πi). That is, if we knew K+, i.e. if we knew how many nonzero features Z contains, and if we knew the probabilities πi, we could draw Z efficiently from it. We take this approach in our analysis: determine K+ and infer the probabilities πi directly from the data. This is more reminiscent of the model used to derive the IBP — a hierarchical Beta-Binomial model, albeit with a variable number of entries:\nj ∈ {n} i ∈ { K+\n} α πi Zij\nIn general, the binary attributes Zij are not observed. Instead, they capture auxiliary structure pertinent to a statistical model of interest. To make matters more concrete, consider the following two models proposed by [18] and [24]. They also serve to showcase the algorithm design in our paper.\nLinear Gaussian Latent Feature Model [18]. The assumption is that we observe vectorial data x. It is generated by linear combination of dictionary atoms A and an associated unknown number of binary causes z, all corrupted by some additive noise . That is, we assume that\nx = Az + where ∼ N (0, σ21) and z ∼ IBP(α). (2) The dictionary matrixA is considered to be fixed but unknown. In this model our goal is to infer both A, σ2 and the probabilities πi associated with the IBP model. Given that, a maximum-likelihood estimate of Z can be obtained efficiently.\nInfinite Sparse Factor Analysis [24]. A second model is that of sparse independent component analysis. In a way, it extends (2) by replacing binary attributes with sparse attributes. That is, instead of z we use the entry-wise product z.∗y. This leads to the model\nx = A(z.∗y) + where ∼ N (0, σ21) , z ∼ IBP(α) and yi ∼ p(y) (3) Again, the goal is to infer A, the probabilities πi and then to associate likely values of Zij and Yij with the data. In particular, [24] make a number of alternative assumptions on p(y), namely either that it is iid Gaussian or that it is iid Laplacian. Note that the scale of y itself is not so important since an equivalent model can always be found by rescaling A suitably.\nNote that in (3) we used the shorthand .∗ to denote point-wise multiplication of two vectors in ’Matlab’ notation. While (2) and (3) appear rather similar, the latter model is considerably more complex since it not only amounts to a sparse signal but also to an additional multiplicative scale. [24] refer to the model as Infinite Sparse Factor Analysis (isFA) or Infinite Independent Component Analysis (iICA) depending on the choice of p(y) respectively."
    }, {
      "heading" : "3 Spectral Characterization",
      "text" : "We are now in a position to define the moments of the associated binary matrix. In our approach we assume that Z ∼ IBP(α). We assume that the number of nonzero attributes k is unknown (but fixed). Our analysis begins by deriving moments for the IBP proper. Subsequently we apply this to the two models described above. All proofs are deferred to the Appendix. For notational convenience we denote by S the symmetrized version of a tensor where care is taken to ensure that existing multiplicities are satisfied. That is, for a generic third order tensor we set S6[A]ijk = Aijk + Akij + Ajki + Ajik + Akji + Aikj . However, if e.g. A = B ⊗ c with Bij = Bji, we only need S3[A]ijk = Aijk +Akij +Ajki to obtain a symmetric tensor."
    }, {
      "heading" : "3.1 Tensorial Moments for the IBP",
      "text" : "A degeneracy in the third order tensor requires that we compute a fourth order moment. We can exclude the cases of πi = 0 and πi = 1 since the former amounts to a nonexistent feature and the latter to a constant offset. We useMi to denote moments of order i and Si to denote diagonal(izable) tensors of order i. Finally, we use π ∈ RK+ to denote the vector of probabilities πi.\nOrder 1 This is straightforward, since we have\nM1 := Ez [z] = π =: S1. (4)\nOrder 2 The second order tensor is given by M2 := Ez [z ⊗ z] = π ⊗ π + diag ( π − π2 ) = S1 ⊗ S1 + diag ( π − π2 ) . (5)\nSolving for the diagonal tensor we have S2 := M2 − S1 ⊗ S1 = diag ( π − π2 ) . (6)\nThe degeneracies {0, 1} of π − π2 = (1− π)π can be ignored since they amount to non-existent and degenerate probability distributions.\nOrder 3 The third order moments yield M3 :=Ez [z ⊗ z ⊗ z] = π ⊗ π ⊗ π + S3 [ π ⊗ diag ( π − π2 )] + diag ( π − 3π2 + 2π3 ) (7)\n=S1 ⊗ S1 ⊗ S1 + S3 [S1 ⊗ S2] + diag ( π − 3π2 + 2π3 ) . (8)\nS3 :=M3 −S3 [S1 ⊗ S2] + S1 ⊗ S1 ⊗ S1 = diag ( π − 3π2 + 2π3 ) . (9)\nNote that the polynomial π − 3π2 + 2π3 = π(2π − 1)(π − 1) vanishes for π = 12 . This is undesirable for the power method — we need to compute a fourth order tensor to exclude this.\nOrder 4 The fourth order moments are\nM4 :=Ez [z ⊗ z ⊗ z ⊗ z] = S1 ⊗ S1 ⊗ S1 ⊗ S1 + S6 [S2 ⊗ S1 ⊗ S1] + S3 [S2 × S2] + S4 [S3 ⊗ S1] + diag ( π − 7π2 + 12π3 − 6π4 ) S4 :=M4 − S1 ⊗ S1 ⊗ S1 ⊗ S1 −S6 [S2 ⊗ S1 ⊗ S1]−S3 [S2 × S2] + S4 [S3 ⊗ S1]\n=diag ( π − 7π2 + 12π3 − 6π4 ) . (10)\nThe roots of the polynomial are { 0, 12 − 1/ √ 12, 12 + 1/ √ 12, 1 }\n. Hence the latent factors and their corresponding πk can be inferred either by S3 or S4."
    }, {
      "heading" : "3.2 Application of the IBP",
      "text" : "The above derivation showed that if we were able to access z directly, we could infer π from it by reading off terms from a diagonal tensor. Unfortunately, this is not quite so easy in practice since z generally acts as a latent attribute in a more complex model. In the following we show how the models of (2) and (3) can be converted into spectral form. We need some notation to indicate multiplications of a tensor M of order k by a set of matrices Ai.\n[T (M,A1, . . . , Ak)]i1,...ik := ∑ j1,...jk Mj1,...jk [A1]i1j1 · . . . · [Ak]ikjk . (11)\nNote that this includes matrix multiplication. For instance, A>1 MA2 = T (M,A1, A2). Also note that in the special case where the matrices Ai are vectors, this amounts to a reduction to a scalar. Any such reduced dimensions are assumed to be dropped implicitly. The latter will become useful in the context of the tensor power method in [6].\nLinear Gaussian Latent Factor Model. When dealing with (2) our goal is to infer both A and π. The main difference is that rather than observing z we have Az, hence all tensors are colored. Moreover, we also need to deal with the terms arising from the additive noise . This yields\nS1 :=M1 = T (π,A) (12)\nS2 :=M2 − S1 ⊗ S1 − σ21 = T (diag(π − π2), A,A) (13) S3 :=M3 − S1 ⊗ S1 ⊗ S1 −S3 [S1 ⊗ S2]−S3 [m1 ⊗ 1] (14)\n=T ( diag ( π − 3π2 + 2π3 ) , A,A,A ) S4 :=M4 − S1 ⊗ S1 ⊗ S1 ⊗ S1 −S6 [S2 ⊗ S1 ⊗ S1]−S3 [S2 ⊗ S2]−S4 [S3 ⊗ S1] (15)\n− σ2S6 [S2 ⊗ 1]−m4S3 [1⊗ 1] =T ( diag ( −6π4 + 12π3 − 7π2 + π ) , A,A,A,A ) Here we used the auxiliary statistics m1 and m4. Denote by v the eigenvector with the smallest eigenvalue of the covariance matrix of x. Then the auxiliary variables are defined as\nm1 :=Ex\n[ x 〈v, (x−E [x])〉2 ] = σ2T (π,A) (16)\nm4 :=Ex [ 〈v, (x−Ex [x])〉4 ] /3 = σ4. (17)\nThese terms are used in a tensor power method to infer both A and π (Appendix A has a derivation).\nInfinite Sparse Factor Analysis. Using the model of (3) it follows that z is a symmetric distribution with mean 0 provided that p(y) has this property. From that it follows that the first and third order moments and tensors vanish, i.e. S1 = 0 and S3 = 0. We have the following statistics: S2 :=M2 − σ21 = T (c · diag(π), A,A) (18) S4 :=M4 −S3 [S2 ⊗ S2]− σ2S6 [S2 ⊗ 1]−m4S3 [1⊗ 1] = T (diag(f(π)), A,A,A,A) .\n(19)\nHere m4 is defined as in (17). Whenever p(y) in (3) is Gaussian, we have c = 1 and f(π) = π−π2. Moreover, whenever p(y) follows the Laplace distribution, we have c = 2 and f(π) = 24π− 12π2.\nLemma 1 Any linear model of the form (2) or (3) with the property that is symmetric and satisfies E[ 2] = E [ 2Gauss ] and E[ 4] = E [ 4Gauss ] the same properties for y, will yield the same moments.\nProof This follows directly from the fact that z, and y are independent and that the latter two have zero mean and are symmetric. Hence the expectations carry through regardless of the actual underlying distribution."
    }, {
      "heading" : "4 Parameter Inference",
      "text" : "Having derived symmetric tensors that contain both A and polynomials of π, we need to separate those two factors and the additive noise, as appropriate. In a nutshell the approach is as follows: we first identify the noise floor using the assumption that the number of nonzero probabilities in π is lower than the dimensionality of the data. Secondly, we use the noise-corrected second order tensor to whiten the data. This is akin to methods used in ICA [12]. Finally, we perform power iterations on the data to obtain S3 and S4, or rather, their applications to data. Note that the eigenvalues in the re-scaled tensors differ slightly since we use S† 1 2\n2 x directly rather than x.\nRobust Tensor Power Method Our reasoning follows that of [6]. It is our goal to obtain an orthogonal decomposition of the tensors Si into an orthogonal matrix V together with a set of corresponding eigenvalues λ such that Si = T [diag(λ), V >, . . . , V >]. This is accomplished by generalizing the Rayleigh quotient and power iterations as described in [6, Algorithm 1]:\nθ ← T [S,1, θ, . . . , θ] and θ ← ‖θ‖−1 θ. (20)\nAlgorithm 1 Excess Correlation Analysis for Linear-Gaussian model with IBP prior Inputs: the moments M1,M2,M3,M4.\n1: Infer K and σ2: 2: Optionally find a subspace R ∈ Rd×K′ with K < K ′ by random projection.\nRange (R) = Range (M2 −M1 ⊗M1) and project down to R 3: Set σ2 := λmin (M2 −M1 ⊗M1) 4: Set S2 = ( M2 −M1 ⊗M1 − σ21 )\nby truncating to eigenvalues larger than 5: Set K = rankS2 6: Set W = UΣ− 1 2 , where [U,Σ] = svd(S2) 7: Whitening: (best carried out by preprocessing x) 8: Set W3 := T (S3,W,W,W ) 9: Set W4 := T (S4,W,W,W,W )\n10: Tensor Power Method: 11: Compute generalized eigenvalues and vectors of W3. 12: Keep all K1 ≤ K (eigenvalue, eigenvector) pairs (λi, vi) of W3 13: Deflate W4 with (λi, vi) for all i ≤ K1 14: Keep all K −K1 (eigenvalue, eigenvector) pairs (λi, vi) of deflated W4 15: Reconstruction: With corresponding eigenvalues {λ1, · · · , λK}, return the set A:\nA =\n{ 1\nZi\n( W † )> vi : vi ∈ Λ } (21)\nwhere Zi = √ πi − π2i with πi = f−1(λi). f(π) = −2π+1√π−π2 if i ∈ [K1] and f(π) = 6π2−6π+1 π−π2 otherwise. (The proof of Equation (21) is provided in the Appendix.)\nIn a nutshell, we use a suitable number of random initialization l, perform a few iterations (v) and then proceed with the most promising candidate for another d iterations. The rationale for picking the best among l candidates is that we need a high probability guarantee that the selected initialization is non-degenerate. After finding a good candidate and normalizing its length we deflate (i.e. subtract) the term from the tensor S.\nExcess Correlation Analysis (ECA) The algorithm for recovering A is shown in Algorithm 1. We first present the method of inferring the number of latent features, K, which can be viewed as the rank of the covariance matrix. An efficient way of avoiding eigendecomposition on a d × d matrix is to find a low-rank approximation R ∈ Rd×K′ such that K < K ′ d and R spans the same space as the covariance matrix. One efficient way to find such matrix is to set R to be\nR = (M2 −M1 ×M1) Θ, (22)\nwhere Θ ∈ Rd×K′ is a random matrix with entries sampled independently from a standard normal. This is described, e.g. by [20]. Since there is noise in the data, it is not possible that we get exactlyK non-zero eigenvalues with the remainder being constant at noise floor σ2. An alternative strategy to thresholding by σ2 is to determineK by seeking the largest slope on the curve of sorted eigenvalues.\nNext, we whiten the observations by multiplying data with W ∈ Rd×K . This is computationally efficient, since we can apply this directly to x, thus yielding third and fourth order tensors W3 and W4 of size k. Moreover, approximately factorizing S2 is a consequence of the decomposition and random projection techniques arising from [20].\nTo find the singular vectors of W3 and W4 we use the robust tensor power method, as described above. From the eigenvectors we found in the last step, A could be recovered with Equation 21. The fact that this algorithm only needs projected tensors makes it very efficient. Streaming variants of the robust tensor power method are subject of future research.\nFurther Details on the projected tensor power method. Explicitly calculating tensors M2,M3,M4 is not practical in high dimensional data. It may not even be desirable to compute the projected variants of M3 and M4, that is, W3 and W4 (after suitable shifts). Instead, we can use\nthe analog of a kernel trick to simplify the tensor power iterations to\nW>T (Ml,1,Wu, . . . ,Wu) = 1\nm m∑ i=1 W>xi 〈xi,Wu〉l−1 = W> m m∑ i=1 xi 〈 W>xi, u 〉l−1 By using incomplete expansions memory complexity and storage are reduced to O(d) per term. Moreover, precomputation is O(d2) and it can be accomplished in the first pass through the data."
    }, {
      "heading" : "5 Concentration of Measure Bounds",
      "text" : "There exist a number of concentration of measure inequalities for specific statistical models using rather specific moments [8]. In the following we derive a general tool for bounding such quantities, both for the case where the statistics are bounded and for unbounded quantities alike. Our analysis borrows from [3] for the bounded case, and from the average-median theorem, see e.g. [2]."
    }, {
      "heading" : "5.1 Bounded Moments",
      "text" : "We begin with the analysis for bounded moments. Denote by φ : X → F a set of statistics on X and let φl be the l-times tensorial moments obtained from l.\nφ1(x) := φ(x); φ2(x) := φ(x)⊗ φ(x); φl(x) := φ(x)⊗ . . .⊗ φ(x) (23)\nIn this case we can define inner products via\nkl(x, x ′) := 〈φl(x), φl(x′)〉 = T [φl(x), φ(x′), . . . , φ(x′)] = 〈φ(x), φ(x′)〉 l = kl(x, x′)\nas reductions of the statistics of order l for a kernel k(x, x′) := 〈φ(x), φ(x′)〉. Finally, denote by\nMl := Ex∼p(x)[φl(x)] and M̂l := 1\nm m∑ j=1 φl(xj) (24)\nthe expectation and empirical averages of φl. Note that these terms are identical to the statistics used in [16] whenever a polynomial kernel is used. It is therefore not surprising that an analogous concentration of measure inequality to the one proven by [3] holds:\nTheorem 2 Assume that the sufficient statistics are bounded via ‖φ(x)‖ ≤ R for all x ∈ X . With probability at most 1− δ the following guarantee holds:\nPr { sup\nu:‖u‖≤1\n∣∣∣T (Ml, u, · · · , u)− T (M̂l, u, · · · , u)∣∣∣ > l} ≤ δ where l ≤ [2 +√−2 log δ]Rl√ m .\nUsing Lemma 1 this means that we have concentration of measure immediately for the moments S1, . . . S4.Details are provided in the appendix. In particular, we need a chaining result (Lemma 4) that allows us to compute bounds for products of terms efficiently. By utilizing an approach similar to [8], overall guarantees for reconstruction accuracy can be derived."
    }, {
      "heading" : "5.2 Unbounded Moments",
      "text" : "We are interested in proving concentration of the following four tensors in (13), (14), (15) and one scalar in (27). Whenever the statistics are unbounded, concentration of moment bounds are less trivial and require the use of subgaussian and gaussian inequalities [22]. We derive a bound for fourth-order subgaussian random variables (previous work only derived up to third order bounds). Lemma 5 and 6 has details on how to obtain such guarantees. We further get the bounds for the tensors based on the concentration of moment in Lemma 7 and 8. Bounds for reconstruction accuracy of our algorithm are provided. The full proof is in the Appendix.\nTheorem 3 (Reconstruction Accuracy) Let ςk [S2] be the k−th largest singular value of S2. Define πmin = argmaxi∈[K] |πi − 0.5|, πmax = argmaxi∈[K] πi and π̃ = ∏ {i:πi≤0.5} πi ∏ {i:πi>0.5}(1−\nπi). Pick any δ, ∈ (0, 1). There exists a polynomial poly(·) such that if sample size m statisfies\nm ≥ poly ( d,K, 1 , log(1/δ), 1\nπ̃ , ς1 [S2] ςK [S2] ,\nK∑ i=1 ‖Ai‖22\nςK [S2] ,\nσ2\nςK [S2] , 1√ πmin − πmin2\n, πmax√\nπmax − π2max\n)\nwith probability greater than 1 − δ, there is a permutation τ on [K] such that the Â returns by Algorithm 1 satifies ∥∥∥Âτ(i) −Ai∥∥∥ ≤ (‖Ai‖2 +√ς1 [S2]) for all i ∈ [K]."
    }, {
      "heading" : "6 Experiments",
      "text" : "We evaluate the algorithm on a number of problems suitable for the two models of (2) and (3). The problems are largely identical to those put forward in [18] in order to keep our results comparable with a more traditional inference approach. We demonstrate that our algorithm is faster, simpler, and achieves comparable or superior accuracy.\nSynthetic data Our goal is to demonstrate the ability to recover latent structure of generated data. Following [18] we generate images via linear noisy combinations of 6 × 6 templates. That is, we use the binary additive model of (2). The goal is to recover both the above images and to assess their respective presence in observed data. Using an additive noise variance of σ2 = 0.5 we are able to recover the original signal quite accurately (from left to right: true signal, signal inferred from 100 samples, signal inferred from 500 samples). Furthermore, as the second row indicates, our algorithm also correctly infers the attributes present in the images.\n0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 Text 1 01 0\nFor a more quantitative evaluation we compared our results to the infinite variational algorithm of [14]. The data is generated using σ ∈ {0.1, 0.2, 0.3, 0.4, 0.5} and with sample size n ∈ {100, 200, 300, 400, 500}. Figure 1 shows that our algorithm is faster and comparatively accurate.\nImage Source Recovery We repeated the same test using 100 photos from [18]. We first reduce dimensionality on the data set by representing the images with 100 principal components and apply our algorithm on the 100-dimensional dataset (see Algorithm 1 for details). Figure 2 shows the result. We used 10 initial iterations 50 random seeds and 30 final iterations 50 in the Robust Power Tensor Method. The total runtime was 0.2788s.\nFigure 2: Results of modeling 100 images from [18] of size 240 × 320 by model (2). Row 1: four sample images containing up to four objects ($20 bill, Klein bottle, prehistoric handaxe, cellular phone). An object basically appears in the same location, but some small variation noise is generated because the items are put into scene by hand; Row 2: Independent attributes, as determined by infinite variational inference of [14] (note, the results in [18] are black and white only); Row 3: Independent attributes, as determined by spectral IBP; Row 4: Reconstruction of the images via spectral IBP. The binary superscripts indicate the items identified in the image.\nOriginal G Spectral isFA MCMC\nFigure 3: Recovery of the source matrix A in model (3) when comparing MCMC sampling and spectral methods. MCMC sampling required 1.72 seconds and yielded a Frobenius distance ‖A−AMCM‖F = 0.77. Our spectral algorithm required 0.77 seconds to achieve a distance ‖A−ASpectral‖F = 0.31.\nGene Expression Data As a first sanity check of the feasibility of our model for (3), we generated synthetic data using x ∈ R7 with k = 4 sources and n = 500 samples, as shown in Figure 3. For a more realistic analysis we used a microarray dataset. The data consisted of 587 mouse liver samples detecting 8565 gene probes, available as dataset GSE2187 as part of NCBI’s Gene Expression Omnibus www.ncbi.nlm.nih.gov/geo. There are four main types of treatments, including Toxicant, Statin, Fibrate and Azole. Figure 4 shows the inferred latent factors arising from expression levels of samples on 10 derived gene signatures. According to the result, the group of fibrate-induced samples and a small group of toxicant-induced samples can be classified accurately by the special patterns. Azole-induced samples have strong positive signals on gene signatures 4 and 8, while statin-induced samples have strong positive signals only on the 9 gene signatures.\nSummary In this paper we introduced a spectral approach to inferring latent parameters in the Indian Buffet Process. We derived tensorial moments for a number of models, provided an efficient inference algorithm, concentration of measure theorems and reconstruction guarantees. All this is backed up by experiments comparing spectral and MCMC methods.\nWe believe that this is a first step towards expanding spectral nonparametric tools beyond the more common Dirichlet Process representations. Applications to more sophisticated models, larger datasets and efficient implementations are subject for future work."
    } ],
    "references" : [ {
      "title" : "Tree-structured stick breaking for hierarchical data",
      "author" : [ "R. Adams", "Z. Ghahramani", "M. Jordan" ],
      "venue" : "Neural Information Processing Systems, pages 19–27,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The space complexity of approximating the frequency moments",
      "author" : [ "N. Alon", "Y. Matias", "M. Szegedy" ],
      "venue" : "Journal of Computers and System Sciences, 58(1):137–147,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Unifying divergence minimization and statistical inference via convex duality",
      "author" : [ "Y. Altun", "A.J. Smola" ],
      "venue" : "H.U. Simon and G. Lugosi, editors, Proc. Annual Conf. Computational Learning Theory, LNCS, pages 139–153. Springer,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Web-scale user modeling for targeting",
      "author" : [ "M. Aly", "A. Hatch", "V. Josifovski", "V.K. Narayanan" ],
      "venue" : "Conference on World Wide Web, pages 3–12. ACM,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Spectral methods for learning multivariate latent tree structure",
      "author" : [ "A. Anandkumar", "K. Chaudhuri", "D. Hsu", "S. Kakade", "L. Song", "T. Zhang" ],
      "venue" : "Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky" ],
      "venue" : "arXiv preprint arXiv:1210.7559,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A tensor spectral approach to learning mixed membership community models",
      "author" : [ "Anima Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade" ],
      "venue" : "In Proc. Annual Conf. Computational Learning Theory,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation",
      "author" : [ "Animashree Anandkumar", "Dean P. Foster", "Daniel Hsu", "Sham M. Kakade", "Yi-Kai Liu" ],
      "venue" : "CoRR, abs/1204.6703,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Variational inference for dirichlet process mixtures",
      "author" : [ "D. Blei", "M. Jordan" ],
      "venue" : "Bayesian Analysis, volume 1, pages 121–144,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Latent Dirichlet allocation",
      "author" : [ "D. Blei", "A. Ng", "M. Jordan" ],
      "venue" : "Journal of Machine Learning Research, 3:993–1022, January",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Hilbert space embeddings of predictive state representations",
      "author" : [ "Byron Boots", "Arthur Gretton", "Geoffrey J Gordon" ],
      "venue" : "In Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Blind signal separation: statistical principles",
      "author" : [ "J.-F. Cardoso" ],
      "venue" : "Proceedings of the IEEE, 90(8):2009–2026,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society B, 39(1):1–22,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Variational inference for the indian buffet process",
      "author" : [ "F. Doshi", "K. Miller", "J. Van Gael", "Y.W. Teh" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track, 5:137–144,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Sharing features among dynamical systems with beta processes",
      "author" : [ "E.B. Fox", "E.B. Sudderth", "M.I. Jordan", "A.S. Willsky" ],
      "venue" : "nips, 22,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "A. Gretton", "K. Borgwardt", "M. Rasch", "B. Schoelkopf", "A. Smola" ],
      "venue" : "JMLR, 13:723–773,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Infinite latent feature models and the indian buffet process",
      "author" : [ "T. Griffiths", "Z. Ghahramani" ],
      "venue" : "Advances in Neural Information Processing Systems 18, pages 475–482,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The indian buffet process: An introduction and review",
      "author" : [ "T. Griffiths", "Z. Ghahramani" ],
      "venue" : "Journal of Machine Learning Research, 12:11851224,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "T.L. Griffiths", "M. Steyvers" ],
      "venue" : "Proceedings of the National Academy of Sciences, 101:5228–5235,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions",
      "author" : [ "N. Halko", "P.G. Martinsson", "J.A. Tropp" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "A spectral algorithm for learning hidden markov models",
      "author" : [ "D. Hsu", "S. Kakade", "T. Zhang" ],
      "venue" : "Proc. Annual Conf. Computational Learning Theory,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Tail inequalities for sums of random matrices that depend on the intrinsic dimension",
      "author" : [ "D. Hsu", "S. Kakade", "T. Zhang" ],
      "venue" : "Electron. Commun. Probab., 17:13,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning mixtures of spherical gaussians: moment methods and spectral decompositions",
      "author" : [ "D. Hsu", "S.M. Kakade" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Infinite sparse factor analysis and infinite independent components analysis",
      "author" : [ "D. Knowles", "Z. Ghahramani" ],
      "venue" : "International Conference on Independent Component Analysis and Signal Separation,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On the method of bounded differences",
      "author" : [ "C. McDiarmid" ],
      "venue" : "Survey in Combinatorics, pages 148–188. Cambridge University Press,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Latent feature models for link prediction",
      "author" : [ "K.T. Miller", "T.L. Griffiths", "M.I. Jordan" ],
      "venue" : "Snowbird, page 2 pages,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Contributions to the mathematical theory of evolution",
      "author" : [ "K. Pearson" ],
      "venue" : "Philosophical Transactions of the Royal Society, pages 71–71,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1894
    }, {
      "title" : "The Volume of Convex Bodies and Banach Space Geometry",
      "author" : [ "G. Pisier" ],
      "venue" : "Cambridge University Press, Cambridge,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Hilbert space embeddings of hidden markov models",
      "author" : [ "L. Song", "B. Boots", "S. Siddiqi", "G. Gordon", "A.J. Smola" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A non-parametric bayesian method for inferring hidden causes",
      "author" : [ "F. Wood", "T.L. Grifths", "Z. Ghahramani" ],
      "venue" : "uai,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "It has a rich history dating back over a century to mixture models for identifying crabs [27] and has served as a key tool for describing diverse sets of distributions ranging from text [10] to images [1] and user behavior [4].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "It has a rich history dating back over a century to mixture models for identifying crabs [27] and has served as a key tool for describing diverse sets of distributions ranging from text [10] to images [1] and user behavior [4].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 0,
      "context" : "It has a rich history dating back over a century to mixture models for identifying crabs [27] and has served as a key tool for describing diverse sets of distributions ranging from text [10] to images [1] and user behavior [4].",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 3,
      "context" : "It has a rich history dating back over a century to mixture models for identifying crabs [27] and has served as a key tool for describing diverse sets of distributions ranging from text [10] to images [1] and user behavior [4].",
      "startOffset" : 223,
      "endOffset" : 226
    }, {
      "referenceID" : 18,
      "context" : "In recent years spectral methods have become a credible alternative to sampling [19] and variational methods [9, 13] for the inference of such structures.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "In recent years spectral methods have become a credible alternative to sampling [19] and variational methods [9, 13] for the inference of such structures.",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "In recent years spectral methods have become a credible alternative to sampling [19] and variational methods [9, 13] for the inference of such structures.",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "In particular, the work of [6, 5, 11, 21, 29] demonstrates that it is possible to infer latent variable structure accurately, despite the problem being nonconvex, thus exhibiting many local minima.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "In particular, the work of [6, 5, 11, 21, 29] demonstrates that it is possible to infer latent variable structure accurately, despite the problem being nonconvex, thus exhibiting many local minima.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "In particular, the work of [6, 5, 11, 21, 29] demonstrates that it is possible to infer latent variable structure accurately, despite the problem being nonconvex, thus exhibiting many local minima.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 20,
      "context" : "In particular, the work of [6, 5, 11, 21, 29] demonstrates that it is possible to infer latent variable structure accurately, despite the problem being nonconvex, thus exhibiting many local minima.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 28,
      "context" : "In particular, the work of [6, 5, 11, 21, 29] demonstrates that it is possible to infer latent variable structure accurately, despite the problem being nonconvex, thus exhibiting many local minima.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "While the issue of spectral inference in Dirichlet Distribution is largely settled [6, 7], the domain of nonparametric tools is much richer and it is therefore desirable to see whether the methods can be extended to other models such as the Indian Buffet Process (IBP).",
      "startOffset" : 83,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "While the issue of spectral inference in Dirichlet Distribution is largely settled [6, 7], the domain of nonparametric tools is much richer and it is therefore desirable to see whether the methods can be extended to other models such as the Indian Buffet Process (IBP).",
      "startOffset" : 83,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "To recover the parameters and latent factors, we use Excess Correlation Analysis (ECA) [8] to whiten the higher order tensors and to reduce their dimensionality.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "We demonstrate how this approach can be used in inferring an IBP structure in the models discussed in [18] and [24].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 23,
      "context" : "We demonstrate how this approach can be used in inferring an IBP structure in the models discussed in [18] and [24].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "Moreover, we show that empirically the spectral algorithm provides higher accuracy and lower runtime than variational methods [14].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 16,
      "context" : "The Indian Buffet Process defines a distribution over equivalence classes of binary matrices Z with a finite number of rows and a (potentially) infinite number of columns [17, 18].",
      "startOffset" : 171,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : "The Indian Buffet Process defines a distribution over equivalence classes of binary matrices Z with a finite number of rows and a (potentially) infinite number of columns [17, 18].",
      "startOffset" : 171,
      "endOffset" : 179
    }, {
      "referenceID" : 14,
      "context" : "This is a very useful strategy and it has led to many applications including structuring Markov transition matrices [15], learning hidden causes with a bipartite graph [30] and finding latent features in link prediction [26].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 29,
      "context" : "This is a very useful strategy and it has led to many applications including structuring Markov transition matrices [15], learning hidden causes with a bipartite graph [30] and finding latent features in link prediction [26].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 25,
      "context" : "This is a very useful strategy and it has led to many applications including structuring Markov transition matrices [15], learning hidden causes with a bipartite graph [30] and finding latent features in link prediction [26].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 17,
      "context" : "Then the probability of generating a particular matrix Z is given by [18]",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "To make matters more concrete, consider the following two models proposed by [18] and [24].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 23,
      "context" : "To make matters more concrete, consider the following two models proposed by [18] and [24].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "Linear Gaussian Latent Feature Model [18].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 23,
      "context" : "In particular, [24] make a number of alternative assumptions on p(y), namely either that it is iid Gaussian or that it is iid Laplacian.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : "[24] refer to the model as Infinite Sparse Factor Analysis (isFA) or Infinite Independent Component Analysis (iICA) depending on the choice of p(y) respectively.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "The latter will become useful in the context of the tensor power method in [6].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "This is akin to methods used in ICA [12].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "Robust Tensor Power Method Our reasoning follows that of [6].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 19,
      "context" : "Moreover, approximately factorizing S2 is a consequence of the decomposition and random projection techniques arising from [20].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "There exist a number of concentration of measure inequalities for specific statistical models using rather specific moments [8].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "Our analysis borrows from [3] for the bounded case, and from the average-median theorem, see e.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "Note that these terms are identical to the statistics used in [16] whenever a polynomial kernel is used.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "It is therefore not surprising that an analogous concentration of measure inequality to the one proven by [3] holds:",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "By utilizing an approach similar to [8], overall guarantees for reconstruction accuracy can be derived.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 21,
      "context" : "Whenever the statistics are unbounded, concentration of moment bounds are less trivial and require the use of subgaussian and gaussian inequalities [22].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "The problems are largely identical to those put forward in [18] in order to keep our results comparable with a more traditional inference approach.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "Following [18] we generate images via linear noisy combinations of 6 × 6 templates.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 13,
      "context" : "For a more quantitative evaluation we compared our results to the infinite variational algorithm of [14].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "Image Source Recovery We repeated the same test using 100 photos from [18].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "Figure 2: Results of modeling 100 images from [18] of size 240 × 320 by model (2).",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "An object basically appears in the same location, but some small variation noise is generated because the items are put into scene by hand; Row 2: Independent attributes, as determined by infinite variational inference of [14] (note, the results in [18] are black and white only); Row 3: Independent attributes, as determined by spectral IBP; Row 4: Reconstruction of the images via spectral IBP.",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 17,
      "context" : "An object basically appears in the same location, but some small variation noise is generated because the items are put into scene by hand; Row 2: Independent attributes, as determined by infinite variational inference of [14] (note, the results in [18] are black and white only); Row 3: Independent attributes, as determined by spectral IBP; Row 4: Reconstruction of the images via spectral IBP.",
      "startOffset" : 249,
      "endOffset" : 253
    } ],
    "year" : 2014,
    "abstractText" : "The Indian Buffet Process is a versatile statistical tool for modeling distributions over binary matrices. We provide an efficient spectral algorithm as an alternative to costly Variational Bayes and sampling-based algorithms. We derive a novel tensorial characterization of the moments of the Indian Buffet Process proper and for two of its applications. We give a computationally efficient iterative inference algorithm, concentration of measure bounds, and reconstruction guarantees. Our algorithm provides superior accuracy and cheaper computation than comparable Variational Bayesian approach on a number of reference problems.",
    "creator" : null
  }
}