{
  "name" : "838e8afb1ca34354ac209f53d90c3a43.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Structure Regularization for Structured Prediction",
    "authors" : [ "Xu Sun" ],
    "emails" : [ "xusun@pku.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Structured prediction models are popularly used to solve structure dependent problems in a wide variety of application domains including natural language processing, bioinformatics, speech recognition, and computer vision. Recently, many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. We argue that this trend could have been misdirected, because our study suggests that complex structures are actually harmful to model accuracy. While it is obvious that intensive structural dependencies can effectively incorporate structural information, it is less obvious that intensive structural dependencies have a drawback of increasing the generalization risk, because more complex structures are easier to suffer from overfitting. Since this type of overfitting is caused by structure complexity, it can hardly be solved by ordinary regularization methods such as L2 and L1 regularization schemes, which is only for controlling weight complexity.\nTo deal with this problem, we propose a simple structure regularization solution based on tag structure decomposition. The proposed method decomposes each training sample into multiple minisamples with simpler structures, deriving a model with better generalization power. The proposed method is easy to implement, and it has several interesting properties: (1) We show both theoretically and empirically that the proposed method can effectively reduce the overfitting risk on structured prediction. (2) The proposed method does not change the convexity of the objective function, such that a convex function penalized with a structure regularizer is still convex. (3) The proposed method has no conflict with the weight regularization. Thus we can apply structure regularization together with weight regularization. (4) The proposed method can accelerate the convergence rate in training.\nThe term structural regularization has been used in prior work for regularizing structures of features, including spectral regularization [1], regularizing feature structures for classifiers [20], and many\nrecent studies on structured sparsity in structured prediction scenarios [11, 8], via adopting mixed norm regularization [10], Group Lasso [22], and posterior regularization [5]. Compared with those prior work, we emphasize that our proposal on tag structure regularization is novel. This is because the term structure in all of the aforementioned work refers to structures of feature space, which is substantially different compared with our proposal on regularizing tag structures (interactions among tags).\nAlso, there are some other related studies. [17] described an interesting heuristic piecewise training method. [19] described a “lookahead\" learning method. Our work differs from [17] and [19] mainly because our work is built on a regularization framework, with arguments and theoretical justifications on reducing generalization risk and improving convergence rate. Also, our method and the theoretical results can fit general graphical models with arbitrary structures, and the detailed algorithm is very different. On generalization risk analysis, related studies include [2, 12] on non-structured classification and [18, 7] on structured classification.\nTo the best of our knowledge, this is the first theoretical result on quantifying the relation between structure complexity and the generalization risk in structured prediction, and this is also the first proposal on structure regularization via regularizing tag-interactions. The contributions of this work1 are two-fold:\n• On the methodology side, we propose a structure regularization framework for structured prediction. We show both theoretically and empirically that the proposed method can effectively reduce the overfitting risk, and at the same time accelerate the convergence rate in training. Our method and the theoretical analysis do not make assumptions based on specific structures. In other words, the method and the theoretical results can apply to graphical models with arbitrary structures, including linear chains, trees, and general graphs.\n• On the application side, for several important natural language processing tasks, our simple method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies as well as substantially faster training speed."
    }, {
      "heading" : "2 Structure Regularization",
      "text" : "A graph of observations (even with arbitrary structures) can be indexed and be denoted by using an indexed sequence of observations O = {o1, . . . , on}. We use the term sample to denote O = {o1, . . . , on}. For example, in natural language processing, a sample may correspond to a sentence of n words with dependencies of tree structures (e.g., in syntactic parsing). For simplicity in analysis, we assume all samples have n observations (thus n tags). In a typical setting of structured prediction, all the n tags have inter-dependencies via connecting each Markov dependency between neighboring tags. Thus, we call n as tag structure complexity or simply structure complexity below.\nA sample is converted to an indexed sequence of feature vectorsx = {x(1), . . . ,x(n)}, wherex(k) ∈ X is of the dimension d and corresponds to the local features extracted from the position/index k. We can use an n× d matrix to represent x ∈ Xn. Let Z = (Xn,Yn) and let z = (x,y) ∈ Z denote a sample in the training data. Suppose a training set is S = {z1 = (x1, y1), . . . , zm = (xm, ym)}, with size m, and the samples are drawn i.i.d. from a distribution D which is unknown. A learning algorithm is a function G : Zm 7→ F with the function space F ⊂ {Xn 7→ Yn}, i.e., G maps a training set S to a function GS : Xn 7→ Yn. We suppose G is symmetric with respect to S, so that G is independent on the order of S.\nStructural dependencies among tags are the major difference between structured prediction and nonstructured classification. For the latter case, a local classification of g based on a position k can be expressed as g(x(k−a), . . . ,x(k+a)), where the term {x(k−a), . . . ,x(k+a)} represents a local window. However, for structured prediction, a local classification on a position depends on the whole input x = {x(1), . . . ,x(n)} rather than a local window, due to the nature of structural dependencies among tags (e.g., graphical models like CRFs). Thus, in structured prediction a local classification on k should be denoted as g(x(1), . . . ,x(n), k). To simplify the notation, we define\ng(x, k) , g(x(1), . . . ,x(n), k) 1See the code at http://klcl.pku.edu.cn/member/sunxu/code.htm\nWe define point-wise cost function c : Y×Y 7→ R+ as c[GS(x, k), y(k)], which measures the cost on a position k by comparing GS(x, k) and the gold-standard tag y(k), and we introduce the point-wise loss as ℓ(GS , z, k) , c[GS(x, k), y(k)] Then, we define sample-wise cost function C : Yn × Yn 7→ R+, which is the cost function with respect to a whole sample, and we introduce the sample-wise loss as\nL(GS , z) , C[GS(x), y] = n∑\nk=1\nℓ(GS , z, k) = n∑\nk=1\nc[GS(x, k), y(k)]\nGiven G and a training set S, what we are most interested in is the generalization risk in structured prediction (i.e., expected average loss) [18, 7]:\nR(GS) = Ez [L(GS , z)\nn ] Since the distribution D is unknown, we have to estimate R(GS) by using the empirical risk:\nRe(GS) = 1\nmn m∑ i=1 L(GS , z i) = 1 mn m∑ i=1 n∑ k=1 ℓ(GS , z i, k)\nTo state our theoretical results, we must describe several quantities and assumptions following prior work [2, 12]. We assume a simple real-valued structured prediction scheme such that the class predicted on position k of x is the sign of GS(x, k) ∈ D.2 Also, we assume the point-wise cost function cτ is convex and τ -smooth such that ∀y1, y2 ∈ D, ∀y∗ ∈ Y\n|cτ (y1, y∗)− cτ (y2, y∗)| ≤ τ |y1 − y2| (1)\nAlso, we use a value ρ to quantify the bound of |GS(x, k) − GS\\i(x, k)| while changing a single sample (with size n′ ≤ n) in the training set with respect to the structured inputx. This ρ-admissible assumption can be formulated as ∀k,\n|GS(x, k)−GS\\i(x, k)| ≤ ρ||GS −GS\\i ||2 · ||x||2 (2)\nwhere ρ ∈ R+ is a value related to the design of algorithm G."
    }, {
      "heading" : "2.1 Structure Regularization",
      "text" : "Most existing regularization techniques are for regularizing model weights/parameters (e.g., a representative regularizer is the Gaussian regularizer or so called L2 regularizer), and we call such regularization techniques as weight regularization.\nDefinition 1 (Weight regularization) Let Nλ : F 7→ R+ be a weight regularization function on F with regularization strength λ, the structured classification based objective function with general weight regularization is as follows:\nRλ(GS) , Re(GS) +Nλ(GS) (3) 2In practice, many popular structured prediction models have a convex and real-valued cost function (e.g.,\nCRFs).\nAlgorithm 1 Training with structure regularization 1: Input: model weights w, training set S, structure regularization strength α 2: repeat 3: S′ ← ∅ 4: for i = 1→ m do 5: Randomly decompose z i ∈ S into mini-samples Nα(z i) = {z(i,1), . . . , z(i,α)} 6: S′ ← S′ ∪Nα(z i) 7: end for 8: for i = 1→ |S′| do 9: Sample z ′ uniformly at random from S′, with gradient ∇gz′(w) 10: w ← w − η∇gz′(w) 11: end for 12: until Convergence 13: return w\nWhile weight regularization is normalizing model weights, the proposed structure regularization method is normalizing the structural complexity of the training samples. As illustrated in Figure 1, our proposal is based on tag structure decomposition, which can be formally defined as follows:\nDefinition 2 (Structure regularization) Let Nα : F 7→ F be a structure regularization function on F with regularization strength α with 1 ≤ α ≤ n, the structured classification based objective function with structure regularization is as follows3:\nRα(GS) , Re[GNα(S)] = 1\nmn m∑ i=1 α∑ j=1 L[GS′ , z(i,j)] = 1 mn m∑ i=1 α∑ j=1 n/α∑ k=1 ℓ[GS′ , z(i,j), k] (4)\nwhere Nα(z i) randomly splits z i into α mini-samples {z(i,1), . . . , z(i,α)}, so that the mini-samples have a distribution on their sizes (structure complexities) with the expected value n′ = n/α. Thus, we get\nS′ = {z(1,1), z(1,2), . . . , z(1,α)︸ ︷︷ ︸ α , . . . , z(m,1), z(m,2), . . . , z(m,α)︸ ︷︷ ︸ α } (5)\nwith mα mini-samples with expected structure complexity n/α. We can denote S′ more compactly as S′ = {z ′1, z ′2, . . . , z ′mα} and Rα(GS) can be simplified as\nRα(GS) , 1\nmn mα∑ i=1 L(GS′ , z ′i) = 1 mn mα∑ i=1 n/α∑ k=1 ℓ[GS′ , z ′ i, k] (6)\nWhen the structure regularization strength α = 1, we have S′ = S and Rα = Re. The structure regularization algorithm (with the stochastic gradient descent setting) is summarized in Algorithm 1. Recall that x = {x(1), . . . ,x(n)} represents feature vectors. Thus, it should be emphasized that the decomposition of x is the decomposition of the feature vectors, not the original observations. Actually the decomposition of the feature vectors is more convenient and has no information loss — decomposing observations needs to regenerate features and may lose some features.\nThe structure regularization has no conflict with the weight regularization, and the structure regularization can be applied together with the weight regularization.\nDefinition 3 (Structure & weight regularization) By combining structure regularization in Definition 2 and weight regularization in Definition 1, the structured classification based objective function is as follows: Rα,λ(GS) , Rα(GS) +Nλ(GS) (7) When α = 1, we have Rα,λ = Re(GS) +Nλ(GS) = Rλ.\nLike existing weight regularization methods, currently our structure regularization is only for the training stage. Currently we do not use structure regularization in the test stage.\n3The notation N is overloaded here. For clarity throughout, N with subscript λ refers to weight regularization function, and N with subscript α refers to structure regularization function."
    }, {
      "heading" : "2.2 Reduction of Generalization Risk",
      "text" : "In contrast to the simplicity of the algorithm, the theoretical analysis is quite technical. In this paper we only describe the major theoretical result. Detailed analysis and proofs are given in the full version of this work [14].\nTheorem 4 (Generalization vs. structure regularization) Let the structured prediction objective function of G be penalized by structure regularization with factor α ∈ [1, n] and L2 weight regularization with factor λ, and the penalized function has a minimizer f :\nf = argmin g∈F Rα,λ(g) = argmin g∈F ( 1 mn mα∑ j=1 Lτ (g,z ′j) + λ 2 ||g||22 ) (8)\nAssume the point-wise loss ℓτ is convex and differentiable, and is bounded by ℓτ (f,z, k) ≤ γ. Assume f(x, k) is ρ-admissible. Let a local feature value be bounded by v such that x(k,q) ≤ v for q ∈ {1, . . . , d}. Then, for any δ ∈ (0, 1), with probability at least 1 − δ over the random draw of the training set S, the generalization risk R(f) is bounded by\nR(f) ≤ Re(f) + 2dτ2ρ2v2n2 mλα + ( (4m− 2)dτ2ρ2v2n2 mλα2 + γ )√α ln δ−1 2m\n(9)\nSince τ, ρ, and v are typically small compared with other variables, especially m, (9) can be approximated as follows by ignoring small terms:\nR(f) ≤ Re(f) +O (dn2√ln δ−1\nλα1.5 √ m\n) (10)\nThe proof is given in the full version of this work [14]. We call the term O ( dn2 √ ln δ−1\nλα1.5 √ m\n) in (10)\nas “overfit-bound\", and reducing the overfit-bound is crucial for reducing the generalization risk bound. First, (10) suggests that structure complexity n can increase the overfit-bound on a magnitude of O(n2), and applying weight regularization can reduce the overfit-bound by O(λ). Importantly, applying structure regularization further (over weight regularization) can additionally reduce the overfit-bound by a magnitude of O(α1.5). Since many applications in practice are based on sparse features, using a sparse feature assumption can further improve the generalization bound. The improved generalization bounds are given in the full version of this work [14]."
    }, {
      "heading" : "2.3 Accelerating Convergence Rates in Training",
      "text" : "We also analyze the impact on the convergence rate of online learning by applying structure regularization. Following prior work [9], our analysis is based on the stochastic gradient descent (SGD) with fixed learning rate. Let g(w) be the structured prediction objective function and w ∈ W is the weight vector. Recall that the SGD update with fixed learning rate η has a form like this:\nwt+1 ← wt − η∇gzt(wt) (11) where gz(wt) is the stochastic estimation of the objective function based on z which is randomly drawn from S. To state our convergence rate analysis results, we need several assumptions following (Nemirovski et al. 2009). We assume g is strongly convex with modulus c, that is, ∀w,w ′ ∈ W ,\ng(w ′) ≥ g(w) + (w ′ −w)T∇g(w) + c 2 ||w ′ −w||2 (12)\nWhen g is strongly convex, there is a global optimum/minimizer w∗. We also assume Lipschitz continuous differentiability of g with the constant q, that is, ∀w,w ′ ∈ W ,\n||∇g(w ′)−∇g(w)|| ≤ q||w ′ −w|| (13) It is also reasonable to assume that the norm of ∇gz(w) has almost surely positive correlation with the structure complexity of z ,4 which can be quantified by a bound κ ∈ R+:\n||∇gz(w)||2 ≤ κ|z | almost surely for ∀w ∈ W (14) 4Many structured prediction systems (e.g., CRFs) satisfy this assumption that the gradient based on a larger\nsample (i.e., n is large) is expected to have a larger norm.\nwhere |z | denotes the structure complexity of z . Moreover, it is reasonable to assume ηc < 1 (15)\nbecause even the ordinary gradient descent methods will diverge if ηc > 1. Then, we show that structure regularization can quadratically accelerate the SGD rates of convergence:\nProposition 5 (Convergence rates vs. structure regularization) With the aforementioned assumptions, let the SGD training have a learning rate defined as η = cϵβα 2\nqκ2n2 , where ϵ > 0 is a convergence tolerance value and β ∈ (0, 1]. Let t be a integer satisfying\nt ≥ qκ 2n2 log (qa0/ϵ)\nϵβc2α2 (16)\nwhere n and α ∈ [1, n] is like before, and a0 is the initial distance which depends on the initialization of the weights w0 and the minimizer w∗, i.e., a0 = ||w0 − w∗||2. Then, after t updates of w it converges to E[g(wt)− g(w∗)] ≤ ϵ.\nThe proof is given in the full version of this work [14]. As we can see, using structure regularization with the strength α can quadratically accelerate the convergence rate with a factor of α2."
    }, {
      "heading" : "3 Experiments",
      "text" : "Diversified Tasks. The natural language processing tasks include (1) part-of-speech tagging, (2) biomedical named entity recognition, and (3) Chinese word segmentation. The signal processing task is (4) sensor-based human activity recognition. The tasks (1) to (3) use boolean features and the task (4) adopts real-valued features. From tasks (1) to (4), the averaged structure complexity (number of observations) n is very different, with n = 23.9, 26.5, 46.6, 67.9, respectively. The dimension of tags |Y| is also diversified among tasks, with |Y| ranging from 5 to 45. Part-of-Speech Tagging (POS-Tagging). Part-of-Speech (POS) tagging is an important and highly competitive task. We use the standard benchmark dataset in prior work [3], with 38,219 training samples and 5,462 test samples. Following prior work [19], we use features based on words and lexical patterns, with 393,741 raw features5. The evaluation metric is per-word accuracy.\nBiomedical Named Entity Recognition (Bio-NER). This task is from the BioNLP-2004 shared task [19]. There are 17,484 training samples and 3,856 test samples. Following prior work [19], we use word pattern features and POS features, with 403,192 raw features in total. The evaluation metric is balanced F-score.\nWord Segmentation (Word-Seg). We use the MSR data provided by SIGHAN-2004 contest [4]. There are 86,918 training samples and 3,985 test samples. The features are similar to [16], with 1,985,720 raw features in total. The evaluation metric is balanced F-score.\nSensor-based Human Activity Recognition (Act-Recog). This is a task based on real-valued sensor signals, with the data extracted from the Bao04 activity recognition dataset [15]. The features are similar to [15], with 1,228 raw features in total. There are 16,000 training samples and 4,000 test samples. The evaluation metric is accuracy.\nWe choose the CRFs [6] and structured perceptrons (Perc) [3], which are arguably the most popular probabilistic and non-probabilistic structured prediction models, respectively. The CRFs are trained using the SGD algorithm,6 and the baseline method is the traditional weight regularization scheme (WeightReg), which adopts the most representative L2 weight regularization, i.e., a Gaussian prior.7 For the structured perceptrons, the baseline WeightAvg is the popular implicit regularization technique based on parameter averaging, i.e., averaged perceptron [3].\n5Raw features are those observation features based only on x, i.e., no combination with tag information. 6In theoretical analysis, following prior work we adopt the SGD with fixed learning rate, as described in Section 2.3. However, since the SGD with decaying learning rate is more commonly used in practice, in experiments we use the SGD with decaying learning rate.\n7We also tested on sparsity emphasized regularization methods, including L1 regularization and Group Lasso regularization [8]. However, we find that in most cases those sparsity emphasized regularization methods have lower accuracy than the L2 regularization.\nThe rich edge features [16] are employed for all methods. All methods are based on the 1st-order Markov dependency. For WeightReg, the L2 regularization strengths (i.e., λ/2 in Eq.(8)) are tuned among values 0.1, 0.5, 1, 2, 5, and are determined on the development data (POS-Tagging) or simply via 4-fold cross validation on the training set (Bio-NER, Word-Seg, and Act-Recog). With this automatic tuning for WeightReg, we set 2, 5, 1 and 5 for POS-Tagging, Bio-NER, Word-Seg, and Act-Recog tasks, respectively."
    }, {
      "heading" : "3.1 Experimental Results",
      "text" : "The experimental results in terms of accuracy/F-score are shown in Figure 2. For the CRF model, the training is convergent, and the results on the convergence state (decided by relative objective change with the threshold value of 0.0001) are shown. For the structured perceptron model, the training is typically not convergent, and the results on the 10’th iteration are shown. For stability of the curves, the results of the structured perceptrons are averaged over 10 repeated runs.\nSince different samples have different size n in practice, we set α being a function of n, so that the generated mini-samples are with fixed size n′ with n′ = n/α. Actually, n′ is a probabilistic distribution because we adopt randomized decomposition. For example, if n′ = 5.5, it means the minisamples are a mixture of the ones with the size 5 and the ones with the size 6, and the mean of the size distribution is 5.5. In the figure, the curves are based on n′ = 1.5, 2.5, 3.5, 5.5, 10.5, 15.5, 20.5.\nAs we can see, the results are quite consistent. It demonstrates that structure regularization leads to higher accuracies/F-scores compared with the existing baselines. We also conduct significance tests based on t-test. Since the t-test for F-score based tasks (Bio-NER and Word-Seg) may be unreliable8, we only perform t-test for the accuracy-based tasks, i.e., POS-Tagging and Act-Recog. For POS-Tagging, the significance test suggests that the superiority of StructReg over WeightReg is very statistically significant, with p < 0.01. For Act-Recog, the significance tests suggest that both the StructReg vs. WeightReg difference and the StructReg vs. WeightAvg difference are extremely statis-\n8Indeed we can convert F-scores to accuracy scores for t-test, but in many cases this conversion is unreliable. For example, very different F-scores may correspond to similar accuracy scores.\ntically significant, with p < 0.0001 in both cases. The experimental results support our theoretical analysis that structure regularization can further reduce the generalization risk over existing weight regularization techniques.\nOur method outperforms the benchmark systems on the three important natural language processing tasks. The POS-Tagging task is a highly competitive task, with many methods proposed, and the best report (without using extra resources) until now is achieved by using a bidirectional learning model in [13],9 with the accuracy 97.33%. Our simple method achieves better accuracy compared with all of those state-of-the-art systems. Furthermore, our method achieves as good scores as the benchmark systems on the Bio-NER and Word-Seg tasks. On the Bio-NER task, [19] achieves 72.28% based on lookahead learning and [21] achieves 72.65% based on reranking. On the Word-Seg task, [4] achieves 97.19% based on maximum entropy classification and our recent work [16] achieves 97.5% based on feature-frequency-adaptive online learning. The comparisons are summarized in Table 1.\nFigure 3 shows experimental comparisons in terms of wall-clock training time. As we can see, the proposed method can substantially improve the training speed. The speedup is not only from the faster convergence rates, but also from the faster processing time on the structures, because it is more efficient to process the decomposed samples with simple structures."
    }, {
      "heading" : "4 Conclusions",
      "text" : "We proposed a structure regularization framework, which decomposes training samples into minisamples with simpler structures, deriving a trained model with regularized structural complexity. Our theoretical analysis showed that this method can effectively reduce the generalization risk, and can also accelerate the convergence speed in training. The proposed method does not change the convexity of the objective function, and can be used together with any existing weight regularization methods. Note that, the proposed method and the theoretical results can fit general structures including linear chains, trees, and graphs. Experimental results demonstrated that our method achieved better results than state-of-the-art systems on several highly-competitive tasks, and at the same time with substantially faster training speed.\nAcknowledgments. This work was supported in part by NSFC (No.61300063).\n9See a collection of the systems at http://aclweb.org/aclwiki/index.php?title=POS_ Tagging_(State_of_the_art)"
    } ],
    "references" : [ {
      "title" : "A spectral regularization framework for multi-task structure learning",
      "author" : [ "A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Ying" ],
      "venue" : "In Proceedings of NIPS’07",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Stability and generalization",
      "author" : [ "O. Bousquet", "A. Elisseeff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms",
      "author" : [ "M. Collins" ],
      "venue" : "In Proceedings of EMNLP’02,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2002
    }, {
      "title" : "A comparative study of parameter estimation methods for statistical natural language processing",
      "author" : [ "J. Gao", "G. Andrew", "M. Johnson", "K. Toutanova" ],
      "venue" : "In Proceedings of ACL’07,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Posterior vs parameter sparsity in latent variable models",
      "author" : [ "J. Graça", "K. Ganchev", "B. Taskar", "F. Pereira" ],
      "venue" : "In Proceedings of NIPS’09,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J. Lafferty", "A. McCallum", "F. Pereira" ],
      "venue" : "In ICML’01,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "Pac-bayes generalization bounds for randomized structured prediction",
      "author" : [ "B. London", "B. Huang", "B. Taskar", "L. Getoor" ],
      "venue" : "In NIPS Workshop on Perturbation, Optimization and Statistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Structured sparsity in structured prediction",
      "author" : [ "A.F.T. Martins", "N.A. Smith", "M.A.T. Figueiredo", "P.M.Q. Aguiar" ],
      "venue" : "In Proceedings of EMNLP’11,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "F. Niu", "B. Recht", "C. Re", "S.J. Wright" ],
      "venue" : "In NIPS’11,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "An efficient projection for l1,infinity regularization",
      "author" : [ "A. Quattoni", "X. Carreras", "M. Collins", "T. Darrell" ],
      "venue" : "In Proceedings of ICML’09,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Convex structure learning in log-linear models: Beyond pairwise potentials",
      "author" : [ "M.W. Schmidt", "K.P. Murphy" ],
      "venue" : "In Proceedings of AISTATS’10,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Learnability and stability in the general learning setting",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan" ],
      "venue" : "In Proceedings of COLT’09,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Guided learning for bidirectional sequence classification",
      "author" : [ "L. Shen", "G. Satta", "A.K. Joshi" ],
      "venue" : "In Proceedings of ACL’07,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Structure regularization for structured prediction: Theories and experiments",
      "author" : [ "X. Sun" ],
      "venue" : "In Technical report,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Large-scale personalized human activity recognition using online multitask learning",
      "author" : [ "X. Sun", "H. Kashima", "N. Ueda" ],
      "venue" : "IEEE Trans. Knowl. Data Eng.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Feature-frequency-adaptive on-line training for fast and accurate natural language processing",
      "author" : [ "X. Sun", "W. Li", "H. Wang", "Q. Lu" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Piecewise pseudolikelihood for efficient training of conditional random fields",
      "author" : [ "C.A. Sutton", "A. McCallum" ],
      "venue" : "In ICML’07,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Max-margin markov networks",
      "author" : [ "B. Taskar", "C. Guestrin", "D. Koller" ],
      "venue" : "In NIPS’03,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2003
    }, {
      "title" : "Learning with lookahead: Can history-based models rival globally optimized models",
      "author" : [ "Y. Tsuruoka", "Y. Miyao", "J. Kazama" ],
      "venue" : "In Conference on Computational Natural Language Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Structural regularized support vector machine: A framework for structural large margin classifier",
      "author" : [ "H. Xue", "S. Chen", "Q. Yang" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Reranking for biomedical named-entity recognition",
      "author" : [ "K. Yoshida", "J. Tsujii" ],
      "venue" : "In ACL Workshop on BioNLP, page 209ĺC216,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "M. Yuan", "Y. Lin" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The term structural regularization has been used in prior work for regularizing structures of features, including spectral regularization [1], regularizing feature structures for classifiers [20], and many",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 19,
      "context" : "The term structural regularization has been used in prior work for regularizing structures of features, including spectral regularization [1], regularizing feature structures for classifiers [20], and many",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 10,
      "context" : "recent studies on structured sparsity in structured prediction scenarios [11, 8], via adopting mixed norm regularization [10], Group Lasso [22], and posterior regularization [5].",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "recent studies on structured sparsity in structured prediction scenarios [11, 8], via adopting mixed norm regularization [10], Group Lasso [22], and posterior regularization [5].",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "recent studies on structured sparsity in structured prediction scenarios [11, 8], via adopting mixed norm regularization [10], Group Lasso [22], and posterior regularization [5].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 21,
      "context" : "recent studies on structured sparsity in structured prediction scenarios [11, 8], via adopting mixed norm regularization [10], Group Lasso [22], and posterior regularization [5].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "recent studies on structured sparsity in structured prediction scenarios [11, 8], via adopting mixed norm regularization [10], Group Lasso [22], and posterior regularization [5].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 16,
      "context" : "[17] described an interesting heuristic piecewise training method.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] described a “lookahead\" learning method.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Our work differs from [17] and [19] mainly because our work is built on a regularization framework, with arguments and theoretical justifications on reducing generalization risk and improving convergence rate.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : "Our work differs from [17] and [19] mainly because our work is built on a regularization framework, with arguments and theoretical justifications on reducing generalization risk and improving convergence rate.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "On generalization risk analysis, related studies include [2, 12] on non-structured classification and [18, 7] on structured classification.",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "On generalization risk analysis, related studies include [2, 12] on non-structured classification and [18, 7] on structured classification.",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "On generalization risk analysis, related studies include [2, 12] on non-structured classification and [18, 7] on structured classification.",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 6,
      "context" : "On generalization risk analysis, related studies include [2, 12] on non-structured classification and [18, 7] on structured classification.",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "To state our theoretical results, we must describe several quantities and assumptions following prior work [2, 12].",
      "startOffset" : 107,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "To state our theoretical results, we must describe several quantities and assumptions following prior work [2, 12].",
      "startOffset" : 107,
      "endOffset" : 114
    }, {
      "referenceID" : 13,
      "context" : "Detailed analysis and proofs are given in the full version of this work [14].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "The proof is given in the full version of this work [14].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "The improved generalization bounds are given in the full version of this work [14].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "Following prior work [9], our analysis is based on the stochastic gradient descent (SGD) with fixed learning rate.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "The proof is given in the full version of this work [14].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "We use the standard benchmark dataset in prior work [3], with 38,219 training samples and 5,462 test samples.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : "Following prior work [19], we use features based on words and lexical patterns, with 393,741 raw features5.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 18,
      "context" : "This task is from the BioNLP-2004 shared task [19].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 18,
      "context" : "Following prior work [19], we use word pattern features and POS features, with 403,192 raw features in total.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "We use the MSR data provided by SIGHAN-2004 contest [4].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "The features are similar to [16], with 1,985,720 raw features in total.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "This is a task based on real-valued sensor signals, with the data extracted from the Bao04 activity recognition dataset [15].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "The features are similar to [15], with 1,228 raw features in total.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "We choose the CRFs [6] and structured perceptrons (Perc) [3], which are arguably the most popular probabilistic and non-probabilistic structured prediction models, respectively.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "We choose the CRFs [6] and structured perceptrons (Perc) [3], which are arguably the most popular probabilistic and non-probabilistic structured prediction models, respectively.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "(7)We also tested on sparsity emphasized regularization methods, including L1 regularization and Group Lasso regularization [8].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "The rich edge features [16] are employed for all methods.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 12,
      "context" : "The POS-Tagging task is a highly competitive task, with many methods proposed, and the best report (without using extra resources) until now is achieved by using a bidirectional learning model in [13],9 with the accuracy 97.",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 20,
      "context" : "28% based on lookahead learning and [21] achieves 72.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "On the Word-Seg task, [4] achieves 97.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 15,
      "context" : "19% based on maximum entropy classification and our recent work [16] achieves 97.",
      "startOffset" : 64,
      "endOffset" : 68
    } ],
    "year" : 2014,
    "abstractText" : "While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via structure decomposition, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies yet with substantially faster training speed.",
    "creator" : null
  }
}