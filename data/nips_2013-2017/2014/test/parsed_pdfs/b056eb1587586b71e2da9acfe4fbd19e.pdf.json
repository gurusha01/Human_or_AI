{
  "name" : "b056eb1587586b71e2da9acfe4fbd19e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Submodular Attribute Selection for Action Recognition in Video",
    "authors" : [ "Jinging Zheng", "Zhuolin Jiang" ],
    "emails" : [ "zjngjng@umiacs.umd.edu", "zhuolin.jiang@huawei.com", "rama@umiacs.umd.edu", "jonathon.phillips@nist.gov" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Action recognition in real-world videos has many potential applications in multimedia retrieval, video surveillance and human computer interaction. In order to accurately recognize human actions from videos, most existing approaches developed various discriminative low-level features, including spatio-temporal interest point (STIP) based features [8, 15], shape and optical flow-based features [19, 5], and trajectory-based representations [28, 33]. Because of large variations in viewpoints, complicated backgrounds, and people performing the actions differently, videos of an action vary greatly. A result of this variability is that conventional low-level features are not able to characterize the rich spatio-temporal structures in real-world action videos. Inspired by recent progress on object recognition [6, 14], multiple high-level semantic concepts called action attributes were introduced in [20, 17] to describe the spatio-temporal evolution of the action, object shapes and human poses, and contextual scenes. Since these action attributes are relatively robust to changes in viewpoints and scenes, they bridge the gap between low-level features and class labels. In this work, we focus on improving action recognition performance of attribute-based representations.\nEven though attribute-based representation appear effective for action recognition, they require humans to generate a list of attributes that may adequately describe a set of actions. From this list, humans then need to assign the action attributes to each class. Previous approaches [20, 17] simply used all the given attributes and ignored the difference in discriminative capability among attributes. This caused two major problems. First, a set of human-labeled attributes may be not be able to\nrepresent and distinguish a set of action classes. This is because humans subjectively annotate action videos with arbitrary attributes. For example, consider the two classes “ApplyEyeMakeup” and “ApplyLipStick” in UCF101 action dataset [30] shown in Figure 1. They have the same humanlabeled attribute set and cannot be distinguished from one another. Second, some manually labeled attributes may be noisy or redundant which leads to degradation in action recognition performance. In addition, their inclusion also increases the feature extraction time. Thus, it would be beneficial to use a smaller subset of attributes while achieving comparable or even improved performance.\nTo overcome the first problem, we propose another type of attributes that we call data-driven attributes. We show that data data-driven attributes are complementary to human-labeled attributes. Instead of using clustering-based algorithms to discover data-driven attributes as in [20], we propose a dictionary-based sparse representation method to discover a large data-driven attribute set. Our learned attributes are more suited to represent all the input data points because our method avoids the problem of hard assignment of data points to clusters. To address the attribute selection problem, we propose to select a compact and discriminative set of attributes from a large set of attributes. Three attribute selection criteria are proposed and then combined to form a submodular objective function. Our method encourages the selected attributes to have strong and similar discrimination capability for all pairs of actions. Furthermore, our method maximizes the sum of maximum coverage that each pairwise class can obtain from the selected attributes."
    }, {
      "heading" : "2 Related Work",
      "text" : "Attribute-based representation for action recognition: Recently, several attribute-based representations have been proposed for improving action recognition performance. Liu et al. [20] modeled attributes as latent variables and searched for the best configuration of attributes for each action using latent SVMs. However, the performance may drop drastically when some attributes are too noisy or redundant. This is because pretrained attribute classifiers from these noisy attributes perform poorly. Li et al. [17] decomposed a video sequence into short-term segments and characterized segments by the dynamics of their attributes. However, since attributes are defined over the entire action video instead of short-term segments, different decomposition of video segments may obtain different attribute dynamics.\nAnother line of work similar to attribute-based methods is based on learning different types of midlevel representations. These mid-level representations usually identify the occurrence of semantic concepts of interest, such as scene types, actions and objects. Fathi et al. [7] proposed to construct mid-level motion features from low-level optical flow features using AdaBoost. Wang et al. [35] modeled a human action as a global root template and a constellation of several parts. Raptis et al. [27] used trajectory clusters as candidates for the parts of an action and assembled these clusters into an action class by graphical modeling. Jain et al. [10] presented a new mid-level representation for videos based on discriminative spatio-temporal patches, which are automatically mined from videos using an exemplar-based clustering approach.\nSubmodularity: Submodular functions are a class of set functions that have the the property of diminishing returns [24]. Given a set E, a set function F : 2E → R is submodular if F (A ∪ v) − F (A) ≥ f(B ∪ v) − F (B) holds for all A ⊆ B ⊆ E and v ∈ E \\ B. The diminishing returns mean that the marginal value of the element v decreases if used in a later stage. Recently, submodular functions have been widely exploited in various applications, such as sensor placements [13], superpixel segmentation [22], document summarization [18], and feature selection [3, 23]. Liu et al. [23] presented a submodular feature selection method for acoustic score spaces based on existing facility location and saturated coverage functions. Krause et al. [12] de-\nveloped a submodular method for selecting dictionary columns from multiple candidates for sparse representation. Iyer et al. [9] designed a new framework for both unconstrained and constrained submodular function optimization. Streeter et al. [31] proposed an online algorithm for maximizing submodular functions. Different from these approaches, we define a novel submodular objective function for attribute selection. Although we only evaluate our approach for action recognition, it can be applied to other recognition tasks that use attribute descriptions."
    }, {
      "heading" : "3 Submodular Attribute Selection",
      "text" : "In this section, we first propose three attribute selection criteria. In order to satisfy these criteria, we define a submodular function based on entropy rate of a random walk and a weighted maximum coverage function. Then we introduce algorithms for the detection of human-labeled attributes and extraction of data-driven attributes."
    }, {
      "heading" : "3.1 Attribute Selection Criteria",
      "text" : "Assume that we have C classes and a large attribute set P = {a1, a2, .., aM} which contains M attributes. The set that includes all combinations of pairwise classes is represented by U = {u1(1, 1), u2(1, 2), ..., ul(i, j), ..., uL(C − 1, C)} where ul(i, j), i < j denotes the pairwise combination of classes i and j, l is the index of this combination in U , and L = C × (C − 1)/2 is the total number of all possible pairwise classes. Here we propose to use the Fisher score to construct an attribute contribution matrix A ∈ RM×L, where an entry Ad,l represents the discrimination capability of attribute ad for differentiating the class pair (i, j) indexed by ul(i, j). Specifically, given the attribute ad and class pair (i, j), let µdk and σ d k be the mean and standard deviation of k-th class and µd be the mean of samples from both classes i and j corresponding to d-th attribute. The Fisher score of attribute ad for differentiating the class pair (i, j) is computed\nas follows: Ad,l(i,j) = ∑ k=i,j nk(µ d k−µ\nd)2∑ k=i,j nkσ 2 k\nwhere l is the index of pairwise classes (i, j) in U , and nk is the number of points from class k. Note that different methods can be used to measure the discrimination capability of ad, such as mutual information and T-test.\nGiven A, we can obtain a row vector r by summing up its elements from each column that are in rows corresponding to selected attributes S. An example of vector r is shown in Figure 2a. We would like to have r satisfy two selection criteria: (1) each entry of r should be as large as possible; and (2) the variance of all entries of r should be small. The first criterion encourages S to provide as much discrimination capability as possible for each pairwise classes. The second criterion makes S have similar discrimination capability for each pairwise classes. These two criteria can be satisfied by maximizing the entropy rate of a random walk on the proposed graphs. Meanwhile, since some attributes may well differentiate the same collection of pairwise classes, it would be redundant to select all these attributes. In other words, one combination of pairwise classes may be repeatedly “covered” (differentiated) by multiple attributes. It is better to select other attributes which can differentiate “uncovered” combinations of pairwise classes. Therefore, we propose the third criterion: the sum of maximum discrimination capability that each pairwise classes can obtain from the selected attributes should be maximized. We will model it as a weighted maximum coverage problem and encourage S to have a maximum coverage of all pairwise classes."
    }, {
      "heading" : "3.2 Entropy Rate-based Attribute Selection",
      "text" : "In order to achieve the first two criteria, we need to construct an undirected graph and maximize the entropy rate of a random walk on this graph. We aim to obtain a subset S so that the attribute-based representation has good discrimination power.\nGraph Construction: We use G = (V,E) to denote an undirected graph where V is the vertex set, and E is the edge set. The vertex vi represents class i and the edge ei,j connecting class i and j represents that class i and j can be differentiated by the selected attribute subset S to some extent. The edge weight for ei,j is defined aswi,j = ∑ d∈S Ad,l, which represents the discrimination capability of S for differentiating class i from class j. The edge weights are symmetric, i.e. wi,j = wj,i. In addition, we add a self-loop ei,i for each vertex vi of G. And the weight for self-loop ei,i is defined as wi,i = ∑ d∈P\\S Ad,l. The total incident weight for each vertex is kept constant so that it produces a stationary distribution for the later proposed random walk on this graph. Note that the addition of these self-loops do not affect the selection of attributes and the graph will change with the selected subset S. Figure 2 gives an example to illustrate the benefits of the entropy rate.\nEntropy Rate: Let X = {Xt|t ∈ T,Xt ∈ V } be a random walk on the graph G = (V,E) with nonnegative discrimination measure w. We use the random walk model from [2] with a transition probability defined as below:\npi,j(S) = { wi,j wi = ∑ d∈S Ad,l wi\nif i 6= j 1− ∑ k:k 6=i wi,k wi = ∑ d∈P\\S Ad,l wi if i = j\n(1)\nwhere S is the selected attribute subset and wi = ∑ m:ei,m∈E wi,m is the sum of incident weights of the vertex vi including the self-loop. The stationary distribution for this random walk is given by µ = (µ1, µ2, ..., µC)\nT = (w1w0 , w2 w0 , ..., wCw0 ) where w0 = ∑C i=1 wi is the sum of the total weights\nincident on all vertices. For a stationary 1st-order Markov chain, the entropy rate which measures the uncertainty of the stochastic processX is given by: H(X) = limt→∞H(Xt|Xt−1, Xt−2, ..., X1) = limt→∞H(Xt|Xt−1) = H(X2|X1). More details can be found in [2]. Consequently, the entropy rate of the random walk X on our proposed graph G = (V,E) can be written as a set function:\nH(S) = ∑ i uiH(X2|X1 = vi) = − ∑ i ui ∑ j pi,j(S)log(pi,j(S)) (2)\nIntuitively, the maximization of the entropy rate will have two properties. First, it encourages the maximization of pi,j(S) where i = 1, ..., C and i 6= j. This can make edge weights wi,j , i 6= j as large as possible, so class i can be easily differentiated from other classes j (i.e., satisfying the first criteria). Second, it makes all class vertices have transition probabilities similar to other connected class vertices, so the discrimination capabilities of class i from other classes are very similar (i.e., satisfying the second criteria). Maximizing the entropy rate of the random walk on the proposed graph can select a subset of attributes that are compact and discriminative for differentiating all pairwise classes. Proposition 3.1. The entropy rate of the random walkH : 2M → R is a submodular function under the proposed graph construction.\nThe observation that adding an attribute in a later stage has a lower increase in the uncertainty establishes the submodularity of the entropy rate. This is because at a later stage, the increased edge weights from the added attribute will be shared with attributes which contribute to the differentiation of the same pair of classes. A detailed proof based on [22] is given in the supplementary section."
    }, {
      "heading" : "3.3 Weighted Maximum Coverage-based Attribute Selection",
      "text" : "We consider a weighted maximum coverage function to achieve the last criteria that the selected subset S should maximize the coverage of all combinations of pairwise classes. For each attribute ad, we define a coverage set Ud ⊆ U which covers all the combinations of pairwise classes that attribute ad can differentiate. Meanwhile, for each element (combination) ul ∈ U that is covered by Ud, we define a coverage weight w(Ud, ul) = Ad,l. Given the universe set U and these coverage sets Ud, d = 1, ...,M , the weighted maximum coverage problem is to select at most K coverage sets, such that the sum of maximum coverage weight each element can obtain from S is maximized. The weighted maximum coverage function is defined as follows:\nQ(S) = ∑ ul∈U max d∈S w(Ud, ul) = ∑ ul∈U max d∈S Ad,l, s.t.NS ≤ K (3)\nwhere NS is the number of attributes in S. Note that the weighted maximum coverage problem is reduced to the well studied set-cover problem when all the coverage weights are equal to be ones. Proposition 3.2. The weighted maximum coverage function Q : 2M → R is a monotonically increasing submodular function under the proposed set representation.\nFor the weighted maximum coverage term, monotonicity is obvious because the addition of any attribute will increase the number of covered elements in U . Submodularity results from the observation that the coverage weights of increased covered elements will be less from adding an attribute in a later stage because some elements may be already covered by previously selected attributes. The proof is given in the supplementary section."
    }, {
      "heading" : "3.4 Objective Function and Optimization",
      "text" : "Combing the entropy rate term and the weighted maximum coverage term, the overall objective function for attribute selection is formulated as follows:\nmaxF(S) = max S H(S) + λQ(S) s.t.NS ≤ K (4)\nwhere λ controls the relative contribution between entropy rate and the weighted maximum coverage term. The objective function is submodular because linear combination of two submodular functions with nonnegative coefficients preserves submodularity [24]. Direct maximization of a submodular\nAlgorithm 1 Submodular Attribute Selection 1: Input: G = (V,E), A and λ 2: Output: S 3: Initialization: S ← ∅ 4: for NS < K and F (S ∪ a)− F (S) ≥ 0 do 5: am = argmaxS∪amF(S ∪ {am})−F(S) 6: S ← am 7: end for\nfunction is an NP-hard problem. However, a greedy algorithm from [24] gives a near-optimal solution with a (1 − 1/e)-approximation bound. The greedy algorithm starts from an empty attribute set S = ∅ ; and iteratively adds one attribute that provides the largest gain for F at each iteration. The iteration stops when the maximum number of selected attributes is obtained or F(S) decreases. Algorithm 1 presents the pseudo code of our algorithm. A naive implementation of this algorithm has the complexity of O(|M |2), because it needs to loop O(|M |) times to add a new attribute and scan through the whole attribute list in each loop. By exploiting the submodularity of the objective function, we use the lazy greedy approach presented in [16] to speed up the optimization process."
    }, {
      "heading" : "3.5 Human-labeled Attribute and Data-driven Attribute Extraction",
      "text" : "Action videos can be characterized by a collection of human-labeled attributes [20]. For example, the action “long-jump” in Olympic Sports Dataset [25] is associated with either the motion attributes (jump forward, motion in the air), or with the scene attributes (e.g., outdoor, track). Given an action\nvideo x, an attribute classifier fa : x → {0, 1} predicts the confidence score of the presence of attribute a in the video. This classifier fa is learned using the training samples of all action classes which have this attribute as positive and the rest as negative. Given a set of attribute classifiers S = {fai(x)}mi=1, an action video x ∈ Rd is mapped to the semantic space O: h : Rd → O = [0, 1]m where h(x) = (h1(x), ..., hm(x))T is a m-dimensional attribute score vector.\nPrevious works [21, 20] on data-driven attribute discovery used k-means or information theoretic clustering algorithms to obtain the clusters as the learned attributes. In this paper, we propose to discover a large initial set of data-driven attributes using a dictionary learning method. Specifically, assume that we have a set of N videos in a n-dimensional feature space X = [x1, ..., xN ], xi ∈ Rn, then a data-driven dictionary is learned by solving the following problem:\nargmin D,Z ||X −DZ||22 s.t. ∀i, ||zi||0 ≤ T (5)\nwhere D = [d1...dK ], di ∈ Rn is the learned attribute dictionary of size K, Z = [zi...zN ], zi ∈ RK are the sparse codes of X , and T specifies the sparsity that each video has fewer than T items in its decomposition. Compared to k-means clustering, this dictionary-based learning scheme avoids the hard assignment of cluster centers to data points. Meanwhile, it doesn’t require the estimation of the probability density function of clusters in information theoretic clustering. Note that our attribute selection framework is very general and different initial attribute extraction methods can be used here."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we validate our method for action recognition on two public datasets: Sports dataset [25] and UCF101 [20] dataset. Specifically, we consider three sets of attributes: humanlabeled attribute set (HLA set), data-driven attribute set (DDA set) and the set mixing both types of attributes (Mixed set). To demonstrate the effectiveness of our selection framework, we compare the result using the selected subset with the result based on the initial set.\nWe also compare our method with other two submodular approaches based on the facility location function (FL) and saturated coverage function (SC) respectively in [23]. These objective functions are defined as follows: Ffa(S) = ∑ i∈V maxj∈S wi,j ,Fsa(S) = ∑ i∈V min{Ci(S), αCi(V)}\nwhere wi,j is a similarity between attribute i and j, Ci(S) = ∑ j∈S wi,j measures the degree that attribute i is “covered” by S and α is a hyperparameter that determines a global saturation threshold. For the two approaches compared against, we consider an undirected k-nearest neighbor graph and use a Gaussian kernel to compute pairwise similaritieswi,j = exp(−βd2i,j) where di,j is the distance between attribute i and j, β = (2〈d2i,j〉)−1 and 〈·〉 denotes expectation over all pairwise distances. Finally, we compare the performance of attribute-based representation with several state-of-the-art approaches on the two datasets."
    }, {
      "heading" : "4.1 Olympic Sports Dataset",
      "text" : "The Olympic Sports dataset contains 783 YouTube video clips of 16 sports activities. We followed the protocol in [20] to extract STIP features [4]. Each action video is finally represented by a 2000- dimensional histogram. We use 40 human-labeled attributes provided by [20]. Three attribute-based representations are constructed as follows: (1) HLA set: For each human-labeled attribute, we train a binary SVM with a histogram intersection kernel. We concatenate confidence scores from all these attribute classifiers into a 40-dimensional vector to represent this video. (2) DDA set: For data-driven attributes, we learn a dictionary of size 457 from all video features using KSVD [1] and each video is represented by a 457-dimensional sparse coefficient vector. (3) Mixed set: This attribute set is obtained by combining HLA set and DDA set.\nWe compare the performance of features based on selected attributes with those based on the initial attribute set. For all the different attribute-based features, we use an SVM with Gaussian kernel for classification. Table 1 shows classification accuracies of different attribute-based representations. Compared with the initial attribute set, the selected attributes have greatly improved the classification accuracy, which demonstrates the effectiveness of our method for selecting a subset of discriminative attributes. Moreover, features based on the Mixed set outperform features based on either HLA set or DDA set. This shows that data-driven attributes are complementary to human-labeled attributes and together they offer a better description of actions. Table 2 shows the per-category average precision (AP) and mean AP of different approaches. It can be seen that our method achieves\nthe best performance. This illustrates the benefits of selecting discriminative attributes and removing noisy and redundant attributes. Note that our method outperforms the method that is most similar to ours [20] which uses complex latent SVMs to combine low-level features, human-labeled attributes and data-driven attributes. Moreover, compared with other dynamic classifiers [25, 17] which account for the dynamics of bag-of-features or action attributes, our method still obtains comparable results. This is because the provided human-labeled attributes are very noisy and they can greatly affect the training of latent SVM and representation of the attribute dynamics.\nFigures 4a 4b 4c show classification accuracies of attribute subsets selected by different submodular selection methods. It can be seen that our method outperforms the other two submodular selection methods for the three different attribute sets. This is because our method prefers attributes with large and similar discrimination capability for differentiating pairwise classes, while the other two methods prefer attributes with large similarity to other attributes (i.e. representative), without explicitly considering the discrimination capabilities of selected attributes. Figure 4d shows the performance curves for a range of λ. We observe that the combination of entropy rate term and maximum coverage term obtains a higher classification accuracy than when only one of them is used. In addition, our approach is insensitive to the selection of λ. Hence we use λ = 0.1 throughout the experiments."
    }, {
      "heading" : "4.2 UCF101 Dataset",
      "text" : "UCF101 dataset contains over 10,000 video clips from 101 different human action categories. We compute the improved version of dense trajectories in [34] and extract three types of descriptors: histogram of oriented gradients (HOG), histogram of optical flow (HOF) and motion boundary his-\ntogram (MBH). We use Fisher vector encoding [26] and obtain 101,376-dimensional histogram to represent each action video. Three different attribute sets and corresponding attribute-based representations are constructed as follows: (1) HLA set: Due to the high dimensionality of features and large number of samples, the linear SVM is trained for the detection of each human-labeled attribute. We concatenate confidence scores from all these attribute classifiers into a 115-dimensional vector to represent a video. (2) DDA set: For data-driven attributes, we first apply PCA to reduce the dimension of histogram descriptors to be 3300 and then learn a dictionary of size 3030. The features based on data-driven attributes are 3030-dimensional sparse coefficient vectors. (3) Mixed set: HLA set plus DDA set.\nFollowing the training and testing dataset partitions proposed in [30], we train a linear SVM and report classification accuracies of different attribute-based representations in Table 1. The selected attribute subset outperforms the initial attribute set again which demonstrates the effectiveness of our proposed attribute selection method. Figure 5 shows the results of attribute subsets selected by different submodular selection methods. Note that this dataset is highly challenging because the training and test videos of the same action have different backgrounds and actors. You can see that our method still substantially outperforms the other two submodular methods. This is because some redundant attributes dominated the selection process and the attributes selected by comparing approaches had very unbalanced discrimination capability for different classes. However, the attributes selected by our method have strong and similar discrimination capability for each class. Table 3 presents the classification accuracies of several state-of-the-art approaches on this dataset. Our method achieves comparable results to the best result 85.9% from [34] which uses complex spatio-temporal pyramids to embed structure information in features. Note that our method also outperforms other methods which make use of complicated and advanced feature extraction and encoding techniques."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We exploited human-labeled attributes and data-driven attributes for improving the performance of action recognition algorithms. We first presented three attribute selection criteria for the selection of discriminative and compact attributes. Then we formulated the selection procedure as one of optimizing a submodular function based on the entropy rate of a random walk and weighted maximum coverage function. Our selected attributes not only have strong and similar discrimination capability for all pairwise classes, but also maximize the sum of largest discrimination capability that each pairwise classes can obtain from the selected attributes. Experimental results on two challenging dataset show that the proposed method significantly outperforms many state-of-the art approaches."
    }, {
      "heading" : "6 Acknowledgements",
      "text" : "The identification of any commercial product or trade name does not imply endorsement or recommendation by NIST. This research was partially supported by a MURI from the Office of Naval research under the Grant 1141221258513."
    } ],
    "references" : [ {
      "title" : "KSVD: An algorithm for designing overcomplete dictionaries for sparse representation",
      "author" : [ "M. Aharon", "M. Elad", "A. Bruckstein" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Elements of Information Theory",
      "author" : [ "T.M. Cover", "J.A. Thomas" ],
      "venue" : "Wiley-Interscience,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Selecting diverse features via spectral regularization",
      "author" : [ "A. Das", "A. Dasgupta", "R. Kumar" ],
      "venue" : "NIPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Behavior recognition via sparse spatio-temporal features",
      "author" : [ "P. Dollar", "V. Rabaud", "G. Cottrell", "S. Belongie" ],
      "venue" : "VS-PETS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Recognizing action at a distance",
      "author" : [ "A.A. Efros", "A.C. Berg", "E.C. Berg", "G. Mori", "J. Malik" ],
      "venue" : "ICCV,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Describing objects by their attributes",
      "author" : [ "A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth" ],
      "venue" : "CVPR,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Action recognition by learning mid-level motion features",
      "author" : [ "A. Fathi", "G. Mori" ],
      "venue" : "CVPR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Actions as space-time shapes",
      "author" : [ "L. Gorelick", "M. Blank", "E. Shechtman", "M. Irani", "R. Basri" ],
      "venue" : "ICCV,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Fast semidifferential-based submodular function optimization",
      "author" : [ "R. Iyer", "S. Jegelka", "J. Bilmes" ],
      "venue" : "ICML,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Representing videos using mid-level discriminative patches",
      "author" : [ "A. Jain", "A. Gupta", "M. Rodriguez", "L. Davis" ],
      "venue" : "CVPR,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Label consistent K-SVD: Learning a discriminative dictionary for recognition",
      "author" : [ "Z. Jiang", "Z. Lin", "L.S. Davis" ],
      "venue" : "PAMI,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Submodular dictionary selection for sparse representation",
      "author" : [ "A. Krause", "V. Cevher" ],
      "venue" : "ICML,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Near-optimal sensor placements in gaussian processes",
      "author" : [ "A. Krause", "A. Singh", "C. Guestrin", "C. Williams" ],
      "venue" : "ICML,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning to detect unseen object classes by between-class attribute transfer",
      "author" : [ "C.H. Lampert", "H. Nickisch", "S. Harmeling" ],
      "venue" : "CVPR,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Space-time interest points",
      "author" : [ "I. Laptev", "T. Lindeberg" ],
      "venue" : "ICCV,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Cost-effective outbreak detection in networks",
      "author" : [ "J. Leskovec", "A. Krause", "C. Guestrin", "C. Faloutsos", "J. VanBriesen", "N. Glance" ],
      "venue" : "KDD,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Recognizing activities by attribute dynamics",
      "author" : [ "W. Li", "N. Vasconcelos" ],
      "venue" : "NIPS,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A class of submodular functions for document summarization",
      "author" : [ "H. Lin", "J. Bilmes" ],
      "venue" : "Proceedings of ACL,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Recognizing actions by shape-motion prototype trees",
      "author" : [ "Z. Lin", "Z. Jiang", "L.S. Davis" ],
      "venue" : "CVPR,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Recognizing human actions by attributes",
      "author" : [ "J. Liu", "B. Kuipers", "S. Savarese" ],
      "venue" : "CVPR,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning semantic visual vocabularies using diffusion distance",
      "author" : [ "J. Liu", "Y. Yang", "M. Shah" ],
      "venue" : "CVPR,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Entropy-rate clustering: Cluster analysis via maximizing a submodular function subject to a matroid constraint",
      "author" : [ "M.-Y. Liu", "O. Tuzel", "S. Ramalingam", "R. Chellappa" ],
      "venue" : "PAMI,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Submodular feature selection for high-dimensional acoustic score spaces",
      "author" : [ "Y. Liu", "K. Wei", "K. Kirchhoff", "Y. Song", "J. Bilmes" ],
      "venue" : "ICASSP,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "An analysis of approximations for maximizing submodular set functionsi",
      "author" : [ "G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Modeling temporal structure of decomposable motion segments for activity classification",
      "author" : [ "J.C. Niebles", "C. wei Chen", "L. Fei-fei" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "Improving the fisher kernel for large-scale image classification",
      "author" : [ "F. Perronnin", "J. Sánchez", "T. Mensink" ],
      "venue" : "ECCV,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Discovering discriminative action parts from mid-level video representations",
      "author" : [ "M. Raptis", "I. Kokkinos", "S. Soatto" ],
      "venue" : "CVPR,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Tracklet descriptors for action modeling and video analysis",
      "author" : [ "M. Raptis", "S. Soatto" ],
      "venue" : "ECCV,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Action bank: A high-level representation of activity in video",
      "author" : [ "S. Sadanand", "J. Corso" ],
      "venue" : "CVPR,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "UCF101: A dataset of 101 human action classes from videos in the wild",
      "author" : [ "K. Soomro", "A.R. Zamir", "M. Shah" ],
      "venue" : "CRCV-TR-12-01,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An online algorithm for maximizing submodular functions",
      "author" : [ "M. Streeter", "D. Golovin" ],
      "venue" : "NIPS,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning latent temporal structure for complex event detection",
      "author" : [ "K.D. Tang", "F.-F. Li", "D. Koller" ],
      "venue" : "CVPR,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Dense trajectories and motion boundary descriptors for action recognition",
      "author" : [ "H. Wang", "A. Kläser", "C. Schmid", "C.-L. Liu" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Action recognition with improved trajectories",
      "author" : [ "H. Wang", "C. Schmid" ],
      "venue" : "ICCV,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Max-margin hidden conditional random fields for human action recognition",
      "author" : [ "Y. Wang", "G. Mori" ],
      "venue" : "CVPR,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Towards good practices for action video encoding",
      "author" : [ "J. Wu", "Y. Zhang", "W. Lin" ],
      "venue" : "ICCV,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Action recognition with actons",
      "author" : [ "J. Zhu", "B. Wang", "X. Yang", "W. Zhang", "Z. Tu" ],
      "venue" : "ICCV,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "In order to accurately recognize human actions from videos, most existing approaches developed various discriminative low-level features, including spatio-temporal interest point (STIP) based features [8, 15], shape and optical flow-based features [19, 5], and trajectory-based representations [28, 33].",
      "startOffset" : 201,
      "endOffset" : 208
    }, {
      "referenceID" : 14,
      "context" : "In order to accurately recognize human actions from videos, most existing approaches developed various discriminative low-level features, including spatio-temporal interest point (STIP) based features [8, 15], shape and optical flow-based features [19, 5], and trajectory-based representations [28, 33].",
      "startOffset" : 201,
      "endOffset" : 208
    }, {
      "referenceID" : 18,
      "context" : "In order to accurately recognize human actions from videos, most existing approaches developed various discriminative low-level features, including spatio-temporal interest point (STIP) based features [8, 15], shape and optical flow-based features [19, 5], and trajectory-based representations [28, 33].",
      "startOffset" : 248,
      "endOffset" : 255
    }, {
      "referenceID" : 4,
      "context" : "In order to accurately recognize human actions from videos, most existing approaches developed various discriminative low-level features, including spatio-temporal interest point (STIP) based features [8, 15], shape and optical flow-based features [19, 5], and trajectory-based representations [28, 33].",
      "startOffset" : 248,
      "endOffset" : 255
    }, {
      "referenceID" : 27,
      "context" : "In order to accurately recognize human actions from videos, most existing approaches developed various discriminative low-level features, including spatio-temporal interest point (STIP) based features [8, 15], shape and optical flow-based features [19, 5], and trajectory-based representations [28, 33].",
      "startOffset" : 294,
      "endOffset" : 302
    }, {
      "referenceID" : 32,
      "context" : "In order to accurately recognize human actions from videos, most existing approaches developed various discriminative low-level features, including spatio-temporal interest point (STIP) based features [8, 15], shape and optical flow-based features [19, 5], and trajectory-based representations [28, 33].",
      "startOffset" : 294,
      "endOffset" : 302
    }, {
      "referenceID" : 5,
      "context" : "Inspired by recent progress on object recognition [6, 14], multiple high-level semantic concepts called action attributes were introduced in [20, 17] to describe the spatio-temporal evolution of the action, object shapes and human poses, and contextual scenes.",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "Inspired by recent progress on object recognition [6, 14], multiple high-level semantic concepts called action attributes were introduced in [20, 17] to describe the spatio-temporal evolution of the action, object shapes and human poses, and contextual scenes.",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "Inspired by recent progress on object recognition [6, 14], multiple high-level semantic concepts called action attributes were introduced in [20, 17] to describe the spatio-temporal evolution of the action, object shapes and human poses, and contextual scenes.",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 16,
      "context" : "Inspired by recent progress on object recognition [6, 14], multiple high-level semantic concepts called action attributes were introduced in [20, 17] to describe the spatio-temporal evolution of the action, object shapes and human poses, and contextual scenes.",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 19,
      "context" : "Previous approaches [20, 17] simply used all the given attributes and ignored the difference in discriminative capability among attributes.",
      "startOffset" : 20,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "Previous approaches [20, 17] simply used all the given attributes and ignored the difference in discriminative capability among attributes.",
      "startOffset" : 20,
      "endOffset" : 28
    }, {
      "referenceID" : 29,
      "context" : "For example, consider the two classes “ApplyEyeMakeup” and “ApplyLipStick” in UCF101 action dataset [30] shown in Figure 1.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "Instead of using clustering-based algorithms to discover data-driven attributes as in [20], we propose a dictionary-based sparse representation method to discover a large data-driven attribute set.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "[20] modeled attributes as latent variables and searched for the best configuration of attributes for each action using latent SVMs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] decomposed a video sequence into short-term segments and characterized segments by the dynamics of their attributes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "[7] proposed to construct mid-level motion features from low-level optical flow features using AdaBoost.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 34,
      "context" : "[35] modeled a human action as a global root template and a constellation of several parts.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[27] used trajectory clusters as candidates for the parts of an action and assembled these clusters into an action class by graphical modeling.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[10] presented a new mid-level representation for videos based on discriminative spatio-temporal patches, which are automatically mined from videos using an exemplar-based clustering approach.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "Submodularity: Submodular functions are a class of set functions that have the the property of diminishing returns [24].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "Recently, submodular functions have been widely exploited in various applications, such as sensor placements [13], superpixel segmentation [22], document summarization [18], and feature selection [3, 23].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 21,
      "context" : "Recently, submodular functions have been widely exploited in various applications, such as sensor placements [13], superpixel segmentation [22], document summarization [18], and feature selection [3, 23].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "Recently, submodular functions have been widely exploited in various applications, such as sensor placements [13], superpixel segmentation [22], document summarization [18], and feature selection [3, 23].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 2,
      "context" : "Recently, submodular functions have been widely exploited in various applications, such as sensor placements [13], superpixel segmentation [22], document summarization [18], and feature selection [3, 23].",
      "startOffset" : 196,
      "endOffset" : 203
    }, {
      "referenceID" : 22,
      "context" : "Recently, submodular functions have been widely exploited in various applications, such as sensor placements [13], superpixel segmentation [22], document summarization [18], and feature selection [3, 23].",
      "startOffset" : 196,
      "endOffset" : 203
    }, {
      "referenceID" : 22,
      "context" : "[23] presented a submodular feature selection method for acoustic score spaces based on existing facility location and saturated coverage functions.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[9] designed a new framework for both unconstrained and constrained submodular function optimization.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 30,
      "context" : "[31] proposed an online algorithm for maximizing submodular functions.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "We use the random walk model from [2] with a transition probability defined as below:",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "A detailed proof based on [22] is given in the supplementary section.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 23,
      "context" : "The objective function is submodular because linear combination of two submodular functions with nonnegative coefficients preserves submodularity [24].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 23,
      "context" : "However, a greedy algorithm from [24] gives a near-optimal solution with a (1 − 1/e)-approximation bound.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 15,
      "context" : "By exploiting the submodularity of the objective function, we use the lazy greedy approach presented in [16] to speed up the optimization process.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : "5 Human-labeled Attribute and Data-driven Attribute Extraction Action videos can be characterized by a collection of human-labeled attributes [20].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 24,
      "context" : "For example, the action “long-jump” in Olympic Sports Dataset [25] is associated with either the motion attributes (jump forward, motion in the air), or with the scene attributes (e.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 20,
      "context" : "Previous works [21, 20] on data-driven attribute discovery used k-means or information theoretic clustering algorithms to obtain the clusters as the learned attributes.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "Previous works [21, 20] on data-driven attribute discovery used k-means or information theoretic clustering algorithms to obtain the clusters as the learned attributes.",
      "startOffset" : 15,
      "endOffset" : 23
    }, {
      "referenceID" : 24,
      "context" : "In this section, we validate our method for action recognition on two public datasets: Sports dataset [25] and UCF101 [20] dataset.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 19,
      "context" : "In this section, we validate our method for action recognition on two public datasets: Sports dataset [25] and UCF101 [20] dataset.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 22,
      "context" : "We also compare our method with other two submodular approaches based on the facility location function (FL) and saturated coverage function (SC) respectively in [23].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 19,
      "context" : "We followed the protocol in [20] to extract STIP features [4].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "We followed the protocol in [20] to extract STIP features [4].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : "We use 40 human-labeled attributes provided by [20].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "(2) DDA set: For data-driven attributes, we learn a dictionary of size 457 from all video features using KSVD [1] and each video is represented by a 457-dimensional sparse coefficient vector.",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "Activity [15] [25] [32] [20] [17] HLA DDA Mixed high-jump 52.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 24,
      "context" : "Activity [15] [25] [32] [20] [17] HLA DDA Mixed high-jump 52.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 31,
      "context" : "Activity [15] [25] [32] [20] [17] HLA DDA Mixed high-jump 52.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "Activity [15] [25] [32] [20] [17] HLA DDA Mixed high-jump 52.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "Activity [15] [25] [32] [20] [17] HLA DDA Mixed high-jump 52.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 19,
      "context" : "Note that our method outperforms the method that is most similar to ours [20] which uses complex latent SVMs to combine low-level features, human-labeled attributes and data-driven attributes.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 24,
      "context" : "Moreover, compared with other dynamic classifiers [25, 17] which account for the dynamics of bag-of-features or action attributes, our method still obtains comparable results.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "Moreover, compared with other dynamic classifiers [25, 17] which account for the dynamics of bag-of-features or action attributes, our method still obtains comparable results.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 33,
      "context" : "We compute the improved version of dense trajectories in [34] and extract three types of descriptors: histogram of oriented gradients (HOG), histogram of optical flow (HOF) and motion boundary his-",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 33,
      "context" : "splits [34] [36] [37] [11] [29] HLA DDA Mixed 1 83.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 35,
      "context" : "splits [34] [36] [37] [11] [29] HLA DDA Mixed 1 83.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 36,
      "context" : "splits [34] [36] [37] [11] [29] HLA DDA Mixed 1 83.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 10,
      "context" : "splits [34] [36] [37] [11] [29] HLA DDA Mixed 1 83.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 28,
      "context" : "splits [34] [36] [37] [11] [29] HLA DDA Mixed 1 83.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 25,
      "context" : "We use Fisher vector encoding [26] and obtain 101,376-dimensional histogram to represent each action video.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 29,
      "context" : "Following the training and testing dataset partitions proposed in [30], we train a linear SVM and report classification accuracies of different attribute-based representations in Table 1.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 33,
      "context" : "9% from [34] which uses complex spatio-temporal pyramids to embed structure information in features.",
      "startOffset" : 8,
      "endOffset" : 12
    } ],
    "year" : 2014,
    "abstractText" : "In real-world action recognition problems, low-level features cannot adequately characterize the rich spatial-temporal structures in action videos. In this work, we encode actions based on attributes that describes actions as high-level concepts e.g., jump forward or motion in the air. We base our analysis on two types of action attributes. One type of action attributes is generated by humans. The second type is data-driven attributes, which are learned from data using dictionary learning methods. Attribute-based representation may exhibit high variance due to noisy and redundant attributes. We propose a discriminative and compact attribute-based representation by selecting a subset of discriminative attributes from a large attribute set. Three attribute selection criteria are proposed and formulated as a submodular optimization problem. A greedy optimization algorithm is presented and guaranteed to be at least (1-1/e)-approximation to the optimum. Experimental results on the Olympic Sports and UCF101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches.",
    "creator" : null
  }
}