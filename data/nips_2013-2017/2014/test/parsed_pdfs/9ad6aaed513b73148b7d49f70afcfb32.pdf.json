{
  "name" : "9ad6aaed513b73148b7d49f70afcfb32.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Efficient Inference of Continuous Markov Random Fields with Polynomial Potentials",
    "authors" : [ "Shenlong Wang", "Alexander G. Schwing", "Raquel Urtasun" ],
    "emails" : [ "slwang@cs.toronto.edu", "aschwing@cs.toronto.edu", "urtasun@cs.toronto.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Graphical models are a convenient tool to illustrate the dependencies among a collection of random variables with potentially complex interactions. Their widespread use across domains from computer vision and natural language processing to computational biology underlines their applicability. Many algorithms have been proposed to retrieve the minimum energy configuration, i.e., maximum a-posteriori (MAP) inference, when the graphical model describes energies or distributions defined on a discrete domain. Although this task is NP-hard in general, message passing algorithms [16] and graph-cuts [4] can be used to retrieve the global optimum when dealing with tree-structured models or binary Markov random fields composed out of sub-modular energy functions.\nIn contrast, graphical models with continuous random variables are much less well understood. A notable exception is Gaussian belief propagation [31], which retrieves the optimum when the potentials are Gaussian for arbitrary graphs under certain conditions of the underlying system. Inspired by discrete graphical models, message-passing algorithms based on discrete approximations in the form of particles [6, 17] or non-linear functions [27] have been developed for general potentials. They are, however, computationally expensive and do not perform well when compared to dedicated algorithms [20]. Fusion moves [11] are a possible alternative, but they rely on the generation of good proposals, a task that is often difficult in practice. Other related work focuses on representing relations on pairwise graphical models [24], or marginalization rather than MAP [13].\nIn this paper we study the case where the potentials are polynomial functions. This is a very general family of models as many applications such as collaborative filtering [8], surface reconstruction [5] and non-rigid registration [30] can be formulated in this way. Previous approaches rely on either polynomial equation system solvers [20], semi-definite programming relaxations [9, 15] or approximate message-passing algorithms [17, 27]. Unfortunately, existing methods either cannot cope with large-scale graphical models, and/or do not have global convergence guarantees.\nIn particular, we exploit the concave-convex procedure (CCCP) [33] to perform inference on continuous Markov random fields (MRFs) with polynomial potentials. Towards this goal, we first show that an arbitrary multivariate polynomial function can be decomposed into a sum of a convex and\na concave polynomial. Importantly, this decomposition can be expressed as a sum-of-squares optimization [10] over polynomial Hessians, which is efficiently solvable via semidefinite programming. Given the decomposition, our inference algorithm proceeds iteratively as follows: at each iteration we linearize the concave part and solve the resulting subproblem efficiently to optimality. Our algorithm inherits the global convergence property of CCCP [25].\nWe demonstrate the effectiveness of our approach in the context of 3D reconstruction, shape from shading and image denoising. Our method proves superior in terms of both computational cost and the energy of the solutions retrieved when compared to approaches such as dual decomposition [20], fusion moves [11] and particle belief propagation [6]."
    }, {
      "heading" : "2 Graphical Models with Continuous Variables and Polynomial Functions",
      "text" : "In this section we first review inference algorithms for graphical models with continuous random variables, as well as the concave-convex procedure. We then prove existence of a concave-convex decomposition for polynomials and provide a construction. Based on this decomposition and construction, we propose a novel inference algorithm for continuous MRFs with polynomial potentials."
    }, {
      "heading" : "2.1 Graphical Models with Polynomial Potentials",
      "text" : "The MRFs we consider represent distributions defined over a continuous domain X = ∏ i Xi, which is a product-space assembled by continuous sub-spaces Xi ⊂ R. Let x ∈ X be the output configuration of interest, e.g., a 3D mesh or a denoised image. Note that each output configuration tuple x = (x1, · · · , xn) subsumes a set of random variables. Graphical models describe the energy of the system as a sum of local scoring functions, i.e., f(x) = ∑ r∈R fr(xr). Each local function fr(xr) : Xr → R depends on a subset of variables xr = (xi)i∈r defined on a domain Xr ⊆ X , which is specified by the restriction often referred to as region r ⊆ {1, . . . , n}, i.e., Xr = ∏ i∈r Xi. We refer toR as the set of all restrictions required to compute the energy of the system. We tackle the problem of maximum a-posteriori (MAP) inference, i.e., we want to find the configuration x∗ having the minimum energy. This is formally expressed as\nx∗ = argmin x ∑ r∈R fr(xr). (1)\nSolving this program for general functions is hard. In this paper we focus on energies composed of polynomial functions. This is a fairly general case, as the energies employed in many applications obey this assumption. Furthermore, for well-behaved continuous non-polynomial functions (e.g., k-th order differentiable) polynomial approximations could be used (e.g., via a Taylor expansion). Let us define polynomials more formally: Definition 1. A d-degree multivariate polynomial f(x) : Rn → R is a finite linear combination of monomials, i.e.,\nf(x) = ∑ m∈M cmx m1 1 x m2 2 · · ·xmnn ,\nwhere we let the coefficient cm ∈ R and the tuple m = (m1, . . . ,mn) ∈M ⊆ Nn with ∑n i=1mi ≤ d ∀m ∈M. The setM subsumes all tuples relevant to define the function f .\nWe are interested in minimizing Eq. (1) where the potential functions fr are polynomials with arbitrary degree. This is a difficult problem as polynomial functions are in general non-convex. Moreover, for many applications of interest we have to deal with a large number of variables, e.g., more than 60,000 when reconstructing shape from shading of a 256 × 256 image. Optimal solutions exist under certain conditions when the potentials are Gaussian [31], i.e., polynomials of degree 2. Message passing algorithms have not been very successful for general polynomials due to the fact that the messages are continuous functions. Discrete [6, 17] and non-parametric [27] approximations have been employed with limited success. Furthermore, polynomial system solvers [20], and moment-based methods [9] cannot scale up to such a large number of variables. Dual-decomposition provides a plausible approach for tackling large-scale problems by dividing the task into many small sub-problems [20]. However, solving a large number of smaller systems is still a bottleneck, and decoding the optimal solution from the sub-problems might be difficult. In contrast, we propose to use the Concave-Convex Procedure (CCCP) [33], which we now briefly review."
    }, {
      "heading" : "2.2 Inference via CCCP",
      "text" : "CCCP is a majorization-minimization framework for optimizing non-convex functions that can be written as the sum of a convex and a concave part, i.e., f(x) = fvex(x) + fcave(x). This framework has recently been used to solve a wide variety of machine learning tasks, such as learning in structured models with latent variables [32, 22], kernel methods with missing entries [23] and sparse principle component analysis [26]. In CCCP, f is optimized by iteratively computing a linearization of the concave part at the current iterate x(i) and solving the resulting convex problem\nx(i+1) = argmin x fvex(x) + x T∇fcave(x(i)). (2)\nThis process is guaranteed to monotonically decrease the objective and it converges globally, i.e., for any point x (see Theorem 2 of [33] and Theorem 8 [25]). Moreover, Salakhutdinov et al. [19] showed that the convergence rate of CCCP, which is between super-linear and linear, depends on the curvature ratio between the convex and concave part. In order to take advantage of CCCP to solve our problem, we need to decompose the energy function into a sum of convex and concave parts. In the next section we show that this decomposition always exists. Furthermore, we provide a procedure to perform this decomposition given general polynomials."
    }, {
      "heading" : "2.3 Existence of a Concave-Convex Decomposition of Polynomials",
      "text" : "Theorem 1 in [33] shows that for all arbitrary continuous functions with bounded Hessian a decomposition into convex and concave parts exists. However, Hessians of polynomial functions are not bounded in Rn. Furthermore, [33] did not provide a construction for the decomposition. In this section we show that for polynomials this decomposition always exists and we provide a construction. Note that since odd degree polynomials are unbounded from below, i.e., not proper, we only focus on even degree polynomials in the following. Let us therefore consider the space spanned by polynomial functions with an even degree d. Proposition 1. The set of polynomial functions f(x) : Rn → R with even degree d, denoted Pnd , is\na topological vector space. Furthermore, its dimension dim(Pnd ) = ( n+ d− 1\nd\n) .\nProof. (Sketch) According to the definition of vector spaces, we know that the set of polynomial functions forms a vector space over R. We can then show that addition and multiplication over the polynomial ring Pnd is continuous. Finally, dim(Pnd ) is equivalent to computing a d-combination with repetition from n elements [3].\nNext we investigate the geometric properties of convex even degree polynomials. Lemma 1. Let the set of convex polynomial functions c(x) : Rn → R with even degree d be Cnd . This subset of Pnd is a convex cone.\nProof. Given two arbitrary convex polynomial functions f and g ∈ Cnd , let h = af+bg with positive scalars a, b ∈ R+. ∀x,y ∈ Rn,∀λ ∈ [0, 1], we have:\nh(λx+ (1− λ)y) = af(λx+ (1− λ)y) + bg(λx+ (1− λ)y) ≤ a(λf(x) + (1− λ)f(y)) + b(λh(x) + (1− λ)h(y)) = λh(x) + (1− λ)h(y).\nTherefore, ∀f, g ∈ Cnd ,∀a, b ∈ R+, we have af + bg ∈ Cnd , i.e., Cnd is a convex cone.\nWe now show that the eigenvalues of the Hessian of f (hence the smallest one) continuously depend on f ∈ Pnd . Proposition 2. For any polynomial function f ∈ Pnd with d ≥ 2, the eigenvalues of its Hessian eig(∇2f(x)) are continuous w.r.t. f in the polynomial space Pnd .\nProof. ∀f ∈ Pnd , given a basis {gi} of Pnd , we obtain the representation f = ∑ i cigi, linear in the coefficients ci. It is easy to see that ∀f ∈ Pnd , the Hessian ∇2f(x) is a polynomial matrix, linear in ci, i.e., ∇2f(x) = ∑ i ci∇2gi(x). Let M(c1, · · · , cn) = ∇2f(x) = ∑ i ci∇2gi(x) define the Hessian as a function of the coefficients (c1, · · · , cn). The eigenvalues eig(M(c1, · · · , cn)) are\nequivalent to the root of the characteristic polynomial of M(c1, · · · , cn), i.e., the set of solutions for det(M − λI) = 0. All the coefficients of the characteristic polynomial are polynomial expressions w.r.t. the entries of M , hence they are also polynomial w.r.t. (c1, · · · , cn) since each entry of M is linear on (c1, · · · , cn). Therefore, the coefficients of the characteristic polynomial are continuously dependent on (c1, · · · , cn). Moreover, the root of a polynomial is continuously dependent on the coefficients of the polynomial [28]. Based on these dependencies, eig(M(c1, · · · , cn)) are continuously dependent on (c1, · · · , cn), and eig(M(c1, · · · , cn)) are continuous w.r.t. f in the polynomial space Pnd .\nThe following proposition illustrates that the relative interior of the convex cone of even degree polynomials is not empty. Proposition 3. For an even degree function space Pnd , there exists a function f(x) ∈ Pnd , such that ∀x ∈ Rn, the Hessian is strictly positive definite, i.e., ∇2f(x) 0. Hence the relative interior of Cnd is not empty.\nProof. Let f(x) = ∑ i x d i + ∑ i x 2 i ∈ Pnd . It follows trivially that\n∇2f(x) = diag ([ d(d− 1)xd−21 + 2, d(d− 1)x d−2 2 + 2, · · · , d(d− 1)xd−2n + 2 ]) 0 ∀x.\nGiven the above two propositions it follows that the dimensionality of Cnd and Pnd is identical. Lemma 2. The dimension of the polynomial vector space is equal to the dimension of the convex even degree polynomial cone having the same degree d and the same number of variables n, i.e., dim(Cnd ) = dim(Pnd ).\nProof. According to Proposition 3, there exists a function f ∈ Pnd , with strictly positive definite Hessian, i.e., ∀x ∈ Rn, eig(∇2f(x)) > 0. Consider a polynomial basis {gi} of Pnd . Consider the vector of eigenvalues E(ĉi) = eig(∇2(f(x) + ĉigi)). According to Proposition 2, E(ĉi) is continuous w.r.t. ĉi, and E(0) is an all-positive vector. According to the definition of continuity, there exists an > 0, such that E(ĉi) > 0, ∀ĉi ∈ {c : |c| < }. Hence, there exists a nonzero constant ĉi such that the polynomial f + ĉigi is also strictly convex. We can construct such a strictly convex polynomial ∀gi. Therefore the polynomial set f + ĉigi is linearly independent and hence a basis of Cnd . This concludes the proof.\nLemma 3. The linear span of the basis of Cnd is Pnd Proof. Suppose Pnd is N -dimensional. According to Lemma 2, Cnd is also N -dimensional. Denote {g1, g2, · · · gN} a basis of Cnd . Assume there exists h ∈ Pnd such that h cannot be linearly represented by {g1, g2, · · · gN}. We have {g1, g2, · · · , gN , h} areN+1 linear independent vectors in Pnd , which is in contradiction with Pnd being N -dimensional.\nTheorem 1. ∀f ∈ Pnd , there exist convex polynomials h, g ∈ Cnd such that f = h− g.\nProof. Let the basis of Cnd be {g1, g2, · · · , gN}. According to Lemma 3, there exist coefficients c1, · · · , cN , such that f = c1g1 + c2g2 + · · · + cNgN . We can partition the coefficients into two sets, according to their sign, i.e., f = ∑ ci≥0 cigi + ∑ cj<0 cjgj . Let h = ∑ ci≥0 cigi and\ng = − ∑ cj<0 cjgj . We have f = h− g, while both h and g are convex polynomials.\nAccording to Theorem 1 there exists a concave-convex decomposition given any polynomial, where both the convex and concave parts are also polynomials with degree no greater than the original polynomial. As long as we can find ( n+ d− 1\nd\n) linearly independent convex polynomial basis\nfunctions for any arbitrary polynomial function f ∈ Pnd , we obtain a valid decomposition by looking at the sign of the coefficients. It is however worth noting that the concave-convex decomposition is not unique. In fact, there is an infinite number of decompositions, trivially seen by adding and subtracting an arbitrary convex polynomial to an existing decomposition.\nFinding a convex basis is however not an easy task, mainly due to the difficulties on checking convexity and the exponentially increasing dimension. Recently, Ahmadi et al. [1] proved that even deciding on the convexity of quartic polynomials is NP-hard.\nAlgorithm 1 CCCP Inference on Continuous MRFs with Polynomial Potentials Input: Initial estimation x0 ∀r find fr(xr) = fr,vex(xr) + fr,cave(xr) via Eq. (4) or via a polynomial basis (Theorem 1) repeat\nsolve x(i+1) = argminx ∑ r fr,vex(xr) + x T∇x( ∑ r∈R fr,cave(x (i) r )) with L-BFGS.\nuntil convergence Output: x∗"
    }, {
      "heading" : "2.4 Constructing a Concave-Convex Decomposition of Polynomials",
      "text" : "In this section we derive an algorithm to construct the concave-convex decomposition of arbitrary polynomials. Our algorithm first constructs the convex basis of the polynomial vector space Pnd before extracting a convex polynomial containing the target polynomial via a sum-of-squares (SOS) program. More formally, given a non-convex polynomial f(x) we are interested in constructing a convex function h(x) = f(x) + ∑ i cigi(x), with gi(x), i = {1, . . . ,m}, the set of all convex monomials with degree no grater than deg(f(x)). From this it follows that fvex = h(x) and fcave = − ∑ i cigi(x). In particular, we want a convex function h(x), with coefficients ci as small as possible: min c wT c s.t. ∇2f(x) + ∑ i ci∇2gi(x) 0 ∀x ∈ Rn, (3)\nwith the objective function being a weighted sum of coefficients. The weight vector w can encode preferences in the minimization, e.g., smaller coefficients for larger degrees. This minimization problem is NP-hard. If it was not, we could decide whether an arbitrary polynomial f(x) is convex by solving such a program, which contradicts the NP-hardness result of [1]. Instead, we utilize a tighter set of constraints, i.e., sum-of-square constraints, which are easier to solve [14]. Definition 2. For an even degree polynomial f(x) ∈ Pnd , with d = 2m, f is an SOS polynomial if and only if there exist g1, . . . , gk ∈ Pnm such that f(x) = ∑k i=1 gi(x) 2.\nThus, instead of solving the NP-hard program stated in Eq. (3), we optimize:\nmin c wT c s.t. ∇2f(x) + ∑ i ci∇2gi(x) ∈ SOS. (4)\nThe set of SOS Hessians is a subset of the positive definite Hessians [9]. Hence, every solution of this problem can be considered a valid construction. Furthermore, the sum-of-squares optimization in Eq. (4) can be formulated as an efficiently solvable semi-definite program (SDP) [10, 9]. It is important to note that the gap between the SOS Hessians and the positive definite Hessians increases as the degree of the polynomials grows. Hence using SOS constraints we might not find a solution, even though there exists one for the original program given in Eq. (3). In practice, SOS optimization works well for monomials and low-degree polynomials. For pairwise graphical models with arbitrary degree polynomials, as well as for graphical models of order up to four with maximum fourth order degree polynomials, we are guaranteed to find a decomposition. This is due to the fact that SOS convexity and polynomial convexity coincide (Theorem 5.2 in [2]). Most practical graphical models are within this set. Known counter-examples [2] are typically found using specific tools.\nWe summarize our algorithm in Alg. 1. Given a graphical model with polynomial potentials with degree at most d, we obtain a concave-convex decomposition by solving Eq. (4). This can be done for the full polynomial or for each non-convex monomial. We then apply CCCP in order to perform inference, where we solve a convex problem at each iteration. In particular, we employ L-BFGS, mainly due to its super-linear convergence and its storage efficiency [12]. In each L-BFGS step, we apply a line search scheme based on the Wolfe conditions [12]."
    }, {
      "heading" : "2.5 Extensions",
      "text" : "Dealing with very large graphs: Motivated by recent progress on accelerating graphical model inference [7, 21, 20], we can handle large-scale problems by employing dual decomposition and using our approach to solve the sub-problems.\nNon-polynomial cases: We have described our method in the context of graphical models with polynomial potentials. It can be extended to the non-polynomial case if the involved functions have\nbounded Hessians, since we can still construct the concave-convex decomposition. For instance, for the Lorentzian regularizer ρ(x) = log(1 + x 2\n2 ), we note that ρ(x) = {log(1 + x2 2 ) + x2 8 } − x2\n8 is a valid concave-convex decomposition. We refer the reader to the supplementary material for a detailed proof. Alternatively, we can approximate any continuous function with polynomials by employing a Taylor expansion around the current iterate, and updating the solution via one CCCP step within a trust region."
    }, {
      "heading" : "3 Experimental Evaluation",
      "text" : "We demonstrate the effectiveness of our approach using three different applications: non-rigid 3D reconstruction, shape from shading and image denoising. We refer the reader to the supplementary material for more figures as well as an additional toy experiment on a densely connected graph with box constraints."
    }, {
      "heading" : "3.1 Non-rigid 3D Reconstruction",
      "text" : "We tackle the problem of deformable surface reconstruction from a single image. Following [30], we parameterize the 3D shape via the depth of keypoints. Let x ∈ RN be the depth of N points. We follow the locally isometric deformation assumption [20], i.e., the distance between neighboring keypoints remains constant as the non-rigid surface deforms. The 3D reconstruction problem is then formulated as\nmin x ∑ (i,j)∈N ( ‖xiqi − xjqj‖2 − d2i,j )2 , (5)\nwhere di,j is the distance between keypoints (given as input), N is the set of all neighboring pixels, xi is the unknown depth of point i, qi = A−1(ui, vi, 1)T is the line-of-sight of pixel i with A denoting the known internal camera parameters. We consider a six-neighborhod system, i.e., up, down, left, right, upper-left and lower-right. Note that each pairwise potential is a four-degree nonconvex polynomial with two random variables. We can easily decompose it into 15 monomials, and perform a concave-convex decomposition given the corresponding convex polynomials (see supplementary material for an example).\nWe first conduct reconstruction experiments on the 100 randomly generated 3 × 3 meshes of [20], where zero-mean Gaussian noise with standard deviation σ = 2 is added to each observed keypoint coordinate. We compare our approach to Fusion Moves [30], particle convex belief propagation (PCBP) [17], L-BFGS as well as dual decomposition with the alternating direction method of multipliers using a polynomial solver (ADMM-Poly) [20]. We employ three different metrics, energy at convergence, running time and root mean square error (RMSE). For L-BFGS and our method, we use a flat mesh as initialization with two rotation angles (0, 0, 0) and (π/4, 0, 0). The convergence criteria is an energy decrease of less than 10−5 or a maximum of 500 iterations is reached. As shown in Table 1 our algorithm achieves lower energy, lower RMSE, and faster running time than ADMM-Poly and PCBP. Furthermore, as shown in Fig. 1(a) the time for running our algorithm to convergence is similar to a single iteration of ADMM-Poly, while we achieve much lower energy.\nWe next reconstruct the real-world 9×9Cardboard sequence [20]. We compare with both ADMMPoly and L-BFGS in terms of energy, time and RMSE. We also compare with the constrained latent variable model of [29], in terms of RMSE. We cannot compare the energy value since the energy function is different. Again, we use a flat mesh as initialization. As shown in Table 2, our algorithm outperforms all baselines. Furthermore, it is more than 20 times faster than ADMM-Poly, which is the second best algorithm. Average energy as a function of time is shown in Fig. 1(b). We refer the reader to Fig. 2 and the video in the supplementary material for a visual comparison between ADMM-Poly and our method. From the first subfigure we observe that our method achieves lower energy for most samples. The second subfigure illustrate the fact that our approach monotonically decreases the energy, as well as our method being much faster than ADMM-Poly."
    }, {
      "heading" : "3.2 Shape-from-Shading",
      "text" : "Following [5, 20], we formulate the shape from shading problem with 3rd-order 4-th degree polynomial functions. Let xi,j = (ui,j , vi,j , wi,j)T be the 3D coordinates of each triangle vertex. Under the Lambertian model assumption, the intensity of a triangle r is represented as: Ir = l1pr+l2qr+l3√\np2r+q 2 r+1\n, where l = (l1, l2, l3)T is the direction of\nthe light, pr and qr are the x and y coordinates of normal vector nr = (pr, qr, 1)T , which is computed as pr =\n(vi,j+1−vi,j)(wi+1,j−wi,j)−(vi+1,j−vi,j)(wi,j+1−wi,j) (ui,j+1−ui,j)(vi+1,j−vi,j)−(ui+1,j−ui,j)(vi,j+1−vi,j) and pr =\n(ui,j+1−ui,j)(wi+1,j−wi,j)−(ui+1,j−ui,j)(wi,j+1−wi,j) (ui,j+1−ui,j)(vi+1,j−vi,j)−(ui+1,j−ui,j)(vi,j+1−vi,j) , respectively. Each clique r represents a triangle, which is constructed by three neighboring points on the grid, i.e., either (xi,j ,xi,j+1,xi+1,j) or (xi,j ,xi,j−1,xi+1,j). Given the rendered image and lighting direction, shape from shading is formulated as\nmin w ∑ r∈R ( (p2r + q 2 r + 1)I 2 r − (l1pr + l2qr + l3)2 )2 . (6)\nWe tested our algorithm on the Vase, Penny and Mozart datasets, where Vase and Penny are 128×128 images and Mozart is a 256 × 256 image with light direction l = (0, 0, 1)T . The energy evolution curve, the inferred shape as well as the rendered and groud-truth images are illustrated in Fig. 3. See the supplementary material for more figures on Penny and Mozart. Our algorithm achieves very low energy, producing very accurate results in only 30 seconds. ADMM-Poly hardly runs on such large-scale data due to the computational cost of the polynomial system solver (more than 2 hours\nper iteration). In order to compare with ADMM-Poly, we also conduct the shape from shading experiment on a scaled 16× 16 version of the Vase data. Both methods retrieve a shape that is very close to the global optimum (0.00027 for ADMM-Poly and 0.00032 for our approach), however, our algorithm is over 500 times faster than ADMM-Poly (2250 seconds for ADMM-Poly and 13.29 seconds for our proposed method). The energy evolution curve on the 16 × 16 re-scaled image in shown in Fig. 1(c)."
    }, {
      "heading" : "3.3 Image Denoising",
      "text" : "We formulate image denoising via minimizing the Fields-of-Experts (FoE) energy [18]. The data term encodes the fact that the recovered image should be close to the noisy input, where closeness is weighted by the noise level σ. Given a pre-learned linear filterbank of ‘experts’ {Ji}i=1,...,K , the image prior term encodes the fact that natural images are Gibbs distributed via p(x) = 1Z exp( ∏ r∈R ∏K i=1(1 + 1 2 (J T i xr) 2)αi). Thus we formulate denoising as\nmin x\nλ\nσ2 ‖x− y‖22 + ∑ r∈R K∑ i=1 αi log(1 + 1 2 (JTi xr) 2), (7)\nwhere y is the noisy image input, x is the clean image estimation, r indexes 5 × 5 cliques and i is the index for each FoE filter. Note that this energy function is not a polynomial function. However, for each FoE model, the Hessian of the energy function log(1 + 12 (J T i xr) 2) is lower bounded by −J T i Ji 8 (proof in the supplementary material). Therefore, we simply add an extra term γx T r xr with γ > JTi Ji\n8 to obtain the concave-convex decomposition log(1+ 1 2 (J T i xr) 2) = {log(1+ 12 (J T i xr) 2)+\nγxTr xr}−γxTr xr. We utilize a pre-trained 5× 5 filterbank with 24 filters, and conduct experiments on the BM3D benchmark 1 with noise level σ = 15. In addition to the other baselines, we compare to the original FoE inference algorithm, which essentially is a first-order gradient descent method with fixed gradient step [18]. For L-BFGS, we set the maximum number of iterations to 10,000, to make sure that the algorithm converges. As shown in Table 3 and Fig. 1(d), our algorithm achieves lower energy than L-BFGS and first-order gradient descent. Furthermore, we see that lower energy does not translate to higher PSNR, showing the limitation of FoE as an image prior."
    }, {
      "heading" : "4 Conclusions",
      "text" : "We investigated the properties of polynomials, and proved that every multivariate polynomial with even degree can be decomposed into a sum of convex and concave polynomials with degree no greater than the original one. Motivated by this property, we exploited the concave-convex procedure to perform inference on continuous Markov random fields with polynomial potentials. Our algorithm is especially fit for solving inference problems on continuous graphical models, with a large number of variables. Experiments on non-rigid reconstruction, shape-from-shading and image denoising validate the effectiveness of our approach. We plan to investigate continuous inference with arbitrary differentiable functions, by making use of polynomial approximations as well as tighter concave-convex decompositions.\n1http://www.cs.tut.fi/˜foi/GCF-BM3D/"
    } ],
    "references" : [ {
      "title" : "Np-hardness of deciding convexity of quartic polynomials and related problems",
      "author" : [ "A.A. Ahmadi", "A. Olshevsky", "P.A. Parrilo", "J.N. Tsitsiklis" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A complete characterization of the gap between convexity and sosconvexity",
      "author" : [ "A.A. Ahmadi", "P.A. Parrilo" ],
      "venue" : "SIAM J. on Optimization,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The geometry of multivariate polynomial division and elimination",
      "author" : [ "K. Batselier", "P. Dreesen", "B.D. Moor" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Fast approximate energy minimization via graph cuts",
      "author" : [ "Y. Boykov", "O. Veksler", "R. Zabih" ],
      "venue" : "PAMI,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Polynomial shape from shading",
      "author" : [ "A. Ecker", "A.D. Jepson" ],
      "venue" : "CVPR,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Particle belief propagation",
      "author" : [ "A.T. Ihler", "D.A. McAllester" ],
      "venue" : "AISTATS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Mrf energy minimization and beyond via dual decomposition",
      "author" : [ "N. Komodakis", "N. Paragios", "G. Tziritas" ],
      "venue" : "PAMI,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Matrix factorization techniques for recommender systems",
      "author" : [ "Y. Koren", "R. Bell", "C. Volinsky" ],
      "venue" : "Computer,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Global optimization with polynomials and the problem of moments",
      "author" : [ "J.B. Lasserre" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Convergent sdp-relaxations in polynomial optimization with sparsity",
      "author" : [ "J.B. Lasserre" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fusion moves for markov random field optimization",
      "author" : [ "V. Lempitsky", "C. Rother", "S. Roth", "A. Blake" ],
      "venue" : "PAMI,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Numerical optimization 2ed",
      "author" : [ "J. Nocedal", "S.J. Wright" ],
      "venue" : "Springer-Verlag,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Belief propagation for continuous state spaces: Stochastic messagepassing with quantitative guarantees",
      "author" : [ "N. Noorshams", "M.J. Wainwright" ],
      "venue" : "JMLR,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Sostools version 3.00 sum of squares optimization toolbox for matlab",
      "author" : [ "A. Papachristodoulou", "J. Anderson", "G. Valmorbida", "S. Prajna", "P. Seiler", "P. Parrilo" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization",
      "author" : [ "P.A. Parrilo" ],
      "venue" : "PhD thesis, Caltech,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Probabilistic reasoning in intelligent systems: networks of plausible inference",
      "author" : [ "J. Pearl" ],
      "venue" : "Morgan Kaufmann,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Convex max-product algorithms for continuous mrfs with applications to protein folding",
      "author" : [ "J. Peng", "T. Hazan", "D. McAllester", "R. Urtasun" ],
      "venue" : "ICML,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Fields of experts",
      "author" : [ "S. Roth", "M.J. Black" ],
      "venue" : "IJCV,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On the convergence of bound optimization algorithms",
      "author" : [ "R. Salakhutdinov", "S. Roweis", "Z. Ghahramani" ],
      "venue" : "UAI,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Continuous inference in graphical models with polynomial energies",
      "author" : [ "M. Salzmann" ],
      "venue" : "CVPR,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributed Message Passing for Large Scale Graphical Models",
      "author" : [ "A.G. Schwing", "T. Hazan", "M. Pollefeys", "R. Urtasun" ],
      "venue" : "CVPR,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Efficient Structured Prediction with Latent Variables for General Graphical Models",
      "author" : [ "A.G. Schwing", "T. Hazan", "M. Pollefeys", "R. Urtasun" ],
      "venue" : "ICML,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Kernel methods for missing variables",
      "author" : [ "A. Smola", "S. Vishwanathan", "T. Hofmann" ],
      "venue" : "AISTATS,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Kernel belief propagation",
      "author" : [ "L. Song", "A. Gretton", "D. Bickson", "Y. Low", "C. Guestrin" ],
      "venue" : "AISTATS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Nonparametric belief propagation",
      "author" : [ "E.B. Sudderth", "A.T. Ihler", "M. Isard", "W.T. Freeman", "A.S. Willsky" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On the continuous dependence of the roots of a polynomial on its coefficients",
      "author" : [ "D.J. Uherka", "A.M. Sergott" ],
      "venue" : "American Mathematical Monthly,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "A constrained latent variable model",
      "author" : [ "A. Varol", "M. Salzmann", "P. Fua", "R. Urtasun" ],
      "venue" : "CVPR,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Soft inextensibility constraints for template-free non-rigid reconstruction",
      "author" : [ "S. Vicente", "L. Agapito" ],
      "venue" : "ECCV,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Correctness of belief propagation in gaussian graphical models of arbitrary topology",
      "author" : [ "Y. Weiss", "W.T. Freeman" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Learning structural svms with latent variables",
      "author" : [ "C.N. Yu", "T. Joachims" ],
      "venue" : "ICML,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The concave-convex procedure",
      "author" : [ "A.L. Yuille", "A. Rangarajan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Although this task is NP-hard in general, message passing algorithms [16] and graph-cuts [4] can be used to retrieve the global optimum when dealing with tree-structured models or binary Markov random fields composed out of sub-modular energy functions.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "Although this task is NP-hard in general, message passing algorithms [16] and graph-cuts [4] can be used to retrieve the global optimum when dealing with tree-structured models or binary Markov random fields composed out of sub-modular energy functions.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 28,
      "context" : "A notable exception is Gaussian belief propagation [31], which retrieves the optimum when the potentials are Gaussian for arbitrary graphs under certain conditions of the underlying system.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "Inspired by discrete graphical models, message-passing algorithms based on discrete approximations in the form of particles [6, 17] or non-linear functions [27] have been developed for general potentials.",
      "startOffset" : 124,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "Inspired by discrete graphical models, message-passing algorithms based on discrete approximations in the form of particles [6, 17] or non-linear functions [27] have been developed for general potentials.",
      "startOffset" : 124,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "Inspired by discrete graphical models, message-passing algorithms based on discrete approximations in the form of particles [6, 17] or non-linear functions [27] have been developed for general potentials.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 19,
      "context" : "They are, however, computationally expensive and do not perform well when compared to dedicated algorithms [20].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "Fusion moves [11] are a possible alternative, but they rely on the generation of good proposals, a task that is often difficult in practice.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 23,
      "context" : "Other related work focuses on representing relations on pairwise graphical models [24], or marginalization rather than MAP [13].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "Other related work focuses on representing relations on pairwise graphical models [24], or marginalization rather than MAP [13].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 7,
      "context" : "This is a very general family of models as many applications such as collaborative filtering [8], surface reconstruction [5] and non-rigid registration [30] can be formulated in this way.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 4,
      "context" : "This is a very general family of models as many applications such as collaborative filtering [8], surface reconstruction [5] and non-rigid registration [30] can be formulated in this way.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 27,
      "context" : "This is a very general family of models as many applications such as collaborative filtering [8], surface reconstruction [5] and non-rigid registration [30] can be formulated in this way.",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 19,
      "context" : "Previous approaches rely on either polynomial equation system solvers [20], semi-definite programming relaxations [9, 15] or approximate message-passing algorithms [17, 27].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "Previous approaches rely on either polynomial equation system solvers [20], semi-definite programming relaxations [9, 15] or approximate message-passing algorithms [17, 27].",
      "startOffset" : 114,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "Previous approaches rely on either polynomial equation system solvers [20], semi-definite programming relaxations [9, 15] or approximate message-passing algorithms [17, 27].",
      "startOffset" : 114,
      "endOffset" : 121
    }, {
      "referenceID" : 16,
      "context" : "Previous approaches rely on either polynomial equation system solvers [20], semi-definite programming relaxations [9, 15] or approximate message-passing algorithms [17, 27].",
      "startOffset" : 164,
      "endOffset" : 172
    }, {
      "referenceID" : 24,
      "context" : "Previous approaches rely on either polynomial equation system solvers [20], semi-definite programming relaxations [9, 15] or approximate message-passing algorithms [17, 27].",
      "startOffset" : 164,
      "endOffset" : 172
    }, {
      "referenceID" : 30,
      "context" : "In particular, we exploit the concave-convex procedure (CCCP) [33] to perform inference on continuous Markov random fields (MRFs) with polynomial potentials.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : "Importantly, this decomposition can be expressed as a sum-of-squares optimization [10] over polynomial Hessians, which is efficiently solvable via semidefinite programming.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 19,
      "context" : "Our method proves superior in terms of both computational cost and the energy of the solutions retrieved when compared to approaches such as dual decomposition [20], fusion moves [11] and particle belief propagation [6].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 10,
      "context" : "Our method proves superior in terms of both computational cost and the energy of the solutions retrieved when compared to approaches such as dual decomposition [20], fusion moves [11] and particle belief propagation [6].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 5,
      "context" : "Our method proves superior in terms of both computational cost and the energy of the solutions retrieved when compared to approaches such as dual decomposition [20], fusion moves [11] and particle belief propagation [6].",
      "startOffset" : 216,
      "endOffset" : 219
    }, {
      "referenceID" : 28,
      "context" : "Optimal solutions exist under certain conditions when the potentials are Gaussian [31], i.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "Discrete [6, 17] and non-parametric [27] approximations have been employed with limited success.",
      "startOffset" : 9,
      "endOffset" : 16
    }, {
      "referenceID" : 16,
      "context" : "Discrete [6, 17] and non-parametric [27] approximations have been employed with limited success.",
      "startOffset" : 9,
      "endOffset" : 16
    }, {
      "referenceID" : 24,
      "context" : "Discrete [6, 17] and non-parametric [27] approximations have been employed with limited success.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 19,
      "context" : "Furthermore, polynomial system solvers [20], and moment-based methods [9] cannot scale up to such a large number of variables.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "Furthermore, polynomial system solvers [20], and moment-based methods [9] cannot scale up to such a large number of variables.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : "Dual-decomposition provides a plausible approach for tackling large-scale problems by dividing the task into many small sub-problems [20].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 30,
      "context" : "In contrast, we propose to use the Concave-Convex Procedure (CCCP) [33], which we now briefly review.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 29,
      "context" : "This framework has recently been used to solve a wide variety of machine learning tasks, such as learning in structured models with latent variables [32, 22], kernel methods with missing entries [23] and sparse principle component analysis [26].",
      "startOffset" : 149,
      "endOffset" : 157
    }, {
      "referenceID" : 21,
      "context" : "This framework has recently been used to solve a wide variety of machine learning tasks, such as learning in structured models with latent variables [32, 22], kernel methods with missing entries [23] and sparse principle component analysis [26].",
      "startOffset" : 149,
      "endOffset" : 157
    }, {
      "referenceID" : 22,
      "context" : "This framework has recently been used to solve a wide variety of machine learning tasks, such as learning in structured models with latent variables [32, 22], kernel methods with missing entries [23] and sparse principle component analysis [26].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 30,
      "context" : ", for any point x (see Theorem 2 of [33] and Theorem 8 [25]).",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : "[19] showed that the convergence rate of CCCP, which is between super-linear and linear, depends on the curvature ratio between the convex and concave part.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "3 Existence of a Concave-Convex Decomposition of Polynomials Theorem 1 in [33] shows that for all arbitrary continuous functions with bounded Hessian a decomposition into convex and concave parts exists.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 30,
      "context" : "Furthermore, [33] did not provide a construction for the decomposition.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "Finally, dim(P d ) is equivalent to computing a d-combination with repetition from n elements [3].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 25,
      "context" : "Moreover, the root of a polynomial is continuously dependent on the coefficients of the polynomial [28].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "[1] proved that even deciding on the convexity of quartic polynomials is NP-hard.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "If it was not, we could decide whether an arbitrary polynomial f(x) is convex by solving such a program, which contradicts the NP-hardness result of [1].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : ", sum-of-square constraints, which are easier to solve [14].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "The set of SOS Hessians is a subset of the positive definite Hessians [9].",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "(4) can be formulated as an efficiently solvable semi-definite program (SDP) [10, 9].",
      "startOffset" : 77,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "(4) can be formulated as an efficiently solvable semi-definite program (SDP) [10, 9].",
      "startOffset" : 77,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "Known counter-examples [2] are typically found using specific tools.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "In particular, we employ L-BFGS, mainly due to its super-linear convergence and its storage efficiency [12].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "In each L-BFGS step, we apply a line search scheme based on the Wolfe conditions [12].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "5 Extensions Dealing with very large graphs: Motivated by recent progress on accelerating graphical model inference [7, 21, 20], we can handle large-scale problems by employing dual decomposition and using our approach to solve the sub-problems.",
      "startOffset" : 116,
      "endOffset" : 127
    }, {
      "referenceID" : 20,
      "context" : "5 Extensions Dealing with very large graphs: Motivated by recent progress on accelerating graphical model inference [7, 21, 20], we can handle large-scale problems by employing dual decomposition and using our approach to solve the sub-problems.",
      "startOffset" : 116,
      "endOffset" : 127
    }, {
      "referenceID" : 19,
      "context" : "5 Extensions Dealing with very large graphs: Motivated by recent progress on accelerating graphical model inference [7, 21, 20], we can handle large-scale problems by employing dual decomposition and using our approach to solve the sub-problems.",
      "startOffset" : 116,
      "endOffset" : 127
    }, {
      "referenceID" : 27,
      "context" : "Following [30], we parameterize the 3D shape via the depth of keypoints.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "We follow the locally isometric deformation assumption [20], i.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 19,
      "context" : "We first conduct reconstruction experiments on the 100 randomly generated 3 × 3 meshes of [20], where zero-mean Gaussian noise with standard deviation σ = 2 is added to each observed keypoint coordinate.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 27,
      "context" : "We compare our approach to Fusion Moves [30], particle convex belief propagation (PCBP) [17], L-BFGS as well as dual decomposition with the alternating direction method of multipliers using a polynomial solver (ADMM-Poly) [20].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "We compare our approach to Fusion Moves [30], particle convex belief propagation (PCBP) [17], L-BFGS as well as dual decomposition with the alternating direction method of multipliers using a polynomial solver (ADMM-Poly) [20].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 19,
      "context" : "We compare our approach to Fusion Moves [30], particle convex belief propagation (PCBP) [17], L-BFGS as well as dual decomposition with the alternating direction method of multipliers using a polynomial solver (ADMM-Poly) [20].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 19,
      "context" : "We next reconstruct the real-world 9×9Cardboard sequence [20].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 26,
      "context" : "We also compare with the constrained latent variable model of [29], in terms of RMSE.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "2 Shape-from-Shading Following [5, 20], we formulate the shape from shading problem with 3rd-order 4-th degree polynomial functions.",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 19,
      "context" : "2 Shape-from-Shading Following [5, 20], we formulate the shape from shading problem with 3rd-order 4-th degree polynomial functions.",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "3 Image Denoising We formulate image denoising via minimizing the Fields-of-Experts (FoE) energy [18].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "In addition to the other baselines, we compare to the original FoE inference algorithm, which essentially is a first-order gradient descent method with fixed gradient step [18].",
      "startOffset" : 172,
      "endOffset" : 176
    } ],
    "year" : 2014,
    "abstractText" : "In this paper, we prove that every multivariate polynomial with even degree can be decomposed into a sum of convex and concave polynomials. Motivated by this property, we exploit the concave-convex procedure to perform inference on continuous Markov random fields with polynomial potentials. In particular, we show that the concave-convex decomposition of polynomials can be expressed as a sum-of-squares optimization, which can be efficiently solved via semidefinite programing. We demonstrate the effectiveness of our approach in the context of 3D reconstruction, shape from shading and image denoising, and show that our method significantly outperforms existing techniques in terms of efficiency as well as quality of the retrieved solution.",
    "creator" : null
  }
}