{
  "name" : "98d6f58ab0dafbb86b083a001561bb34.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Time–Data Tradeoffs by Aggressive Smoothing",
    "authors" : [ "John J. Bruer", "Joel A. Tropp", "Volkan Cevher", "Stephen R. Becker" ],
    "emails" : [ "*jbruer@cms.caltech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "This paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization. As the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly. This work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems."
    }, {
      "heading" : "1 Introduction",
      "text" : "It once seemed obvious that the running time of an algorithm should increase with the size of the input. But recent work in machine learning has led us to question this dogma. In particular, Shalev-Shwartz and Srebro [1] showed that their algorithm for learning a support vector classifier actually becomes faster when they increase the amount of training data. Other researchers have identified related tradeoffs [2, 3, 4, 5, 6, 7, 8, 9]. Together, these works support an emerging perspective in statistical computation that treats data as a computational resource that we can exploit to improve algorithms for estimation and learning.\nIn this paper, we consider statistical algorithms based on convex optimization. Our primary contribution is the following proposal:\nAs the amount of available data increases, we can smooth statistical optimization problems more and more aggressively. We can solve the smoothed problems significantly faster without any increase in statistical risk.\nIndeed, many statistical estimation procedures balance the modeling error with the complexity of the model. When we have very little data, complexity regularization is essential to fit an accurate model. When we have a large amount of data, we can relax the regularization without compromising the quality of the model. In other words, excess data offers us an opportunity to accelerate the statistical optimization. We propose to use smoothing methods [10, 11, 12] to implement this tradeoff.\nWe develop this idea in the context of the regularized linear inverse problem (RLIP) with random data. Nevertheless, our ideas apply to a wide range of problems. We pursue a more sophisticated example in a longer version of this work [13].\nJJB’s and JAT’s work was supported under ONR award N00014-11-1002, AFOSR award FA9550-09-10643, and a Sloan Research Fellowship. VC’s work was supported in part by the European Commission under Grant MIRG-268398, ERC Future Proof, SNF 200021-132548, SNF 200021-146750 and SNF CRSII2-147633. SRB was previously with IBM Research, Yorktown Heights, NY 10598 during the completion of this work."
    }, {
      "heading" : "1.1 The regularized linear inverse problem",
      "text" : "Let x\\ ∈ Rd be an unknown signal, and let A ∈ Rm×d be a known measurement matrix. Assume that we have access to a vector b ∈ Rm of m linear samples of that signal given by\nb := Ax\\ .\nGiven the pair (A,b), we wish to recover the original signal x\\ .\nWe consider the case whereA is fat (m < d), so we cannot recover x\\ without additional information about its structure. Let us introduce a proper convex function f : Rd → R ∪ {+∞} that assigns small values to highly structured signals. Using the regularizer f , we construct the estimator\nx̂ := arg min x f (x) subject to Ax = b. (1)\nWe declare the estimator successful when x̂ = x\\ , and we refer to this outcome as exact recovery.\nWhile others have studied (1) in the statistical setting, our result is different in character from previous work. Agarwal, Negahban, and Wainwright [14] showed that gradient methods applied to problems like (1) converge in fewer iterations due to increasing restricted strong convexity and restricted smoothness as sample size increases. They did not, however, discuss a time–data tradeoff explicitly, nor did they recognize that the overall computational cost may rise as the problem sizes grow.\nLai and Yin [15], meanwhile, proposed relaxing the regularizer in (1) based solely on some norm of the underlying signal. Our relaxation, however, is based on the sample size as well. Our method results in better performance as sample size increases: a time–data tradeoff.\nThe RLIP (1) provides a good candidate for studying time–data tradeoffs because recent work in convex geometry [16] gives a precise characterization of the number of samples needed for exact recovery. Excess samples allow us to replace the optimization problem (1) with one that we can solve faster. We do this for sparse vector and low-rank matrix recovery problems in Sections 4 and 5."
    }, {
      "heading" : "2 The geometry of the time–data tradeoff",
      "text" : "In this section, we summarize the relevant results that describe the minimum sample size required to solve the regularized linear inverse problem (1) exactly in a statistical setting."
    }, {
      "heading" : "2.1 The exact recovery condition and statistical dimension",
      "text" : "We can state the optimality condition for (1) in a geometric form; cf. [17, Prop. 2.1]. Fact 2.1 (Exact recovery condition). The descent cone of a proper convex function f : Rd → R∪{+∞} at the point x is the convex cone\nD ( f ;x) := ⋃ τ>0 { y ∈ Rd : f (x + τy) ≤ f (x) } .\nThe regularized linear inverse problem (1) exactly recovers the unknown signal x\\ if and only if\nD ( f ;x\\ ) ∩ null(A) = {0}. (2)\nWe illustrate this condition in Figure 1(a).\nTo determine the number of samples we need to ensure that the exact recovery condition (2) holds, we must quantify the “size” of the descent cones of the regularizer f . Definition 2.2 (Statistical dimension [16, Def. 2.1]). Let C ∈ Rd be a convex cone. Its statistical dimension δ(C) is defined as\nδ(C) := E [ ‖ΠC (g)‖2 ] ,\nwhere g ∈ Rd has independent standard Gaussian entries, and ΠC is the projection operator onto C. When the measurement matrix A is sufficiently random, Amelunxen et al. [16] obtain a precise characterization of the number m of samples required to achieve exact recovery.\nFact 2.3 (Exact recovery condition for the random RLIP [16, Thm. II]). Assume that the null space of the measurement matrixA ∈ Rm×d in the RLIP (1) is oriented uniformly at random. (In particular, a matrix with independent standard Gaussian entries has this property.) Then\nm ≥ δ ( D ( f ;x\\ ) ) + Cη √ d =⇒ exact recovery holds with probability ≥ 1 − η;\nm ≤ δ ( D ( f ;x\\ ) ) − Cη √ d =⇒ exact recovery holds with probability ≤ η,\nwhere Cη := √ 8 log(4/η).\nIn words, the RLIP undergoes a phase transition when the number m of samples equals δ(D ( f ;x\\ )). Any additional samples are redundant, so we can try to exploit them to identify x\\ more quickly."
    }, {
      "heading" : "2.2 A geometric opportunity",
      "text" : "Chandrasekaran and Jordan [6] have identified a time–data tradeoff in the setting of denoising problems based on Euclidean projection onto a constraint set. They argue that, when they have a large number of samples, it is possible to enlarge the constraint set without increasing the statistical risk of the estimator. They propose to use a discrete sequence of relaxations based on algebraic hierarchies.\nWe have identified a related opportunity for a time–data tradeoff in the RLIP (1). When we have excess samples, we may replace the regularizer f with a relaxed regularizer f̃ that is easier to optimize. In contrast to [6], we propose to use a continuous sequence of relaxations based on smoothing.\nFigure 1 illustrates the geometry of our time–data tradeoff. When the number of samples exceeds δ(D ( f ;x\\ )), Fact 2.3 tells us that the situation shown in Figure 1(a) holds with high probability. This allows us to enlarge the sublevel sets of the regularizer while still satisfying the exact recovery condition, as shown in Figure 1(b). A suitable relaxation allows us to solve the problem faster. Our geometric motivation is similar with [6] although our relaxation method is totally unrelated."
    }, {
      "heading" : "3 A time–data tradeoff via dual-smoothing",
      "text" : "This section presents an algorithm that can exploit excess samples to solve the RLIP (1) faster."
    }, {
      "heading" : "3.1 The dual-smoothing procedure",
      "text" : "The procedure we use applies Nesterov’s primal-smoothing method from [11] to the dual problem; see [12]. Given a regularizer f , we introduce a family { fµ : µ > 0} of strongly convex majorants:\nfµ (x) := f (x) + µ\n2 ‖x‖2 .\nAlgorithm 3.1 Auslender–Teboulle applied to the dual-smoothed RLIP Input: measurement matrixA, observed vector b\n1: z0 ← 0, z̄0 ← z0, θ0 ← 1 2: for k = 0,1,2, . . . do 3: yk ← (1 − θk )zk + θk z̄k 4: xk ← arg minx f (x) + µ2 ‖x‖2 − 〈yk , Ax − b〉 5: z̄k+1 ← z̄k + µ‖A‖2θ (b −Axk ) 6: zk+1 ← (1 − θk )zk + θk z̄k+1 7: θk+1 ← 2/(1 + (1 + 4/θ2k )1/2) 8: end for\nIn particular, the sublevel sets of fµ grow as µ increases. We then replace f with fµ in the original RLIP (1) to obtain new estimators of the form\nx̂µ := arg min x fµ (x) subject to Ax = b. (3)\nThe Lagrangian of the convex optimization problem (3) becomes\nLµ (x,z) = f (x) + µ2 ‖x‖ 2 − 〈z, Ax − b〉 ,\nwhere the Lagrange multiplier z is a vector in Rm . This gives a family of dual problems: maximize gµ (z) := min\nx Lµ (x,z) subject to z ∈ Rm . (4)\nSince fµ is strongly convex, the Lagrangian L has a unique minimizer xz for each dual point z: xz := arg min\nx Lµ (x,z). (5)\nStrong duality holds for (3) and (4) by Slater’s condition [18, Sec. 5.2.3]. Therefore, if we solve the dual problem (4) to obtain an optimal dual point, (5) returns the unique optimal primal point.\nThe dual function is differentiable with ∇gµ (z) = b −Axz , and the gradient is Lipschitz-continuous with Lipschitz constant Lµ no larger than µ−1 ‖A‖2; see [12, 11]. Note that Lµ is decreasing in µ, and so we call µ the smoothing parameter."
    }, {
      "heading" : "3.2 Solving the smoothed dual problem",
      "text" : "In order to solve the smoothed dual problem (4), we apply the fast gradient method from Auslender and Teboulle [19]. We present the pseudocode in Algorithm 3.1.\nThe computational cost of the algorithm depends on two things: the number of iterations necessary for convergence and the cost of each iteration. The following result bounds the error of the primal iterates xk with respect to the true signal x\\ . The proof is in the supplemental material. Proposition 3.1 (Primal convergence of Algorithm 3.1). Assume that the exact recovery condition holds for the primal problem (3). Algorithm 3.1 applied to the smoothed dual problem (4) converges to an optimal dual point z?µ . Let x ? µ be the corresponding optimal primal point given by (5). Then the sequence of primal iterates {xk } satisfies ‖x\\ − xk ‖ ≤ 2 ‖A‖ ‖z ? µ ‖\nµ · k .\nThe chosen regularizer affects the cost of Algorithm 3.1, line 4. Fortunately, this step is inexpensive for many regularizers of interest. Since the matrix–vector productAxk in line 5 dominates the other vector arithmetic, each iteration requires O(md) arithmetic operations."
    }, {
      "heading" : "3.3 The time–data tradeoff",
      "text" : "Proposition 3.1 suggests that increasing the smoothing parameter µ leads to faster convergence of the primal iterates of the Auslender–Teboulle algorithm. The discussion in Section 2.2 suggests that, when we have excess samples, we can increase the smoothing parameter while maintaining exact recovery. Our main technical proposal combines these two observations:\nAs the number m of measurements in the RLIP (1) increases, we smooth the dual problem (4) more and more aggressively while maintaining exact recovery. The Auslender–Teboulle algorithm can solve these increasingly smoothed problems faster.\nIn order to balance the inherent tradeoff between smoothing and accuracy, we introduce the maximal smoothing parameter µ(m). For a sample size m, µ(m) is the largest number satisfying\nδ ( D ( fµ(m);x\\ ) ) ≤ m. (6)\nChoosing a smoothing parameter µ ≤ µ(m) ensures that we do not cross the phase transition of our RLIP. In practice, we need to be less aggressive in order to avoid the “transition region”. The following two sections provide examples that use our proposal to achieve a clear time–data tradeoff."
    }, {
      "heading" : "4 Example: Sparse vector recovery",
      "text" : "In this section, we apply the method outlined in Section 3 to the sparse vector recovery problem."
    }, {
      "heading" : "4.1 The optimization problem",
      "text" : "Assume that x\\ is a sparse vector. The `1 norm serves as a convex proxy for sparsity, so we choose it as the regularizer in the RLIP (1). This problem is known as basis pursuit, and it was proposed by Chen et al. [20]. It has roots in geophysics [21, 22].\nWe apply the dual-smoothing procedure from Section 3 to obtain the relaxed primal problem, which is equivalent to the elastic net of Zou and Hastie [23]. The smoothed dual is given by (4).\nTo determine the exact recovery condition, Fact 2.3, for the dual-smoothed RLIP (3), we must compute the statistical dimension of the descent cones of fµ . We provide an accurate upper bound.\nProposition 4.1 (Statistical dimension bound for the dual-smoothed `1 norm). Let x ∈ Rd with s nonzero entries, and define the normalized sparsity ρ := s/d. Then\n1 d δ ( D ( fµ ;x) ) ≤ inf τ≥0   ρ [ 1 + τ2(1 + µ ‖x‖`∞ )2 ] + (1 − ρ) √ 2 π ∫ ∞ τ (u − τ)2e−u2/2 du   .\nThe proof is provided in the supplemental material. Figure 2 shows the statistical dimension and maximal smoothing curves for sparse vectors with ±1 entries. In order to apply this result we only need estimates of the magnitude and sparsity of the signal.\nTo apply Algorithm 3.1 to this problem, we must calculate an approximate primal solution xz from a dual point z (Algorithm 3.1, line 4). This step can be written as\nxz ← µ(m)−1 · SoftThreshold(ATz,1),\nwhere [SoftThreshold(x, t)]i = sgn (xi ) ·max {|xi | − t,0}. Algorithm 3.1, line 5 dominates the total cost of each iteration."
    }, {
      "heading" : "4.2 The time–data tradeoff",
      "text" : "We can obtain theoretical support for the existence of a time–data tradeoff in the sparse recovery problem by adapting Proposition 3.1. See the supplemental material for the proof.\nProposition 4.2 (Error bound for dual-smoothed sparse vector recovery). Let x\\ ∈ Rd with s nonzero entries, m be the sample size, and µ(m) be the maximal smoothing parameter (6). Given a measurement matrixA ∈ Rm×d , assume the exact recovery condition (2) holds for the dual-smoothed sparse vector recovery problem. Then the sequence of primal iterates from Algorithm 3.1 satisfies\n‖x\\ − xk ‖ ≤ 2d\n1 2 κ(A) [ ρ · (1 + µ(m) ‖x\\ ‖`∞ )2 + (1 − ρ) ] 1 2\nµ(m) · k ,\nwhere ρ := s/d is the normalized sparsity of x\\ , and κ(A) is the condition number of the matrixA.\nFor a fixed number k of iterations, as the number m of samples increases, Proposition 4.2 suggests that the error decreases like 1/µ(m). This observation suggests that we can achieve a time–data tradeoff by smoothing."
    }, {
      "heading" : "4.3 Numerical experiment",
      "text" : "Figure 3 shows the results of a numerical experiment that compares the performance difference between current numerical practice and our aggressive smoothing approach.\nMost practitioners use a fixed smoothing parameter µ that depends on the ambient dimension or sparsity but not on the sample size. For the constant smoothing case, we choose µ = 0.1 based on the recommendation in [15]. It is common, however, to see much smaller choices of µ [24, 25].\nIn contrast, our method exploits excess samples by smoothing the dual problem more aggressively. We set the smoothing parameter µ = µ(m)/4. This heuristic choice is small enough to avoid the phase transition of the RLIP while large enough to reap performance benefits. Our forthcoming work [13] addressing the case of noisy samples provides a more principled way to select this parameter.\nIn the experiment, we fix both the ambient dimension d = 40 000 and the normalized sparsity ρ = 5%. To test each smoothing approach, we generate and solve 10 random sparse vector recovery models for each value of the sample size m = 12 000,14 000,16 000, . . . ,38 000. Each random model comprises a Gaussian measurement matrixA and a random sparse vector x\\ whose nonzero entires are ±1 with equal probability. We stop Algorithm 3.1 when the relative error ‖x\\ − xk ‖ / ‖x\\ ‖ is less than 10−3. This condition guarantees that both methods maintain the same level of accuracy.\nIn Figure 3(a), we see that for both choices of µ, the average number of iterations decreases as sample size increases. When we plot the total computational cost1 in Figure 3(b), we see that the constant smoothing method cannot overcome the increase in cost per iteration. In fact, in this example, it would be better to throw away excess data when using constant smoothing. Meanwhile, our aggressive smoothing method manages to decrease total cost as sample size increases. The maximal speedup achieved is roughly 2.5×. We note that if the matrixA were orthonormal, the cost of both smoothing methods would decrease as sample sizes increase. In particular, the uptick seen at m = 38 000 in Figure 3 would disappear (but our method would maintain roughly the same relative advantage over constant smoothing). This suggests that the condition number κ(A) indeed plays an important role in determining the computational cost. We believe that using a Gaussian matrixA is warranted here as statistical models often use independent subjects.\nLet us emphasize that we use the same algorithm to test both smoothing approaches, so the relative comparison between them is meaningful. The observed improvement shows that we have indeed achieved a time–data tradeoff by aggressive smoothing."
    }, {
      "heading" : "5 Example: Low-rank matrix recovery",
      "text" : "In this section, we apply the method outlined in Section 3 to the low-rank matrix recovery problem."
    }, {
      "heading" : "5.1 The optimization problem",
      "text" : "Assume that X \\ ∈ Rd1×d2 is low-rank. Consider a known measurement matrix A ∈ Rm×d , where d := d1d2. We are given linear measurements of the form b = A · vec(X \\ ), where vec returns the (column) vector obtained by stacking the columns of the input matrix. Fazel [26] proposed using the Schatten 1-norm ‖·‖S1 , the sum of the matrix’s singular values, as a convex proxy for rank. Therefore, we follow Recht et al. [27] and select f = ‖·‖S1 as the regularizer in the RLIP (1). The low-rank matrix recovery problem has roots in control theory [28].\nWe apply the dual-smoothing procedure to obtain the approximate primal problem and the smoothed dual problem, replacing the squared Euclidean norm in (3) with the squared Frobenius norm.\nAs in the sparse vector case, we must compute the statistical dimension of the descent cones of the strongly convex regularizer fµ . In the case where the matrixX is square, the following is an accurate upper bound for this quantity. (The non-square case is addressed in the supplemental material.) Proposition 5.1 (Statistical dimension bound for the dual-smoothed Schatten 1-norm). Let X ∈ Rd1×d1 have rank r, and define the normalized rank ρ := r/d1. Then\n1 d21 δ ( D ( fµ ;X) ) ≤ inf 0≤τ≤2 { ρ + (1 − ρ) [ ρ ( 1 + τ2(1 + µ ‖X ‖)2 ) +\n(1 − ρ) 12π [ 24(1 + τ2) cos−1(τ/2) − τ(26 + τ2)\n√ 4 − τ2 ] ]} + o (1) ,\nas d1 → ∞ while keeping the normalized rank ρ constant. 1We compute total cost as k · md, where k is the number of iterations taken, and md is the dominant cost of each iteration.\nThe proof is provided in the supplemental material. The plots of the statistical dimension and maximal smoothing curves closely resemble those of the `1 norm and are in the supplemental material as well.\nIn this case, Algorithm 3.1, line 4 becomes [12, Sec. 4.3]\nXz ← µ(m)−1 · SoftThresholdSingVal(mat(ATz),1), where mat is the inverse of the vec operator. Given a matrixX with SVD U · diag(σ) · V T ,\nSoftThresholdSingVal(X , t) = U · diag (SoftThreshold(σ, t)) · V T . Algorithm 3.1, line 5 dominates the total cost of each iteration."
    }, {
      "heading" : "5.2 The time–data tradeoff",
      "text" : "When we adapt the error bound in Proposition 3.1 to this specific problem, the result is nearly same as in the `1 case (Proposition 4.2). For completeness, we include the full statement of the result in the supplementary material, along with its proof. Our experience with the sparse vector recovery problem suggests that a tradeoff should exist for the low-rank matrix recovery problem as well."
    }, {
      "heading" : "5.3 Numerical experiment",
      "text" : "Figure 4 shows the results of a substantially similar numerical experiment to the one performed for sparse vectors. Again, current practice dictates using a smoothing parameter that has no dependence on the sample size m [29]. In our tests, we choose the constant parameter µ = 0.1 recommended by [15]. As before, we compare this with our aggressive smoothing method that selects µ = µ(m)/4.\nIn this case, we use the ambient dimension d = 200 × 200 and set the normalized rank ρ = 5%. We test each method with 10 random trials of the low-rank matrix recovery problem for each value of the sample size m = 11 250,13 750,16 250, . . . ,38 750. The measurement matrices are again Gaussian, and the nonzero singular values of the random low-rank matricesX \\ are 1. We solve each problem with Algorithm 3.1, stopping when the relative error in the Frobenius norm is smaller than 10−3.\nIn Figure 4, we see that both methods require fewer iterations for convergence as sample size increases. Our aggressive smoothing method additionally achieves a reduction in total computational cost, while the constant method does not. The observed speedup from exploiting the additional samples is 5.4×. The numerical results show that we have indeed identified a time–data tradeoff via smoothing. While this paper considers only the regularized linear inverse problem, our technique extends to other settings. Our forthcoming work [13] addresses the case of noisy measurements, provides a connection to statistical learning problems, and presents additional examples."
    } ],
    "references" : [ {
      "title" : "SVM optimization: inverse dependence on training set size",
      "author" : [ "S. Shalev-Shwartz", "N. Srebro" ],
      "venue" : "In Proceedings of the 25th Annual International Conference on Machine Learning (ICML",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "L. Bottou", "O. Bousquet" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "High-dimensional analysis of semidefinite relaxations for sparse principal components",
      "author" : [ "A.A. Amini", "M.J. Wainwright" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Oracle inequalities for computationally adaptive model selection",
      "author" : [ "A. Agarwal", "P.L. Bartlett", "J.C. Duchi" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Computational Lower Bounds for",
      "author" : [ "Q. Berthet", "P. Rigollet" ],
      "venue" : "Sparse PCA. arXiv,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Computational and statistical tradeoffs via convex relaxation",
      "author" : [ "V. Chandrasekaran", "M.I. Jordan" ],
      "venue" : "Proc. Natl. Acad. Sci. USA,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "More data speeds up training time in learning halfspaces over sparse vectors",
      "author" : [ "A. Daniely", "N. Linial", "S. Shalev-Shwartz" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "On statistics, computation and scalability",
      "author" : [ "M.I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Computation-Risk Tradeoffs for Covariance-Thresholded Regression",
      "author" : [ "D. Shender", "J. Lafferty" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Problem complexity and method efficiency in optimization. A Wiley-Interscience Publication",
      "author" : [ "A.S. Nemirovsky", "D.B. Yudin" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1983
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Math. Program.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "Templates for convex cone problems with applications to sparse signal recovery",
      "author" : [ "S.R. Becker", "E.J. Candès", "M.C. Grant" ],
      "venue" : "Math. Program. Comput.,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Time–Data Tradeoffs by Smoothing",
      "author" : [ "J.J. Bruer", "J.A. Tropp", "V. Cevher", "S.R. Becker" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Fast Global Convergence of Gradient Methods for High-Dimensional Statistical Recovery",
      "author" : [ "A. Agarwal", "S. Negahban", "M.J. Wainwright" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Augmented l(1) and Nuclear-Norm Models with a Globally Linearly Convergent Algorithm",
      "author" : [ "M.-J. Lai", "W. Yin" ],
      "venue" : "SIAM J. Imaging Sci.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Living on the edge: A geometric theory of phase transitions in convex optimization",
      "author" : [ "D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp" ],
      "venue" : "Information and Inference,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "The Convex Geometry of Linear Inverse Problems",
      "author" : [ "V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky" ],
      "venue" : "Found. Comput. Math.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Convex optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "Interior gradient and proximal methods for convex and conic optimization",
      "author" : [ "A. Auslender", "M. Teboulle" ],
      "venue" : "SIAM J. Optim.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Atomic decomposition by basis pursuit",
      "author" : [ "S.S. Chen", "D.L. Donoho", "M.A. Saunders" ],
      "venue" : "SIAM J. Sci. Comput.,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1998
    }, {
      "title" : "Robust modeling with erratic data",
      "author" : [ "J.F. Claerbout", "F. Muir" ],
      "venue" : "Geophysics, 38(5):826–844,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1973
    }, {
      "title" : "Linear Inversion of Band-Limited Reflection Seismograms",
      "author" : [ "F. Santosa", "W.W. Symes" ],
      "venue" : "SIAM J. Sci. Stat. Comput.,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1986
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "H. Zou", "T. Hastie" ],
      "venue" : "J. R. Stat. Soc. Ser. B Stat. Methodol.,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "Linearized Bregman Iterations for Compressed Sensing",
      "author" : [ "J.-F. Cai", "S. Osher", "Z. Shen" ],
      "venue" : "Math. Comp.,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    }, {
      "title" : "Fast linearized Bregman iteration for compressive sensing and sparse denoising",
      "author" : [ "S. Osher", "Y. Mao", "B. Dong", "W. Yin" ],
      "venue" : "Commun. Math. Sci.,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "Matrix rank minimization with applications",
      "author" : [ "M. Fazel" ],
      "venue" : "PhD thesis, Stanford University,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2002
    }, {
      "title" : "Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization",
      "author" : [ "B. Recht", "M. Fazel", "P.A. Parrilo" ],
      "venue" : "SIAM Rev.,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "On the rank minimization problem over a positive semidefinite linear matrix inequality",
      "author" : [ "M. Mesbahi", "G.P. Papavassilopoulos" ],
      "venue" : "IEEE Trans. Automat. Control,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1997
    }, {
      "title" : "A singular value thresholding algorithm for matrix completion",
      "author" : [ "J.-F. Cai", "E.J. Candès", "Z. Shen" ],
      "venue" : "SIAM J. Optim.,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In particular, Shalev-Shwartz and Srebro [1] showed that their algorithm for learning a support vector classifier actually becomes faster when they increase the amount of training data.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Other researchers have identified related tradeoffs [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "Other researchers have identified related tradeoffs [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "Other researchers have identified related tradeoffs [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "Other researchers have identified related tradeoffs [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "Other researchers have identified related tradeoffs [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "Other researchers have identified related tradeoffs [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "Other researchers have identified related tradeoffs [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "Other researchers have identified related tradeoffs [2, 3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "We propose to use smoothing methods [10, 11, 12] to implement this tradeoff.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 10,
      "context" : "We propose to use smoothing methods [10, 11, 12] to implement this tradeoff.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "We propose to use smoothing methods [10, 11, 12] to implement this tradeoff.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : "We pursue a more sophisticated example in a longer version of this work [13].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "Agarwal, Negahban, and Wainwright [14] showed that gradient methods applied to problems like (1) converge in fewer iterations due to increasing restricted strong convexity and restricted smoothness as sample size increases.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "Lai and Yin [15], meanwhile, proposed relaxing the regularizer in (1) based solely on some norm of the underlying signal.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "The RLIP (1) provides a good candidate for studying time–data tradeoffs because recent work in convex geometry [16] gives a precise characterization of the number of samples needed for exact recovery.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "[16] obtain a precise characterization of the number m of samples required to achieve exact recovery.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "Chandrasekaran and Jordan [6] have identified a time–data tradeoff in the setting of denoising problems based on Euclidean projection onto a constraint set.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "In contrast to [6], we propose to use a continuous sequence of relaxations based on smoothing.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "Our geometric motivation is similar with [6] although our relaxation method is totally unrelated.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "The procedure we use applies Nesterov’s primal-smoothing method from [11] to the dual problem; see [12].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 11,
      "context" : "The procedure we use applies Nesterov’s primal-smoothing method from [11] to the dual problem; see [12].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "The dual function is differentiable with ∇gμ (z) = b −Axz , and the gradient is Lipschitz-continuous with Lipschitz constant Lμ no larger than μ−1 ‖A‖2; see [12, 11].",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 10,
      "context" : "The dual function is differentiable with ∇gμ (z) = b −Axz , and the gradient is Lipschitz-continuous with Lipschitz constant Lμ no larger than μ−1 ‖A‖2; see [12, 11].",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : "In order to solve the smoothed dual problem (4), we apply the fast gradient method from Auslender and Teboulle [19].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : "We apply the dual-smoothing procedure from Section 3 to obtain the relaxed primal problem, which is equivalent to the elastic net of Zou and Hastie [23].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 23,
      "context" : "It is common, however, to see much smaller choices of μ [24, 25].",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 24,
      "context" : "It is common, however, to see much smaller choices of μ [24, 25].",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 12,
      "context" : "Our forthcoming work [13] addressing the case of noisy samples provides a more principled way to select this parameter.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 25,
      "context" : "Fazel [26] proposed using the Schatten 1-norm ‖·‖S1 , the sum of the matrix’s singular values, as a convex proxy for rank.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 26,
      "context" : "[27] and select f = ‖·‖S1 as the regularizer in the RLIP (1).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "The low-rank matrix recovery problem has roots in control theory [28].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 28,
      "context" : "Again, current practice dictates using a smoothing parameter that has no dependence on the sample size m [29].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 12,
      "context" : "Our forthcoming work [13] addresses the case of noisy measurements, provides a connection to statistical learning problems, and presents additional examples.",
      "startOffset" : 21,
      "endOffset" : 25
    } ],
    "year" : 2014,
    "abstractText" : "This paper proposes a tradeoff between sample complexity and computation time that applies to statistical estimators based on convex optimization. As the amount of data increases, we can smooth optimization problems more and more aggressively to achieve accurate estimates more quickly. This work provides theoretical and experimental evidence of this tradeoff for a class of regularized linear inverse problems.",
    "creator" : null
  }
}