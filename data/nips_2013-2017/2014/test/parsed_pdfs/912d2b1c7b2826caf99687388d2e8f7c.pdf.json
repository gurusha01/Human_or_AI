{
  "name" : "912d2b1c7b2826caf99687388d2e8f7c.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Efficient Structured Matrix Rank Minimization",
    "authors" : [ "Adams Wei Yu", "Wanli Ma", "Yaoliang Yu", "Jaime G. Carbonell" ],
    "emails" : [ "jgc}@cs.cmu.edu,", "suvrit@tuebingen.mpg.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Many practical tasks involve finding models that are both simple and capable of explaining noisy observations. The model complexity is sometimes encoded by the rank of a parameter matrix, whereas physical and system level constraints could be encoded by a specific matrix structure. Thus, rank minimization subject to structural constraints has become important to many applications in machine learning, control theory, and signal processing [10, 22]. Applications include collaborative filtering [23], system identification and realization [19, 21], multi-task learning [28], among others.\nThe focus of this paper is on problems where in addition to being low-rank, the parameter matrix must satisfy additional linear structure. Typically, this structure involves Hankel, Toeplitz, Sylvester, Hessenberg or circulant matrices [4, 11, 19]. The linear structure describes interdependencies between the entries of the estimated matrix and helps substantially reduce the degrees of freedom.\nAs a concrete example consider a linear time-invariant (LTI) system where we are estimating the parameters of an autoregressive moving-average (ARMA) model. The order of this LTI system, i.e., the dimension of the latent state space, is equal to the rank of a Hankel matrix constructed by the process covariance [20]. A system of lower order, which is easier to design and analyze, is usually more desirable. The problem of minimum order system approximation is essentially a structured matrix rank minimization problem. There are several other applications where such linear structure is of great importance—see e.g., [11] and references therein. Furthermore, since (enhanced) structured matrix completion also falls into the category of rank minimization problems, the results in our paper can as well be applied to specific problems in spectral compressed sensing [6], natural language processing [1], computer vision [8] and medical imaging [24].\nFormally, we study the following (block) structured rank minimization problem:\nmin y 1 2kA(y) bk 2 F + µ · rank(Qm,n,j,k(y)). (1)\nHere, y = (y1, ..., yj+k 1) is an m⇥ n(j + k 1) matrix with yt 2 Rm⇥n for t = 1, ..., j + k 1, A : Rm⇥n(j+k 1) ! Rp is a linear map, b 2 Rp, Q\nm,n,j,k (y) 2 Rmj⇥nk is a structured matrix whose elements are linear functions of y\nt ’s, and µ > 0 controls the regularization. Throughout this paper, we will use M = mj and N = nk to denote the number of rows and columns of Q\nm,n,j,k\n(y).\nProblem (1) is in general NP-hard [21] due to the presence of the rank function. A popular approach to address this issue is to use the nuclear norm k · k⇤, i.e., the sum of singular values, as a convex surrogate for matrix rank [22]. Doing so turns (1) into a convex optimization problem:\nmin y 1 2kA(y) bk 2 F + µ · kQm,n,j,k(y)k⇤. (2)\nSuch a relaxation has been combined with various convex optimization procedures in previous work, e.g., interior-point approaches [17, 18] and first-order alternating direction method of multipliers (ADMM) approaches [11]. However, such algorithms are computationally expensive. The cost per iteration of an interior-point method is no less than O(M2N2), and that of typical proximal and ADMM style first-order methods in [11] is O(min(N2M,NM2)); this high cost arises from each iteration requiring a full Singular Value Decomposition (SVD). The heavy computational cost of these methods prevents them from scaling to large problems.\nContributions. In view of the efficiency and scalability limitations of current algorithms, the key contributions of our paper are as follows.\n• We formulate the structured rank minimization problem differently, so that we still find lowrank solutions consistent with the observations, but substantially more scalably.\n• We customize the generalized conditional gradient (GCG) approach of Zhang et al. [27] to our new formulation. Compared with previous first-order methods, the cost per iteration is O(MN) (linear in the data size), which is substantially lower than methods that require full SVDs.\n• Our approach maintains a convergence rate of O 1 ✏ and thus achieves an overall complexity\nof O MN\n✏\n, which is by far the lowest in terms of the dependence of M or N for general struc-\ntured rank minimization problems. It also empirically proves to be a state-of-the-art method for (but clearly not limited to) stochastic system realization and spectral compressed sensing.\nWe note that following a GCG scheme has another practical benefit: the rank of the intermediate solutions starts from a small value and then gradually increases, while the starting solutions obtained from existing first-order methods are always of high rank. Therefore, GCG is likely to find a lowrank solution faster, especially for large size problems.\nRelated work. Liu and Vandenberghe [17] adopt an interior-point method on a reformulation of (2), where the nuclear norm is represented via a semidefinite program. The cost of each iteration in [17] is no less than O(M2N2). Ishteva et al. [15] propose a local optimization method to solve the weighted structured rank minimization problem, which still has complexity as high as O(N3Mr2) per iteration, where r is the rank. This high computational cost prevents [17] and [15] from handling large-scale problems. In another recent work, Fazel et al. [11] propose a framework to solve (2). They derive several primal and dual reformulations for the problem, and propose corresponding first-order methods such as ADMM, proximal-point, and accelerated projected gradient. However, each iteration of these algorithms involves a full SVD of complexity O(min(M2N,N2M)), making it hard to scale them to large problems. Signoretto et al. [25] reformulate the problem to avoid full SVDs by solving an equivalent nonconvex optimization problem via ADMM. However, their method requires subroutines to solve linear equations per iteration, which can be time-consuming for large problems. Besides, there is no guarantee that their method will converge to the global optimum.\nThe conditional gradient (CG) (a.k.a. Frank-Wolfe) method was proposed by Frank and Wolfe [12] to solve constrained problems. At each iteration, it first solves a subproblem that minimizes a linearized objective over a compact constraint set and then moves toward the minimizer of the cost function. CG is efficient as long as the linearized subproblem is easy to solve. Due to its simplicity and scalability, CG has recently witnessed a great surge of interest in the machine learning and optimization community [16]. In another recent strand of work, CG was extended to certain regularized (non-smooth) problems as well [3, 13, 27]. In the following, we will show how a generalized CG method can be adapted to solve the structured matrix rank minimization problem."
    }, {
      "heading" : "2 Problem Formulation and Approach",
      "text" : "In this section we reformulate the structured rank minimization problem in a way that enables us to apply the generalized conditional gradient method, which we subsequently show to be much more efficient than existing approaches, both theoretically and experimentally. Our starting point is that in most applications, we are interested in finding a “simple” model that is consistent with\nthe observations, but the problem formulation itself, such as (2), is only an intermediate means, hence it need not be fixed. In fact, when formulating our problem we can and we should take the computational concerns into account. We will demonstrate this point first."
    }, {
      "heading" : "2.1 Problem Reformulation",
      "text" : "The major computational difficulty in problem (2) comes from the linear transformation Q m,n,j,k (·) inside the trace norm regularizer. To begin with, we introduce a new matrix variable X 2 Rmj⇥nk and remove the linear transformation by introducing the following linear constraint\nQ m,n,j,k (y) = X. (3) For later use, we partition the matrix X into the block form\nX :=\n2\n664\nx11 x12 · · · x1k x21 x22 · · · x2k\n... ... ... x j1 xj2 · · · xjk\n3\n775 with xil 2 R m⇥n for i = 1, ..., j, l = 1, ..., k. (4)\nWe denote by x := vec(X) 2 Rmjk⇥n the vector obtained by stacking the columns of X blockwise, and by X := mat(x) 2 Rmj⇥nk the reverse operation. Since x and X are merely different reorderings of the same object, we will use them interchangeably to refer to the same object.\nWe observe that any linear (or slightly more generally, affine) structure encoded by the linear transformation Q\nm,n,j,k (·) translates to linear constraints on the elements of X (such as the sub-blocks in (4) satisfying say x12 = x21), which can be represented as linear equations Bx = 0, with an appropriate matrix B that encodes the structure of Q. Similarly, the linear constraint in (3) that relates y and X , or equivalently x, can also be written as the linear constraint y = Cx for a suitable recovery matrix C. Details on constructing matrix B and C can be found in the appendix. Thus, we reformulate (2) into\nmin x2Rmjk⇥n\n1 2kA(Cx) bk 2 F + µkXk⇤ (5)\ns.t. Bx = 0. (6) The new formulation (5) is still computationally inconvenient due to the linear constraint (6). We resolve this difficulty by applying the penalty method, i.e., by placing the linear constraint into the objective function after composing with a penalty function such as the squared Frobenius norm:\nmin x2Rmjk⇥n\n1 2kA(Cx) bk 2 F + 2 kBxk 2 F + µkXk⇤. (7)\nHere > 0 is a penalty parameter that controls the inexactness of the linear constraint. In essence, we turn (5) into an unconstrained problem by giving up on satisfying the linear constraint exactly. We argue that this is a worthwhile trade-off for (i) By letting \" 1 and following a homotopy scheme the constraint can be satisfied asymptotically; (ii) If exactness of the linear constraint is truly desired, we could always post-process each iterate by projecting to the constraint manifold using Cproj (see appendix); (iii) As we will show shortly, the potential computational gains can be significant, enabling us to solve problems at a scale which is not achievable previously. Therefore, in the sequel we will focus on solving (7). After getting a solution for x, we recover the original variable y through the linear relation y = Cx. As shown in our empirical studies (see Section 3), the resulting solution Q\nm,n,j,k (y) indeed enjoys the desirable low-rank property even with a moderate penalty parameter . We next present an efficient algorithm for solving (7)."
    }, {
      "heading" : "2.2 The Generalized Conditional Gradient Algorithm",
      "text" : "Observing that the first two terms in (7) are both continuously differentiable, we absorb them into a common term f and rewrite (7) in the more familiar compact form:\nmin X2Rmj⇥nk (X) := f(X) + µkXk⇤, (8)\nwhich readily fits into the framework of the generalized conditional gradient (GCG) [3, 13, 27]. In short, at each iteration GCG successively linearizes the smooth function f , finds a descent direction by solving the (convex) subproblem\nZ k 2 arg min kZk⇤1 hZ,rf(X k 1)i, (9)\nAlgorithm 1 Generalized Conditional Gradient for Structured Matrix Rank Minimization 1: Initialize U0, V0; 2: for k = 1, 2, ... do 3: (u\nk , v k ) top singular vector pair of rf(U k 1Vk 1);\n4: set ⌘ k\n2/(k + 1), and ✓ k by (13); 5: Uinit ( p 1 ⌘ k U k 1, p ✓ k u k ); Vinit ( p 1 ⌘ k V k 1, p ✓ k v k\n); 6: (U\nk , V k ) argmin (U, V ) using initializer (Uinit, Vinit); 7: end for\nand then takes the convex combination X k = (1 ⌘ k )X k 1+⌘k(↵kZk) with a suitable step size ⌘k and scaling factor ↵ k\n. Clearly, the efficiency of GCG heavily hinges on the efficacy of solving the subproblem (9). In our case, the minimal objective is simply the matrix spectral norm of rf(X\nk ) and the minimizer can be chosen as the outer product of the top singular vector pair. Both can be computed essentially in linear time O(MN) using the Lanczos algorithm [7].\nTo further accelerate the algorithm, we adopt the local search idea in [27], which is based on the variational form of the trace norm [26]:\nkXk⇤ = 12 min{kUk 2 F + kV k2F : X = UV }. (10)\nThe crucial observation is that (10) is separable and smooth in the factor matrices U and V , although not jointly convex. We alternate between the GCG algorithm and the following nonconvex auxiliary problem, trying to get the best of both ends:\nmin U,V\n(U, V ), where (U, V ) = f(UV ) + µ2 (kUk 2 F + kV k2F). (11)\nSince our smooth function f is quadratic, it is easy to carry out a line search strategy for finding an appropriate ↵\nk in the convex combination X k+1 = (1 ⌘k)Xk+⌘k(↵kZk) =: (1 ⌘k)Xk+✓kZk,\nwhere ✓ k\n= argmin ✓ 0 h k (✓) (12)\nis the minimizer of the function (on ✓ 0) h k (✓) := f((1 ⌘ k )X k + ✓Z k ) + µ(1 ⌘ k )kX k\nk⇤ + µ✓. (13) In fact, h\nk (✓) upper bounds the objective function at (1 ⌘ k )X k + ✓Z k . Indeed, using convexity, ((1 ⌘\nk )X k + ✓Z k ) = f((1 ⌘ k )X k + ✓Z k ) + µk(1 ⌘ k )X k + ✓Z k k⇤  f((1 ⌘\nk )X k + ✓Z k ) + µ(1 ⌘ k )kX k k⇤ + µ✓kZkk⇤  f((1 ⌘\nk )X k + ✓Z k ) + µ(1 ⌘ k )kX k k⇤ + µ✓ (as kZkk⇤  1) = h\nk\n(✓).\nThe reason to use the upper bound h k (✓), instead of the true objective ((1 ⌘ k )X k + ✓Z k ), is to avoid evaluating the trace norm, which can be quite expensive. More generally, if f is not quadratic, we can use the quadratic upper bound suggested by the Taylor expansion. It is clear that ✓\nk in (12) can be computed in closed-form.\nWe summarize our procedure in Algorithm 1. Importantly, we note that the algorithm explicitly maintains a low-rank factorization X = UV throughout the iteration. In fact, we never need the product X , which is a crucial step in reducing the memory footage for large applications. The maintained low-rank factorization also allows us to more efficiently evaluate the gradient and its spectral norm, by carefully arranging the multiplication order. Finally, we remark that we need not wait until the auxiliary problem (11) is fully solved; we can abort this local procedure whenever the gained improvement does not match the devoted computation. For the convergence guarantee we establish in Theorem 1 below, only the descent property (U\nk V k )  (U k 1Vk 1) is needed.\nThis requirement can be easily achieved by evaluating , which, unlike the original objective , is computationally cheap."
    }, {
      "heading" : "2.3 Convergence analysis",
      "text" : "Having presented the generalized conditional gradient algorithm for our structured rank minimization problem, we now analyze its convergence property. We need the following standard assumption.\nAssumption 1 There exists some norm k · k and some constant L > 0, such that for all A,B 2 RN⇥M and ⌘ 2 (0, 1), we have\nf((1 ⌘)A+ ⌘B)  f(A) + ⌘hB A,rf(A)i+ L⌘ 2\n2 kB Ak 2.\nMost standard loss functions, such as the quadratic loss we use in this paper, satisfy Assumption 1.\nWe are ready to state the convergence property of Algorithm 1 in the following theorem. To make the paper self-contained, we also reproduce the proof in the appendix.\nTheorem 1 Let Assumption 1 hold, X be arbitrary, and X k be the k-th iterate of Algorithm 1 applied on the problem (7), then we have\n(X k ) (X)  2C k + 1 , (14)\nwhere C is some problem dependent absolute constant.\nThus for any given accuracy ✏ > 0, Algorithm 1 will output an ✏-approximate (in the sense of function value) solution in at most O(1/✏) steps."
    }, {
      "heading" : "2.4 Comparison with existing approaches",
      "text" : "We briefly compare the efficiency of Algorithm 1 with the state-of-the-art approaches; more thorough experimental comparisons will be conducted in Section 3 below. The per-step complexity of our algorithm is dominated by the subproblem (9) which requires only the leading singular vector pair of the gradient. Using the Lanczos algorithm this costs O(MN) arithmetic operations [16], which is significantly cheaper than the O(min(M2N,N2M)) complexity of [11] (due to their need of full SVD). Other approaches such as [25] and [17] are even more costly."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we present empirical results using our algorithms. Without loss of generality, we focus on two concrete structured rank minimization problems: (i) stochastic system realization (SSR); and (ii) 2-D spectral compressed sensing (SCS). Both problems involve minimizing the rank of two different structured matrices. For SSR, we compare different first-order methods to show the speedups offered by our algorithm. In the SCS problem, we show that our formulation can be generalized to more complicated linear structures and effectively recover unobserved signals."
    }, {
      "heading" : "3.1 Stochastic System Realization",
      "text" : "Model. The SSR problem aims to find a minimal order autoregressive moving-average (ARMA) model, given the observation of noisy system output [11]. As a discrete linear time-invariant (LTI) system, an AMRA process can be represented by the following state-space model\ns t+1 = Dst + Eut, zt = Fst + ut, t = 1, 2, ..., T, (15)\nwhere s t 2 Rr is the hidden state variable, u t 2 Rn is driving white noise with covariance matrix G, and z\nt 2 Rn is the system output that is observable at time t. It has been shown in [20] that the system order r equals the rank of the block-Hankel matrix (see appendix for definition) constructed by the exact process covariance y\ni = E(z t zT t+i), provided that the number of blocks per column, j,\nis larger than the actual system order. Determining the rank r is the key to the whole problem, after which, the parameters D,E, F,G can be computed easily [17, 20]. Therefore, finding a low order system is equivalent to minimizing the rank of the Hankel matrix above, while remaining consistent with the observations.\nSetup. The meaning of the following parameters can be seen in the text after E.q. (1). We follow the experimental setup of [11]. Here, m = n, p = n⇥ n(j + k 1), while v = (v1, v2, ..., vj+k 1) denotes the empirical process covariance calculated as v\ni = 1 T P T i t=1 zt+iz T t\n, for 1  i  k and 0 otherwise. Let w = (w1, w2, ..., wj+k 1) be the observation matrix, where the wi are all 1’s for 1  i  k, indicating the whole block of v\ni\nis observed, and all 0’s otherwise (for unobserved\nblocks). Finally, A(y) = vec(w y), b = vec(w v), Q(y) = H n,n,j,k (y), where is the elementwise product and is H\nn,n,j,k (·) the Hankel matrix (see Appendix for the corresponding B and C). Data generation. Each entry of the matrices D 2 Rr⇥r, E 2 Rr⇥n, F 2 Rn⇥r is sampled from a Gaussian distribution N(0, 1). Then they are normalized to have unit nuclear norm. The initial state vector s0 is drawn from N(0, Ir) and the input white noise ut from N(0, In). The measurement noise is modeled by adding an ⇠ term to the output z\nt , so the actual observation is z t = z t + ⇠, where each entry of ⇠ 2 Rn is a standard Gaussian noise, and is the noise level. Throughout this experiment, we set T = 1000, = 0.05, the maximum iteration limit as 100, and the stopping criterion as kx\nk+1 xkkF < 10 3 or | k+1 k||min( k+1, k)| < 10 3. The initial iterate is a matrix of all\nones.\nAlgorithms. We compare our approach with the state-of-the-art competitors, i.e., the first-order methods proposed in [11]. Other methods, such as those in [15, 17, 25] suffer heavier computation cost per iteration, and are thus omitted from comparison. Fazel et al. [11] aim to solve either the primal or dual form of problem (2), using primal ADMM (PADMM), a variant of primal ADMM (PADMM2), a variant of dual ADMM (DADMM2), and a dual proximal point algorithm (DPPA). As for solving (7), we implemented generalized conditional gradient (GCG) and its local search variant (GCGLS). We also implemented the accelerated projected gradient with singular value thresholding (APG-SVT) to solve (8) by adopting the FISTA [2] scheme. To fairly compare both lines of methods for different formulations, in each iteration we track their objective values, the squared loss 1 2kA(Cx) bk 2 F (or 1 2kA(y) bk 2 F), and the rank of the Hankel matrix Hm,n,j,k(y). Since square loss measures how well the model fits the observations, and the Hankel matrix rank approximates the system order, comparison of these quantities obtained by different methods is meaningful.\nResult 1: Efficiency and Scalability. We compare the performance of different methods on two sizes of problems, and the result is shown in Figure 2. The most important observation is, our approach GCGLS/GCG significantly outperform the remaining competitors in term of running time. It is easy to see from Figure 2(a) and 2(b) that both the objective value and square loss by GCGLS/GCG drop drastically within a few seconds and is at least one order of magnitude faster than the runner-up competitor (DPPA) to reach a stable stage. The rest of baseline methods cannot even approach the minimum values achieved by GCGLS/GCG within the iteration limit. Figure 2(d) and 2(e) show that such advantage is amplified as size increases, which is consistent with the theoretical finding. Then, not surprisingly, we observe that the competitors become even slower if the problem size continues growing. Hence, we only test the scalability of our approach on larger sized problems, with the running time reported in Figure 1. We can see that the running time of GCGLS grows linearly w.r.t. the size MN , again consistent with previous analysis.\nResult 2: Rank of solution. We also report the rank of H\nn,n,j,k (y) versus the running time in Figure 2(c) and 2(f), where y = Cx if we solve (2) or y directly comes from the solution of (7). The rank is computed as the number of singular values larger than 10 3. For the GCGLS/GCG, the iterate starts from a low rank estimation and then gradually approaches the true one. However, for other competitors, the iterate first jumps to a full rank matrix and the rank of later iterate drops gradually. Given that the solution is intrinsically of low rank, GCGLS/GCG will probably find the desired one more efficiently. In view of this, the working memory of GCGLS is usually much smaller than the competitors, as it uses two low rank matrices U, V to represent but never materialize the solution until necessary."
    }, {
      "heading" : "3.2 Spectral Compressed Sensing",
      "text" : "In this part we apply our formulation and algorithm to another application, spectral compressed sensing (SCS), a technique that has by now been widely used in digital signal processing applications [6, 9, 29]. We show in particular that our reformulation (7) can effectively and rapidly recover partially observed signals.\nModel. The problem of spectral compressed sensing aims to recover a frequency-sparse signal from a small number of observations. The 2-D signal Y (k, l), 0 < k  n1, 0 < l  n2 is supposed to be the superposition of r 2-D sinusoids of arbitrary frequencies, i.e. (in the DFT form)\nY (k, l) = rX\ni=1\nd i\nej2⇡(kf1i+lf2i) = rX\ni=1\nd i\n(ej2⇡f1i)k(ej2⇡f2i)l (16)\nwhere d i is the amplitudes of the i-th sinusoid and (f xi , f yi ) is its frequency.\nInspired by the conventional matrix pencil method [14] for estimating the frequencies of sinusoidal signals or complex sinusoidal (damped) signals, the authors in [6] propose to arrange the observed data into a 2-fold Hankel matrix whose rank is bounded above by r, and formulate the 2-D spectral compressed sensing problem into a rank minimization problem with respect to the 2-fold Hankel structure. This 2-fold structure is a also linear structure, as we explain in the appendix. Given limited observations, this problem can be viewed as a matrix completion problem that recovers a low-rank matrix from partially observed entries while preserving the pre-defined linear structure. The trace norm heuristic for rank (·) is again used here, as it is proved by [5] to be an exact method for matrix completion provided that the number of observed entries satisfies the corresponding information theoretic bound.\nSetup. Given a partial observed signal Y with ⌦ as the observation index set, we adopt the formulation (7) and thus aim to solve the following problem:\nmin X2RM⇥N\n1 2 kP⌦(mat(Cx)) P⌦(Y )k2F + 2 kBxk2F + µkXk⇤ (17)\nwhere x = vec(X), mat(·) is the inverse of the vectorization operator on Y . In this context, as before, A = P⌦, b = P⌦(Y ), where P⌦(Y ) only keeps the entries of Y in the index set ⌦ and vanishes the others, Q(Y ) = H(2)\nk1,k2 (Y ) is the two-fold Hankel matrix, and corresponding B and\nC can be found in the appendix to encode H(2) k1,k2 (Y ) = X . Further, the size of matrix here is M = k1k2, N = (n1 k1 + 1)(n2 k2 + 1). Algorithm. We apply our generalized conditional gradient method with local search (GCGLS) to solve the spectral compressed sensing problem, using the reformulation discussed above. Following\nthe experiment setup in [6], we generate a ground truth data matrix Y 2 R101⇥101 through a superposition of r = 6 2-D sinusoids, randomly reveal 20% of the entries, and add i.i.d Gaussian noise with amplitude signal-to-noise ratio 10.\nResult. The results on the SCS problem are shown in Figure 3. The generated true 2-D signal Y is shown in Figure 3(a) using the jet colormap. The 20% observed entries of Y are shown in Figure 3(b), where the white entries are unobserved. The signal recovered by our GCGLS algorithm is shown in Figure 3(c). Comparing with the true signal in Figure 3(a), we can see that the result of our CGCLS algorithm is pretty close to the truth. To demonstrate the result more clearly, we extract a single column as a 1-D signals for further inspection. Figure 3(d) plots the original signal (blue line) as well as the observed ones (red dot), both from the first column of the 2-D signals. In 3(e), the recovered signal is represented by the red dashed dashed curve. It matches the original signal with significantly large portion, showing the success of our method in recovering partially observed 2-D signals from noise. Since the 2-fold structure used in this experiment is more complicated than that in the previous SSR task, this experiment further validates our algorithm on more complicated problems."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we address the structured matrix rank minimization problem. We first formulate the problem differently, so that it is amenable to adapt the Generalized Conditional Gradient Method. By doing so, we are able to achieve the complexity O(MN) per iteration with a convergence rate O 1 ✏ . Then the overall complexity is by far the lowest compared to state-of-the-art methods for the structured matrix rank minimization problem. Our empirical studies on stochastic system realization and spectral compressed sensing further confirm the efficiency of the algorithm and the effectiveness of our reformulation."
    } ],
    "references" : [ {
      "title" : "Spectral learning of general weighted automata via constrained matrix completion",
      "author" : [ "B. Balle", "M. Mohri" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM J. Imaging Sciences,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "A generalized conditional gradient method and its connection to an iterative shrinkage method",
      "author" : [ "K. Bredies", "D.A. Lorenz", "P. Maass" ],
      "venue" : "Computational Optimization and Applications,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Signal enhancement: A composite property mapping algorithm",
      "author" : [ "J.A. Cadzow" ],
      "venue" : "IEEE Transactions on Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1988
    }, {
      "title" : "The power of convex relaxation: near-optimal matrix completion",
      "author" : [ "E.J. Candès", "T. Tao" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Spectral compressed sensing via structured matrix completion",
      "author" : [ "Y. Chen", "Y. Chi" ],
      "venue" : "In ICML,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Lanczos Algorithms for Large Symmetric Eigenvalue Computations",
      "author" : [ "J.K. Cullum", "R.A. Willoughby" ],
      "venue" : "Vol. 1. Elsevier,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2002
    }, {
      "title" : "A rank minimization approach to video inpainting",
      "author" : [ "T. Ding", "M. Sznaier", "O.I. Camps" ],
      "venue" : "In ICCV, pages",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Spectral compressive sensing",
      "author" : [ "M.F. Duarte", "R.G. Baraniuk" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Matrix rank minimization with applications",
      "author" : [ "M. Fazel" ],
      "venue" : "PhD thesis, Stanford University,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2002
    }, {
      "title" : "Hankel matrix rank minimization with applications to system identification and realization",
      "author" : [ "M. Fazel", "T.K. Pong", "D. Sun", "P. Tseng" ],
      "venue" : "SIAM J. Matrix Analysis Applications,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "An algorithm for quadratic programming",
      "author" : [ "M. Frank", "P. Wolfe" ],
      "venue" : "Naval Research Logistics Quarterly,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1956
    }, {
      "title" : "Conditional gradient algorithms for machine learning",
      "author" : [ "Z. Harchaoui", "A. Juditsky", "A. Nemirovski" ],
      "venue" : "In NIPS Workshop on Optimization for ML.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Estimating two-dimensional frequencies by matrix enhancement and matrix pencil",
      "author" : [ "Y. Hua" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1992
    }, {
      "title" : "Factorization approach to structured low-rank approximation with applications",
      "author" : [ "M. Ishteva", "K. Usevich", "I. Markovsky" ],
      "venue" : "SIAM J. Matrix Analysis Applcations,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Revisiting Frank-Wolfe: Projection-free sparse convex optimization",
      "author" : [ "M. Jaggi" ],
      "venue" : "In ICML, pages 427–435,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Semidefinite programming methods for system realization and identification",
      "author" : [ "Z. Liu", "L. Vandenberghe" ],
      "venue" : "In CDC,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Interior-point method for nuclear norm approximation with application to system identification",
      "author" : [ "Z. Liu", "L. Vandenberghe" ],
      "venue" : "SIAM J. Matrix Analysis Applications,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Nuclear norm system identification with missing inputs and outputs",
      "author" : [ "Z. Liu", "A. Hansson", "L. Vandenberghe" ],
      "venue" : "Systems & Control Letters,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Vector ARMA estimation: a reliable subspace approach",
      "author" : [ "J. Mari", "P. Stoica", "T. McKelvey" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2000
    }, {
      "title" : "Structured low-rank approximation and its applications",
      "author" : [ "I. Markovsky" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "B. Recht", "M. Fazel", "P.A. Parrilo" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Fast maximum margin matrix factorization for collaborative prediction",
      "author" : [ "J.D.M. Rennie", "N. Srebro" ],
      "venue" : "In ICML,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "Calibrationless parallel imaging reconstruction based on structured low-rank matrix completion",
      "author" : [ "P.J. Shin", "P.E. Larson", "M.A. Ohliger", "M. Elad", "J.M. Pauly", "D.B. Vigneron", "M. Lustig" ],
      "venue" : "Magnetic Resonance in Medicine,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "An SVD-free approach to a class of structured low rank matrix optimization problems with application to system identification",
      "author" : [ "M. Signoretto", "V. Cevher", "J.A. Suykens" ],
      "venue" : "Technical report, K.U.Leuven,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "N. Srebro", "J.D.M. Rennie", "T. Jaakkola" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2004
    }, {
      "title" : "Accelerated training for matrix-norm regularization: A boosting approach",
      "author" : [ "X. Zhang", "Y. Yu", "D. Schuurmans" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Multi-task learning: theory, algorithms, and applications",
      "author" : [ "J. Zhou", "J. Chen", "J. Ye" ],
      "venue" : "SIAM Data Mining Tutorial,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Graph spectral compressed sensing",
      "author" : [ "X. Zhu", "M. Rabbat" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Thus, rank minimization subject to structural constraints has become important to many applications in machine learning, control theory, and signal processing [10, 22].",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : "Thus, rank minimization subject to structural constraints has become important to many applications in machine learning, control theory, and signal processing [10, 22].",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : "Applications include collaborative filtering [23], system identification and realization [19, 21], multi-task learning [28], among others.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "Applications include collaborative filtering [23], system identification and realization [19, 21], multi-task learning [28], among others.",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 20,
      "context" : "Applications include collaborative filtering [23], system identification and realization [19, 21], multi-task learning [28], among others.",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 27,
      "context" : "Applications include collaborative filtering [23], system identification and realization [19, 21], multi-task learning [28], among others.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "Typically, this structure involves Hankel, Toeplitz, Sylvester, Hessenberg or circulant matrices [4, 11, 19].",
      "startOffset" : 97,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : "Typically, this structure involves Hankel, Toeplitz, Sylvester, Hessenberg or circulant matrices [4, 11, 19].",
      "startOffset" : 97,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "Typically, this structure involves Hankel, Toeplitz, Sylvester, Hessenberg or circulant matrices [4, 11, 19].",
      "startOffset" : 97,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : ", the dimension of the latent state space, is equal to the rank of a Hankel matrix constructed by the process covariance [20].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "Furthermore, since (enhanced) structured matrix completion also falls into the category of rank minimization problems, the results in our paper can as well be applied to specific problems in spectral compressed sensing [6], natural language processing [1], computer vision [8] and medical imaging [24].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, since (enhanced) structured matrix completion also falls into the category of rank minimization problems, the results in our paper can as well be applied to specific problems in spectral compressed sensing [6], natural language processing [1], computer vision [8] and medical imaging [24].",
      "startOffset" : 252,
      "endOffset" : 255
    }, {
      "referenceID" : 7,
      "context" : "Furthermore, since (enhanced) structured matrix completion also falls into the category of rank minimization problems, the results in our paper can as well be applied to specific problems in spectral compressed sensing [6], natural language processing [1], computer vision [8] and medical imaging [24].",
      "startOffset" : 273,
      "endOffset" : 276
    }, {
      "referenceID" : 23,
      "context" : "Furthermore, since (enhanced) structured matrix completion also falls into the category of rank minimization problems, the results in our paper can as well be applied to specific problems in spectral compressed sensing [6], natural language processing [1], computer vision [8] and medical imaging [24].",
      "startOffset" : 297,
      "endOffset" : 301
    }, {
      "referenceID" : 20,
      "context" : "Problem (1) is in general NP-hard [21] due to the presence of the rank function.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 21,
      "context" : ", the sum of singular values, as a convex surrogate for matrix rank [22].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 16,
      "context" : ", interior-point approaches [17, 18] and first-order alternating direction method of multipliers (ADMM) approaches [11].",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 17,
      "context" : ", interior-point approaches [17, 18] and first-order alternating direction method of multipliers (ADMM) approaches [11].",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : ", interior-point approaches [17, 18] and first-order alternating direction method of multipliers (ADMM) approaches [11].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "The cost per iteration of an interior-point method is no less than O(M2N2), and that of typical proximal and ADMM style first-order methods in [11] is O(min(N2M,NM2)); this high cost arises from each iteration requiring a full Singular Value Decomposition (SVD).",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : "Liu and Vandenberghe [17] adopt an interior-point method on a reformulation of (2), where the nuclear norm is represented via a semidefinite program.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "The cost of each iteration in [17] is no less than O(M2N2).",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 14,
      "context" : "[15] propose a local optimization method to solve the weighted structured rank minimization problem, which still has complexity as high as O(N3Mr2) per iteration, where r is the rank.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "This high computational cost prevents [17] and [15] from handling large-scale problems.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : "This high computational cost prevents [17] and [15] from handling large-scale problems.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 24,
      "context" : "[25] reformulate the problem to avoid full SVDs by solving an equivalent nonconvex optimization problem via ADMM.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "Frank-Wolfe) method was proposed by Frank and Wolfe [12] to solve constrained problems.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "Due to its simplicity and scalability, CG has recently witnessed a great surge of interest in the machine learning and optimization community [16].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "In another recent strand of work, CG was extended to certain regularized (non-smooth) problems as well [3, 13, 27].",
      "startOffset" : 103,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "In another recent strand of work, CG was extended to certain regularized (non-smooth) problems as well [3, 13, 27].",
      "startOffset" : 103,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : "In another recent strand of work, CG was extended to certain regularized (non-smooth) problems as well [3, 13, 27].",
      "startOffset" : 103,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Observing that the first two terms in (7) are both continuously differentiable, we absorb them into a common term f and rewrite (7) in the more familiar compact form: min X2Rmj⇥nk (X) := f(X) + μkXk⇤, (8) which readily fits into the framework of the generalized conditional gradient (GCG) [3, 13, 27].",
      "startOffset" : 289,
      "endOffset" : 300
    }, {
      "referenceID" : 12,
      "context" : "Observing that the first two terms in (7) are both continuously differentiable, we absorb them into a common term f and rewrite (7) in the more familiar compact form: min X2Rmj⇥nk (X) := f(X) + μkXk⇤, (8) which readily fits into the framework of the generalized conditional gradient (GCG) [3, 13, 27].",
      "startOffset" : 289,
      "endOffset" : 300
    }, {
      "referenceID" : 26,
      "context" : "Observing that the first two terms in (7) are both continuously differentiable, we absorb them into a common term f and rewrite (7) in the more familiar compact form: min X2Rmj⇥nk (X) := f(X) + μkXk⇤, (8) which readily fits into the framework of the generalized conditional gradient (GCG) [3, 13, 27].",
      "startOffset" : 289,
      "endOffset" : 300
    }, {
      "referenceID" : 6,
      "context" : "Both can be computed essentially in linear time O(MN) using the Lanczos algorithm [7].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 26,
      "context" : "To further accelerate the algorithm, we adopt the local search idea in [27], which is based on the variational form of the trace norm [26]: kXk⇤ = (1)2 min{kUk 2 F + kV k(2)F : X = UV }.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 25,
      "context" : "To further accelerate the algorithm, we adopt the local search idea in [27], which is based on the variational form of the trace norm [26]: kXk⇤ = (1)2 min{kUk 2 F + kV k(2)F : X = UV }.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : "Using the Lanczos algorithm this costs O(MN) arithmetic operations [16], which is significantly cheaper than the O(min(M2N,N2M)) complexity of [11] (due to their need of full SVD).",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 10,
      "context" : "Using the Lanczos algorithm this costs O(MN) arithmetic operations [16], which is significantly cheaper than the O(min(M2N,N2M)) complexity of [11] (due to their need of full SVD).",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 24,
      "context" : "Other approaches such as [25] and [17] are even more costly.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 16,
      "context" : "Other approaches such as [25] and [17] are even more costly.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "The SSR problem aims to find a minimal order autoregressive moving-average (ARMA) model, given the observation of noisy system output [11].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 19,
      "context" : "It has been shown in [20] that the system order r equals the rank of the block-Hankel matrix (see appendix for definition) constructed by the exact process covariance y",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "Determining the rank r is the key to the whole problem, after which, the parameters D,E, F,G can be computed easily [17, 20].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "Determining the rank r is the key to the whole problem, after which, the parameters D,E, F,G can be computed easily [17, 20].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 10,
      "context" : "We follow the experimental setup of [11].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : ", the first-order methods proposed in [11].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : "Other methods, such as those in [15, 17, 25] suffer heavier computation cost per iteration, and are thus omitted from comparison.",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "Other methods, such as those in [15, 17, 25] suffer heavier computation cost per iteration, and are thus omitted from comparison.",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 24,
      "context" : "Other methods, such as those in [15, 17, 25] suffer heavier computation cost per iteration, and are thus omitted from comparison.",
      "startOffset" : 32,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "[11] aim to solve either the primal or dual form of problem (2), using primal ADMM (PADMM), a variant of primal ADMM (PADMM2), a variant of dual ADMM (DADMM2), and a dual proximal point algorithm (DPPA).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "We also implemented the accelerated projected gradient with singular value thresholding (APG-SVT) to solve (8) by adopting the FISTA [2] scheme.",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "In this part we apply our formulation and algorithm to another application, spectral compressed sensing (SCS), a technique that has by now been widely used in digital signal processing applications [6, 9, 29].",
      "startOffset" : 198,
      "endOffset" : 208
    }, {
      "referenceID" : 8,
      "context" : "In this part we apply our formulation and algorithm to another application, spectral compressed sensing (SCS), a technique that has by now been widely used in digital signal processing applications [6, 9, 29].",
      "startOffset" : 198,
      "endOffset" : 208
    }, {
      "referenceID" : 28,
      "context" : "In this part we apply our formulation and algorithm to another application, spectral compressed sensing (SCS), a technique that has by now been widely used in digital signal processing applications [6, 9, 29].",
      "startOffset" : 198,
      "endOffset" : 208
    }, {
      "referenceID" : 13,
      "context" : "Inspired by the conventional matrix pencil method [14] for estimating the frequencies of sinusoidal signals or complex sinusoidal (damped) signals, the authors in [6] propose to arrange the observed data into a 2-fold Hankel matrix whose rank is bounded above by r, and formulate the 2-D spectral compressed sensing problem into a rank minimization problem with respect to the 2-fold Hankel structure.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 5,
      "context" : "Inspired by the conventional matrix pencil method [14] for estimating the frequencies of sinusoidal signals or complex sinusoidal (damped) signals, the authors in [6] propose to arrange the observed data into a 2-fold Hankel matrix whose rank is bounded above by r, and formulate the 2-D spectral compressed sensing problem into a rank minimization problem with respect to the 2-fold Hankel structure.",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "The trace norm heuristic for rank (·) is again used here, as it is proved by [5] to be an exact method for matrix completion provided that the number of observed entries satisfies the corresponding information theoretic bound.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "the experiment setup in [6], we generate a ground truth data matrix Y 2 R101⇥101 through a superposition of r = 6 2-D sinusoids, randomly reveal 20% of the entries, and add i.",
      "startOffset" : 24,
      "endOffset" : 27
    } ],
    "year" : 2014,
    "abstractText" : "We study the problem of finding structured low-rank matrices using nuclear norm regularization where the structure is encoded by a linear map. In contrast to most known approaches for linearly structured rank minimization, we do not (a) use the full SVD; nor (b) resort to augmented Lagrangian techniques; nor (c) solve linear systems per iteration. Instead, we formulate the problem differently so that it is amenable to a generalized conditional gradient method, which results in a practical improvement with low per iteration computational cost. Numerical results show that our approach significantly outperforms state-of-the-art competitors in terms of running time, while effectively recovering low rank solutions in stochastic system realization and spectral compressed sensing problems.",
    "creator" : null
  }
}