{"title": "Blossom Tree Graphical Models", "abstract": "We combine the ideas behind trees and Gaussian graphical models to form a new nonparametric family of graphical models. Our approach is to attach nonparanormal blossoms\", with arbitrary graphs, to a collection of nonparametric trees. The tree edges are chosen to connect variables that most violate joint Gaussianity. The non-tree edges are partitioned into disjoint groups, and assigned to tree nodes using a nonparametric partial correlation statistic. A nonparanormal blossom is then \"grown\" for each group using established methods based on the graphical lasso. The result is a factorization with respect to the union of the tree branches and blossoms, defining a high-dimensional joint density that can be efficiently estimated and evaluated on test points. Theoretical properties and experiments with simulated and real data demonstrate the effectiveness of blossom trees.\"", "id": "da8ce53cf0240070ce6c69c48cd588ee", "authors": ["Zhe Liu", "John Lafferty"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Authors propose a method of estimating a graphical model for continuous data that blends the following three, established ideas: 1) assume the data follows a multivariate Gaussian and estimate using the graphical lasso; 2) do not assume the data follows a multivariate Gaussian and instead use a Gaussian copula, the nonparanormal, to allow arbitrary single variable marginals; or 3) assume a specific tree-structured factorization and model arbitrary bivariate marginals along the tree structure.\n\nThe proposed method introduces the blossom tree, which is a specific factorization of the model into a collection of densely connected blossom components that are connected by a specific set of tree edges. In particular, each blossom is connected (via a pedicel node) to at most one tree edge. The blossom components are modeled as sparse multivariate Gaussians (or using the non-paranormal copula) and the tree edges are modeled as arbitrary bivariate distributions with single variable marginals that are consistent with the marginal of any blossom pedicel to which they are attached.\n\nThe main idea of this paper, the factorization of a high-dimensional graphical model via the blossom tree structure appears to be unique and the ability to flexibly model components of a high-dimensional graphical model as non-Gaussian seems appealing. However, I\u2019m not convinced that the blossom-tree factorization adds any real value. In other words, why should I do all of this work to learn a blossom-tree model, over a non-paranomral model, a sparse multivariate gaussian or a forest-tree model? The authors only considered simulated data, constructed using a process favorable for their proposed factorization.\n\nI found the result in Theorem 3.1 a bit confusing. It tells us about the quality of estimating the negentropy. Why is this useful for the joint estimation problem? What does it tell us about the estimates produced using the blossom-tree factorization?\n\nIn addition, the proposed method of learning the blossom-tree structure seems a bit weak. Learning a sequence of blossom tree models, one non-blossom edge at a time, seems unreasonable for any moderately sized problem. In addition, please be specific about the complexity of the proposed method, relative to learning using graphical lasso, non paranormal and forest-trees. \n\nFinally, I think the experiments section could use a real boost. Running on at least one real-world data set would help convince the reader that there is real value in this factorization.\n\nDetailed comments:\n- I think that calling this a blossom tree is a poor choice. The word blossom was introduced by Edmonds for the matching problem and refers to an odd-sized set of vertices. \n- In the experiments, why are the \u20180\u2019 trunks not equal to the graphical lasso? If you don\u2019t add any trunks isn\u2019t there just one blossom, modeled as a multivariate gaussian?\n- Runtimes for the different methods should also be reported. \n- Line 60: The statement \u201cmaintain tractable inference without placing limitations on the independence graph\u201d is a bit misleading. They do encourage the graph to be sparse; they are just not structural like, say a graph laplacian.  Interesting article that is well written and appears to be introduce a novel factorization for graphical models of continuous data. However, the lack of experimentation on real-world data failed to convince me of the utility of the blossom tree factorization, while the proposed greed blossom tree construction method seems to intractable to be useful.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes blending several methods for statistically modeling \nmultivariable continuous data into a method called blossom tree. The different methods include forest models and the graphical lasso. The resulting type of model seems novel and quite flexible and the proposed algorithm may be able to efficiently estimate it from high-dimensional non-Gaussian data. The paper is mostly clear but not easy to follow, in part because the way the model is constructed seems a bit ad-hoc, in part because many results are left for the supplementary material. It will be helpful to make the paper more principled and self-contained. Experimental results are shown only for simulated data that are constructed with the proposed model in mind. It would be useful to see a stronger experimental section including a substantial result(s) on a real dataset with e.g. glasso benchmark.  Mostly clearly written paper presenting a new and potentially useful method to flexibly model non-Gaussian multidimensional continuous data. The experimental demonstration is lacking.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper continues a line of work in graphical models to go beyond\nGaussian modeling of real-valued attributes. The apparent novelty is\nthe combination of a previous algorithm for non-parametric forest\ndensity estimation, which rests on a non-parametric generalization of\nthe classical Chow-Liu algorithm for learning tree graphical models, with the standard\ngraphical LASSO (glasso), resulting on the authors call a \"blossom tree\ngraphical model.\" The paper states a type of statistical-consistency\n(theoretical) result, and a few experiments.\n\n* On Section 3: Can the authors' comment on how much the stated result\nsays about *machine learning* properties (i.e., PAC or\nlarge-deviation, as opposed to data-size-asymptotic bounds)? Can the\nauthors say anything at all about the properties of the constant N?\nWhat would be an expression of a lower or upper bound, even if exact?\nJust like the typical asymptotic convergence as the number of samples\ngoes to infinity (e.g., CLT), it is hard to know when the\nlarge-deviation bounds of the kind provided in the paper start being\nvalid. Without such information regarding N and the exact number\nof samples n, while it may be an interesting and useful statistical\nresult, it does not appear very useful to machine learning, IMHO.\n\nHence, I would have Section 3 moved fully to the supplementary\nmaterial and replace it with more experimental evaluations. For example,\nI would have moved the sections Supplementary Simulations 1 and 2 into\nthe main body of the paper. But even then, a slightly more thorough\nevaluation would be useful such as experimenting by changing the class\nor form of the underlying ground-truth density, and the class of\nunderlying graphs, whenever applicable, the number of samples, and the\nnumber of attributes/variables. \n\n* Given the nature of the paper, a reference to the Chow-Liu \nalgorithm, from 1968, seems warranted, even if only in passing. \n\nC. K. Chow, and C. N. Liu, \"Approximating Discrete Probability\nDistributions with Dependence Trees,\" IEEE Transactions of Information\nTheory, vol. IT-14, no. 3, May 1968\n\nDespite the different statistical setups/models (i.e., discrete\nvs. continuous), and the fact that the work in the submitted paper\ngoes beyond simple trees/forests/blossom representations, it is\nundeniable that the core of the underlying approach has its roots on\nChow-Liu's work.\n\n* It'd have been nice to see experiments on real-world data such as\nthose on microarray data that Liu et al (JMLR, 2011) performed. Is\nthe problem public access to the data? What about other datasets\n(e.g., fMRI)?\n\n* I am curious as the core distinctions among the following work and\nforest density estimation, which serves as the foundation for the submission?\n\n- Marwan Mattar, and Erik Learned-Miller. Improved generative models for\ncontinuous image features through tree-structured non-parametric\ndistributions. UMass Amherst Technical Report 06-57, 10 pages, 2006.\n\n- Mihai Datcu, Farid Melgani, Andrea Piardi, and Sebastiano\nB. Serpico, \"Multisource Data Classification With Dependence Trees,\"\nIEEE Transactions on Geoscience and Remote Sensing, vol. 40, no. 3,\npp. 609-617, March 2002. \n\nI am interested in directly ML-relevant, as opposed to statistical,\ndistinctions.\n\n* It has been quite a long time since Chow and Liu's work in 1968. Hence,\nI was originally skeptical that no one had actually tried anything like what the\nauthors' propose. If it exists, I have not been able to find it. So,\nwhile somewhat surprising, I must give the authors their due for the\nnovelty and originality of their proposed approach... Now, I may\nchange my mind later, should I find a reference :) ... All kidding aside,\neven if I find an old reference, it does not take much away from the\nauthors' originality/novelty, because no one in recent years have\nproposed the technique, despite the intense attention to the problem\nof modeling continuous/real-valued attributes going back a decade, at least!\n\n A nice, clean, interesting, and surprisingly novel approach tomodeling multiple real-valued attributes/random variables. It can benefitfrom further experimental/empirical evaluations.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
