{"title": "Efficient Structured Matrix Rank Minimization", "abstract": "We study the problem of finding structured low-rank matrices using nuclear norm regularization where the structure is encoded by a linear map. In contrast to most known approaches for linearly structured rank minimization, we do not (a) use the full SVD; nor (b) resort to augmented Lagrangian techniques; nor (c) solve linear systems per iteration. Instead, we formulate the problem differently so that it is amenable to a generalized conditional gradient method, which results in a practical improvement with low per iteration computational cost. Numerical results show that our approach significantly outperforms state-of-the-art competitors in terms of running time, while effectively recovering low rank solutions in stochastic system realization and spectral compressed sensing problems.", "id": "912d2b1c7b2826caf99687388d2e8f7c", "authors": ["Adams Wei Yu", "Wanli Ma", "Yaoliang Yu", "Jaime Carbonell", "Suvrit Sra"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Summary: The paper describes an efficient optimization approach to find structured low-rank matrices.\nThe structure is encoded by a linear map and enforcing low rank is achieved by adding to the cost function the nuclear norm of the structured matrix.\nThe cost function is optimized with a generalized conditional gradient algorithm.\nBy using a factorization of the large structured matrix the optimization is accelerated further. This comes at the cost of having to solve a non-convex auxiliary problem, optimizing the two factor matrices. But this can be solved efficiently following ideas of Zhang et al. (NIPS2012). \nA theoretical analysis indicates that the whole algorithm has a remarkably low complexity and for several sample problems it is shown that this approach is at least an order of magnitude faster than the next best competitor. It also scales well to very large problems; speed comparisons are shown for matrix sizes up to over 300M elements. An advantage is that the factor matrices start from low ranks while alternative methods tend to start from full rank.\n\nQuality: A thorough theoretical analysis is provided for computational complexity and convergence rate that show significant improvements over alternative methods. This is also confirmed in experimental results where the method outperforms consistently the competing methods.\n\nClarity: Overall the paper is well written, but the material is complex and the reader has to go through the supplementary file as well as through some of the cited papers to appreciate what is happening. \n\nOriginality: The method described here combines in a creative way several ideas from previous algorithms to come up with a new solution that is surprisingly efficient.\n\nSignificance: The problem that is addressed here is of significance for various applications and the results show convincing improvements over state of the art. \n The paper describes an efficient optimization approach to find structured low-rank matrices. A theoretical analysis indicates considerable improvements over alternative methods and experimental results show an order of magnitude improvement in speed over competing techniques.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This work considers a seemingly new, and faster, way to deal\n\twith nuclear-norm regularised estimation of low-rank matrices.\n\tThe work is well motivated. The authors take time to carefully\n\texplain the main contributions and how these fit in with the\n\texisting literature. \n\n\tThe authors perform a reasonably simple mathematical trick to\n\tmanipulate the form of the problem. The main work appears to be\n\tin the exploitation of this trick using their knowledge of the\n\tconditional gradient algorithm. \n  A somewhat novel algrotihm is proposed to estimate low-rank matrices. The algorithm offers a speed-up compared to existing methods.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "** I have read the authors\u2019 response and they have provided satisfactory comments addressing my major concerns. I discuss some of these below (denoting added comments with **). I have read the other reviews and my rating of the paper hasn\u2019t changed; I think it is a strong paper and should be accepted. I do not feel that the theoretical or experimental contribution pushes it into the oral/spotlight category.**\n\nThe paper addresses the problem of finding structured low rank matrices for the case when the structure is encoded using a linear map and nuclear norm regularization is employed. The authors propose a modified formulation of the problem which makes it amenable to application of the generalized conditional gradient method. They provide an algorithm that incorporates other methods that accelerate convergence and show that it converges. Empirical studies suggest that the algorithm is significantly faster than its competitors and can scale to much larger problems. \n\nStrengths:\n\n(1) The paper addresses an optimization problem that lies at the core of multiple machine learning and signal processing tasks and the authors provide a novel procedure that can identify a suitable solution much faster than state-of-the-art approaches. The empirical studies demonstrate the efficacy of the approach for two useful application problems. \n\nWeaknesses:\n\n(1) The authors achieve the significant reduction in computational cost by modifying the objective function so that it is no longer directly penalizes the rank of the structured matrix. The authors make three reasonable arguments for doing so shortly after stating the objective function in (7). The first two argue that the linear constraint could be satisfied asymptotically through a homotopy scheme or exactly through a projection, but the paper doesn\u2019t explore either of these options, either empirically or theoretically. The theoretical result really addresses the convergence of the algorithm in terms of minimizing the modified objective function; it doesn\u2019t provide a sense of the discrepancy between the identified solution and the more natural original formulation. Graphs 2(c) and 2(f) show some slightly strange behaviour in the rank of the solution as it converges. This behaviour is not discussed at all and it is not clear when the algorithm should be terminated to identify an appropriate solution. \n\n** The authors have argued that \u201cany formulation that achieves this goal should be sufficient for the application\u201d. I think this is a reasonable argument, but at the moment the paper is presented as though (7) is a relaxation/approximation of (5) \u2013 and is effectively trying to find the same solution. I think it would be useful to add a sentence or two making it clear that this is an alternative utility function that targets the same overall objective (low-rank model). **\n** With regard to the experimental results and the behaviour of the algorithm, it would be good to add some extra discussion either in the main paper or in the supplementary material expanding on what the authors have provided in their response\u201d **\n\nComments:\n\nWhy does Theorem 1 cite [28]? Although there is a similar theorem in [28], with a similar proof approach, this doesn\u2019t seem to be taken directly from there? What does ([28]) mean?\n\n** The authors acknowledge that the citation should appear in the proof, after a statement that the proof technique is inspired by the method in [28]. **\n The paper addresses an optimization problem that lies at the core of multiple machine learning and signal processing tasks and the authors provide a novel procedure that can identify a suitable solution much faster than state-of-the-art approaches. This is achieved by modifying the objective function and the paper would be improved if it provided more in-depth discussion and analysis of the impact of this modification.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
