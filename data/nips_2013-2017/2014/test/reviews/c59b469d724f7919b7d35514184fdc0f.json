{"title": "A Latent Source Model for Online Collaborative Filtering", "abstract": "", "id": "c59b469d724f7919b7d35514184fdc0f", "authors": ["Guy Bresler", "George H. Chen", "Devavrat Shah"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Summary\n\nThis paper proposes a recommendation algorithm based on reinforcement learning, which pursues both exploration and exploitation in an online setting. Also, the authors introduce a theoretical framework analyzing the proposed algorithm.\nBasic assumptions in this paper are 1) the users are (clearly) grouped into k typical user types, and 2) likable items are clearly separated.\nUnder these assumptions, this paper proved that the proposed algorithm performs well.\n\n\nContribution\n\nAs the authors claimed, there has been a lot of online recommendation systems in literature, but there was few theoretical analysis about them.\nAlthough the proposed algorithm is quite simple, it is indeed meaningful to have a theoretical frame to anlayze a recommendation system in an online setting.\n\n\nSome issues\n\n1. It is well known that users (or items) are grouped into several categories. This paper makes use of this fact (or hypothesis) for analyzing the algorithm, not for the algorithm itself. How can we make use of this in recommendation task itself?\n\n2. When users (or items) are clustered, we usually think of a top-down clustering, without allowing for each user or item to be a member of more than one clusters. A recent paper below finds a similar set of users or items in bottom-up fashion. How can we apply the proposed framework when clusters can overlap?\n J. Lee, S. Kim, G. Lebanon, Y. Singer. Local Low-Rank Matrix Approximation, ICML 2013.\n\n3. In algorithm 1, alpha is defined as some value in (0, 4/7]. How this particular value 4/7 was decided?\n\n4. The definition of \\epsilon_R(n) and \\epsilon_J(t) make sense as they are, but it seems no evidence that this actually works better than constants for those. Can you prove or show by experiment this? Also, did you try to find the best \\alpha? The performance may be affected by the choice of \\alpha, so including the experimental result varying \\alpha would be useful.\n\n5. (minor) Three lines in Figure 1(b) are not distinguishable when printed by black-and-white printers. Can you change the shape of each line?\n\n6. (minor) Line 413: [4] study --> [4] studies (or studied)\n\n7. (minor) Line 419: Another related work is by [12], who study --> Another related work is [12], which studied This paper proposes a simple online recommendation algorithm and a theoretical analysis for the proposed method. I believe this paper is worth to be published in NIPS.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "** COMMENTS AFTER AUTHOR FEEDBACK **\n\nThis is a high quality paper.\n\nMinor comment. Not addressed in the rebuttal is\n\n\"1. The definition of neighbourhood is very unclear, because of the use of \u201cjointly\u201d. Line 178 ... contradicts Lemma 8\u2019s proof (Line 576).\"\n\nwhich might be good to clarify in the paper.\n\n--------------------------------------------------\nDisclaimer: the reviewer is not an expert in Learning Theory, but in Collaborative Filtering. In assessing the paper\u2019s theoretical significance, he/she will submit to the viewpoint of expert Learning Theorists. \n\nMAIN IDEAS OF THE PAPER:\n\nWe know that collaborative filtering works in practice: users who buy A also tend to buy B. The paper takes a bold stab at providing theoretical guarantees on how well it works, albeit in a restricted and slightly hypothetical setting. The hypothetical setting is a simplified sketch of reality, necessary to obtain a theoretically analysable handle on the problem.\n\nThe paper analyses a very simple collaborative filtering algorithm, where collaborative filtering relies on looking for a set of neighbouring users with the most similar like/dislike patters (within some cosine distance) and using their extra like/dislike votes to score items to provide next item to a user. The goal is to maximize the expected number of liked items presented to each user within a fixed time window of 1 to T recommendations.\n\nThe paper shows that the users are \u201clearnable\u201d after around log(km) = log(number of user types) + log(number of items) rounds of recommendations (up to log^{7/3}(km), depending on a parameter choice). Essentially in time logarithmic in the number of items, roughly optimal performance can be expected. The whole set-up relies on some conditions, like having enough users.\n\nThe result relies on two additional steps in the collaborative filtering algorithm: (1) random exploration, and (2) structured exploration. Both are necessary, (1) to interrogate potential future items for recommendation, and (2) to have enough of an overlap in like/dislike items to learn about the similarity between users.\n\nThe experimental results try to simulate a real scenario by streaming a curated subset of the Netflix prize data to the data set\u2019s users. In the simulation, the average cumulative reward is higher than those of two existing methods (PAF and DM, incidentally neither of which has ever been used in practice, to my knowledge and those of peers).\n\nRELATIONSHIP TO PREVIOUS WORK:\n\nThe paper is set apart by using two exploration steps, one of which learns the similarity between users (setting it apart from standard multi-arm bandits). Furthermore, no arm can be pulled more than once.\n\nQUALITY and CLARITY:\n\nThe paper is very well explained, and beautifully written.\n\nI worked through most of the proofs in the supplementary material, and they seem technically correct.\n\nI would like to comment on some textual issues for the benefit of the authors:\n\n1. The definition of neighbourhood is very unclear, because of the use of \u201cjointly\u201d. Line 178 suggests that *only* user-item indexes that were recommended in the joint exploration step in the algorithm are eligible for defining neighbours (and hence cosine similarity). Why are other overlapping items not considered in defining the neighbourhood? The confusion arises as Line 178 is intentional, but if we read Lemma 8\u2019s proof (Line 576), it contradicts Line 178, as now jointly means the items that two users rated in common, and *not* the subset of that set, which was tagged with \u201cjointly\u201d in your Algorithm (see Line 157).\n\n2. Sec 2, Lines 103 and others. The paper exclaims the point that one should not recommend a consumed item to a user again (especially a disliked one). This is rather obvious in the recommendations community, and I assume you drive home the point to distinguish your work from a standard multi-arm bandit setting?\n\n3. Typos.\nLine 165 should refer to \\tilde{p}_{ui} and not index j?\nLine 524 (Lemma 5\u2019s proof). \\bar{Z}_s = 1 / s^\\alpha \u2013 Z_s would give a zero mean.\nLine 610 [unnecessary inequality in proof]\nLine 634 user u has is...\n\nORIGINALITY and SIGNIFICANCE:\n\nThis paper is the first attempt (to my knowledge) to characterize learning rates in a (pseudo-) real recommendations setting.\n\nThe (new) practical take-home message is that joint exploration is required for the algorithm (and maybe more generally?) to achieve optimality in time logarithmic in the number of items.\n\nPRACTICAL WEAKNESSES TO BE AWARE OF:\n\nTo achieve the main results (that is, how long and how much should one explore and exploit before attaining optimal performance) the proofs and algorithm rely on a number of assumptions. These assumptions already give useful insights (like how many users one needs relative to an item catalogue size and tolerance), but from a practitioner\u2019s viewpoint, they break down on many levels.\n\n1. The assumption that users belong to each type with odds 1/k is invalid. In reality, these are typically according some power law.\n\n2. Random (and joint) exploration, whilst theoretically appealing (and necessary) is a dangerous game, as it can hurt user retention in systems like those described in the Introduction. The algorithm doesn\u2019t make provision for a user never returning because of it.\n\n3. From calibration plots in real systems, we know that condition A1 does not hold in practice. There are pairs for which the odds are really, well, coinflip!\n\nThese are not criticisms, but differences to be aware of.\n A beautifully written paper that shows that the users are \u201clearnable\u201d after around log(number of user types) + log(number of items) rounds of recommendations, given a simple algorithm. The result proof relies on a number of conditions to be met, which are essentially a \u201csimplification of the real world\u201d, and tells us that structured (joint) exploration of users is a requirement.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Quality and originality: The focus on the online setting in recommendation is welcome \u2014 this is a genuinely challenging issue in domains like movies, books and music \u2014 and I find the introductory material to be of high quality. The performance guarantees and proofs seem correct to me, but I\u2019m not at all an expert in that area. The Collaborative-Greedy model itself is less convincing. Though it outperforms two other models, the datasets used are so small that it\u2019s hard to get an intuition as to how it would work for more realistic datasets (Netflix prize, Million Song dataset). The authors show awareness of this pint in observing that a full validation would require an actual interactive online recommendation system. Also, the point of the paper seemed more to be a theoretical exploration of possibilities, and the datasets seem large enough to support the authors\u2019 claims. I find the final claim of the paper to be particularly thoughtful: that two types of exploration are useful for learning mixed distributions in an active learning setting.\n\nClarity and Significance: This paper was pretty far from my area of research. I found the paper to be \u201clocally clear\u201d (i.e. I think I understood each individual paragraph) but I fear I\u2019m missing some of the bigger picture.  This paper explores the online setting in collaborative filtering, presents a model and learning problem for online recommendation, provides performance guarantees (with proofs) and discusses some experimental results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
