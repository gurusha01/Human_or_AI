{"title": "Global Belief Recursive Neural Networks", "abstract": "Recursive Neural Networks have recently obtained state of the art performance on several natural language processing tasks. However, because of their feedforward architecture they cannot correctly predict phrase or word labels that are determined by context. This is a problem in tasks such as aspect-specific sentiment classification which tries to, for instance, predict that the word Android is positive in the sentence Android beats iOS. We introduce global belief recursive neural networks (GB-RNNs) which are based on the idea of extending purely feedforward neural networks to include one feedbackward step during inference. This allows phrase level predictions and representations to give feedback to words. We show the effectiveness of this model on the task of contextual sentiment analysis. We also show that dropout can improve RNN training and that a combination of unsupervised and supervised word vector representations performs better than either alone. The feedbackward step improves F1 performance by 3% over the standard RNN on this task, obtains state-of-the-art performance on the SemEval 2013 challenge and can accurately predict the sentiment of specific entities.", "id": "1415db70fe9ddb119e23e9b2808cde38", "authors": ["Romain Paulus", "Richard Socher", "Christopher D. Manning"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper introduces \"belief propagation recursive neural\n\tnetworks\". This kind of networks extends the \"feedforward\" (bottom-up)\n\trecursive networks with a backward (top-down) step during\n\tinference. This allows phrase level predictions and embeddings to\n\tgive feedback to word embeddings and labels. \n \n\tThis paper is overall well written. The model is well\n\tmotivated and described. This extends the idea of the\n\tBidirectional Recursive Neural Networks introduced by Irsoy\n\tand Cardie 2008 (note that the reference to that paper is\n\tincomplete). For instance one contribution is the introduction of hybrid word vectors. \n Maybe this is the most important difference with previous work ? \n Moreover, experimental comparisons are not completely fair: were the RNN and B-RNN also use hybrid vectors ? how the \"best\" models are tuned and selected ? How the single best is selected ? It is worth noticing that the others methods do not use an ensemble. These points must be clarified. \n\n The term \"belief propagation\" is misleading and\n\tmaybe the term forward-backward could be well suited. \n\n\tSection 3.5 (Training) is a little bit too short and could\n\tprovide more details. For instance, I guess that the training\n\tuse the back-propagation through structure algorithm (the paper\n\tof Goller 1996 could be cited). This algorithm implies that\n\tthe recursive model is unfolded. In this case, this yields a\n\tvery deep network to \"reach\" the word embedding part that is\n\tupdated. If I understand correctly, I think that the authors\n\tcould provide more details on this very important step. \n \n\tSection 3.6 is a little bit confused and the difference\n\tbetween this model and the wok of Irsoy and Cardie must be\n\tclarified. \n\n\tSection 4.1 must be improved to be understood by readers\n\toutside of the NLP community. In the task 2 of Semeval 2013\n\tthere are 2 subtasks: contextual and message polarity. I\n\tguess that you address the contextual polarity task. With a\n\tbetter presentation of the task, one can even understand the\n\tparser constraints.\n \n\tMaybe, sections 4.3 and 4.4 could be merged. \n\n\n This paper is overall well written and describes an interesting variant of the Bidirectional recursive model.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In NLP, recursive neural networks (RNNs) have been used to produce representations snippets of text by recursively combining pairs of representations of words/shorter snippets. The process starts with the representation of words and proceeds up a pre-specified parse tree until the representation of the entire snippet is obtained at the root node. The unidirectional nature of this process, however, does not allow the information to be propagated down the tree from the larger to the smaller contexts. This paper introduces a bidirectional extension of RNNs in which the upward pass through the tree is followed by a downward pass, augmenting the representations of all the nodes with the information from larger contexts. The resulting system achieves state-of-the-art performance on Task 2 of SemEval 2012.\n\nThe extension of the established RNN-based approach proposed in the paper is simple, elegant and effective. Though the high-level idea is quite similar to the one from [18], the computation performed on the downward pass in the paper is different and the resulting performance appears to be superior.\n\nThe method is well motivated and clearly presented and the paper is nicely written in general. The experimental results appear to be excellent, but I have some concerns about the experimental setup. Was the result for the alternative bidirectional RNN model from [18] reported in Table 2 obtained using dropout and the additional training data? Were the hybrid vectors also used in that case? Also, while the ensemble result is impressive, it does not really belong in a table comparing the performance of individual models.\n\nWhat does the \"widely used mix of both\" refer to in Section 4.4? A well executed paper based on a simple and elegant idea.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes using both the encoding and decoding phrase vectors of a recursive neural network auto-encoder in a sentiment classification task. The authors refer to this as belief propagation, although it is not an instance of this inference algorithm and as such this name serves to confuse the reader. In general this is a well written paper, however the motivation for the proposed model is vague and lacks a clear theoretical justification.\n\nThe key weakness of this paper is the evaluation. There is no reason to present an ensemble result in the context of this evaluation, none of the benchmarks are ensembles and this obfuscates the result. The exact composition of the models compared is also not clear. Key questions that should be clarified are: how were the models tuned and the ensemble/best models selected? How were the benchmarks in Table 2 tuned? Were they using the same augmented data as the BP-RNN? Were the RNN and Bidirectional-RNN also using hybrid word vectors?\nIn the absence of this information I would guess that the second entry in Table 3 is the most comparable to the benchmarks in Tables 1&2 when assessing the BP-RNN architecture.\n\nMinor points:\n- I am surprised that an off the shelf parser was used to parse tweets. I would be interested to know how accurate it is on this data.\n- the Irsoy and Cardie citation is incomplete (as well as a number of others).\n This paper presents a recursive neural network model for classifying phrasal sentiment in context. The results appear reasonable, but the model lacks a strong motivation and the experimental methodology is not entirely clear.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
