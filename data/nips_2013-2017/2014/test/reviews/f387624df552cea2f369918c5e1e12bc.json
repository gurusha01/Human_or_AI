{"title": "Best-Arm Identification in Linear Bandits", "abstract": "", "id": "f387624df552cea2f369918c5e1e12bc", "authors": ["Marta Soare", "Alessandro Lazaric", "Remi Munos"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "Problem definition:- The paper studies best-arm identification in linear bandits. In this problem there is a hidden unknown parameter \\theta*\\in R^d and a finite set of arms X\\subseteq R^d. When an arm is pulled you observe a reward x^T\\theta*+epsilon where epsilon is a zero mean i.i.d noise with bounded range. The goal is to identify argmax_{x\\in X} x^T\\theta* with the least number of samples. \n\n\nResults: The paper characterizes the sample complexity of static and dynamic allocation strategies to identify the best arm. \n\nQuality of the paper:- The techniques appear reasonably sound. In the experiments section it would also be nice to compare to the existing algorithms for linear bandits. (i.e run the existing algorithm for linear bandit till \\hat{S}\\subseteq C(x) and then count the number of steps to achieve this). The reason this comparison is important is that the paper mentions at the beginning of the paper that unlike MAB, the strategies to minimize sample complexity of best arm identification of LB could be different from regret minimization. \n\nClarity of the paper:- The paper looks reasonably well written. \n\nSignificance:- The paper proposes and solves a reasonably general problem. The paper does not necessarily do a good job mentioning applications, but I think it looks like an abstract problem which should have good applications. \n The paper studies best-arm identification in linear bandits. It characterizes the sample complexity of static and dynamic allocation strategies to identify the best arm.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper is one of the first to study best arm identification in bandit models with correlated arms. The authors consider the fixed-confidence setting. A new complexity notion, that takes into account the correlation between arms, is proposed, and algorithms that almost attain this complexity are proposed. Some of them are based on a static allocation strategy (i.e. that does not depend on the rewards collected), and one of them is adaptive. \n\nFirst, this paper is not exactly the first on best arm identification in linear bandit problem. Indeed,\n\n* Hoffmann, Shahriari, De Freitas, On correlation and budget constraints in model-based bandit optimization with application to automatic machine learning (AISTATS 2014)\n\nconsider a linear bandit model with gaussian noise. However, they study the fixed-budget setting, and the complexity term featured in their work involves a sum over all arms of a squared gap (that is, it does not really take into account the correlations induced by the linear structured).\n\nThe theoretical results of the paper rely on Propositions 1 and 2 given in Section 2. The statement of these results are a bit imprecise, and it should be mentionned whether n is fixed or not. Indeed, the result extracted from [1] given as Proposition 2 should be \n\nP(\\forall n\\in\\N, (2) holds) \\geq 1-\\delta,\n \nwhereas Proposition 1 should be written as: for every n, for every fixed sequence x_n,\n\nP( (1) holds) \\geq 1- \\delta. \n\nThis makes a difference, as an extra union bound over n might be required when using Proposition 1, as even when the sampling rule is static, the sampling rule is adaptive. Also a precise reference for Proposition 1 should be given. \n\nThe terminology should be made more precise. In best arm identification, in the fixed-confidence setting, algorithms consists in two elements: a sampling rule and a stopping rule (and also a recommendation rule, but quite naturally, when stopping you always choose the best arm as if the current least-square estimate were the true parameter). The \"static and adaptive strategies\" mentioned in the paper relates only to the sampling strategy as the stopping strategy of the G-allocation strategy is adaptive. In the proof of Theorem 1, a union bound over n seems to be missing (see above comment on Proposition 1): you need the display of line 674 to hold for all n, hence the union bound will bring you a log(CK^2/t^2\\delta) for some constant C for example. Besides, in the statement of Theorem 1, 'beta-approximation' is not defined (neither in the text nor in Lemma 5 and 6).\n\nAbout the oracle: Section 3 is a bit confusing as when you have access to the parameter \\theta^*, there is nothing left to do. I understand that you assume that the algorithms are based on confidence intervals and stop when one of the confidence intervals is contained on the C(x). You then study which shape these confidence intervals should have in order to be as quickly as possible included in C(x^*). Still the definition of the oracle appears quite imprecise (again if \\theta^* was known the problem would be solved). Usually the definition of a complexity in best arm identification comes from a lower bound on the number of sample used, for any $\\delta$-PAC algorithm: you don't really have such a result here and I don't see what justifies the claim \"We characterize the complexity of the problem\" in the introduction. \n\nThe algorithms are built on allocation strategies coming from experimental design theory (many of the results of Appendix B are adapted from [16]) which are hard to implement and must be approximated. It is not clear what is the overall numerical complexity of the proposed algorithm and how it scales with the different parameters. \n\nIn the numerical experiments section, it is always frustrating to see comparison only between the methods introduced in the paper. As such, this section is not very informative. In particular, an obvious benchmark would have been to compare to an algorithm designed for best arm identification in classic (unstructured) bandit problems (LUCB, UGapE, etc.) This is particularly true as the problem chosen in the experiments is such that the arms are orthogonal, except for two of them; a situation which is very close to the classic d-armed bandit model. Comments about the choice of such a particular setting for the simulations would also be appreciable.\n\nMinor comments\n\n* There is an overlap in the notation: in the paragraph \"the setting\", \\Delta is defined in two ways, \\Delta(x) and \\Delta(y) (one beeing functions of vectors, the others of directions): it might be more convenient for the reader to distinguish explicitly these two notions.\n* In Footnote 1 \"used of all direction y\": should it be \"for all vector x\"?\n* Line 232, \"inthe\" (should be separated).\n* In the boxed environment of page 4 (Figure 2), it should be Eq. 24 in place of Eq. 22 on the second line.\n* Figures 2 and 3 (which actually define the algorithms) refer to Equations that only appear in the supplementary material; please make the main paper self-contained. \n This paper addresses the challenging issue of best-arm identification in linear bandit models. It is novel and uses some insights from the theory of optimal design. Still it does not really provide a generic notion of sample complexity and provides methods whose complexity appears to be high. The experimental results are also very limited.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper deals with the pure exploration variant of linear bandits. Here, the input consists of K vectors of d dimensions. The player is allowed to query vectors and obtain a sample from a distribution whose mean is a fixed linear function of the vector. The objective is to find the arm with maximum value w.h.p using a minimal number of queries.\n\nThe authors provide a solution to the problem based on the experiment design framework. They offer two static allocation strategies, meaning methods for querying the arms that are not adaptive to the outcome of the queries. These strategies provide a result analogous to that of the uniform strategy w.r.t the classic best arm identification problem. The authors also provide a dynamic strategy achieving a provable result that can outperform the static strategy in some scenarios. In addition to the proposed solution, a problem-dependent optimal strategy is given that matches that of the classic best arm identification problem.\n\nThe main body of the paper is in general well written. Some of the proofs in the appendix can use a few more details but they hold, as far as I checked. The studied problem is interesting and the paper provides an extensive theoretical study of it. However, I found the experiments a bit lacking. The result for the adaptive sampling technique is a bit weak in the sense that there is a separation between it and static methods only in a very specific case in which, among other requirements, finding the second best arm is a much easier task than finding the best arm. It is an interesting question whether the true performance of the adaptive setting is indeed superior to the static only in this restricted scenario or whether it is superior in other scenarios as well. Proving such a statement may prove to be difficult but there should be experiments aimed to answer this question. Instead the experiments are made w.r.t an example carefully built to match the conditions in which the adaptive algorithm is provably superior.\n\n\nComments / typos:\n\u2022\tProposition 1: should be \u201cw.p. 1-\\delta, for all x \\in \u2026\u201d rather than the other way\n\u2022\tLine 101: D^k \u2013 small k rather than large K\n\u2022\tEquations 3,4,5: It seems that the expression in the log be K/\\delta rather than K^2/\\delta. \n\u2022\tGiven the optimal strategy, it seems rather easy to prove a lower bound for the sample complexity\n\u2022\tIn Lemma 2, H_LB is bounded from above and below. It would be interesting to see for some example data sets how close it is to the upper and lower bound\n\u2022\tTheorem 1: N^G should be defined\n\u2022\tLine 269: The comment after theorem 2 is too vauge. It seems that there is some expression \u201cNG < ExpressionG < the given bound\u201d and \u201cNXY < ExpressionXY < the given bound\u201d that is missing, since the bounds as given are exactly the same\n\u2022\tLine 352: missing space\n\u2022\tSection B: Wouldn\u2019t it be easier to simply sample from the strategy provided by the convex program? Matrix chernoff boundes should help ensure (w.h.p) that the sample converged well\n\u2022\tProof of lemma 6: Theorem 2.6 of the mentioned paper does not necessarily apply for D-optimal design. Also, Some reference is needed for the claim that G and D optimal design are equivalent, even w.r.t approximations (as the D-optimal problem is only approximated by the greedy strategy)\n\u2022\tEquation 30: should be <, not \\leq\n\u2022\tLine 753: should be n_j > M^*\n\n\n Overall I recommend accepting the paper since the problem is interesting and the analysis is solid and extensive. However, the quality of the paper can potentially increase if the authors can manage to add more meaningful experiments, either on real data or on more plausible synthetic data.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
