{"title": "Submodular Attribute Selection for Action Recognition in Video", "abstract": "In real-world action recognition problems, low-level features cannot adequately characterize the rich spatial-temporal structures in action videos. In this work, we encode actions based on attributes that describes actions as high-level concepts: \\textit{e.g.}, jump forward and motion in the air. We base our analysis on two types of action attributes. One type of action attributes is generated by humans. The second type is data-driven attributes, which is learned from data using dictionary learning methods. Attribute-based representation may exhibit high variance due to noisy and redundant attributes. We propose a discriminative and compact attribute-based representation by selecting a subset of discriminative attributes from a large attribute set. Three attribute selection criteria are proposed and formulated as a submodular optimization problem. A greedy optimization algorithm is presented and guaranteed to be at least (1-1/e)-approximation to the optimum. Experimental results on the Olympic Sports and UCF101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches.", "id": "b056eb1587586b71e2da9acfe4fbd19e", "authors": ["Jingjing Zheng", "Zhuolin Jiang", "Rama Chellappa", "Jonathon P. Phillips"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper presents an algorithm for attribute\n (feature) selection, applied to activity recognition. The\n approach defines 3 criteria for choosing attributes for\n recognition: high discrimination between pairwise classes, similarity\n in the amount of discrimination over all pairs of classes, and\n coverage of all pairs of classes. The first two of these\n criteria are formulated using a random walk approach, the latter\n using a set-cover approach. Finally, a greedy optimization\n strategy is used to choose an attribute subset, starting from\n the empty set.\n\n This paper presents interesting research. The main contribution of\n the paper is the algorithm for feature selection. This\n algorithm seems novel, and is an interesting combination of\n random walk, set-cover, and greedy optimization. The\n experimental results are also good -- the method shows\n improvements over existing methods and baselines including no\n feature selection and alternative strategies. There are some\n missing details in the experiments but the results seem solid.\n\n My main concern about the paper is the motivation/intuition for\n the approach -- the pieces seem chosen to try to use submodular\n optimization. The discussion on lines 133-145 describes 3\n criteria. I wasn't clear on the motivation for criterion 2 --\n why can't some attributes be better for certain classes than\n others? Can't a final classifier choose combinations? It seems\n that this second criterion complicates the optimization, and\n necessitates the use of the proposed techniques.\n\n The resulting submodular optimization amounts to a\n greedy approach. It seems one could do similar greedy\n optimization for other fomulations, for example ones with\n criterion 2. It would be interesting to know whether this\n criterion is important empirically.\n\n Overall, this concern is not fundamental; I think the paper is\n very good, and should be accepted.\n\n Other comments:\n\n - I haven't gone over the details in supplementary material --\n the intuitive explanations in the paper for these seemed\n reasonable though.\n\n - Typo \"Combing\"\n\n - I didn't understand where the sparse coding (Eq. 5) is used in\n the paper. The experiments refer to KSVD for the DDA set.\n Sparse coding of attributes doesn't seem to be a key component\n of the paper, but if this is used it should be compared to\n k-means or alternatives. (Though there isn't space for this in\n a conference version.)\n\n - How are hyperparamters set in the experiments? Are they tuned\n by looking at the test (i.e. cross-validation) accuracies?\n\n - The experimental results seem better than previous work.\n Where are the numbers in Table 2 from? [20] is shown with a\n per-activity AP list, and mAP of 71.6. In [20], Fig. 10(b)\n shows mAP of 74.38%. This would seem to be better than what is\n in this paper.\n\n\n - The related work section is thorough, though the work of Fu et\n al., who also do similar attribute learning could be added:\n\nLearning multi-modal latent attributes\nY. Fu, T. Hospedales, T. Xiang and S. Gong\nIEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI 2013)\n\n\n\n Novel algorithm for attribute or feature selection.The motivation for part of the approach is not entirely clear,but the method is novel and seems effective empirically.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper addresses the problem of action recognition in video. The authors encode actions based on two types of attributes that describe the actions as high-level concepts: 1) Human Labeled Attributes (HLA) and 2) Data Driven Attributes (DDA). The authors propose a method to select a subset of the attributes that are discriminative and compact by combining submodular objectives that promote discrimination through the entropy rate-based attribute selection and coverage through the weighted maximum coverage-based attribute selection. The authors present experiments on the Olympic Sports and UCF101 datasets, two difficult datasets for action recognition and compare to a set of baselines and state-of-the-art approaches.\n\nThe paper is mostly well written, except a few sections. The problem is of significance as it is a standard problem in computer vision, and the experiments are performed on relevant and difficult datasets. The overall formulation of the attribute selection problem as a weighted maximum coverage problem and entropy rate maximization for a random walk is novel and interesting.\n\nThe core problem tackled in this paper seems to be the feature selection problem, as the authors try to select a subset of attribute classifiers that perform well. However, this a standard problem and there are many simple approaches for doing this, such as approaches with L1 regularization like the lasso, L1 regularized hinge loss, elastic net. I would encourage the authors to make direct comparisons to these simple approaches, instead of focusing only on other submodular selection methods. \n\nState-of-the-art results reported in other works are significantly higher due to the use of better dense feature vectors (90.2% on Olympic dataset as reported in [34], also other numbers Jiang et al. (ECCV 12), Jain et al (CVPR 13), Gaidon et al. (BMVC 12)are close to 80%). It would be interesting to see if the proposed attribute selection method leads to an improved performance, when combined with these dense features.\n\nFurther, this work proposes a general method for feature selection, and is not specific to action classes. Hence, to truly justify the potential of this work, I would encourage the authors to run experiments in other domains in vision, which use attributes as well (refer to [11] for other datasets).\n\nThere are a few mistakes in the equations, which should be corrected if the paper is selected for publication:\n- The equation for A_{d,l} in L129 is not clear. What is $u^d$ and $u_k^d$ in this equation? Are these supposed to be $\\mu$. What does mean and standard deviation of a \"class\" refer to in L127?\n- Similarly, Eq. 2, u_i should perhaps be \\mu_i\n- L161: Define T\n\n The overall formulation of attribute selection through the use of submodular objective functions which encourages the selection of discriminative attributes is novel and interesting. The authors address a standard feature selection problem, but restrict the experiments only to video action datasets, and do not compare to state-of-the-art results which use better features than the ones used in the paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The proposed paper would like to use attributes as a mid-level feature representation for doing action recognition (AR). Currently, the attributes used in AR are all human assigned without accounting for discriminative ability. This paper proposes a data-driven approach to \"discover\", or rather select a limited set of discriminative attributes from a larger set.\n\nQUALITY:\n\nThe formulation of the attribute selection criteria seems correct. What is missing is the motivation for keeping the relationships only pairwise and not either higher-order or one-vs-all? \n\nThe overall claims made in the introduction are very much grander than what the results can substantiate. In particular, the claims of human-labelled attributes (HLA) being \"arbitrary\", and have noise and or redundancy which may \"lead to degradation in action recognition performance\" does not give enough credit to HLAs. The attributes are typically selected their semantic meaning and plays a role once hierarchies of activities are taken into place, e.g. applying eye makeup and applying licpstick both have the same attributes as they both fall into the category of applying makeup. Furthermore, as the experimental results show in tables 2 & 3, with the exception of a few classes, the data-driven attributes (DDA) are actually much weaker than HLA, with some results being significantly worse. IT is only in the \"mixed\" case in which there are improvements to be found, suggesting that the DDA are at best complementary to the HLA, and alone are insufficient. As such, claims that one can find a more compact set of attributes are not substantiated. \n\nCLARITY:\n\nline 102 - 103: while the application area is interesting, more relevant to know *HOW* they have been used or applied in such applications, e.g. for optimization, classification, etc.\n\nline 129: what is u_k^d and u^d?\n\nHow is lambda in equation 4 determined and what impact does it have on the results?\n\nORIGINALITY: the paper states that they define a \"novel submodular objective function for attribute selection\", but how it is novel is not clear. Furthermore, please comment on what distinguishes this submodularity optimization from the other approaches listed in the related works section and especially [22] and [23] in which the method is compared. \n\nSIGNIFICANCE:\nThis paper would be of interest to those who work in action recognition and or attribute representations, both of which are sub-topics in computer vision.  An interesting approach for determining data-driven attribute representations. A good paper which needs only some refinement of writing in the introduction / motivation", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
