{"title": "Consistency of weighted majority votes", "abstract": "We revisit from a statistical learning perspective the classical decision-theoretic problem of weighted expert voting. In particular, we examine the consistency (both asymptotic and finitary) of the optimal Nitzan-Paroush weighted majority and related rules. In the case of known expert competence levels, we give sharp error estimates for the optimal rule. When the competence levels are unknown, they must be empirically estimated. We provide frequentist and Bayesian analyses for this situation. Some of our proof techniques are non-standard and may be of independent interest. The bounds we derive are nearly optimal, and several challenging open problems are posed. Experimental results are provided to illustrate the theory.", "id": "cd14821dab219ea06e2fd1a2df2e3582", "authors": ["Daniel Berend", "Aryeh Kontorovich"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper considers weighted majority algorithm and establishes consistency (error rate of the aggregator tending to zero) results under two settings: (1) when the competence level (risk of each expert) is known in advance and (2) when it is estimated. For case (2), frequentist and Bayesian methods for estimating the competence level are provided.\n\nFor case (1), consistency is established in terms of providing upper and lower bounds on the error rate of the aggregator, which involve standard calculations ( apart from the fact that upper bound is established by invoking a result by Kearns and Saul, instead of Hoeffding's inequality). For case (2) under the frequentist setting, an independent set of labeled inputs is used to estimate the competence level of each expert. Depending on the confidence level, two estimates (and corresponding analyses) are provided. Under Bayesian setting, the estimator proposed is more or less standard and the analysis is entirely left as an open problem.\n\nThe main problem with the paper is the novelty aspect of the work. The estimators provided and the analyses provided are based on standard tools and there is no 'novel theoretical contribution' as such. From a practical perspective, assuming access to labeled samples in order to estimate the competence level serves as a drawback compared to the Dawid-Skene method, using which both competence level and ground truth could be estimated without labels (apart from the references provided in the paper, two related references are [1] and [2] below). But still, the problem considered in the paper is very relevant (especially due to several crowd-sourcing applications). An advantage of weighted majority method over the Dawid-Skene model, is the former makes no modeling assumptions. Hence a theoretical analysis characterizing conditions when weighted majority performs better compared to the David-Skene method would make the paper much stronger. \n\n[1] http://jmlr.org/papers/v11/donmez10a.html\n[2] http://arxiv.org/abs/1310.5764  This paper considers weighted majority algorithm and establishes consistency (error rate of the aggregator tending to zero) results. Drawback of the paper is the novelty aspect - there is no novel theoretical contribution as such or particularly striking practical implications of the analysis provided.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper considers the problem of combining experts' guesses in a\nclassification setting. Unlike the sequential, adversarial setting,\nthe problem is based on static guesses with known or estimated\naccuracies (competencies). In particular, the authors show conditions\nthat guarantee when the Nitzan-Paroush weighted majority rule is\nconsistent.\n\n\nThe paper is well-written and very interesting. The results are very\nclean and rigorous. The authors provide lots of intuition and even\npoint out interesting open problems along the way. Interestingly,\nstandard concentration of measure results like Hoeffding's inequality\nand Bernstein's inequality are not adequate. Instead, less known but\nsharper bounds are used.\n Clear, interesting analysis if the weighted majority rulein the non-sequential setting.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper analyzes the Nitzan-Paroush strategy for assigning weights to conditionally (on the outcome) independent experts, which is optimal in expectation. The authors provide exponential concentration inequalities (both upper and lower bounds on probabilities) for the error of this optimal rule. The upper bounds are sharp and derived based on the Kearns-Saul inequality. They continue to analyze the situation where the quality of the experts is not given and must be estimated from data, giving frequentist procedures (which are analyzed) and Bayesian procedures (which are not analyzed). \n\nQUALITY\n\nThe results are interesting, nontrivial and appear correct, as far as I checked. I was struck with the use of the Kearns-Saul inequality, which (although I have seen it before and know it was designed for other goals) seems almost magically suitable for application in this problem. The exponential bound Theorem 1(i) is quite strong, being linear in the nr of experts who are correct with probability > 1/2 + epsilon, for fixed epsilon > 0. \n\nI do have some questions/small issues about the estimators, both the frequentist and the Bayesian ones. \n\nFirst, as a very minor point, I'd rather call 'adaptive' 'fully empirical' and 'nonadaptive' 'oracle' by the way, that seems to more correctly describe the difference.\n\nSecond:\nTheorem 7: adaptive, high-confidence, frequentist case:\n \nThis theorem is only useful if the authors can show that the event R will hold with large probability for sufficiently large sample sizes. (Otherwise it may be that we always have to refrain from making a high-confidence decision, making the results useless).\nThe authors should say some more about the sample sizes when we can expect R to hold, given the (oracle, nonadaptive) probabilities\n(so that 'if we are lucky about the real probabilities, then with high probability we get a situation in which the adaptive bound is useful')\n \n\nAbout the Bayesian method: this seems to be based on using the standard, unconditional Naive Bayes model in which the experts opinions are viewed as X-data and the outcome as Y. But I guess that most Bayesians would condition on the X's and use conditional likelihood - and then Naive Bayes becomes logistic regression, which often works better anyway. The authors should say something about this additional possibility. Is there any reason why they have not considered it? \n\n\n\nWhy naive Bayes and not logistic regression approach?\n\n\n\nCLARITY\n\nThe paper is quite well-written; I am not an expert in the probabilistic weighted-majority analysis but had no trouble following the paper. \n\nORIGINALITY\n\nThe results are somewhat original, the *proof techniques* are very original. \n\nSIGNIFICANCE\n\nReasonably high, esp. given the use of the Kearns-Saul inequality. The authors give concentration bounds on the error of weighted majority voting with optimal weights, and show how to learn the weights from data. Results are interesting, proof technique even more so.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
