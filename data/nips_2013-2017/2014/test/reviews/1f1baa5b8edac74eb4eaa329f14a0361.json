{"title": "Zero-shot recognition with unreliable attributes", "abstract": "In principle, zero-shot learning makes it possible to train an object recognition model simply by specifying the category's attributes. For example, with classifiers for generic attributes like striped and four-legged, one can construct a classifier for the zebra category by enumerating which properties it possesses --- even without providing zebra training images. In practice, however, the standard zero-shot paradigm suffers because attribute predictions in novel images are hard to get right. We propose a novel random forest approach to train zero-shot models that explicitly accounts for the unreliability of attribute predictions. By leveraging statistics about each attribute\u2019s error tendencies, our method obtains more robust discriminative models for the unseen classes. We further devise extensions to handle the few-shot scenario and unreliable attribute descriptions. On three datasets, we demonstrate the benefit for visual category learning with zero or few training examples, a critical domain for rare categories or categories defined on the fly.", "id": "1f1baa5b8edac74eb4eaa329f14a0361", "authors": ["Dinesh Jayaraman", "Kristen Grauman"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "The paper strives to bridge the gap between the theory and practice of attribute-based zero-shot learning. The theory is that novel classes can be recognized automatically using pre-trained attribute predictors; in practice, however, learning these attribute classifiers can be as difficult or even more so than learning the object classes themselves. \n\nRandom forests are trained to predict unseen classes from attribute vectors,\nand the training procedure takes into account the reliability of the attribute\ndetectors by propagating a validation set through each decision tree at training\ntime. The authors show how the method can be extended to handle training with a\nfew training examples of test categories. The method achieves state-of-the-art\nresults on several attribute datasets.\n\nQUALITY: The authors do a really nice job of handling this fundamental zero-shot learning problem using a random forest framework. The model is elegant and theoretically sound. Results on 3 standard datasets are strong. The authors do a nice job of performing ablation studies, introducing artificial noise, evaluating several setting (zero- versus few-shot learning), and comparing with the literature.\n\nCLARITY: The paper is very well written. One confusion was in Section 3.2.1: when the threshold t is introduced, the attribute signatures are still binary, so t can be any value 0 < t < 1 without changing anything in equation (2). Then it is not clear in lines 207-212 how a novel test example can be propagated down the tree, since t seems to be ill-defined. This is cleared up in later sections.\n\nORIGINALITY: This is a nice application of a random forest framework to an important problem.\n\nSIGNIFICANCE: The paper addresses a fundamental problem in zero-shot learning.\n The paper is strong, interesting, and sound. The results on 3 datasets in a variety of settings are convincing. The paper deserves to be published.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a new random forest based method for zero-shot learning that better accommodates the uncertainty in attribute classifier predictions. Uncertainty is measured via the performance (true positive rate, false positive rate, etc.,) of attribute classifiers on a held-out validation set. The paper proposes to use this performance information in the information gain computation during the learning of the random forest (for pursuing a given category-attribute signature along multiple paths of a decision tree), thereby accommodating the \"uncertainty\" of attribute predictions. \n\nQuality: The proposed ideas are supported well with relevant empirical/quantitative analysis. However it lacks in qualitative analysis (For e.g., I would have liked to see the types of attributes and the categories that are effected/improved by this method.) Also it is unclear how many of the parameters involved were selected (for e.g., why was the 80%-20% split for training-validation chosen, etc.,)\n\nClarity: The paper is well-written and easy to read. \n\nOriginality: While most previous works have addressed the problem of \"attribute strength\", this paper claims to focus on the lesser explored problem of \"attribute reliability\". Although many learning algorithms/techniques implicitly account for classifier unreliability, I believe this work is novel in terms of explicit unreliability handling.\n\nSignificance: While the proposed method (random forest) is not new, the idea (of modeling unreliability by measuring classifier performance on validation data to improve zero-shot learning performance) introduced in this paper is interesting/thought-provoking to researchers working on attributes.\n While the idea and the method introduced in this paper are not significantly novel, their application towards improving zero-shot learning is revealing/interesting.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a method for attribute-based zero-shot learning using random forests. Attribute classifiers are first trained using SVMs. A random forest is trained on class-attribute signature vectors, while using a validation set of images with attribute-labels is used to estimate attribute reliability at each node of the tree. An extension to few-shot learning is also proposed.\n\nClarity: The paper is very clear and well-written. It does a good job summarizing related work.\n\nOriginality: The novelty of this paper is somewhat low, but not too low to be accepted. One issue is that the claim that the proposed approach deals with unreliable attribute predictions while earlier work does not is a bit overstated or misleading. For example, direct attribute prediction [7] (DAP)--the baseline method used in this paper and many others--has some similar abilities to deal with attribute uncertainty. Both approaches use the basic approach: \na) Train attribute classifiers on image features (identical for both papers)\nb) Train a 2nd class probability estimator as a function of attribute predictions, using a validation set to estimate the reliability of attribute classifiers ([6] uses Platt scaling and a probabilistic model to combine attributes, whereas the proposed approach uses a new random forest-based method)\n\nIn this way, I don't find the proposed approach to be a fundamentally new way of handling attribute uncertainty. What it does differently than earlier work is that it uses a more complex model for modelling attribute noise: 1) it uses an ROC-based estimate instead of a sigmoidal model like Platt scaling, and 2) whereas [6] treats attributes as independent, the proposed approach effectively models joint statistics in attribute errors, since the validation set is propagated down trees. In this way, I think it would be a little more appropriate to describe the approach as a better way to model attribute correlation (I don't know this area well, but Scheirer et al. \"Multi-Attribute Spaces: Calibration for Attribute Fusion and Similarity Search\" might be one related paper that could be cited).\n\nAt the same time, one possible weakness of the proposed approach in comparison to methods that treat attributes independently is that it seems like one would need a large validation set to obtain valid probability estimates as it gets subdivided when propagated down decision trees. This issue was glossed over in the paper, and proposed approach for dealing with this (line 323) seems heuristic. It would have been nice to see more experiments evaluating the effect of training and validation set size.\n\nQuality: In general, the quality of the paper is good and the technical choices and descriptions make sense. The experiments are appropriate and support the claims of the paper, although the level of quantitative improvement isn't high enough to accept the paper solely on the merit of empirical results. The extension to few-shot learning (Section 3.3) felt less polished than the rest of the paper. Whereas the rest of the paper was clear and explanations were lengthy, this section was very brief. To me, a method that could combine class-attribute priors and image attribute estimates when estimating probabilities p(a|...) might make more intuitive sense then a weighted combination of information gains (Eq. 6). This is a well written paper, with technical decisions that make sense, and adequate experiments; however, the main selling point of the paper as a new way of handling unreliable attributes is somewhat questionable.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
