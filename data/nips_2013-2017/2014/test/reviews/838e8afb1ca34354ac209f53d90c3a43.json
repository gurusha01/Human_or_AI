{"title": "Structure Regularization for Structured Prediction", "abstract": "While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based overfitting, we propose a structure regularization framework via \\emph{structure decomposition}, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control overfitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies yet with substantially faster training speed.", "id": "838e8afb1ca34354ac209f53d90c3a43", "authors": ["Xu Sun"], "conference": "NIPS2014", "accepted": true, "reviews": [{"comments": "This paper proposes a new regularization method for structured prediction. The idea is relatively straightforward: a linear chain model is segmented into smaller subchains, each of which is added as an independent training example. \n\nTheorems are provided (with proofs in the supplement) showing how this regularization can reduce generalization risk and accelerate convergence rates. Empirical comparisons with state of the art approaches suggest that the resulting method is both faster and more accurate. The accuracy improvements are small, but these are all well-studied tasks where small improvements can have impact.\n\nMy primary concern is the assumption of a linear chain. Is this a limitation of the method? If not, the notation should be generalized to accept graphical models of arbitrary structure. Otherwise, the article should be written assuming a linear chain. There are obvious additional difficulties when considering more complex models --- even if the scope of this paper is limited to linear chains, I think this is still a sufficient contribution. \n\nA secondary concern is that the paper should be better placed in the context of related work. How does this approach relate to other work that approximates structure at training time? E.g., piecewise training (Sutton & McCallum, ICML 2007) or Wainright's \"Estimating the \u201cWrong\u201d Graphical Model\" paper (JMLR 2006). In general, please better situate this submission in the context of related work.\n\nFinally, the paper could use a more thorough edit to remove some distracting errors and add additional clarifications. Some suggestions:\n\n- Proposition 3: it is not clear why the learning rate is defined as such. Please motivate and clarify.\n\n- Is Figure 2 accuracy or F1? The text seems to confuse these terms.\n\n- I assume only the best performing StructReg results are used for computing significance? Surely it is not significant for all values of alpha.\n\n058: confliction -> conflict\n078: two-folds -> two-fold\n098: local window of $o_k$: \"Window\" typically denotes features of adjacent nodes in a chain, but this notation suggests the features are of observation $o_k$. Please clarify.\n138: draw -> drawn\n164: Please define $N_\\lambda$ in Eq. 1\n193: I believe $g$ in Eq. 5 should be G, to be consistent with Eq. 4.\n201: focus -> focuses\n220: xvalue -> value (?)\n229: value is bounded -> value be bounded\n236: simplified as -> approximated by (?) Since you're ignoring small terms.\n\n\n This paper proposes a new regularization method for structured prediction, providing theoretical and empirical evidence of its efficacy. While the approach is straightforward, it appears to perform quite well, and has an interesting theoretical foundation; I feel this paper is a valuable contribution to structured classification problems.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper investigates decomposing the structured output object into smaller objects in Structured Output prediction setting. Theoretical analysis is provided to show that the decomposition approach has better generalization then regular SO models. In case of CRF, convergence rates are shown to be faster for optimization with SGD. Experiments on sequence models with 4 different applications show superior/competitive (3/1) performance to the state-of-the art systems in the literature.\n\nI find the theoretical analysis of the the decompositional approach refreshing, since there has been various proposal of this approach algorithmically without any theoretical analysis. One problem that I see is that the provided bounds are much looser than the existing analysis of SO prediction models for hinge-loss, \n[Taskar et al, Max-Margin Markov Networks, NIPS 2003,\nMcAllister, Generalization bounds and consistency for Structured Labeling, in Predicting Structured Data, 2006]\nwhere the generalization bound is log(l) opposed to l4 in this paper with l being the size of the structured object. Given that, it is not clear to me how the analysis of the decompositional approach would translate in the more informative analysis. \n\nIn the experimental side, the results look very impressive. In particular for segmentation tasks (NER and Word Segmentation), a consistent labeling of the segmentation is fairly important [in BIO terms, I(In) label does not mean anything if there is no B(Begin) term]. The figures seem to suggest that in Chinese word segmentation for CRFs, simply looking at the bigram labels yields the best performance (46.6/20) and similarly for NER (26.5/~10). [Can the authors pls comment whether this is a correct reading of the comparisons?]. I am assuming here, that during testing the same decomposition algorithm is applied. I have also notice that comparing Table 1, the performance of the model on the (test?) data, is very much aligned with the best values of the hyperparameter in Figure 2, which is stated to be set on development data. It would be great if the authors could provide some discussions on these two aspect.\n\nThe decomposition of the model is done randomly for each instance (Algorithm 1). Such decomposition naturally translates sequence models. Sequence modeling is still the most representative SO prediction problem. However, it would be great to provide the reader (some possibly informal discussion) what are the consequence of the random decomposition approach for models with clique size larger than 2. A decompositional approach to SO prediction supported with (rather uninformative) theoretical analysis and strong empirical analysis.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary: The authors propose \u201ctag structure regularization\u201d, a novel regularization scheme whereby at training time, a structured prediction model is penalized based on losses on sub-structures of the training example. The key is that the model is forced to make predictions for each sub-structure independently, so it cannot rely on long-reaching information from other parts of the graph. They provide analysis based on stability arguments that the regularization strength decreases bounds on generalization error and increases the convergence rate of SGD. On several sequential prediction tasks they show that varying the regularization strength can produce state-of-the-art accuracies on several tasks and at the same time speed up training.\n\nMajor comments: (+ Pros, - Cons)\n\n+ On the one hand, the central idea in this paper is very interesting. The authors argue that by breaking up training samples into sub-samples during learning, we can increase the generalization of structured models. Intuitively, this makes a certain kind of sense: the structured model is regularized to rely more on local information than incoming messages from the rest of the graph. This might prevent errors from propagating, and results in a \u201csimpler\u201d model. Another way to think about it is that if a model can make accurate predictions without relying on passing messages, the \u201ctag structure regularization\u201d will choose that simpler model, while more standard approaches do not have such preference.\n\n- On the other hand, the actual theoretical analysis in this paper seems like it isn\u2019t making the correct assumptions or taking the right approach to analysis. Yes, increasing alpha does reduce the bound -- but at the tightest setting alpha = n, the bound is still far looser than other structured prediction generalization bounds. E.g. the original MMMN paper (Taskar et al.) had a logarithmic complexity in both the multi class label size and the number of variables (l in that paper, n here). More recently London et al. used PAC Bayes analysis to show that for *templated* models, increasing the size of the example actually **decreases** generalization error: in the limit, one could learn an entire templated model from a single example. So, from that perspective, using n as the measure of structure complexity makes no sense, since in most applications (including those in this paper) feature templates are used (bi-grams, etc.). So while I believe that their analysis is technically correct, the authors must reconcile their analysis with previous work in order for this paper to make sense.\n\n- n^4 seems like an awfully large term for a generalization bound: following the supplemental, it seems like it stems from the fact that the bounds rely on decomposing loss linearly in Lemma 6, and then due to having to multiple the norms of 2 examples (which could be O(n)) in addition to that, as well as decomposing the regularization term linearly (O(n/alpha)). To me, that suggests that this is really not the right approach to take here.\n\n- The aforementioned previous work uses a more subtle measure of graph complexity in their bounds, based on concentration inequalities, that measures the maximum dependence of one variable on the others in the graph. It seems like a better approach to analysis would be to relate the novel regularization to the resulting complexity of the learned model in terms of a complexity measure like that, where one can assume some sort of templating. Instead of just assuming complexity = n.\n\n+ I hate to say this (because this idea is so trendy), but the tag structure regularization reminds me a lot of dropout: essentially, for each example, you generate new examples by removing edges in the graph. In the case of sequences, this creates disconnected components, but one could imagine more generally just removing edges. So in that sense I think this paper does help shed light on other ideas in the field.\n\n+ All that being said about the theory, the experimental results are very strong, and the idea is simple enough to be easy to experiment with and verify. While the analysis seems like it takes the wrong approach, the idea is simple and interesting, and the experiments are strong. I think it would benefit the community to see it.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
