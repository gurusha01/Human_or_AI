{
  "name" : "194cf6c2de8e00c05fcf16c498adc7bf.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Variational Laws of Visual Attention for Dynamic Scenes",
    "authors" : [ "Dario Zanca", "Marco Gori" ],
    "emails" : [ "dario.zanca@unifi.it", "marco@diism.unisi.it" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Eye movements in humans constitute an essential mechanism to disentangle the tremendous amount of information that reaches the retina every second. This mechanism in adults is very sophisticated. In fact, it involves both bottom-up processes, which depend on raw input features, and top-down processes, which include task dependent strategies [2; 3; 4]. It turns out that visual attention is interwound with high level cognitive processes, so as its deep understanding seems to be trapped into a sort of eggs-chicken dilemma. Does visual scene interpretation drive visual attention or the other way around? Which one “was born” first? Interestingly, this dilemma seems to disappears in newborns: despite their lack of knowledge of the world, they exhibit mechanisms of attention to extract relevant information from what they see [5]. Moreover, there are evidences that the very first fixations are highly correlated among adult subjects who are presented with a new input [25]. This shows that they still share a common mechanism that drive early fixations, while scanpaths diverge later under top-down influences.\nMany attempts have been made in the direction of modeling visual attention. Based on the feature integration theory of attention [14], Koch and Ullman in [9] assume that human attention operates in the early representation, which is basically a set of feature maps. They assume that these maps are then combined in a central representation, namely the saliency map, which drives the attention mechanisms. The first complete implementation of this scheme was proposed by Itti et al. in [10]. In that paper, feature maps for color, intensity and orientation are extracted at different scales. Then center-surround differences and normalization are computed for each pixel. Finally, all this information is combined linearly in a centralized saliency map. Several other models have been proposed by the computer vision community, in particular to address the problem of refining saliency maps estimation. They usually differ in the definition of saliency, while they postulate a centralized control of the attention mechanism through the saliency map. For instance, it has been claimed that\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nthe attention is driven according to a principle of information maximization [16] or by an opportune selection of surprising regions [17]. A detailed description of the state of the art is given in [8]. Machine learning approaches have been used to learn models of saliency. Judd et al. [1] collected 1003 images observed by 15 subjects and trained an SVM classifier with low-, middle-, and high-level features. More recently, automatic feature extraction methods with convolutional neural networks achieved top level performance on saliency estimation [26; 18].\nMost of the referred papers share the idea that saliency is the product of a global computation. Some authors also provide scanpaths of image exploration, but to simulate them over the image, they all use the procedure defined by [9]. The winner-take-all algorithm is used to select the most salient location for the first fixation. Then three rules are introduced to select the next location: inhibition-of-return, similarity preference, and proximity preference. An attempt of introducing biological biases has been made by [6] to achieve more realistic saccades and improve performance.\nIn this paper, we present a novel paradigm in which visual attention emerges from a few unifying functional principles. In particular, we assume that attention is driven by the curiosity for regions with many details, and by the need to achieve brightness invariance, which leads to fixation and motion tracking. These principles are given a mathematical expression by a variational approach based on a generalization of least action, whose stationary point leads to the correspondent Euler-Lagrange differential equations of the focus of attention. The theory herein proposed offers an intriguing model for capturing a mechanisms behind saccadic eye movements, as well as object tracking within the same framework. In order to compare our results with the state of the art in the literature, we have also computed the saliency map by counting the visits in each pixel over a given time window, both on static and dynamic scenes. It is worth mentioning that while many papers rely on models that are purposely designed to optimize the approximation of the saliency map, for the proposed approach such a computation is obtained as a byproduct of a model of scanpath.\nThe paper is organized as follows. Section 2 provides a mathematical description of the model and the Euler-Lagrange equations of motion that describe attention dynamics. The technical details, including formal derivation of the motion equations, are postponed to the Appendix. In the Section 3 we describe the experimental setup and show performance of the model in a task of saliency detection on two popular dataset of images [12; 11] and one dataset of videos [27]. Some conclusions and critical analysis are finally drawn in Section 4."
    }, {
      "heading" : "2 The model",
      "text" : "In this section, we propose a model of visual attention that takes place in the earliest stage of vision, which we assume to be completely data driven. We begin discussing the driving principles."
    }, {
      "heading" : "2.1 Principles of visual attention",
      "text" : "The brightness signal b(t, x) can be thought of as a real-valued function\nb : R+ × R2 → R (1) where t is the time and x = (x1, x2) denotes the position. The scanpath over the visual input is defined as x : R+ → R2 (2) The scanpath x(t) will be also referred to as trajectory or observation.\nThree fundamental principles drive the model of attention. They lead to the introduction of the correspondent terms of the Lagrangian of the action.\ni) Boundedness of the trajectory Trajectory x(t) is bounded into a defined area (retina). This is modeled by a harmonic oscillator at the borders of the image which constraints the motion within the retina1:\nV (x) = k ∑ i=1,2 ( (li − xi)2 · [xi > li] + (xi)2 · [xi < 0] ) (3)\n1Here, we use Iverson’s notation, according to which if p is a proposition then [p] = 1 if p=true and [p] = 0 otherwise\nwhere k is the elastic constant, li is the i-th dimension of the rectangle which represents the retina2.\nii) Curiosity driven principle Visual attention is attracted by regions with many details, that is where the magnitude of the gradient of the brightness is high. In addition to this local field, the role of peripheral information is included by processing a blurred version p(t, x) of the brightness b(t, x). The modulation of these two terms is given by\nC(t, x) = b2x cos 2(ωt) + p2x sin 2(ωt), (4)\nwhere bx and px denote the gradient w.r.t. x. Notice that the alternation of the local and peripheral fields has a fundamental role in avoiding trapping into regions with too many details.\niii) brightness invariance Trajectories that exhibit brightness invariance are motivated by the need to perform fixation. Formally, we impose the constraint ḃ = bt + bxẋ = 0. This is in fact the classic constraint that is widely used in computer vision for the estimation of the optical flow [20]. Its soft-satisfaction can be expressed by the associated term\nB(t, x, ẋ) = ( bt + bxẋ )2 . (5)\nNotice that, in the case of static images, bt = 0, and the term is fully satisfied for trajectory x(t) whose velocity ẋ is perpendicular to the gradient, i.e.when the focus is on the borders of the objects. This kind of behavior favors coherent fixation of objects. Interestingly, in case of static images, the model can conveniently be simplified by using the upper bound of the brightness as follows:\nB(t, x, ẋ) = ḃ2(t, x) = (∂bt + bxẋ) 2 ≤\n≤ 2b2t + 2b2xẋ2 := B̄(t, x, ẋ) (6)\nThis inequality comes from the parallelogram law of Hilbert spaces. As it will be seen the rest of the paper, this approximation significantly simplifies the motion equations."
    }, {
      "heading" : "2.2 Least Action Principle",
      "text" : "Visual attention scanpaths are modeled as the motion of a particle of mass m within a potential field. This makes it possible to construct the generalized action\nS = ∫ T 0 L(t, x, ẋ) dt (7)\nwhere L = K − U , where K is the kinetic energy\nK(ẋ) = 1\n2 mẋ2 (8)\nand U is a generalized potential energy defined as\nU(t, x, ẋ) = V (x)− ηC(t, x) + λB(t, x, ẋ). (9)\nHere, we assume that η, λ > 0. Notice, in passing that while V and B get the usual sign of potentials, C comes with the flipped sign. This is due to the fact that, whenever it is large, it generates an attractive field. In addition, we notice that the brightness invariance term is not a truly potential, since it depends on both the position and the velocity. However, its generalized interpretation as a “potential” comes from considering that it generates a force field. In order to discover the trajectory we look for a stationary point of the action in Eq. (7), which corresponds to the Euler-Lagrange equations\nd\ndt\n∂L ∂ẋi = ∂L ∂xi , (10)\n2A straightforward extension can be given for circular retina.\nwhere i = 1, 2 for the two motion coordinates. The right-hand term in (10) can be written as\n∂L ∂x = ηCx − Vx − λBx. (11)\nLikewise we have d\ndt\n∂L ∂ẋ = mẍ− λ d dt Bẋ (12)\nso as the general motion equation turns out to be\nmẍ− λ d dt Bẋ + Vx − ηCx + λBx = 0. (13)\nThese are the general equations of visual attention. In the Appendix we give the technical details of the derivations. Throughout the paper, the proposed model is referred to as the EYe MOvement Laws (EYMOL)."
    }, {
      "heading" : "2.3 Parameters estimation with simulated annealing",
      "text" : "Different choices of parameters lead to different behaviors of the system. In particular, weights can emphasize the contribution of curiosity or brightness invariance terms. To better control the system we use two different parameters for the curiosity term, namely ηb and ηp, to weight b and p contributions respectively. The best values for the three parameters (ηb, ηp, λ) are estimated using the algorithm of simulated annealing (SA). This method allows to perform iterative improvements, starting from a known state i. At each step, the SA considers some neighbouring state j of the current state, and probabilistically moves to the new state j or stays on the current state i. For our specific problem, we limit our search to a parallelepiped-domain D of possible values, due to theoretical bounds and numerical3 issues. Distance between states i and j is proportional with a temperature T , which is initialized to 1 and decreases over time as Tk = α ∗ Tk−1, where k identifies the iteration step, and 0 << α < 1. The iteration step is repeated until the system reaches a state that is good enough for the application, which in our case is to maximize the NSS similarity between human saliency maps and simulated saliency maps. Only a batch of a 100 images from CAT2000-TRAIN is used to perform the SA algorithm4. This batch is created by randomly selecting 5 images from each of the 20 categories of the dataset. To start the SA, parameters are initialized in the middle point of the 3-dimensional parameters domain D. The process is repeated 5 times, on different sub-samples, to select 5 parameters configurations. Finally, those configurations together with the average configuration are tested on the whole dataset, to select the best one.\nAlgorithm 1 In the psedo-code, P() is the acceptance probability and score() is computed as the average of NSS scores on the sample batch of 100 images.\n1: procedure SIMULATEDANNEALING 2: Select an initial state i ∈ D 3: T ← 1 4: do 5: Generate random state j, neighbor of i 6: if P(score(i), score(j)) ≥ Random(0, 1) then 7: i← j 8: end if 9: T ← α ∗ T\n10: while T ≥ 0.01 11: end procedure\n3Too high values for ηb or ηp produce numerically unstable and unrealistic trajectories for the focus of attention.\n4Each step of the SA algorithm needs evaluation over all the selected images. Considering the whole dataset would be very expensive in terms of time."
    }, {
      "heading" : "3 Experiments",
      "text" : "To quantitative evaluate how well our model predicts human fixations, we defined an experimental setup for salient detection both in images and in video. We used images from MIT1003 [1], MIT300 [12] and CAT2000 [11], and video from SFU [27] eye-tracking database. Many of the design choices were common to both experiments; when they differ, it is explicitly specified."
    }, {
      "heading" : "3.1 Input pre-processing",
      "text" : "All input images are converted to gray-scale. Peripheral input p is implemented as a blurred versions of the brightness b. This blurred version is obtained by convolving the original gray-scale image with a Gaussian kernel. For the images only, an algorithm identifies the rectangular zone of the input image in which the totality of information is contained in order to compute li in (14). Finally both b and p are multiplied by a Gaussian blob centered in the middle of the frame in order to make brightness gradients smaller as we move toward periphery and produce a center bias."
    }, {
      "heading" : "3.2 Saliency maps computation",
      "text" : "Differently by many of the most popular methodologies in the state-of-the-art [10; 16; 1; 24; 18], the saliency map is not itself the central component of our model but it can be naturally calculated from the visual attention laws in (13). The output of the model is a trajectory determined by a system of two second ordered differential equations, provided with a set of initial conditions. Since numerical integration of (13) does not raise big numerical difficulties, we used standard functions of the python scientific library SciPy [21].\nSaliency map is then calculated by summing up the most visited locations during a sufficiently large number of virtual observations. For images, we collected data by running the model 199 times, each run was randomly initialized almost at the center of the image and with a small random velocity, and integrated for a running time corresponding to 1 second of visual exploration. For videos, we collected data by running the model 100 times, each run was initialized almost at the center of the first frame of the clip and with a small random velocity.\nModel that have some blur and center bias on the saliency map can improve their score with respect to some metrics. A grid search over blur radius and center parameter σ have been used, in order to maximize AUC-Judd and NSS score on the training data of CAT2000 in the case of images, and on SFU in case of videos."
    }, {
      "heading" : "3.3 Saliency detection on images",
      "text" : "Two versions of the the model have been evaluated. The first version V1 implementing brightness invariance in the approximated form (6), the second version V2 implementing the brightness invariance in its exact form, as described in the Appendix. Model V1 and V2 have been compared on the MIT1003 and CAT2000-TRAIN datasets, since they provide public data about fixations. Parameters estimation have been conducted independently for the two models and the best configuration for each one is used in this comparison. Results are statistically equivalent (see Table2) and this proves that, in the case of static images, the approximation is very good and does not cause loss in the score. For further experiments we decided to use the approximated form V1 due to its simpler form of the equation that also reduces time of computation.\nModel V1 has been evaluated in two different dataset of eye-tracking data: MIT300 and CAT2000TEST. In this case, scores were officially provided by MIT Saliency Benchmark Team [15]. Description of the metrics used is provided in [13]. Table 2 and Table 3 shows the scores of our\nmodel compared with five other popular method [10; 16; 1; 24; 18], which have been selected to be representative of different approaches. Despite its simplicity, our model reaches best score in half of the cases and for different metrics."
    }, {
      "heading" : "3.4 Saliency detection on dynamic scenes",
      "text" : "We evaluated our model in a task of saliency detection with the dataset SFU [27]. The dataset contains 12 clips and fixations of 15 observers, each of them have watched twice every video. Table 4 provides a comparison with other four model. Also in this case, despite of its simplicity and even if it was not designed for the specific task, our model competes well with state-of-the-art models. Our model can be easily run in real-time to produce an attentive scanpath. In some favorable case, it shows evidences of tracking moving objects on the scene."
    }, {
      "heading" : "4 Conclusions",
      "text" : "In this paper we investigated how human attention mechanisms emerge in the early stage of vision, which we assume completely data-driven. The proposed model consists of differential equations, which provide a real-time model of scanpath. These equations are derived in a generalized framework of least action, which nicely resembles related derivations of laws in physics. A remarkable novelty concerns the unified interpretation of curiosity-driven movements and the brightness invariance term for fixation and tracking, that are regarded as mechanisms that jointly contribute to optimize the acquisition of visual information. Experimental results on both image and video datasets of saliency are very promising, especially if we consider that the proposed theory offers a truly model of eye movements, whereas the computation of the saliency maps only arises as a byproduct.\nIn future work, we intend to investigate behavioural data, not only in terms of saliency maps, but also by comparing actual generated scanpaths with human data in order to discover temporal correlations. We aim at providing the integration of the presented model with a theory of feature extraction that is still expressed in terms of variational-based laws of learning [29]."
    }, {
      "heading" : "Appendix: Euler-Lagrange equations",
      "text" : "In this section we explicitly compute the differential laws of visual attention that describe the visual attention scanpath, as the Euler-Lagrange equations of the action functional (7).\nFirst, we compute the partial derivatives of the different contributions w.r.t. x, in order to compute the exact contributions of (11). For the retina boundaries,\nVx = k ∑ i=1,2 ( − 2 (li − xi) · [xi > li] + 2xi · [xi < 0] ) (14)\nThe curiosity term (4)\nCx =2cos 2(ωt)bx · bxx + 2sin2(ωt)px · pxx (15)\nFor the term of brightness invariance,\nBx = ∂\n∂x (bt + bxẋ)\n2 (16)\n= 2 (bt + bxẋ) (btx + bxxẋ) (17)\nSince we assume b ∈ C2(t, x), by the Schwarz’s theorem5, we have that btx = bxt, so that Bx = 2 (bt + bxẋ) (bxt + bxxẋ) (18)\n= 2(ḃ)(ḃx) (19)\nWe proceed by computing the contribution in (12). Derivative w.r.t. ẋ of the brightness invariance term is\nBẋ = ∂\n∂ẋ (bt + bxẋ)\n2 (20)\n= 2 (bt + bxẋ) bx (21)\n= 2(ḃ)(bx) (22)\nSo that, total derivative w.r.t. t can be write as\nd dt Bẋ =2\n( b̈bx + ḃḃx ) (23)\nWe observe that b̈ ≡ b̈(t, x, ẋ, ẍ) is the only term which depends on second derivatives of x. Since we are interested in expressing EL in an explicit form for the variable ẍ, we explore more closely its contribution\nb̈(t, x, ẋ, ẍ) = d\ndt ḃ (24)\n= d\ndt (bt + bxẋ) (25)\n=ḃt + ḃx · ẋ+ bx · ẍ (26) (27)\nSubstituting it in (23) we have\nd dt Bẋ =2\n( (ḃt + ḃx · ẋ+ bx · ẍ)bx + ḃḃx ) (28)\n=2 ( (ḃt + ḃx · ẋ)bx + ḃḃx ) + 2(bx · ẍ)bx (29)\n5Schwarz’s theorem states that, if f : Rn → R has continuous second partial derivatives at any given point in Rn, then ∀i, j ∈ {1, ..., n} it holds fxixj = fxjxi\nSo that, from (12) we get\nd\ndt\n∂L ∂ẋ = mẍ− 2λ\n( (ḃt + ḃx · ẋ)bx + ḃḃx + (bx · ẍ)bx ) (30)\nEuler-Lagrange equations. Combining (11) and (30), we get Euler-Lagrange equation of attention mẍ− 2λ ( (ḃt + ḃx · ẋ)(bx) + (ḃ)(ḃx) + (bx · ẍ)bx ) = ηCx − Vx − λBx (31)\nIn order to obtain explicit form for the variable ẍ, we re-write the equation as to move to the left all contributes which do not depend on that variable.\nmẍ− 2λ(bx · ẍ)bx =ηCx − Vx − λBx + 2λ((ḃt + ḃx · ẋ)(bx) + (ḃ)(ḃx)) (32) = ηCx − Vx + 2λ(ḃt + ḃx · ẋ)(bx)︸ ︷︷ ︸\nA=(A1,A2)\n(33)\nIn matrix form, the equation is( mẍ1 mẍ2 ) − ( 2λ(bx1 ẍ1 + bx2 ẍ2)bx1 2λ(bx1 ẍ1 + bx2 ẍ2)bx2 ) = ( A1 A2 ) (34)\nwhich gives us the system of two differential equations{ mẍ1 − 2λ(bx1 ẍ1 + bx2 ẍ2)bx1 = A1 mẍ2 − 2λ(bx1 ẍ1 + bx2 ẍ2)bx2 = A2\n(35)\nGrouping by same variable,{ (m− 2λb2x1)ẍ1 − 2λ(bx1bx2)ẍ2 = A1 −2λ(bx1bx2)ẍ1 + (m− 2λb2x2)ẍ2 = A2\n(36)\nWe define\nD = ∣∣∣∣(m− 2λb2x1) −2λ(bx1bx2)−2λ(bx1bx2) (m− 2λb2x2) ∣∣∣∣ (37)\nD1 = ∣∣∣∣A1 −2λ(bx1bx2)A2 (m− 2λb2x2) ∣∣∣∣ , D2 = ∣∣∣∣(m− 2λb2x1) A1−2λ(bx1bx2) A2 ∣∣∣∣ (38) By the Cramer’s method we get differential equation of visual attention for the two spatial component, i.e.  ẍ1 = D1 D\nẍ2 = D2 D\n(39)\nNotice that, this raise to a further condition over the parameter λ. In particular, in the case values of b(t, x) are normalized in the range [0, 1], it imposes to chose\nD 6= 0 =⇒ λ < m 4\n(40)\nIn fact,\nD = (m− 2λb2x1)(m− 2λb 2 x2)− 4λ 2(bx1bx2) 2 (41) = m ( m− 2λ(b2x1 + b 2 x1) ) (42)\nFor values of bx = 0, we have that\nD = m2 > 0 (43) so that ∀t, we must impose\nD > 0. (44)\nIf λ > 0, then\nm− 2λ(b2x1 + b 2 x1) > 0 (45)\nλ < m\n2(b2x1 + b 2 x1)\n(46)\nThe quantity on the right reaches its minimum at m\n4 , so that the condition\n0 < λ < m\n4 (47)\nguarantees the well-posedness of the problem."
    } ],
    "references" : [ {
      "title" : "Learning to Predict Where Humans Look",
      "author" : [ "T. Judd", "K. Ehinger", "F. Durand", "A. Torralba" ],
      "venue" : "IEEE International Conference on Computer Vision",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Computational modelling of visual attention",
      "author" : [ "L. Itti", "C. Koch" ],
      "venue" : "Nature Reviews Neuroscience,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Visual Attention: Bottom-Up Versus Top-Down",
      "author" : [ "C.E. Connor", "H.E. Egeth", "S. Yantis" ],
      "venue" : "Current Biology,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Interactions of Top-Down and Bottom-Up Mechanisms in Human Visual Cortex",
      "author" : [ "S. McMains", "S. Kastner" ],
      "venue" : "Society for Neuroscience,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Characteristics of saccades in human infants",
      "author" : [ "L. Hainline", "J. Turkel", "I. Abramov", "E. Lemerise", "C.M. Harris" ],
      "venue" : "Vision Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1984
    }, {
      "title" : "Saccadic model of eye movements for free-viewing condition",
      "author" : [ "O. Le Meur", "Z. Liu" ],
      "venue" : "Vision Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Calculus of Variation",
      "author" : [ "I.M. Gelfand", "S.V. Fomin" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1993
    }, {
      "title" : "State-of-the-Art in Visual Attention Modeling",
      "author" : [ "A. Borji", "L. Itti" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Shifts in selective visual attention: towards the underlying neural circuitry",
      "author" : [ "C. Koch", "S. Ullman" ],
      "venue" : "Springer Human Neurobiology,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1985
    }, {
      "title" : "A Model of Saliency-Based Visual Attention for Rapid Scene Analysis",
      "author" : [ "L. Itti", "C. Koch" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1998
    }, {
      "title" : "CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research",
      "author" : [ "A. Borji", "L. Itti" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "A Benchmark of Computational Models of Saliency to Predict Human Fixations",
      "author" : [ "T. Judd", "F. Durand", "A. Torralba" ],
      "venue" : "MIT Technical Report",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "What do different evaluation metrics tell us about saliency models? arXiv:1604.03605",
      "author" : [ "Z. Bylinskii", "T. Judd", "A. Oliva", "A. Torralba" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "A Feature Integration Theory of Attention",
      "author" : [ "A.M. Treisman", "G. Gelade" ],
      "venue" : "Cognitive Psychology,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1980
    }, {
      "title" : "Attention based on information maximization",
      "author" : [ "N. Bruce", "J. Tsotsos" ],
      "venue" : "J. Vis.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2007
    }, {
      "title" : "Bayesian Surprise Attracts Human Attention",
      "author" : [ "L. Itti", "P. Baldi" ],
      "venue" : "Vision Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images",
      "author" : [ "E. Vig", "M. Dorr", "D. Cox" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Determining optical flow",
      "author" : [ "B.K.P. Horn", "B.G. Schunck" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1981
    }, {
      "title" : "SciPy: Open source scientific tools for Python. http://www.scipy.org",
      "author" : [ "E. Jones", "O. Travis", "P. Peterson" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2001
    }, {
      "title" : "Saliency detection: a Boolean map approach",
      "author" : [ "J. Zhang", "S. Sclaroff" ],
      "venue" : "Proc. of the IEEE International Conference on Computer Vision",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Predicting Human Eye Fixations via an LSTM-based Saliency Attentive Model. http://arxiv.org/abs/1611.09571",
      "author" : [ "M. Cornia", "L. Baraldi", "G. Serra", "R. Cucchiara" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "On the relationship between optical variability, visual saliency, and eye fixations",
      "author" : [ "A. Garcia-Diaz", "V. Leborán", "X.R. Fdez-Vida", "X.M. Pardo" ],
      "venue" : "Journal of Vision,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Visual correlates of fixation selection: Effects of scale and time",
      "author" : [ "B.W. Tatler", "R.J. Baddeley", "I.D. Gilchrist" ],
      "venue" : "Vision Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2005
    }, {
      "title" : "Eye-Tracking Database for a Set of Standard Video Sequences",
      "author" : [ "H. Hadizadeh", "M.J. Enriquez", "I.V. Bajic" ],
      "venue" : "IEEE Transactions on Image Processing",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Learning to Detect Video Saliency With HEVC Features",
      "author" : [ "M. Xu", "L. Jiang", "Z. Ye", "Z. Wang" ],
      "venue" : "IEEE Transactions on Image Processing",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2017
    }, {
      "title" : "On-line Learning on Temporal Manifolds. AI*IA 2016 Advances in Artificial Intelligence Springer International Publishing, pp 321–333",
      "author" : [ "M. Maggini", "A. Rossi" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "In fact, it involves both bottom-up processes, which depend on raw input features, and top-down processes, which include task dependent strategies [2; 3; 4].",
      "startOffset" : 147,
      "endOffset" : 156
    }, {
      "referenceID" : 2,
      "context" : "In fact, it involves both bottom-up processes, which depend on raw input features, and top-down processes, which include task dependent strategies [2; 3; 4].",
      "startOffset" : 147,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : "In fact, it involves both bottom-up processes, which depend on raw input features, and top-down processes, which include task dependent strategies [2; 3; 4].",
      "startOffset" : 147,
      "endOffset" : 156
    }, {
      "referenceID" : 4,
      "context" : "Does visual scene interpretation drive visual attention or the other way around? Which one “was born” first? Interestingly, this dilemma seems to disappears in newborns: despite their lack of knowledge of the world, they exhibit mechanisms of attention to extract relevant information from what they see [5].",
      "startOffset" : 304,
      "endOffset" : 307
    }, {
      "referenceID" : 22,
      "context" : "Moreover, there are evidences that the very first fixations are highly correlated among adult subjects who are presented with a new input [25].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "Based on the feature integration theory of attention [14], Koch and Ullman in [9] assume that human attention operates in the early representation, which is basically a set of feature maps.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "Based on the feature integration theory of attention [14], Koch and Ullman in [9] assume that human attention operates in the early representation, which is basically a set of feature maps.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "the attention is driven according to a principle of information maximization [16] or by an opportune selection of surprising regions [17].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "the attention is driven according to a principle of information maximization [16] or by an opportune selection of surprising regions [17].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "A detailed description of the state of the art is given in [8].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "[1] collected 1003 images observed by 15 subjects and trained an SVM classifier with low-, middle-, and high-level features.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "More recently, automatic feature extraction methods with convolutional neural networks achieved top level performance on saliency estimation [26; 18].",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 8,
      "context" : "Some authors also provide scanpaths of image exploration, but to simulate them over the image, they all use the procedure defined by [9].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "An attempt of introducing biological biases has been made by [6] to achieve more realistic saccades and improve performance.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "In the Section 3 we describe the experimental setup and show performance of the model in a task of saliency detection on two popular dataset of images [12; 11] and one dataset of videos [27].",
      "startOffset" : 151,
      "endOffset" : 159
    }, {
      "referenceID" : 10,
      "context" : "In the Section 3 we describe the experimental setup and show performance of the model in a task of saliency detection on two popular dataset of images [12; 11] and one dataset of videos [27].",
      "startOffset" : 151,
      "endOffset" : 159
    }, {
      "referenceID" : 23,
      "context" : "In the Section 3 we describe the experimental setup and show performance of the model in a task of saliency detection on two popular dataset of images [12; 11] and one dataset of videos [27].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 17,
      "context" : "This is in fact the classic constraint that is widely used in computer vision for the estimation of the optical flow [20].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "0137) Table 1: Results on MIT1003 [1] and CAT2000-TRAIN [11] of the two different version of EYMOL.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "0137) Table 1: Results on MIT1003 [1] and CAT2000-TRAIN [11] of the two different version of EYMOL.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "We used images from MIT1003 [1], MIT300 [12] and CAT2000 [11], and video from SFU [27] eye-tracking database.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "We used images from MIT1003 [1], MIT300 [12] and CAT2000 [11], and video from SFU [27] eye-tracking database.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "We used images from MIT1003 [1], MIT300 [12] and CAT2000 [11], and video from SFU [27] eye-tracking database.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 23,
      "context" : "We used images from MIT1003 [1], MIT300 [12] and CAT2000 [11], and video from SFU [27] eye-tracking database.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "Differently by many of the most popular methodologies in the state-of-the-art [10; 16; 1; 24; 18], the saliency map is not itself the central component of our model but it can be naturally calculated from the visual attention laws in (13).",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 14,
      "context" : "Differently by many of the most popular methodologies in the state-of-the-art [10; 16; 1; 24; 18], the saliency map is not itself the central component of our model but it can be naturally calculated from the visual attention laws in (13).",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "Differently by many of the most popular methodologies in the state-of-the-art [10; 16; 1; 24; 18], the saliency map is not itself the central component of our model but it can be naturally calculated from the visual attention laws in (13).",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "Differently by many of the most popular methodologies in the state-of-the-art [10; 16; 1; 24; 18], the saliency map is not itself the central component of our model but it can be naturally calculated from the visual attention laws in (13).",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 16,
      "context" : "Differently by many of the most popular methodologies in the state-of-the-art [10; 16; 1; 24; 18], the saliency map is not itself the central component of our model but it can be naturally calculated from the visual attention laws in (13).",
      "startOffset" : 78,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : "Since numerical integration of (13) does not raise big numerical difficulties, we used standard functions of the python scientific library SciPy [21].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 12,
      "context" : "Description of the metrics used is provided in [13].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "MIT300 AUC SIM EMD CC NSS KL Itti-Koch [10], implem.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "53 Table 2: Results on MIT300 [12] provided by MIT Saliency Benchmark Team [15].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 9,
      "context" : "CAT2000-TEST AUC SIM EMD CC NSS KL Itti-Koch [10], implem.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "67 Table 3: Results on CAT2000 [11] provided by MIT Saliency Benchmark Team [15].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "model compared with five other popular method [10; 16; 1; 24; 18], which have been selected to be representative of different approaches.",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 14,
      "context" : "model compared with five other popular method [10; 16; 1; 24; 18], which have been selected to be representative of different approaches.",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "model compared with five other popular method [10; 16; 1; 24; 18], which have been selected to be representative of different approaches.",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 21,
      "context" : "model compared with five other popular method [10; 16; 1; 24; 18], which have been selected to be representative of different approaches.",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "model compared with five other popular method [10; 16; 1; 24; 18], which have been selected to be representative of different approaches.",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 23,
      "context" : "We evaluated our model in a task of saliency detection with the dataset SFU [27].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "SFU Eye-Tracking Database EYMOL Itti-Koch [10] Surprise [17] Judd Model [1] HEVC [28] Mean AUC 0.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : "SFU Eye-Tracking Database EYMOL Itti-Koch [10] Surprise [17] Judd Model [1] HEVC [28] Mean AUC 0.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "SFU Eye-Tracking Database EYMOL Itti-Koch [10] Surprise [17] Judd Model [1] HEVC [28] Mean AUC 0.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 24,
      "context" : "SFU Eye-Tracking Database EYMOL Itti-Koch [10] Surprise [17] Judd Model [1] HEVC [28] Mean AUC 0.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 23,
      "context" : "41 Table 4: Results on the video dataset SFU [27].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 25,
      "context" : "We aim at providing the integration of the presented model with a theory of feature extraction that is still expressed in terms of variational-based laws of learning [29].",
      "startOffset" : 166,
      "endOffset" : 170
    } ],
    "year" : 2017,
    "abstractText" : "Computational models of visual attention are at the crossroad of disciplines like cognitive science, computational neuroscience, and computer vision. This paper proposes a model of attentional scanpath that is based on the principle that there are foundational laws that drive the emergence of visual attention. We devise variational laws of the eye-movement that rely on a generalized view of the Least Action Principle in physics. The potential energy captures details as well as peripheral visual features, while the kinetic energy corresponds with the classic interpretation in analytic mechanics. In addition, the Lagrangian contains a brightness invariance term, which characterizes significantly the scanpath trajectories. We obtain differential equations of visual attention as the stationary point of the generalized action, and we propose an algorithm to estimate the model parameters. Finally, we report experimental results to validate the model in tasks of saliency detection.",
    "creator" : null
  }
}