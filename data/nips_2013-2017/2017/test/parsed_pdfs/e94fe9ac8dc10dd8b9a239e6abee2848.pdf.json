{
  "name" : "e94fe9ac8dc10dd8b9a239e6abee2848.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Approximation Algorithms for $\\ell_0$-Low Rank Approximation",
    "authors" : [ "Karl Bringmann", "Pavel Kolev", "David P. Woodruff" ],
    "emails" : [ "kbringma@mpi-inf.mpg.de", "pkolev@mpi-inf.mpg.de", "dwoodruf@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Low rank approximation of an m × n matrix A is an extremely well-studied problem, where the goal is to replace the matrix A with a rank-k matrix A′ which well-approximates A, in the sense that ‖A− A′‖ is small under some measure ‖ · ‖. Since any rank-k matrix A′ can be written as U · V , where U is m× k and V is k × n, this allows for a significant parameter reduction. Namely, instead of storingA, which hasmn entries, one can store U and V , which have only (m+n)k entries in total. Moreover, when computing Ax, one can first compute V x and then U(V x), which takes (m+ n)k instead of mn time. We refer the reader to several surveys [19, 24, 40] for references to the many results on low rank approximation.\nWe focus on approximation algorithms for the low-rank approximation problem, i.e. we seek to output a rank-k matrixA′ for which ‖A−A′‖ ≤ α‖A−Ak‖, whereAk = argminrank(B)=k‖A−B‖ is the best rank-k approximation to A, and the approximation ratio α is as small as possible. One of the most widely studied error measures is the Frobenius norm ‖A‖F = ( ∑m i=1 ∑n j=1A 2 i,j)\n1/2, for which the optimal rank-k approximation can be obtained via the singular value decomposition (SVD). Using randomization and approximation, one can compute an α = 1 + -approximation, for any > 0, in time much faster than the min(mn2,mn2) time required for computing the SVD, namely, in O(‖A‖0 + n · poly(k/ )) time [9, 26, 29], where ‖A‖0 denotes the number of non-zero entries\n∗This work has been funded by the Cluster of Excellence “Multimodal Computing and Interaction” within the Excellence Initiative of the German Federal Government.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nof A. For the Frobenius norm ‖A‖0 time is also a lower bound, as any algorithm that does not read nearly all entries of A might not read a very large entry, and therefore cannot achieve a relative error approximation.\nThe rank-k matrix Ak obtained by computing the SVD is also optimal with respect to any rotationally invariant norm, such as the operator and Schatten-p norms. Thus, such norms can also be solved exactly in polynomial time. Recently, however, there has been considerable interest [10, 3, 32] in obtaining low rank approximations for NP-hard error measures such as the entrywise `p-norm\n‖A‖p = (∑ i,j |Ai,j |p )1/p\n, where p ≥ 1 is a real number. Note that for p < 1, this is not a norm, though it is still a well-defined quantity. For p =∞, this corresponds to the max-norm or Chebyshev norm. It is known that one can achieve a poly(k log(mn))-approximation in poly(mn) time for the low-rank approximation problem with entrywise `p-norm for every p ≥ 1 [36, 8].\n1.1 `0-Low Rank Approximation\nA natural variant of low rank approximation which the results above do not cover is that of `0-low rank approximation, where the measure ‖A‖0 is the number of non-zero entries. In other words, we seek a rank-k matrixA′ for which the number of entries (i, j) withA′i,j 6= Ai,j is as small as possible. Letting OPT = minrank(B)=k ∑ i,j δ(Ai,j 6= A′i,j), where δ(Ai,j 6= A′i,j) = 1 if Ai,j 6= A′i,j and 0 otherwise, we would like to output a rank-k matrix A′ for which there are at most αOPT entries (i, j) with A′i,j 6= Ai,j . Approximation algorithms for this problem are essential since solving the problem exactly is NP-hard [12, 14], even when k = 1 and A is a binary matrix.\nThe `0-low rank approximation problem is quite natural for problems with no underlying metric, and its goal is to minimize the number of disagreeing data positions with a low rank matrix. Indeed, this error measure directly answers the following question: if we are allowed to ignore some data - outliers or anomalies - what is the best low-rank model we can get? One well-studied case is whenA is binary, but A′ and its factors U and V need not necessarily be binary. This is called unconstrained Binary Matrix Factorization in [18], which has applications to association rule mining [20], biclustering structure identification [42, 43], pattern discovery for gene expression [34], digits reconstruction [25], mining high-dimensional discrete-attribute data [21, 22], market based clustering [23], and document clustering [43]. There is also a body of work on Boolean Matrix Factorization which restricts the factors to also be binary, which is referred to as constrained Binary Matrix Factorization in [18]. This is motivated in applications such as classifying text documents and there is a large body of work on this, see, e.g. [28, 31].\nThe `0-low rank approximation problem coincides with a number of problems in different areas. It exactly coincides with the famous matrix rigidity problem over the reals, which asks for the minimal number OPT of entries of A that need to be changed in order to obtain a matrix of rank at most k. The matrix rigidity problem is well-studied in complexity theory [15, 16, 39] and parameterized complexity [13]. These works are not directly relevant here as they do not provide approximation algorithms. There are also other variants of `0-low rank approximation, corresponding to cases such as when A is binary, A′ = UV is required to have binary factors U and V , and multiplication is either performed over a binary field [41, 17, 12, 30], or corresponds to an OR of ANDs. The latter is known as the Boolean model [4, 12, 27, 33, 35, 38]. These different notions of inner products lead to very different algorithms and results for the `0-low rank approximation problem. However, all these models coincide in the special and important case in which A is binary and k = 1. This case was studied in [20, 34, 18], as their algorithm for k = 1 forms the basis for their successful heuristic for general k, e.g. the PROXIMUS technique [20].\nAnother related problem is robust PCA [6], in which there is an underlying matrix A that can be written as a low rank matrix L plus a sparse matrix S [7]. Candès et al. [7] argue that both components are of arbitrary magnitude, and we do not know the locations of the non-zeros in S nor how many there are. Moreover, grossly corrupted observations are common in image processing, web data analysis, and bioinformatics where some measurements are arbitrarily corrupted due to occlusions, malicious tampering, or sensor failures. Specific scenarios include video surveillance, face recognition, latent semantic indexing, and ranking of movies, books, etc. [7]. These problems have the common theme of being an arbitrary magnitude sparse perturbation to a low rank matrix with no natural underlying metric, and so the `0-error measure (which is just the Hamming distance, or number of disagreements) is appropriate. In order to solve robust PCA in practice, Candès et al. [7]\nrelaxed the `0-error measure to the `1-norm. Understanding theoretical guarantees for solving the original `0-problem is of fundamental importance, and we study this problem in this paper.\nFinally, interpreting 00 as 0, the `0-low rank approximation problem coincides with the aforementioned notion of entrywise `p-approximation when p = 0. It is not hard to see that previous work [8] for general p ≥ 1 fails to give any approximation factor for p = 0. Indeed, critical to their analysis is the scale-invariance property of a norm, which does not hold for p = 0 since `0 is not a norm."
    }, {
      "heading" : "1.2 Our Results",
      "text" : "We provide approximation algorithms for the `0-low rank approximation problem which significantly improve the running time or approximation factor of previous work. In some cases our algorithms even run in sublinear time, i.e., faster than reading all non-zero entries of the matrix. This is provably impossible for other measures such as the Frobenius norm and more generally, any `p-norm for p > 0. For k > 1, our approximation algorithms are, to the best of our knowledge, the first with provable guarantees for this problem.\nFirst, for k = 1, we significantly improve the polynomial running time of previous (2 + )- approximations for this problem. The best previous algorithm due to Jiang et al. [18] was based on the observation that there exists a column u of A spanning a 2-approximation. Therefore, solving the problem minv ‖A−uv‖0 for each column u of A yields a 2-approximation, where for a matrix B the measure ‖B‖0 counts the number of non-zero entries. The problem minv ‖A− uv‖0 decomposes into ∑ i mini ‖A:,i − viu‖0, where A:,i is the i-th column of A, and vi the i-th entry of vector v. The optimal vi is the mode of the ratios Ai,j/uj , where j ranges over indices in {1, 2, . . . ,m} with uj 6= 0. As a result, one can find a rank-1 matrix uvT providing a 2-approximation in O(‖A‖0n) time, which was the best known running time. Somewhat surprisingly, we show that one can achieve sublinear time for solving this problem. Namely, we obtain a (2 + )-approximation in (m + n) poly( −1ψ−1 log(mn)) time, for any > 0, where ψ = OPT /‖A‖0. This significantly improves upon the earlier O(‖A‖0n) time for not too small and ψ. Our result should be contrasted to Frobenius norm low rank approximation, for which Ω(‖A‖0) time is required even for k = 1, as otherwise one might miss a very large entry in A. Since `0-low rank approximation is insensitive to the magnitude of entries of A, we bypass this general impossibility result.\nNext, still considering the case of k = 1, we show that if the matrix A is binary, a well-studied case coinciding with the abovementioned GF (2) and Boolean models, we obtain an approximation algorithm parameterized in terms of the ratio ψ = OPT /‖A‖0, showing it is possible in time (m+n)ψ−1 poly(log(mn)) to obtain a (1 +O(ψ))-approximation. Note that our algorithm is again sublinear, unlike all algorithms in previous work. Moreover, when A is itself very well approximated by a low rank matrix, then ψ may actually be sub-constant, and we obtain a significantly better (1 + o(1))-approximation than the previous best known 2-approximations. Thus, we simultaneously improve the running time and approximation factor. We also show that the running time of our algorithm is optimal up to poly(log(mn)) factors by proving that any (1 + O(ψ))-approximation succeeding with constant probability must read Ω((m+ n)ψ−1) entries of A in the worst case.\nFinally, for arbitrary k > 1, we first give an impractical algorithm that runs in time nO(k) and achieves an α = poly(k)-approximation. To the best of our knowledge this is the first approximation algorithm for the `0-low rank approximation problem with any non-trivial approximation factor. To make our algorithm practical, we reduce the running time to poly(mn), with an exponent independent of k, if we allow for a bicriteria solution. In particular, we allow the algorithm to output a matrix A′ of somewhat larger rank O(k log(n/k)), for which ‖A−A′‖0 ≤ O(k2 log(n/k)) minrank(B)=k ‖A−B‖0. Although we do not obtain rank exactly k, many of the motivations for finding a low rank approximation, such as reducing the number of parameters and fast matrix-vector product, still hold if the output rank is O(k log(n/k)). We are not aware of any alternative algorithms which achieve poly(mn) time and any provable approximation factor, even for bicriteria solutions."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "For an matrix A ∈ Am×n with entries Ai,j , we write Ai,: for its i-th row and A:,j for its j-th column.\nInput Formats We always assume that we have random access to the entries of the given matrix A, i.e. we can read any entry Ai,j in constant time. For our sublinear time algorithms we need more efficient access to the matrix, specifically the following two variants:\n(1) We say that we are given A with column adjacency arrays if we are given arrays B1, . . . , Bn and lengths `1, . . . , `n such that for any 1 ≤ k ≤ `j the pair Bj [k] = (i, Ai,j) stores the row i containing the k-th nonzero entry in column j as well as that entry Ai,j . This is a standard representation of matrices used in many applications. Note that given only these adjacency arrays B1, . . . , Bn, in order to access any entry Ai,j we can perform a binary search over Bj , and hence random access to any matrix entry is in time O(log n). Moreover, we assume to have random access to matrix entries in constant time, and note that this is optimistic by at most a factor O(log n). (2) We say that we are given matrix A with row and column sums if we can access the numbers∑ j Ai,j for i ∈ [m] and ∑ iAi,j for j ∈ [n] in constant time (and, as always, access any entry Ai,j in constant time). Notice that storing the row and column sums takes O(n + m) space, and thus while this might not be standard information it is very cheap to store.\nWe show that the first access type even allows to sample from the set of nonzero entries uniformly in constant time.\nLemma 1. Given a matrix A ∈ Rm×n with column adjacency arrays, after O(n) time preprocessing we can sample a uniformly random nonzero entry (i, j) from A in time O(1).\nThe proof of this lemma, as well as most other proofs in this extended abstract, can be found in the full version of the paper.\n3 Algorithms for Real `0-rank-k\nGiven a matrix A ∈ Rm×n, the `0-rank-k problem asks to find a matrix A′ with rank k such that the difference between A and A′ measured in `0 norm is minimized. We denote the optimum value by\nOPT(k) def = min rank(A′)=k ‖A−A′‖0 = min U∈Rm×k, V ∈Rk×n ‖A− UV ‖0 . (1)\nIn this section, we establish several new results on the `0-rank-k problem. In Subsection 3.1, we prove a structural lemma that shows the existence of k columns which provide a (k + 1)-approximation to OPT(k), and we also give an Ω(k)-approximation lower bound for any algorithm that selects k columns from the input matrix A. In Subsection 3.2, we give an approximation algorithm that runs in poly(nk,m) time and achieves an O(k2)-approximation. To the best of our knowledge, this is the first algorithm with provable non-trivial approximation guarantees. In Subsection 3.3, we design a practical algorithm that runs in poly(n,m) time with an exponent independent of k, if we allow for a bicriteria solution."
    }, {
      "heading" : "3.1 Structural Results",
      "text" : "We give a new structural result for `0 showing that any matrix A contains k columns which provide a (k + 1)-approximation for the `0-rank-k problem (1).\nLemma 2. Let A ∈ Rm×n be a matrix and k ∈ [n]. There is a subset J (k) ⊂ [n] of size k and a matrix Z ∈ Rk×n such that ‖A−A:,J(k)Z‖0 ≤ (k + 1)OPT(k).\nProof. Let Q(0) be the set of columns j with UV:,j = 0, and let R(0) def = [n] \\Q(0). Let S(0) def= [n], T (0) def = ∅. We split the value OPT(k) into OPT(S(0), R(0)) def= ‖AS(0),R(0) − UVS(0),R(0)‖0 and OPT(S(0), Q(0)) def = ‖AS(0),Q(0) − UVS(0),Q(0)‖0 = ‖AS(0),Q(0)‖0.\nSuppose OPT(S(0), R(0)) ≥ |S(0)||R(0)|/(k + 1). Then, for any subset J (k) it follows that minZ‖A−AS(0),J(k)Z‖0 ≤ |S(0)||R(0)|+ ‖AS(0),Q(0)‖0 ≤ (k + 1)OPT(k). Otherwise, there is a column i(1) such that ∥∥AS(0),i(1) − (UV )S(0),i(1)∥∥0 ≤ OPT(S(0), R(0))/|R(0)| ≤ OPT(k)/|R(0)|.\nLet T (1) be the set of indices on which (UV )S(0),i(1) and AS(0),i(1) disagree, and similarly S(1) def = S(0)\\T (1) on which they agree. Then we have |T (1)| ≤ OPT(k)/|R(0)|. Hence, in the submatrix T (1) × R(0) the total error is at most |T (1)| · |R(0)| ≤ OPT(k). Let R(1), D(1) be a partitioning of R(0) such that AS(1),j is linearly dependent on AS(1),i(1) iff j ∈ D(1). Then by selecting column A:,i(1) the incurred cost on matrix S(1) ×D(1) is zero. For the remaining submatrix S(`) ×R(`), we perform a recursive call of the algorithm.\nWe make at most k recursive calls, on instances S(`) × R(`) for ` ∈ {0, . . . , k − 1}. In the `th iteration, either OPT(S(`), R(`)) ≥ |S(`)||R(`)|/(k + 1 − `) and we are done, or there is a column i(`+1) which partitions S(`) into S(`+1), T (`+1) and R(`) into R(`+1), D(`+1) such that |S(`+1)| ≥ m · ∏` i=0(1− 1 k+1−i ) = k−` k+1 ·m and for every j ∈ D\n(`) the column AS(`+1),j belongs to the span of {AS(`+1),i(t)}`+1t=1 .\nSuppose we performed k recursive calls. We show now that the incurred cost in submatrix S(k)×R(k) is at most OPT(S(k), R(k)) ≤ OPT(k). By construction, the sub-columns {AS(k),i}i∈I(k) are linearly independent, where I(k) = {i(1), . . . , i(k)} is the set of the selected columns, and AS(k),I(k) = (UV )S(k),I(k) . Since rank(AS(k),I(k)) = k, it follows that rank(US(k),:) = k, rank(V:,I(k)) = k and the matrix V:,I(k) ∈ Rk×k is invertible. Hence, for matrix Z = (V:,I(k))−1V:,Rk we have OPT(S(k), R(k)) = ‖ASk,Rk −ASk,IkZ‖0.\nThe statement follows by noting that the recursive calls accumulate a total cost of at most k ·OPT(k) in the submatrices T (`+1) × R(`) for ` ∈ {0, 1, . . . , k − 1}, as well as cost at most OPT(k) in submatrix S(k) ×R(k).\nWe also show that any algorithm that selects k columns of a matrix A incurs at least an Ω(k)approximation for the `0-rank-k problem.\nLemma 3. Let k ≤ n/2. Suppose A = (Gk×n; In×n) ∈ R(n+k)×n is a matrix composed of a Gaussian random matrix G ∈ Rk×n with Gi,j ∼ N(0, 1) and identity matrix In×n. Then for any subset J (k) ⊂ [n] of size k, we have minZ∈Rk×n‖A−A:,J(k)Z‖0 = Ω(k) ·OPT(k)."
    }, {
      "heading" : "3.2 Basic Algorithm",
      "text" : "We give an impractical algorithm that runs in poly(nk,m) time and achieves anO(k2)-approximation. To the best of our knowledge this is the first approximation algorithm for the `0-rank-k problem with non-trivial approximation guarantees. Theorem 4. Given A ∈ Rm×n and k ∈ [n] we can compute in O(nk+1m2kω+1) time a set of k indices J (k) ⊂ [n] and a matrix Z ∈ Rk×n such that ‖A−A:,J(k)Z‖0 ≤ O(k2) ·OPT(k).\nOur result relies on a subroutine by Berman and Karpinski [5] (attributed also to Kannan in that paper) which given a matrix U and a vector b approximates minx ‖Ux − b‖0 in polynomial time. Specifically, we invoke in our algorithm the following variant of this result established by Alon, Panigrahy, and Yekhanin [2]. Theorem 5. [2] There is an algorithm that given A ∈ Rm×k and b ∈ Rm outputs in O(m2kω+1) time a vector z ∈ Rk such that w.h.p. ‖Az − b‖0 ≤ k ·minx ‖Ax− b‖0."
    }, {
      "heading" : "3.3 Bicriteria Algorithm",
      "text" : "Our main contribution in this section is to design a practical algorithm that runs in poly(n,m) time with an exponent independent of k, if we allow for a bicriteria solution. Theorem 6. Given A ∈ Rm×n and k ∈ [1, n], there is an algorithm that in expected poly(m,n) time outputs a subset of indices J ⊂ [n] with |J | = O(k log(n/k)) and a matrix Z ∈ R|J|×n such that ‖A−A:,JZ‖0 ≤ O(k 2 log(n/k)) ·OPT(k).\nThe structure of the proof follows a recent approximation algorithm [8, Algorithm 3] for the `p-low rank approximation problem, for any p ≥ 1. We note that the analysis of [8, Theorem 7] is missing an\nO(log1/p n) approximation factor, and naïvely provides an O(k log1/p n)-approximation rather than the stated O(k)-approximation. Further, it might be possible to obtain an efficient algorithm yielding an O(k2 log k)-approximation for Theorem 6 using unpublished techniques in [37]; we leave the study of obtaining the optimal approximation factor to future work.\nThere are two critical differences with the proof of [8, Theorem 7]. We cannot use the earlier [8, Theorem 3] which shows that any matrixA contains k columns which provide anO(k)-approximation for the `p-low rank approximation problem, since that proof requires p ≥ 1 and critically uses scale-invariance, which does not hold for p = 0. Our combinatorial argument in Lemma 2 seems fundamentally different than the maximum volume submatrix argument in [8] for p ≥ 1. Second, unlike for `p-regression for p ≥ 1, the `0-regression problem minx ‖Ux−b‖0 given a matrix U and vector b is not efficiently solvable since it corresponds to a nearest codeword problem, which is NP-hard [1]. Thus, we resort to an approximation algorithm for `0-regression, based on ideas for solving the nearest codeword problem in [2, 5].\nNote that OPT(k) ≤ ‖A‖0. Since there are only mn + 1 possibilities of OPT(k), we can assume we know OPT(k) and we can run the Algorithm 1 below for each such possibility, obtaining a rank-O(k log n) solution, and then outputting the solution found with the smallest cost. This can be further optimized by forming instead O(log(mn)) guesses of OPT(k). One of these guesses is within a factor of 2 from the true value of OPT(k), and we note that the following argument only needs to know OPT(k) up to a factor of 2.\nWe start by defining the notion of approximate coverage, which is different than the corresponding notion in [8] for p ≥ 1, due to the fact that `0-regression cannot be efficiently solved. Consequently, approximate coverage for p = 0 cannot be efficiently tested. Let Q ⊆ [n] and M = A:,Q be an m× |Q| submatrix of A. We say that a column M:,i is (S,Q)-approximately covered by a submatrix M:,S of M , if |S| = 2k and minx ‖M:,Sx−M:,i‖0 ≤ 100(k+1)OPT (k)\n|Q| .\nLemma 7. (Similar to [8, Lemma 6], but using Lemma 2) LetQ ⊆ [n] andM = A:,Q be a submatrix of A. Suppose we select a subset R of 2k uniformly random columns of M . Then with probability at least 1/3, at least a 1/10 fraction of the columns of M are (R,Q)-approximately covered.\nProof. To show this, as in [8], consider a uniformly random column index i not in the set R. Let T def = R ∪ {i} and η def= minrank(B)=k‖M:,T −B‖0. Since T is a uniformly random subset of 2k + 1\ncolumns of M , ET η ≤ (2k+1)OPT\n(k) M |Q| ≤ (2k+1)OPT(k) |Q| . Let E1 be the event η ≤ 10(2k+1)OPT(k)\n|Q| . Then, by a Markov bound, Pr[E1] ≥ 9/10. Fix a configuration T = R ∪ {i} and let L(T ) ⊂ T be the subset guaranteed by Lemma 2 such that |L(T )| = k and minX‖M:,L(T )X−M:,T ‖0 ≤ (k + 1) minrank(B)=k‖M:,T −B‖0. Notice that Ei [ minx‖M:,L(T )x−M:,i‖0 | T ] = 12k+1 minX‖M:,L(T )X −M:,T ‖0, and thus by the law of total\nprobability we have ET [ minx ‖M:,L(T )x−M:,i‖0 ] ≤ (k+1)η2k+1 .\nLet E2 denote the event that minx ‖M:,Lx−M:,i‖0 ≤ 10(k+1)η2k+1 . By a Markov bound, Pr[E2] ≥ 9/10. Further, as in [8], let E3 be the event that i /∈ L. Observe that there are ( k+1 k ) ways to choose a subset\nR′ ⊂ T such that |R′| = 2k and L ⊂ R′. Since there are ( 2k+1 2k ) ways to choose R′, it follows that Pr[L ⊂ R | T ] = k+12k+1 > 1/2. Hence, by the law of total probability, we have Pr[E3] > 1/2.\nAs in [8], Pr[E1 ∧ E2 ∧ E3] > 2/5, and conditioned on E1 ∧ E2 ∧ E3, minx ‖M:,Rx −M:,i‖0 ≤ minx ‖M:,Lx −M:,i‖0 ≤ 10(k+1)η2k+1 ≤ 100(k+1)OPT(k)\n|Q| , where the first inequality uses that L is a subset of R given E3, and so the regression cost cannot decrease, while the second inequality uses the occurrence of E2 and the final inequality uses the occurrence of E1. As in [8], if Zi is an indicator random variable indicating whether i is approximately covered by R, and Z = ∑ i∈Q Zi, then ER[Z] ≥ 2|Q| 5 and ER[|Q| − Z] ≤ 3|Q| 5 . By a Markov bound, Pr[|Q| − Z ≥ 9|Q|10 ] ≤ 2 3 . Thus, probability at least 1/3, at least a 1/10 fraction of the columns of M are (R,Q)-approximately covered.\nAlgorithm 1 Selecting O(k log(n/k)) columns of A. Require: An integer k, and a matrix A. Ensure: O(k log(n/k)) columns of A APPROXIMATELYSELECTCOLUMNS (k,A):\nif number of columns of A ≤ 2k then return all the columns of A else repeat\nLet R be a set of 2k uniformly random columns of A until at least (1/10)-fraction columns of A are nearly approximately covered Let AR be the columns of A not nearly approximately covered by R return R ∪ APPROXIMATELYSELECTCOLUMNS(k,AR)\nend if\nGiven Lemma 7, we are ready to prove Theorem 6. As noted above, a key difference with the corresponding [8, Algorithm 3] for `p and p ≥ 1, is that we cannot efficiently test if a column i is approximately covered by a set R. We will instead again make use of Theorem 5.\nProof of Theorem 6. The computation of matrix Z force us to relax the notion of (R,Q)approximately covered to the notion of (R,Q)-nearly-approximately covered as follows: we say that a column M:,i is (R,Q)-nearly-approximately covered if, the algorithm in Theorem 5 returns a vector z such that ‖M:,Rz −M:,i‖0 ≤ 100(k+1) 2OPT(k)\n|Q| . By the guarantee of Theorem 5, if M:,i is (R,Q)-approximately covered then it is also w.h.p. (R,Q)-nearly-approximately covered.\nSuppose Algorithm 1 makes t iterations and let A:,∪ti=1Ri and Z be the resulting solution. We bound now its cost. LetB0 = [n], and consider the i-th iteration of Algorithm 1. We denote byRi a set of 2k uniformly random columns of Bi−1, by Gi a set of columns that is (Ri, Bi−1)-nearly-approximately covered, and by Bi = Bi−1\\{Gi ∪ Ri} a set of the remaining columns. By construction, |Gi| ≥ |Bi−1|/10 and |Bi| ≤ 910 |Bi−1| − 2k < 9 10 |Bi−1|. Since Algorithm 1 terminates when Bt+1 ≤ 2k, we have 2k < |Bt| < (1 − 110 ) tn, and thus the number of iterations t ≤ 10 log(n/2k). By\nconstruction, |Gi| = (1 − αi)|Bi−1| for some αi ≤ 9/10, and so ∑t i=1 |Gi| |Bi−1| ≤ t ≤ 10 log n 2k . Since minx(j)‖A:,Rix(j)−A:,j‖0 ≤ 100(k+1)2OPT(k) |Bi−1| , we have ∑t i=1 ∑ j∈Gi‖A:,Riz\n(j)−A:,j‖0 ≤∑t i=1 ∑ j∈Gi k ·minx(j)‖A:,Rix (j) −A:,j‖0 ≤ O ( k2 · log n2k ) ·OPT(k).\nBy Lemma 7, the expected number of iterations of selecting a set Ri such that |Gi| ≥ 1/10|Bi−1| is O(1). Since the number of recursive calls t is bounded by O(log(n/k)), it follows by a Markov bound that Algorithm 1 chooses O(k log(n/k)) columns in total. Since the approximation algorithm of Theorem 5 runs in polynomial time, our entire algorithm has expected polynomial time.\n4 Algorithm for Real `0-rank-1\nGiven a matrix A ∈ Rm×n, the `0-rank-1 problem asks to find a matrix A′ with rank 1 such that the difference between A and A′ measured in `0 norm is minimized. We denote the optimum value by\nOPT(1) def = min rank(A′)=1 ‖A−A′‖0 = min u∈Rm, v∈Rn ‖A− uvT ‖0. (2)\nIn the trivial case when OPT(1) = 0, there is an optimal algorithm that runs in time O(‖A‖0) and finds the exact rank-1 decomposition uvT of a matrix A. In this work, we focus on the case when OPT(1) ≥ 1. We show that Algorithm 2 yields a (2 + )-approximation factor and runs in nearly linear time in ‖A‖0, for any constant > 0. Furthermore, a variant of our algorithm even runs in sublinear time, if ‖A‖0 is large and ψ def = OPT(1)/‖A‖0 is not too small. In particular, we obtain time o(‖A‖0) when OPT(1) ≥ ( −1 log(mn))4 and ‖A‖0 ≥ n( −1 log(mn))4.\nAlgorithm 2 Input: A ∈ Rm×n and ∈ (0, 0.1). 1. Partition the columns of A into weight-classes S = {S(0), . . . , S(logn+1)} such that S(0) contains all columns j with ‖A:,j‖0 = 0 and S(i) contains all columns j with 2i−1 ≤ ‖A:,j‖0 < 2i. 2. For each weight-class S(i) do:\n2.1 Sample a set C(i) of Θ( −2 log n) elements uniformly at random from S(i). 2.2 Find a vector z(j) ∈ Rn such that ‖A−A:,j [z(j)]T ‖0 ≤ ( 1 + 15 ) minv ‖A−A:,jvT ‖0, for\neach column A:,j ∈ C(i). 3. Compute a (1 + 15 )-approximation Yj of ‖A−A:,j [z (j)]T ‖0 for every j ∈ ⋃ i∈[|S|] C\n(i). Return: the pair (A:,j , z(j)) corresponding to the minimal value Yj .\nTheorem 8. There is an algorithm that, given A ∈ Rm×n with column adjacency arrays and OPT(1) ≥ 1, and given ∈ (0, 0.1], runs w.h.p. in time O (( n logm 2 + min { ‖A‖0, n+ ψ−1 logn 2 }) log2 n 2 ) and outputs a column A:,j and a vector z that\nsatisfy w.h.p. ‖A−A:,jzT ‖0 ≤ (2 + )OPT(1). The algorithm also computes a value Y satisfying w.h.p. (1− )OPT(1) ≤ Y ≤ (2 + 2 )OPT(1).\nThe only steps for which the implementation details are not immediate are Steps 2.2 and 3. We will discuss them in Sections 4.1 and 4.2, respectively. Note that the algorithm from Theorem 8 selects a column A:,j and then finds a good vector z such that the product A:,jzT approximates A. We show that the approximation guarantee 2 + is essentially tight for algorithms following this pattern.\nLemma 9. There exist a matrix A ∈ Rn×n such that minz‖A − A:,jzT ‖0 ≥ 2(1 − 1/n)OPT(1), for every column A:,j ."
    }, {
      "heading" : "4.1 Implementing Step 2.2",
      "text" : "The Step 2.2 of Algorithm 2 uses the following sublinear procedure, given in Algorithm 3. Lemma 10. Given A ∈ Rm×n, u ∈ Rm and ∈ (0, 1) we can compute in O( −2n logm) time a vector z ∈ Rn such that w.h.p. ‖A:,i − ziu‖0 ≤ (1 + ) minvi‖A:,i − viu‖0 for every i ∈ [n].\nAlgorithm 3 Input: A ∈ Rm×n, u ∈ Rm and ∈ (0, 1). Let Z def= Θ( −2 logm), N def= supp(u), and p def= Z/|N |. 1. Select each index i ∈ N with probability p and let S be the resulting set. 2. Compute a vector z ∈ Rn such that zj = arg minr∈R‖AS,j − r · uS‖0 for all j ∈ [n]. Return: vector z."
    }, {
      "heading" : "4.2 Implementing Step 3",
      "text" : "In Step 3 of Algorithm 2 we want to compute a (1 + 15 )-approximation Yj of ‖A− A:,j [z (j)]T ‖0 for every j ∈ ⋃ i∈[|S|] C\n(i). We present two solutions, an exact algorithm (see Lemma 11) and a sublinear time sampling-based algorithm (see Lemma 13). Lemma 11. Suppose A,B ∈ Rm×n are represented by column adjacency arrays. Then, we can compute in O(‖A‖0 + n) time the measure ‖A−B‖0.\nFor our second, sampling-based implementation of Step 3, we make use of an algorithm by Dagum et al. [11] for estimating the expected value of a random variable. We note that the runtime of their algorithm is a random variable, the magnitude of which is bounded w.h.p. within a certain range.\nTheorem 12. [11] Let X be a random variable taking values in [0, 1] with µ def= E[X] > 0. Let 0 < , δ < 1 and ρX = max{Var[X], µ}. There is an algorithm with sample access to X that computes an estimator µ̃ in time t such that for a universal constant c we have: i) Pr[(1− )µ ≤ µ̃ ≤ (1 + )µ] ≥ 1− δ, and ii) Pr[t ≥ c −2 log(1/δ)ρX/µ2] ≤ δ.\nWe state now our key technical insight, on which we build upon our sublinear algorithm.\nLemma 13. There is an algorithm that, given A,B ∈ Rm×n with column adjacency arrays and ‖A−B‖0 ≥ 1, and given > 0, computes an estimator Z that satisfies w.h.p. (1− )‖A−B‖0 ≤ Z ≤ (1 + )‖A−B‖0. The algorithm runs w.h.p. in time O(n+ −2 ‖A‖0+‖B‖0‖A−B‖0 log n}).\nWe present now our main result in this section.\nTheorem 14. There is an algorithm that, given A ∈ Rm×n with column adjacency arrays and OPT(1) ≥ 1, and given j ∈ [n], v ∈ Rm and ∈ (0, 1), outputs an estimator Y that satisfies w.h.p. (1 − )‖A − A:,jvT ‖0 ≤ Y ≤ (1 + )‖A − A:,jvT ‖0. The algorithm runs w.h.p. in time O(min{‖A‖0, n+ −2ψ−1 log n}), where ψ = OPT(1)/‖A‖0.\nTo implement Step 3 of Algorithm 2, we simply apply Theorem 14 with A, and v = z(j) to each sampled column j ∈ ⋃ 0≤i≤logn+1 C (i).\n5 Algorithms for Boolean `0-rank-1\nOur goal is to compute an approximate solution of the Boolean `0-rank-1 problem, defined by:\nOPT = OPTA def = min u∈{0,1}m, v∈{0,1}n ‖A− uvT ‖0, where A ∈ {0, 1}m×n. (3)\nIn practice, approximating a matrix A by a rank-1 matrix uvT makes most sense if A is close to being rank-1. Hence, the above optimization problem is most relevant when OPT ‖A‖0. In this section, we focus on the case OPT/‖A‖0 ≤ φ for sufficiently small φ > 0. We prove the following. Theorem 15. Given A ∈ {0, 1}m×n with row and column sums, and given φ ∈ (0, 1/80] with OPT/‖A‖0 ≤ φ, we can compute vectors ũ, ṽ with ‖A− ũṽT ‖0 ≤ (1 + 5φ)OPT + 37φ2‖A‖0 in time O(min{‖A‖0 +m+ n, φ−1(m+ n) log(mn)}).\nIn combination with Theorem 8 we obtain the following.\nTheorem 16. Given A ∈ {0, 1}m×n with column adjacency arrays and with row and column sums, for ψ = OPT/‖A‖0 we can compute vectors ũ, ṽ with ‖A − ũṽT ‖0 ≤ (1 + 500ψ)OPT in time w.h.p. O(min{‖A‖0 +m+ n, ψ−1(m+ n)} · log3(mn)).\nA variant of the algorithm from Theorem 15 can also be used to solve the Boolean `0-rank-1 problem exactly. This yields the following theorem, which in particular shows that the problem is in polynomial time when OPT ≤ O (√ ‖A‖0 log(mn) ) .\nTheorem 17. Given a matrix A ∈ {0, 1}m×n, if OPTA/‖A‖0 ≤ 1/240 then we can exactly solve the Boolean `0-rank-1 problem in time 2O(OPT/ √ ‖A‖0) · poly(mn).\n6 Lower Bounds for Boolean `0-rank-1\nWe give now a lower bound of Ω(n/φ) on the number of samples of any 1 +O(φ)-approximation algorithm for the Boolean `0-rank-1 problem, where OPT/‖A‖0 ≤ φ as before. Theorem 18. Let C ≥ 1. Given an n × n Boolean matrix A with column adjacency arrays and with row and column sums, and given √ log(n)/n φ ≤ 1/100C such that OPTA/‖A‖0 ≤ φ, computing a (1 + Cφ)-approximation of OPTA requires to read Ω(n/φ) entries of A.\nThe technical core of our argument is the following lemma.\nLemma 19. Let φ ∈ (0, 1/2). Let X1, . . . , Xk be Boolean random variables with expectations p1, . . . , pk, where pi ∈ {1/2− φ, 1/2 + φ} for each i. Let A be an algorithm which can adaptively obtain any number of samples of each random variable, and which outputs bits bi for every i ∈ [1 : k]. Suppose that with probability at least 0.95 over the joint probability space of A and the random samples, A outputs for at least a 0.95 fraction of all i that bi = 1 if pi = 1/2 + φ and bi = 0 otherwise. Then, with probability at least 0.05, A makes Ω(k/φ2) samples in total."
    } ],
    "references" : [ {
      "title" : "More on average case vs approximation complexity",
      "author" : [ "Michael Alekhnovich" ],
      "venue" : "Computational Complexity,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Deterministic approximation algorithms for the nearest codeword problem. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, 12th International Workshop",
      "author" : [ "Noga Alon", "Rina Panigrahy", "Sergey Yekhanin" ],
      "venue" : "APPROX 2009, and 13th International Workshop,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Computing a nonnegative matrix factorization - provably",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ravi Kannan", "Ankur Moitra" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Discovery of optimal factors in binary data via a novel method of matrix decomposition",
      "author" : [ "Radim Belohlávek", "Vilém Vychodil" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Approximating minimum unsatisfiability of linear equations",
      "author" : [ "Piotr Berman", "Marek Karpinski" ],
      "venue" : "In Proceedings of the Thirteenth Annual ACM-SIAM Symposium on Discrete Algorithms, January",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "Emmanuel J Candès", "Xiaodong Li", "Yi Ma", "John Wright" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "Emmanuel J. Candès", "Xiaodong Li", "Yi Ma", "John Wright" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Algorithms for $\\ell_p$ low-rank approximation",
      "author" : [ "Flavio Chierichetti", "Sreenivas Gollapudi", "Ravi Kumar", "Silvio Lattanzi", "Rina Panigrahy", "David P. Woodruff" ],
      "venue" : "In Proceedings of the 34th International Conference on Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2017
    }, {
      "title" : "Low rank approximation and regression in input sparsity time",
      "author" : [ "Kenneth L. Clarkson", "David P. Woodruff" ],
      "venue" : "In Symposium on Theory of Computing Conference,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Input sparsity and hardness for robust subspace approximation",
      "author" : [ "Kenneth L. Clarkson", "David P. Woodruff" ],
      "venue" : "In IEEE 56th Annual Symposium on Foundations of Computer Science,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "An optimal algorithm for monte carlo estimation",
      "author" : [ "Paul Dagum", "Richard M. Karp", "Michael Luby", "Sheldon M. Ross" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2000
    }, {
      "title" : "Low Rank Approximation of Binary Matrices: Column Subset Selection and Generalizations",
      "author" : [ "C. Dan", "K. Arnsfelt Hansen", "H. Jiang", "L. Wang", "Y. Zhou" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Matrix rigidity: Matrix theory from the viewpoint of parameterized complexity",
      "author" : [ "Fedor V. Fomin", "Daniel Lokshtanov", "S.M. Meesum", "Saket Saurabh", "Meirav Zehavi" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2017
    }, {
      "title" : "On the complexity of robust PCA and `1-norm low-rank matrix approximation",
      "author" : [ "Nicolas Gillis", "Stephen A. Vavasis" ],
      "venue" : "CoRR, abs/1509.09236,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Using the notions of separability and independence for proving the lower bounds on the circuit complexity (in russian)",
      "author" : [ "D. Grigoriev" ],
      "venue" : "Notes of the Leningrad branch of the Steklov Mathematical Institute,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1976
    }, {
      "title" : "Using the notions of separability and independence for proving the lower bounds on the circuit complexity",
      "author" : [ "D. Grigoriev" ],
      "venue" : "Journal of Soviet Math.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1980
    }, {
      "title" : "ICA over finite fields separability and algorithms",
      "author" : [ "Harold W. Gutch", "Peter Gruber", "Arie Yeredor", "Fabian J. Theis" ],
      "venue" : "Signal Processing,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "A clustering approach to constrained binary matrix factorization",
      "author" : [ "Peng Jiang", "Jiming Peng", "Michael Heath", "Rui Yang" ],
      "venue" : "In Data Mining and Knowledge Discovery for Big Data,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Proximus: a framework for analyzing very high dimensional discrete-attributed datasets",
      "author" : [ "Mehmet Koyutürk", "Ananth Grama" ],
      "venue" : "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2003
    }, {
      "title" : "Compression, clustering, and pattern discovery in very high-dimensional discrete-attribute data sets",
      "author" : [ "Mehmet Koyutürk", "Ananth Grama", "Naren Ramakrishnan" ],
      "venue" : "IEEE Trans. Knowl. Data Eng.,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2005
    }, {
      "title" : "Nonorthogonal decomposition of binary matrices for bounded-error data compression and analysis",
      "author" : [ "Mehmet Koyutürk", "Ananth Grama", "Naren Ramakrishnan" ],
      "venue" : "ACM Transactions on Mathematical Software (TOMS),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2006
    }, {
      "title" : "A general model for clustering binary data",
      "author" : [ "Tao Li" ],
      "venue" : "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "Randomized algorithms for matrices and data",
      "author" : [ "Michael W. Mahoney" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Modeling dyadic data with binary latent factors",
      "author" : [ "Edward Meeds", "Zoubin Ghahramani", "Radford M. Neal", "Sam T. Roweis" ],
      "venue" : "In Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    }, {
      "title" : "Low-distortion subspace embeddings in inputsparsity time and applications to robust linear regression",
      "author" : [ "Xiangrui Meng", "Michael W. Mahoney" ],
      "venue" : "In Symposium on Theory of Computing Conference,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    }, {
      "title" : "The discrete basis problem",
      "author" : [ "Pauli Miettinen", "Taneli Mielikäinen", "Aristides Gionis", "Gautam Das", "Heikki Mannila" ],
      "venue" : "IEEE Trans. Knowl. Data Eng.,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2008
    }, {
      "title" : "MDL4BMF: minimum description length for boolean matrix factorization",
      "author" : [ "Pauli Miettinen", "Jilles Vreeken" ],
      "venue" : "TKDD, 8(4):18:1–18:31,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "OSNAP: faster numerical linear algebra algorithms via sparser subspace embeddings",
      "author" : [ "Jelani Nelson", "Huy L. Nguyen" ],
      "venue" : "In 54th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    }, {
      "title" : "Generalized Independent Component Analysis Over Finite Alphabets",
      "author" : [ "A. Painsky", "S. Rosset", "M. Feder" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Boolean Matrix Factorization and Noisy Completion via Message Passing",
      "author" : [ "S. Ravanbakhsh", "B. Poczos", "R. Greiner" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "Weighted low rank approximations with provable guarantees",
      "author" : [ "Ilya P. Razenshteyn", "Zhao Song", "David P. Woodruff" ],
      "venue" : "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "A simple algorithm for topic identification in 0-1 data",
      "author" : [ "Jouni K. Seppänen", "Ella Bingham", "Heikki Mannila" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2003
    }, {
      "title" : "Mining discrete patterns via binary matrix factorization",
      "author" : [ "Bao-Hong Shen", "Shuiwang Ji", "Jieping Ye" ],
      "venue" : "In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2009
    }, {
      "title" : "Noisy-or component analysis and its application to link analysis",
      "author" : [ "Tomás Singliar", "Milos Hauskrecht" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2006
    }, {
      "title" : "Low rank approximation with entrywise",
      "author" : [ "Zhao Song", "David P. Woodruff", "Peilin Zhong" ],
      "venue" : "`1-norm error. CoRR,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2016
    }, {
      "title" : "Entrywise low rank approximation of general functions, 2018",
      "author" : [ "Zhao Song", "David P. Woodruff", "Peilin Zhong" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2018
    }, {
      "title" : "The role mining problem: finding a minimal descriptive set of roles",
      "author" : [ "Jaideep Vaidya", "Vijayalakshmi Atluri", "Qi Guo" ],
      "venue" : "In 12th ACM Symposium on Access Control Models and Technologies,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2007
    }, {
      "title" : "Graph-theoretic arguments in low-level complexity",
      "author" : [ "Leslie G. Valiant" ],
      "venue" : "In Mathematical Foundations of Computer Science",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 1977
    }, {
      "title" : "Sketching as a tool for numerical linear algebra",
      "author" : [ "David P. Woodruff" ],
      "venue" : "Foundations and Trends in Theoretical Computer Science,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2014
    }, {
      "title" : "Independent component analysis over galois fields of prime order",
      "author" : [ "Arie Yeredor" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2011
    }, {
      "title" : "Binary matrix factorization for analyzing gene expression data",
      "author" : [ "Zhong-Yuan Zhang", "Tao Li", "Chris Ding", "Xian-Wen Ren", "Xiang-Sun Zhang" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2010
    }, {
      "title" : "Binary matrix factorization with applications",
      "author" : [ "Zhongyuan Zhang", "Tao Li", "Chris Ding", "Xiangsun Zhang" ],
      "venue" : "In Data Mining,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "We refer the reader to several surveys [19, 24, 40] for references to the many results on low rank approximation.",
      "startOffset" : 39,
      "endOffset" : 51
    }, {
      "referenceID" : 38,
      "context" : "We refer the reader to several surveys [19, 24, 40] for references to the many results on low rank approximation.",
      "startOffset" : 39,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "Using randomization and approximation, one can compute an α = 1 + -approximation, for any > 0, in time much faster than the min(mn(2),mn(2)) time required for computing the SVD, namely, in O(‖A‖0 + n · poly(k/ )) time [9, 26, 29], where ‖A‖0 denotes the number of non-zero entries ∗This work has been funded by the Cluster of Excellence “Multimodal Computing and Interaction” within the Excellence Initiative of the German Federal Government.",
      "startOffset" : 218,
      "endOffset" : 229
    }, {
      "referenceID" : 24,
      "context" : "Using randomization and approximation, one can compute an α = 1 + -approximation, for any > 0, in time much faster than the min(mn(2),mn(2)) time required for computing the SVD, namely, in O(‖A‖0 + n · poly(k/ )) time [9, 26, 29], where ‖A‖0 denotes the number of non-zero entries ∗This work has been funded by the Cluster of Excellence “Multimodal Computing and Interaction” within the Excellence Initiative of the German Federal Government.",
      "startOffset" : 218,
      "endOffset" : 229
    }, {
      "referenceID" : 27,
      "context" : "Using randomization and approximation, one can compute an α = 1 + -approximation, for any > 0, in time much faster than the min(mn(2),mn(2)) time required for computing the SVD, namely, in O(‖A‖0 + n · poly(k/ )) time [9, 26, 29], where ‖A‖0 denotes the number of non-zero entries ∗This work has been funded by the Cluster of Excellence “Multimodal Computing and Interaction” within the Excellence Initiative of the German Federal Government.",
      "startOffset" : 218,
      "endOffset" : 229
    }, {
      "referenceID" : 9,
      "context" : "Recently, however, there has been considerable interest [10, 3, 32] in obtaining low rank approximations for NP-hard error measures such as the entrywise `p-norm ‖A‖p = (∑ i,j |Ai,j | )1/p , where p ≥ 1 is a real number.",
      "startOffset" : 56,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "Recently, however, there has been considerable interest [10, 3, 32] in obtaining low rank approximations for NP-hard error measures such as the entrywise `p-norm ‖A‖p = (∑ i,j |Ai,j | )1/p , where p ≥ 1 is a real number.",
      "startOffset" : 56,
      "endOffset" : 67
    }, {
      "referenceID" : 30,
      "context" : "Recently, however, there has been considerable interest [10, 3, 32] in obtaining low rank approximations for NP-hard error measures such as the entrywise `p-norm ‖A‖p = (∑ i,j |Ai,j | )1/p , where p ≥ 1 is a real number.",
      "startOffset" : 56,
      "endOffset" : 67
    }, {
      "referenceID" : 34,
      "context" : "It is known that one can achieve a poly(k log(mn))-approximation in poly(mn) time for the low-rank approximation problem with entrywise `p-norm for every p ≥ 1 [36, 8].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "It is known that one can achieve a poly(k log(mn))-approximation in poly(mn) time for the low-rank approximation problem with entrywise `p-norm for every p ≥ 1 [36, 8].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 11,
      "context" : "Approximation algorithms for this problem are essential since solving the problem exactly is NP-hard [12, 14], even when k = 1 and A is a binary matrix.",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "Approximation algorithms for this problem are essential since solving the problem exactly is NP-hard [12, 14], even when k = 1 and A is a binary matrix.",
      "startOffset" : 101,
      "endOffset" : 109
    }, {
      "referenceID" : 17,
      "context" : "This is called unconstrained Binary Matrix Factorization in [18], which has applications to association rule mining [20], biclustering structure identification [42, 43], pattern discovery for gene expression [34], digits reconstruction [25], mining high-dimensional discrete-attribute data [21, 22], market based clustering [23], and document clustering [43].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "This is called unconstrained Binary Matrix Factorization in [18], which has applications to association rule mining [20], biclustering structure identification [42, 43], pattern discovery for gene expression [34], digits reconstruction [25], mining high-dimensional discrete-attribute data [21, 22], market based clustering [23], and document clustering [43].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 40,
      "context" : "This is called unconstrained Binary Matrix Factorization in [18], which has applications to association rule mining [20], biclustering structure identification [42, 43], pattern discovery for gene expression [34], digits reconstruction [25], mining high-dimensional discrete-attribute data [21, 22], market based clustering [23], and document clustering [43].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 41,
      "context" : "This is called unconstrained Binary Matrix Factorization in [18], which has applications to association rule mining [20], biclustering structure identification [42, 43], pattern discovery for gene expression [34], digits reconstruction [25], mining high-dimensional discrete-attribute data [21, 22], market based clustering [23], and document clustering [43].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 32,
      "context" : "This is called unconstrained Binary Matrix Factorization in [18], which has applications to association rule mining [20], biclustering structure identification [42, 43], pattern discovery for gene expression [34], digits reconstruction [25], mining high-dimensional discrete-attribute data [21, 22], market based clustering [23], and document clustering [43].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 23,
      "context" : "This is called unconstrained Binary Matrix Factorization in [18], which has applications to association rule mining [20], biclustering structure identification [42, 43], pattern discovery for gene expression [34], digits reconstruction [25], mining high-dimensional discrete-attribute data [21, 22], market based clustering [23], and document clustering [43].",
      "startOffset" : 236,
      "endOffset" : 240
    }, {
      "referenceID" : 19,
      "context" : "This is called unconstrained Binary Matrix Factorization in [18], which has applications to association rule mining [20], biclustering structure identification [42, 43], pattern discovery for gene expression [34], digits reconstruction [25], mining high-dimensional discrete-attribute data [21, 22], market based clustering [23], and document clustering [43].",
      "startOffset" : 290,
      "endOffset" : 298
    }, {
      "referenceID" : 20,
      "context" : "This is called unconstrained Binary Matrix Factorization in [18], which has applications to association rule mining [20], biclustering structure identification [42, 43], pattern discovery for gene expression [34], digits reconstruction [25], mining high-dimensional discrete-attribute data [21, 22], market based clustering [23], and document clustering [43].",
      "startOffset" : 290,
      "endOffset" : 298
    }, {
      "referenceID" : 21,
      "context" : "This is called unconstrained Binary Matrix Factorization in [18], which has applications to association rule mining [20], biclustering structure identification [42, 43], pattern discovery for gene expression [34], digits reconstruction [25], mining high-dimensional discrete-attribute data [21, 22], market based clustering [23], and document clustering [43].",
      "startOffset" : 324,
      "endOffset" : 328
    }, {
      "referenceID" : 41,
      "context" : "This is called unconstrained Binary Matrix Factorization in [18], which has applications to association rule mining [20], biclustering structure identification [42, 43], pattern discovery for gene expression [34], digits reconstruction [25], mining high-dimensional discrete-attribute data [21, 22], market based clustering [23], and document clustering [43].",
      "startOffset" : 354,
      "endOffset" : 358
    }, {
      "referenceID" : 17,
      "context" : "There is also a body of work on Boolean Matrix Factorization which restricts the factors to also be binary, which is referred to as constrained Binary Matrix Factorization in [18].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 14,
      "context" : "The matrix rigidity problem is well-studied in complexity theory [15, 16, 39] and parameterized complexity [13].",
      "startOffset" : 65,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "The matrix rigidity problem is well-studied in complexity theory [15, 16, 39] and parameterized complexity [13].",
      "startOffset" : 65,
      "endOffset" : 77
    }, {
      "referenceID" : 37,
      "context" : "The matrix rigidity problem is well-studied in complexity theory [15, 16, 39] and parameterized complexity [13].",
      "startOffset" : 65,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "The matrix rigidity problem is well-studied in complexity theory [15, 16, 39] and parameterized complexity [13].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 39,
      "context" : "There are also other variants of `0-low rank approximation, corresponding to cases such as when A is binary, A′ = UV is required to have binary factors U and V , and multiplication is either performed over a binary field [41, 17, 12, 30], or corresponds to an OR of ANDs.",
      "startOffset" : 221,
      "endOffset" : 237
    }, {
      "referenceID" : 16,
      "context" : "There are also other variants of `0-low rank approximation, corresponding to cases such as when A is binary, A′ = UV is required to have binary factors U and V , and multiplication is either performed over a binary field [41, 17, 12, 30], or corresponds to an OR of ANDs.",
      "startOffset" : 221,
      "endOffset" : 237
    }, {
      "referenceID" : 11,
      "context" : "There are also other variants of `0-low rank approximation, corresponding to cases such as when A is binary, A′ = UV is required to have binary factors U and V , and multiplication is either performed over a binary field [41, 17, 12, 30], or corresponds to an OR of ANDs.",
      "startOffset" : 221,
      "endOffset" : 237
    }, {
      "referenceID" : 28,
      "context" : "There are also other variants of `0-low rank approximation, corresponding to cases such as when A is binary, A′ = UV is required to have binary factors U and V , and multiplication is either performed over a binary field [41, 17, 12, 30], or corresponds to an OR of ANDs.",
      "startOffset" : 221,
      "endOffset" : 237
    }, {
      "referenceID" : 3,
      "context" : "The latter is known as the Boolean model [4, 12, 27, 33, 35, 38].",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "The latter is known as the Boolean model [4, 12, 27, 33, 35, 38].",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 25,
      "context" : "The latter is known as the Boolean model [4, 12, 27, 33, 35, 38].",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 31,
      "context" : "The latter is known as the Boolean model [4, 12, 27, 33, 35, 38].",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 33,
      "context" : "The latter is known as the Boolean model [4, 12, 27, 33, 35, 38].",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 36,
      "context" : "The latter is known as the Boolean model [4, 12, 27, 33, 35, 38].",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 18,
      "context" : "This case was studied in [20, 34, 18], as their algorithm for k = 1 forms the basis for their successful heuristic for general k, e.",
      "startOffset" : 25,
      "endOffset" : 37
    }, {
      "referenceID" : 32,
      "context" : "This case was studied in [20, 34, 18], as their algorithm for k = 1 forms the basis for their successful heuristic for general k, e.",
      "startOffset" : 25,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : "This case was studied in [20, 34, 18], as their algorithm for k = 1 forms the basis for their successful heuristic for general k, e.",
      "startOffset" : 25,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "Another related problem is robust PCA [6], in which there is an underlying matrix A that can be written as a low rank matrix L plus a sparse matrix S [7].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 6,
      "context" : "Another related problem is robust PCA [6], in which there is an underlying matrix A that can be written as a low rank matrix L plus a sparse matrix S [7].",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "[7] argue that both components are of arbitrary magnitude, and we do not know the locations of the non-zeros in S nor how many there are.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "It is not hard to see that previous work [8] for general p ≥ 1 fails to give any approximation factor for p = 0.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : "[18] was based on the observation that there exists a column u of A spanning a 2-approximation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "Our result relies on a subroutine by Berman and Karpinski [5] (attributed also to Kannan in that paper) which given a matrix U and a vector b approximates minx ‖Ux − b‖0 in polynomial time.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "Specifically, we invoke in our algorithm the following variant of this result established by Alon, Panigrahy, and Yekhanin [2].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "[2] There is an algorithm that given A ∈ Rm×k and b ∈ R outputs in O(m(2)k) time a vector z ∈ R such that w.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 35,
      "context" : "Further, it might be possible to obtain an efficient algorithm yielding an O(k(2) log k)-approximation for Theorem 6 using unpublished techniques in [37]; we leave the study of obtaining the optimal approximation factor to future work.",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 7,
      "context" : "Our combinatorial argument in Lemma 2 seems fundamentally different than the maximum volume submatrix argument in [8] for p ≥ 1.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "Second, unlike for `p-regression for p ≥ 1, the `0-regression problem minx ‖Ux−b‖0 given a matrix U and vector b is not efficiently solvable since it corresponds to a nearest codeword problem, which is NP-hard [1].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 1,
      "context" : "Thus, we resort to an approximation algorithm for `0-regression, based on ideas for solving the nearest codeword problem in [2, 5].",
      "startOffset" : 124,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "Thus, we resort to an approximation algorithm for `0-regression, based on ideas for solving the nearest codeword problem in [2, 5].",
      "startOffset" : 124,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "We start by defining the notion of approximate coverage, which is different than the corresponding notion in [8] for p ≥ 1, due to the fact that `0-regression cannot be efficiently solved.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "To show this, as in [8], consider a uniformly random column index i not in the set R.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "Further, as in [8], let E3 be the event that i / ∈ L.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 7,
      "context" : "As in [8], Pr[E1 ∧ E2 ∧ E3] > 2/5, and conditioned on E1 ∧ E2 ∧ E3, minx ‖M:,Rx −M:,i‖0 ≤ minx ‖M:,Lx −M:,i‖0 ≤ 10(k+1)η 2k+1 ≤ 100(k+1)OPT |Q| , where the first inequality uses that L is a subset of R given E3, and so the regression cost cannot decrease, while the second inequality uses the occurrence of E2 and the final inequality uses the occurrence of E1.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "As in [8], if Zi is an indicator random variable indicating whether i is approximately covered by R, and Z = ∑ i∈Q Zi, then ER[Z] ≥ 2|Q| 5 and ER[|Q| − Z] ≤ 3|Q| 5 .",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 10,
      "context" : "[11] for estimating the expected value of a random variable.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Let X be a random variable taking values in [0, 1] with μ def = E[X] > 0.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "We study the `0-Low Rank Approximation Problem, where the goal is, given an m× n matrix A, to output a rank-k matrix A′ for which ‖A′ −A‖0 is minimized. Here, for a matrix B, ‖B‖0 denotes the number of its non-zero entries. This NPhard variant of low rank approximation is natural for problems with no underlying metric, and its goal is to minimize the number of disagreeing data positions. We provide approximation algorithms which significantly improve the running time and approximation factor of previous work. For k > 1, we show how to find, in poly(mn) time for every k, a rank O(k log(n/k)) matrix A′ for which ‖A′ −A‖0 ≤ O(k log(n/k)) OPT. To the best of our knowledge, this is the first algorithm with provable guarantees for the `0-Low Rank Approximation Problem for k > 1, even for bicriteria algorithms. For the well-studied case when k = 1, we give a (2+ )-approximation in sublinear time, which is impossible for other variants of low rank approximation such as for the Frobenius norm. We strengthen this for the well-studied case of binary matrices to obtain a (1 +O(ψ))-approximation in sublinear time, where ψ = OPT /‖A‖0. For small ψ, our approximation factor is 1 + o(1).",
    "creator" : null
  }
}