{
  "name" : "2131f8ecf18db66a758f718dc729e00e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Semisupervised Clustering, AND-Queries and Locally Encodable Source Coding",
    "authors" : [ "Arya Mazumdar" ],
    "emails" : [ "arya@cs.umass.edu", "soumyabratap@umass.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Suppose we have n elements, and the ith element has a labelXi ∈ {0, 1, . . . , k−1},∀i ∈ {1, . . . , n}. We consider the task of learning the labels of the elements (or learning the label vector). This can also be easily thought of as a clustering problem of n elements into k clusters, where there is a ground-truth clustering1. There exist various approaches to this problem in general. In many cases some similarity values between pair of elements are known (a high similarity value indicate that they are in the same cluster). Given these similarity values (or a weighted complete graph), the task is equivalent to to graph clustering; when perfect similarity values are known this is equivalent to finding the connected components of a graph.\nA recent approach to clustering has been via crowdsourcing. Suppose there is an oracle (expert labelers, crowd workers) with whom we can make pairwise queries of the form “do elements u and v belong to the same cluster?”. We will call this the ‘same cluster’ query (as per [4]). Based on the answers from the oracle, we then try to reconstruct the labeling or clustering. This idea has seen a recent surge of interest especially in the entity resolution research (see, for e.g. [33, 30, 8, 20]). Since each query to crowd workers cost time and money, a natural objective will be to minimize the number of queries to the oracle and still recover the clusters exactly. Carefully designed adaptive and interactive querying algorithms for clustering has also recently been developed [33, 30, 8, 22, 21]. In\n1The difference between clustering and learning labels is that in the case of clustering it is not necessary to know the value of the label for a cluster. Therefore any unsupervised labeling algorithm will be a clustering algorithm, however the reverse is not true. In this paper we are mostly concerned about the labeling problem, hence our algorithms (upper bounds) are valid for clustering as well.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nparticular, the query complexity for clustering with a k-means objective had recently been studied in [4], and there are significant works in designing optimal crowdsourcing schemes in general (see, [12, 13, 28, 34, 15]). Note that, a crowd worker may potentially handle more than two elements at a time; however it is of interest to keep the number of elements involved in a query as small as possible. As an example, recent work in [31] considers triangle queries (involving three elements in a query). Also crowd workers can compute some simple functions on this small set of inputs - instead of answering a ‘same cluster’ query. But again it is desirable that the answer the workers provide to be simple, such as a binary answer.\nThe queries to the oracle can be asked adaptively or non-adaptively. For the clustering problem, both the adaptive version and the nonadaptive versions have been studied. While both versions have obvious advantages and disadvantages, for crowdsourcing applications it is helpful to have a parallelizable querying scheme in most scenarios for faster response-rate and real time analysis. In this paper, we concentrate on the nonadaptive version of the problem, i.e., we perform the labeling algorithm after all the query answers are all obtained.\nBudgeted crowdsourcing problems can be quite straight-forwardly viewed as a canonical sourcecoding or source-channel coding problem of information theory (e.g., see the recent paper [14]). A main contribution of our paper is to view this as a locally encodable source coding problem: a data compression problem where each compressed bit depends only on a constant number of input bits. The notion of locally encodable source coding is not well-studied even within information theory community, and the only place where it is mentioned to the best of our knowledge is in [23], although the focus of that paper is a related notion of smooth encoding. Another related notion of local decoding seem to be much more well-studied [19, 18, 16, 26, 6, 25, 5, 32].\nBy posing the querying problem as such we can get a number of information theoretic lower bounds on the number of queries required to recover the correct labeling. We also provide nonadaptive schemes that are near optimal. Another of our main contributions is to show that even within queries with binary answers, ‘same cluster’ queries (or XOR queries) may not be the best possible choice. A smaller number of queries can be achieved for approximate recovery by using what we call an AND query. Among our settings, we also consider the case when the oracle gives incorrect answers with some probability. A simple scheme to reduce errors in this case could be to take a majority vote after asking the same question to multiple different crowd workers. However, often that is not sufficient. Experiments on several real datasets (see [21]) with answers collected from Amazon Mechanical Turk [9, 29] show that majority voting could even increase the errors. Interestingly, such an observation has been made by a recent paper as well [27, Figure 1]. The probability of error of a query answer may also be thought of as the aggregated answer after repeating the query several times. Once the answer has been aggregated, it cannot change – and thus it reduces to the model where repeating the same question multiple times is not allowed. On the other hand, it is usually assumed that the answers to different queries are independently erroneous (see [10]). Therefore we consider the case where repetition of a same query multiple times is not allowed2, however different queries can result in erroneous answers independently.\nIn this case, the best known algorithms need O(n log n) queries to perform the clustering with two clusters [21]. We show that by employing our AND querying method (1− δ)-proportion of all labels in the label vector will be recovered with only O(n log 1δ ) queries.\nAlong the way, we also provide new information theoretic results on fundamental limits of locally encodable source coding. While the the related notion of locally decodable source code [19, 16, 26, 6], as well as smooth compression [23, 26] have been studied, there was no nontrivial result known related to locally encodable codes in general. Although the focus of this paper is primarily theoretical, we also perform a real crowdsourcing experiment to validate our algorithm."
    }, {
      "heading" : "2 Problem Setup and Information Theoretic View",
      "text" : "For n elements, consider a label vector X ∈ {0, . . . , k − 1}n, where Xi, the ith entry of X , is the label of the ith element and can take one of k possible values. Suppose P (Xi = j) = pj∀j and Xi’s are independent. In other words, the prior distribution of the labels is given by the vector\n2Independent repetition of queries is also theoretically not interesting, as by repeating any query just O(logn) times one can reduce the probability of error to near zero.\np ≡ (p0, . . . , pk−1).For the special case of k = 2, we denote p0 ≡ 1 − p and p1 ≡ p. While we emphasize on the case of k = 2 our results extends in the case of larger k, as will be mentioned.\nA query Q : {0, . . . , k − 1}∆ → {0, 1} is a deterministic function that takes as argument ∆ labels, ∆ n, and outputs a binary answer. While the query answer need not be binary, we restrict ourselves mostly to this case for being the most practical choice.\nSuppose a total ofm queries are made and the query answers are given by Y ∈ {0, 1}m. The objective is to reconstruct the label vector X from Y , such that the number of queries m is minimized.\nWe assume our recovery algorithms to have the knowledge of p. This prior distribution, or the relative sizes of clusters, is usually easy to estimate by subsampling a small (O(log n)) subset of elements and performing a complete clustering within that set (by, say, all pairwise queries). In many prior works, especially in the recovery algorithms of popular statistical models such as stochastic block model, it is assumed that the relative sizes of the clusters are known (see [1]).\nWe also consider the setting where query answers may be erroneous with some probability of error. For crowdsourcing applications, this is a valid assumption since many times even expert labelers can make errors, and such assumption can be made. To model this we assume each entry of Y is flipped independently with some probability q. Such independence assumption has been used many times previously to model errors in crowdsourcing systems (see, e.g., [10]). While this may not be the perfect model, we do not allow a single query to be repeated multiple times in our algorithms (see the Introduction for a justification). For the analysis of our algorithm we just need to assume that the answers to different queries are independent. While we analyze our algorithms under these assumptions for theoretical guarantees, it turns out that even in real crowdsourcing situations our algorithmic results mostly follow the theoretical results, giving further validation to the model.\nFor the k = 2 case, and when q = 0 (perfect oracle), it is easy to see that n queries are sufficient for the task. One simply compares every element with the first element. This does not extend to the case when k > 2: for perfect recovery, and without any prior, one must make O(n2) queries in this case. When q > 0 (erroneous oracle), it has been shown that a total number of O(γnk log n) queries are sufficient [21], where γ is the ratio of the sizes of the largest and smallest clusters.\nInformation theoretic view. The problem of learning a label vector x from queries is very similar to the canonical source coding (data compression) problem from information theory. In the source coding problem, a (possibly random) vector X is ‘encoded’ into a usually smaller length binary vector called the compressed vector3 Y ∈ {0, 1}m. The decoding task is to again obtain X from the compressed vector Y . It is known that if X is distributed according to p, then m ≈ nH(p) is both necessary and sufficient to recover x with high probability, where H(p) = − ∑ i pi log pi is the entropy of p.\nWe can cast our problem in this setting naturally, where entries of Y are just answers to queries made on X . The main difference is that in source coding each Yi may potentially depend on all the entries of X; while in the case of label learning, each Yi may depend on only ∆ of the xis.\nWe call this locally encodable source coding. This terminology is analogous to the recently developed literature on locally decodable source coding [19, 16]. It is called locally encodable, because each compressed bit depend only on ∆ of the source (input) bits. For locally decodable source coding, each bit of the reconstructed sequence X̂ depends on at most a prescribed constant number ∆ of bits from the compressed sequence. Another closely related notion is that of ‘smooth compression’, where each source bit contributes to at most ∆ compressed bits [23]. Indeed, in [23], the notion of locally encodable source coding is also present where it was called robust compression. We provide new information theoretic lower bounds on the number of queries required to reconstruct X exactly and approximately for our problem.\nFor the case when there are only two labels, the ‘same cluster’ query is equivalent to an Boolean XOR operation between the labels. There are some inherent limitations to these functions that prohibit the ‘same cluster’ queries to achieve the best possible number of queries for the ‘approximate’ recovery of labeling problem. We use an old result by Massey [17] to establish this limitation. We show that, instead using an operation like Boolean AND, much smaller number of queries are able to recover most of the labels correctly.\n3The compressed vector is not necessarily binary, nor it is necessarily smaller length.\nWe also consider the case when the oracle gives faulty answer, or Y is corrupted by some noise (the binary symmetric channel). This setting is analogous to the problem of joint source-channel coding. However, just like before, each encoded bit must depend on at most ∆ bits. We show that for the approximate recovery problem, AND queries are again performing substantially well. In a real crowdsourcing experiment, we have seen that if crowd-workers have been provided with the same set of pairs and being asked for ‘same cluster’ queries as well as AND queries, the error-rate of AND queries is lower. The reason is that for a correct ‘no’ answer in an AND query, a worker just need to know one of the labels in the pair. For a ‘same cluster’ query, both the labels must be known to the worker for any correct answer.\nThere are multiple reasons why one would ask for a ‘combination’ or function of multiple labels from a worker instead of just asking for a label itself (a ‘label-query’). Note that, asking for labels will never let us recover the clusters in less than n queries, whereas, as we will see, the queries that combine labels will. Also in case of erroneous answer with AND queries or ‘same cluster’ queries, we have the option of not repeating a query, and still reduce errors. No such option is available with direct label-queries.\nContributions. In summary our contributions can be listed as follows.\n1. Noiseless queries and exact recovery (Sec. 3.1): For two clusters, we provide a querying scheme that asks αn, α < 1 number of nonadaptive pairwise ‘same cluster’ queries, and recovers the all labels with high probability, for a range of prior probabilities. We also provide a new lower bound that is strictly better than nH(p) for some p.\n2. Noiseless queries and approximate recovery (Sec. 3.2): We provide a new lower bound on the number of queries required to recover (1− δ) fraction of the labels δ > 0. We also show that ‘same cluster’ queries are insufficient, and propose a new querying strategy based on AND operation that performs substantially better.\n3. Noisy queries and approximate recovery (Sec. 3.3). For this part we assumed the query answer to be k-ary (k ≥ 2) where k is the number of clusters. This section contains the main algorithmic result that uses the AND queries as main primitive. We show that, even in the presence of noise in the query answers, it is possible to recover (1− δ) proportion of all labels correctly with only O(n log kδ ) nonadaptive queries. We validate this theoretical result in a real crowdsourcing experiment in Sec. 4."
    }, {
      "heading" : "3 Main results and Techniques",
      "text" : ""
    }, {
      "heading" : "3.1 Noiseless queries and exact recovery",
      "text" : "In this scenario we assume the query answer from oracle to be perfect and we wish to get back the all of the original labels exactly without any error. Each query is allowed to take only ∆ labels as input. When ∆ = 2, we are allowed to ask only pairwise queries. Let us consider the case when there are only two labels, i.e., k = 2. That means the labels Xi ∈ {0, 1}, 1 ≤ i ≤ n, are iid Bernoulli(p) random variable. Therefore the number of queries m that are necessary and sufficient to recover all the labels with high probability is approximately nh(p) ± o(n) where h(x) ≡ −x log2 x− (1−x) log2(1−x) is the binary entropy function. However the sufficiency part here does not take into account that each query can involve only ∆ labels.\nQuerying scheme: We use the following type of queries. For each query, labels of ∆ elements are given to the oracle, and the oracle returns a simple XOR operation of the labels. Note, for ∆ = 2, our queries are just ‘same cluster’ queries.\nTheorem 1. There exists a querying scheme with m = n(h(p)+o(1)) log2 1 α queries of above type, where α = 12 (1 + (1 − 4p(1 − p)) ∆), such that it will be possible to recover all the labels with high probability by a Maximum Likelihood decoder.\nProof. Let the number of queries asked is m. Let us define Q to be the random binary query matrix of dimension m× n where each row has exactly ∆ ones, other entries being zero. Now for a label vector X we can represent the set of query outputs by Y = QX mod 2. Now if we use Maximum Likelihood Decoding then we will not make an error as long as the query output vector is different\nfor every X that belong to the typical set4 of X . Let us define a ‘bad event’ for two different label vectors X1 and X2 to be the event QX1 = QX2 or Q(X1 +X2) = 0 mod 2 because in that case we will not be able to differentiate between those two sequences. Now consider a random ensemble of matrices where in each row ∆ positions are chosen uniformly randomly from the n positions to be 1. In this random ensemble, the probability of a ‘bad event’ for any two fixed typical label vectors X1 and X2 is going to be(∑\ni=0:∆ i even\n( nr(p) i )( n−nr(p) ∆−i )(\nn ∆ ) )m ≤ ( 12 ((n∆)+ (n−2nr(p)∆ ))(n ∆ ) )m ≤ (1 2 (1 + (1− 2r(p))∆) )m ,\nwhere r(p) = 2p(1− p). This is because , X1 +X2 mod 2 has r(p) = 2p(1− p) ones with high probability since they are typical vectors.\nNow we have to use the ‘coding theoretic’ idea of expurgation to complete the analysis. From linearity of expectation, the expected number of ‘bad events’ is going to be(\nT\n2\n)( 1\n2 (1 + (1− 2r(p))∆)\n)m ,\nwhere T is the size of the typical set and T ≤ 2n(h(p)+o(1)). If this expected number of ‘bad events’ is smaller than T then for every ‘bad event’, we can throw out 1 label vector and there will be no more bad events. This will imply perfect recovery, as long as(\nT\n2\n)( 1\n2 (1 + (1− 2r(p))∆)\n)m < T.\nSubstituting the upper bound for T , we have that perfect recovery is possible as long as, mn > (h(p)+o(1)− log 2 n )/(log 1 α ). Now if we take to be of the form n\n−β for β > 0 then asymptotically we will have a vanishing fraction of typical label vectors which will be expurgated and log n → 0. Therefore m = n(h(p)+o(1))\nlog 1α queries will going to recover all the labels with high probability. Hence\nthere must exist a querying scheme with m = n(h(p)+o(1)) log 1α queries that will work.\nThe number of sufficient queries guaranteed by the above theorem is strictly less than n for all 0.0694 ≤ p < 0.5 even for ∆ = 2. Note that, with ∆ = 2, by querying the first element with all others nonadaptively (total n − 1 queries), it is possible to deduce the two clusters. In contrast, if one makes just random ‘same cluster’ queries, then O(n log n) queries are required to recover the clusters with high probability (see, e.g., [2]).\nNow we provide a lower bound on the number of queries required.\nTheorem 2. The minimum number of queries necessary to recover all labels with high probability is at least by nh(p) ·max{1,maxρ (1−ρ)\nh( (1−ρ)r(p)∆ ρ ) } where r(p) ≡ 2p(1− p).\nProof. Every query involves at most ∆ elements. Therefore the average number of queries an element is part of is ∆mn . Therefore 1− ρ fraction of all the elements (say the set S ⊂ {1, . . . , n}) are part of less than ∆mρn queries. Now consider the set {1, . . . , n} \\ S. Consider all typical label vectors C ∈ {0, 1}n such that their projection on {1, . . . , n} \\ S is a fixed typical sequence. We know that there are 2n(1−ρ)h(p) such sequences. Let X0 be one of these sequences. Now, almost all sequences of C must have a distance of n(1−ρ)r(p)+o(n) from X0. Let Y 0 be the corresponding query outputs when X0 is the input. Now any query output for input belonging to C must reside in a Hamming ball of radius (1− ρ)r(p)∆m\nρ from Y 0. Therefore we must have mh(\n(1−ρ)r(p)∆ ρ ) > n(1− ρ)h(p).\nThis lower bound is better than the naive nh(p) for p < 0.03475 when ∆ = 2.\n4Here a typical set of labels is all such label vectors where the number of ones is between np− n2/3 and np+ n2/3.\nFor ∆ = 2, the plot of the corresponding upper and lower bounds have been shown in Figure 1. The main takeaway from this part is that, by exploiting the prior probabilities (or relative cluster sizes), it is possible to know the labels with strictly less than n queries (and close to the lower bound for p ≥ 0.3), even with pairwise ‘same cluster’ queries."
    }, {
      "heading" : "3.2 Noiseless queries and approximate recovery",
      "text" : "Again let us consider the case when k = 2, i.e., only two possible labels. Let X ∈ {0, 1}n be the label vector. Suppose we have a querying algorithm that, by using m queries, recovers a label vector X̂ . Definition. We call a querying algorithm to be (1 − δ)good if for any label vector, at least (1 − δ)n labels are correctly recovered. This means for any label-recovered label pair X, X̂ , the Hamming distance is at most δn. For an almost equivalent definition, we can define a distortion function d(X, X̂) = X + X̂ mod 2, for any two labels X, X̂ ∈ {0, 1}. We can see that Ed(X, X̂) = Pr(X 6= X̂), which we want to be bounded by δ.\nUsing standard rate-distortion theory [7], it can be seen that, if the queries could involve an arbitrary number of elements then with m queries it is possible to have a (1− δ̃(m/n))-good scheme where δ̃(γ) ≡ h−1(h(p) − γ). When each query is allowed to take only at most ∆ inputs, we have the following lower bound for (1− δ)-good querying algorithms. Theorem 3. In any (1 − δ)-good querying scheme with m queries where each query can take as input ∆ elements, the following must be satisfied (below h′(x) = dh(x)dx ):\nδ ≥ δ̃ (m n ) +\nh(p)− h(δ̃(mn )) h′(δ̃(mn ))(1 + e ∆h′(δ̃(mn )))\nThe proof of this theorem is quite involved, and we have included it in the appendix in the supplementary material.\nOne of the main observation that we make is that the ‘same cluster’ queries are highly inefficient for approximate recovery. This follows from a classical result of Ancheta and Massey [17] on the limitation of linear codes as rate-distortion codes.\nRecall that, the ‘same cluster’ queries are equivalent to XOR operation in the binary field, which is a linear operation on GF (2). We rephrase a conjecture by Massey in our terminology. Conjecture 1 (‘same cluster’ query lower bound). For any (1− δ)-good scheme with m ‘same cluster’ queries (∆ = 2), the following must be satisfied: δ ≥ p(1− mnh(p) ).\nThis conjecture is known to be true at the point p = 0.5 (equal sized clusters). We have plotted these two lower bounds in Figure 2.\nNow let us provide a querying scheme with ∆ = 2 that will provably be better than ‘same cluster’ schemes.\nQuerying scheme: AND Queries: We define the AND query Q : {0, 1}2 → {0, 1} as Q(X,X ′) = X ∧\nX ′, where X,X ′ ∈ {0, 1}, so that Q(X,X ′) = 1 only when both the elements have labels equal to 1. For each pairwise query the oracle will return this AND operation of the labels. Theorem 4. There exists a (1− δ)-good querying scheme with m pairwise AND queries such that\nδ = pe− 2m n + n∑ d=1 e− 2m n ( 2mn ) d d! d∑ k=1 ( n k ) f(k, d) nd (1− p)kp\nwhere f(k, d) = ∑k i=0(−1)i ( k i ) (k − i)d.\nProof. Assume p < 0.5 without loss of generality. Consider a random bipartite graph where each ‘left’ node represent an element labeled according to the label vector X ∈ {0, 1}n and each ‘right’ node represents a query. All the query answers are collected in Y ∈ {0, 1}m. The graph has right-degree exactly equal to 2. For each query the two inputs are selected uniformly at random without replacement.\nRecovery algorithm: For each element we look at the queries that involves it and estimate its label as 1 if any of the query answers is 1 and predict 0 otherwise. If there are no queries that involves the element, we simply output 0 as the label.\nSince the average left-degree is 2mn and since all the edges from the right nodes are randomly and independently thrown, we can model the degree of each left-vertex by a Poisson distribution with the mean λ = 2mn . We define element j to be a two-hop-neighbor of i if there is at least one query which involved both the elements i and j . Under our decoding scheme we only have an error when the label of i, Xi = 1 and the labels of all its two-hop-neighbors are 0. Hence the probability of error for estimating Xi can be written as, Pr(Xi 6= X̂i) = ∑ d Pr(degree(i) = d) Pr(Xi 6= X̂i | degree(i) = d). Now let us estimate Pr(Xi 6= X̂i | degree(i) = d). We further condition the error on the event that there are k distinct two-hop-neighbors (lets call the number of distinct neighbors of i as Dist(i)) and hence we have that Pr(Xi 6= X̂i | degree(i) = d) = ∑d k=1 Pr(Dist(i) =\nk) Pr(Xi 6= X̂i|degree(i) = d,Dist(i) = k) = ∑d k=1 ( n k ) f(k,d) nd\np(1− p)k. Now using the Poisson assumption we get the statement of the theorem.\nThe performance of this querying scheme is plotted against the number of queries for prior probabilities p = 0.5 in Figure 2.\nComparison with ‘same cluster’ queries: We see in Figure 2 that the AND query scheme beats the ‘same cluster’ query lower bound for a range of query-performance trade-off in approximate recovery for p = 12 . For smaller p, this range of values of δ increases further. If we randomly choose ‘same cluster’ queries and then resort to maximum likelihood decoding (note that, for AND queries, we present a simple decoding) then O(n log n) queries are still required even if we allow for δ proportion of incorrect labels (follows from [11]). The best performance for ‘same cluster’ query in approximate recovery that we know of for small δ is given by: m = n(1− δ) (neglect nδ elements and just query the n(1− δ) remaining elements with the first element). However, such a scheme can be achieved by AND queries as well in a similar manner. Therefore, there is no point in the query vs δ plot that we know of where ‘same cluster’ query achievability outperforms AND query achievability."
    }, {
      "heading" : "3.3 Noisy queries and approximate recovery",
      "text" : "This section contains our main algorithmic contribution. In contrast to the previous sections here we consider the general case of k ≥ 2 clusters. Recall that the label vector X ∈ {0, 1, . . . , k − 1}n, and the prior probability of each label is given by the probabilities p = (p0, . . . , pk−1). Instead of binary output queries, in this part we consider an oracle that can provide one of k different answers. We consider a model of noise in the query answer where the oracle provides correct answer with probability 1− q, and any one of the remaining incorrect answers with probability qk−1 . Note that we do not allow the same query to be asked to the oracle multiple time (see Sec. 2 for justification). We also define a (1− δ)-good approximation scheme exactly as before. Querying Scheme: We only perform pairwise queries. For a pair of labels X and X ′ we define a query Y = Q(X,X ′) ∈ {0, 1, . . . , k − 1}. For our algorithm we define the Q as\nQ(X,X ′) = { i if X = X ′ = i 0 otherwise. } We can observe that for k = 2, this query is exactly same as the binary AND query that we defined in the previous section. In our querying scheme, we make a total of nd2 queries, for an integer d > 1. We design a d-regular graph G(V,E) where V = {1, . . . , n} is the set of elements that we need to label. We query all the pairs of elements (u, v) ∈ E. Under this querying scheme, we propose to use Algorithm 1 for reconstructions of labels.\nTheorem 5. The querying scheme with m = O(n log kδ ) queries and Algorithm 1 is (1− δ)-good for approximate recovery of labels from noisy queries.\nAlgorithm 1 Noisy query approximate recovery with nd2 queries\nRequire: PRIOR p ≡ (p0, . . . , pk−1) Require: Query Answers Yu,v : (u, v) ∈ E\nfor i ∈ [1, 2, . . . , k − 1] do Ci = dq k−1 + dpi 2 ( 1− qkk−1 ) end for for u ∈ V do\nfor i ∈ [1, 2, . . . , k − 1] do Nu,i = ∑d v=1 1{Yu,v = i}\nif Nu,i ≥ dCie then Xu ← i Assigned← True break\nend if end for if ¬ Assigned then Xu ← 0\nend if end for\nWe can come up with more exact relation between number of queries m = nd2 , δ, p, q and k. This is deferred to the appendix in the supplementary material.\nProof of Theorem 5. The total number of queries is m = nd2 . Now for a particular element u ∈ V , we look at the values of d noisy oracle answers {Yu,v}dv=1. We have, E(Nu,i) = dqk−1 + dpi ( 1 − qkk−1 ) when the true label of u is i 6= 0. When the true label is something else, E(Nu,i) = dqk−1 . There is an obvious gap between these expectations. Clearly when the true label is i, the probability of error in assignment of the label of u is given by, Pi ≤ ∑ j:j 6=i,j 6=0 Pr(Nu,j > Cj) + Pr(Nu,i < Ci) ≤ cke−2d 2\nfor some constants c and depending on the gap, from Chernoff bound. And when the true label is 0, the probability of error is P0 ≤ ∑ j:j 6=0 P (Nu,j > Cj) ≤ c′ke−2d ′2 , for\nsome constants c′, ′. Let δ = ∑ i piPi, we can\neasily observe that d scales as O(log kδ ). Hence the total number of queries is nd2 = O(n log k δ ).\nThe only thing that remains to be proved is that the number of incorrect labels is δn with high probability. Let Zu be the event that element u has been incorrectly labeled. Then EZu = δ. The total number of incorrectly labeled elements is Z = ∑ u Zu. We have EZ = nδ. Now define\nZu ∼ Zv if Zu and Zv are dependent. Now ∆∗ ≡ ∑ Zu∼Zv Pr(Zu|Zv) ≤ d\n2 + d because the maximum number of nodes dependent with Zu are its 1-hop and 2-hop neighbors. Now using Corollary 4.3.5 in [3], it is evident that Z = EZ = nδ almost always.\nThe theoretical performance guarantee of Algorithm 1 (a detailed version of Theorem 5 is in the supplementary material) for k = 2 is shown in Figures 3 and 4. We can observe from Figure 3 that for a particular q, incorrect labeling decreases as p becomes higher. We can also observe from Figure 4 that if q = 0.5 then the incorrect labeling is 50% because the complete information from the oracle is lost. For other values of q, we can see that the incorrect labeling decreases with increasing d.\nWe point out that ‘same cluster’ queries are not a good choice here, because of the symmetric nature of XOR due to which there is no gap between the expected numbers (contrary to the proof of Theorem 5) which we had exploited in the algorithm to a large extent.\nLastly, we show that Algorithm 1 can work without knowing the prior distribution and only with the knowledge of relative sizes of the clusters. The ground truth clusters can be adversarial as long as they maintain the relative sizes.\nTheorem 6. Suppose we have ni, the number of elements with label i, i = 0, 1, . . . , k − 1, as input instead of the priors. By taking a random permutation over the nodes while constructing the d-regular graph, Algorithm 1 will be (1 − δ)-good approximation with m = O(n log kδ ) queries as n → ∞ when we set pi = nin .\nThe proof of this theorem is deferred to the appendix in the supplementary material.\nFigure 4: Recovery error for a fixed p, q and varying d\nFigure 5: Algorithm 1 on real crowdsourced dataset"
    }, {
      "heading" : "4 Experiments",
      "text" : "Though our main contribution is theoretical we have verified our work by using our algorithm on a real dataset created by local crowdsourcing. We first picked a list of 100 ‘action’ movies and 100 ‘romantic’ movies from IMDB (http://www.imdb.com/list/ls076503982/ and http://www.imdb.com/list/ls058479560/). We then created the queries as given in the querying scheme of Sec. 3.3 by creating a d-regular graph (where d is even). To create the graph we put all the movies on a circle and took a random permutation on them in a circle. Then for each node we connected d2 edges on either side to its closest neighbors in the permuted circular list. This random permutation will allow us to use the relative sizes of the clusters as priors as explained in Sec. 3.3. Using d = 10 , we have nd2 = 1000 queries with each query being the following question: Are both the movies ‘action’ movies?. Now we divided these 1000 queries into 10 surveys (using SurveyMonkey platform) with each survey carrying 100 queries for the user to answer. We used 10 volunteers to fill up the survey. We instructed them not to check any resources and answer the questions spontaneously and also gave them a time limit of a maximum of 10 minutes. The average finish time of the surveys were 6 minutes. The answers represented the noisy query model since some of the answers were wrong. In total, we have found 105 erroneous answers in those 1000 queries. For each movie we evaluate the d query answer it is part of, and use different thresholds T for prediction. That is, if there are more than T ‘yes’ answers among those d answers we classified the movie as ‘action’ movie and a ‘romantic’ movie otherwise.The theoretical threshold for predicting an ‘action’ movie is T = 2 for oracle error probability q = 0.105, p = 0.5 and d = 10 . But we compared other thresholds as well.\nWe now used Algorithm 1 to predict the true label vector from a subset of queries by taking d̃ edges for each node where d̃ < d and d̃ is even i.e d̃ ∈ {2, 4, 6, 8, 10}. Obviously, for d̃ = 2 , the thresholds T = 3, 4 is meaningless as we always estimate the movie as ‘romantic’ and hence the distortion starts from 0.5 in that case. We plotted the error for each case against the number of queries (nd̃2 ) and also plotted the theoretical distortion obtained from our results for k = 2 labels and p = 0.5, q = 0.105. We compare these results along with the theoretical distortion that we should have for q = 0.105. All these results have been compiled in Figure 5 and we can observe that the distortion is decreasing with the number of queries and\nthe gap between the theoretical result and the experimental results is small for T = 2. These results validate our theoretical results and our algorithm to a large extent.\nWe have also asked ‘same cluster’ queries with the same set of 1000 pairs to the participants to find that the number of erroneous responses to be 234 (whereas with AND queries it was 105). This substantiates the claim that AND queries are easier to answer for workers. Since this number of errors is too high, we have compared the performance of ‘same cluster’ queries with AND queries and our algorithm in a synthetically generated dataset with two hundred elements (Figure 6). For recovery with ‘same cluster’ queries, we have used the popular spectral clustering algorithm with normalized cuts [24]. The detailed results obtained can be found in Figure 7 in the supplementary material.\nAcknowledgements: This research is supported in parts by NSF Awards CCF-BSF 1618512, CCF 1642550 and an NSF CAREER Award CCF 1642658. The authors thank Barna Saha for many discussions on the topics of this paper. The authors also thank the volunteers who participated in the crowdsourcing experiments for this paper."
    } ],
    "references" : [ {
      "title" : "Exact recovery in the stochastic block model",
      "author" : [ "E. Abbe", "A.S. Bandeira", "G. Hall" ],
      "venue" : "IEEE Trans. Information Theory, 62(1):471–487,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Community recovery in hypergraphs",
      "author" : [ "K. Ahn", "K. Lee", "C. Suh" ],
      "venue" : "Communication, Control, and Computing (Allerton), 2016 54th Annual Allerton Conference on, pages 657–663. IEEE,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The probabilistic method",
      "author" : [ "N. Alon", "J.H. Spencer" ],
      "venue" : "John Wiley & Sons,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Clustering with same-cluster queries",
      "author" : [ "H. Ashtiani", "S. Kushagra", "S. Ben-David" ],
      "venue" : "Advances In Neural Information Processing Systems, pages 3216–3224,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Are bitvectors optimal",
      "author" : [ "H. Buhrman", "P.B. Miltersen", "J. Radhakrishnan", "S. Venkatesh" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Sparse graph codes for compression, sensing, and secrecy",
      "author" : [ "V.B. Chandar" ],
      "venue" : "PhD thesis, Massachusetts Institute of Technology,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Elements of information theory, 2nd Ed",
      "author" : [ "T.M. Cover", "J.A. Thomas" ],
      "venue" : "John Wiley & Sons,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Online entity resolution using an oracle",
      "author" : [ "D. Firmani", "B. Saha", "D. Srivastava" ],
      "venue" : "PVLDB, 9(5):384–395,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Fault-tolerant entity resolution with the crowd",
      "author" : [ "A. Gruenheid", "B. Nushi", "T. Kraska", "W. Gatterbauer", "D. Kossmann" ],
      "venue" : "CoRR, abs/1512.00537,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fault-tolerant entity resolution with the crowd",
      "author" : [ "A. Gruenheid", "B. Nushi", "T. Kraska", "W. Gatterbauer", "D. Kossmann" ],
      "venue" : "arXiv preprint arXiv:1512.00537,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Achieving exact cluster recovery threshold via semidefinite programming: Extensions",
      "author" : [ "B. Hajek", "Y. Wu", "J. Xu" ],
      "venue" : "IEEE Transactions on Information Theory, 62(10):5918–5937,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Iterative learning for reliable crowdsourcing systems",
      "author" : [ "D.R. Karger", "S. Oh", "D. Shah" ],
      "venue" : "Advances in neural information processing systems, pages 1953–1961,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Budget-optimal task allocation for reliable crowdsourcing systems",
      "author" : [ "D.R. Karger", "S. Oh", "D. Shah" ],
      "venue" : "Operations Research, 62(1):1–24,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fundamental limits of budget-fidelity trade-off in label crowdsourcing",
      "author" : [ "F. Lahouti", "B. Hassibi" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 5059–5067,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Variational inference for crowdsourcing",
      "author" : [ "Q. Liu", "J. Peng", "A.T. Ihler" ],
      "venue" : "Advances in neural information processing systems, pages 692–700,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On locally decodable source coding",
      "author" : [ "A. Makhdoumi", "S.-L. Huang", "M. Médard", "Y. Polyanskiy" ],
      "venue" : "Communications (ICC), 2015 IEEE International Conference on, pages 4394–4399. IEEE,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Joint source and channel coding",
      "author" : [ "J.L. Massey" ],
      "venue" : "Technical report, DTIC Document,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Update-efficiency and local repairability limits for capacity approaching codes",
      "author" : [ "A. Mazumdar", "V. Chandar", "G.W. Wornell" ],
      "venue" : "IEEE Journal on Selected Areas in Communications, 32(5):976–988,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Local recovery in data compression for general sources",
      "author" : [ "A. Mazumdar", "V. Chandar", "G.W. Wornell" ],
      "venue" : "Information Theory (ISIT), 2015 IEEE International Symposium on, pages 2984– 2988. IEEE,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A theoretical analysis of first heuristics of crowdsourced entity resolution",
      "author" : [ "A. Mazumdar", "B. Saha" ],
      "venue" : "The Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Clustering with noisy queries",
      "author" : [ "A. Mazumdar", "B. Saha" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 31,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Query complexity of clustering with side information",
      "author" : [ "A. Mazumdar", "B. Saha" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 31,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Smooth compression, gallager bound and nonlinear sparse-graph codes",
      "author" : [ "A. Montanari", "E. Mossel" ],
      "venue" : "Information Theory, 2008. ISIT 2008. IEEE International Symposium on, pages 2474–2478. IEEE,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "A.Y. Ng", "M.I. Jordan", "Y. Weiss" ],
      "venue" : "Advances in neural information processing systems, pages 849–856,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Compressing sparse sequences under local decodability constraints",
      "author" : [ "A. Pananjady", "T.A. Courtade" ],
      "venue" : "Information Theory (ISIT), 2015 IEEE International Symposium on, pages 2979–2983. IEEE,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Succincter",
      "author" : [ "M. Patrascu" ],
      "venue" : "Foundations of Computer Science, 2008. FOCS’08. IEEE 49th Annual IEEE Symposium on, pages 305–313. IEEE,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A solution to the single-question crowd wisdom problem",
      "author" : [ "D. Prelec", "H.S. Seung", "J. McCoy" ],
      "venue" : "Nature, 541(7638):532–535,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Reliable crowdsourcing for multi-class labeling using coding theory",
      "author" : [ "A. Vempaty", "L.R. Varshney", "P.K. Varshney" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing, 8(4):667–679,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Entity resolution with crowd errors",
      "author" : [ "V. Verroios", "H. Garcia-Molina" ],
      "venue" : "31st IEEE International Conference on Data Engineering, ICDE 2015, Seoul, South Korea, April 13-17, 2015, pages 219–230,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Crowdsourcing algorithms for entity resolution",
      "author" : [ "N. Vesdapunt", "K. Bellare", "N. Dalvi" ],
      "venue" : "PVLDB, 7(12):1071–1082,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Crowdsourced clustering: Querying edges vs triangles",
      "author" : [ "R.K. Vinayak", "B. Hassibi" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1316–1324,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Bit-probe lower bounds for succinct data structures",
      "author" : [ "E. Viola" ],
      "venue" : "SIAM Journal on Computing, 41(6):1593–1604,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Crowder: Crowdsourcing entity resolution",
      "author" : [ "J. Wang", "T. Kraska", "M.J. Franklin", "J. Feng" ],
      "venue" : "PVLDB, 5(11):1483–1494,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning from the wisdom of crowds by minimax entropy",
      "author" : [ "D. Zhou", "S. Basu", "Y. Mao", "J.C. Platt" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2195–2203,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "We will call this the ‘same cluster’ query (as per [4]).",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 32,
      "context" : "Carefully designed adaptive and interactive querying algorithms for clustering has also recently been developed [33, 30, 8, 22, 21].",
      "startOffset" : 112,
      "endOffset" : 131
    }, {
      "referenceID" : 29,
      "context" : "Carefully designed adaptive and interactive querying algorithms for clustering has also recently been developed [33, 30, 8, 22, 21].",
      "startOffset" : 112,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "Carefully designed adaptive and interactive querying algorithms for clustering has also recently been developed [33, 30, 8, 22, 21].",
      "startOffset" : 112,
      "endOffset" : 131
    }, {
      "referenceID" : 21,
      "context" : "Carefully designed adaptive and interactive querying algorithms for clustering has also recently been developed [33, 30, 8, 22, 21].",
      "startOffset" : 112,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "Carefully designed adaptive and interactive querying algorithms for clustering has also recently been developed [33, 30, 8, 22, 21].",
      "startOffset" : 112,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "particular, the query complexity for clustering with a k-means objective had recently been studied in [4], and there are significant works in designing optimal crowdsourcing schemes in general (see, [12, 13, 28, 34, 15]).",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "particular, the query complexity for clustering with a k-means objective had recently been studied in [4], and there are significant works in designing optimal crowdsourcing schemes in general (see, [12, 13, 28, 34, 15]).",
      "startOffset" : 199,
      "endOffset" : 219
    }, {
      "referenceID" : 12,
      "context" : "particular, the query complexity for clustering with a k-means objective had recently been studied in [4], and there are significant works in designing optimal crowdsourcing schemes in general (see, [12, 13, 28, 34, 15]).",
      "startOffset" : 199,
      "endOffset" : 219
    }, {
      "referenceID" : 27,
      "context" : "particular, the query complexity for clustering with a k-means objective had recently been studied in [4], and there are significant works in designing optimal crowdsourcing schemes in general (see, [12, 13, 28, 34, 15]).",
      "startOffset" : 199,
      "endOffset" : 219
    }, {
      "referenceID" : 33,
      "context" : "particular, the query complexity for clustering with a k-means objective had recently been studied in [4], and there are significant works in designing optimal crowdsourcing schemes in general (see, [12, 13, 28, 34, 15]).",
      "startOffset" : 199,
      "endOffset" : 219
    }, {
      "referenceID" : 14,
      "context" : "particular, the query complexity for clustering with a k-means objective had recently been studied in [4], and there are significant works in designing optimal crowdsourcing schemes in general (see, [12, 13, 28, 34, 15]).",
      "startOffset" : 199,
      "endOffset" : 219
    }, {
      "referenceID" : 30,
      "context" : "As an example, recent work in [31] considers triangle queries (involving three elements in a query).",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 22,
      "context" : "The notion of locally encodable source coding is not well-studied even within information theory community, and the only place where it is mentioned to the best of our knowledge is in [23], although the focus of that paper is a related notion of smooth encoding.",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 18,
      "context" : "Another related notion of local decoding seem to be much more well-studied [19, 18, 16, 26, 6, 25, 5, 32].",
      "startOffset" : 75,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "Another related notion of local decoding seem to be much more well-studied [19, 18, 16, 26, 6, 25, 5, 32].",
      "startOffset" : 75,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "Another related notion of local decoding seem to be much more well-studied [19, 18, 16, 26, 6, 25, 5, 32].",
      "startOffset" : 75,
      "endOffset" : 105
    }, {
      "referenceID" : 25,
      "context" : "Another related notion of local decoding seem to be much more well-studied [19, 18, 16, 26, 6, 25, 5, 32].",
      "startOffset" : 75,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "Another related notion of local decoding seem to be much more well-studied [19, 18, 16, 26, 6, 25, 5, 32].",
      "startOffset" : 75,
      "endOffset" : 105
    }, {
      "referenceID" : 24,
      "context" : "Another related notion of local decoding seem to be much more well-studied [19, 18, 16, 26, 6, 25, 5, 32].",
      "startOffset" : 75,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "Another related notion of local decoding seem to be much more well-studied [19, 18, 16, 26, 6, 25, 5, 32].",
      "startOffset" : 75,
      "endOffset" : 105
    }, {
      "referenceID" : 31,
      "context" : "Another related notion of local decoding seem to be much more well-studied [19, 18, 16, 26, 6, 25, 5, 32].",
      "startOffset" : 75,
      "endOffset" : 105
    }, {
      "referenceID" : 20,
      "context" : "Experiments on several real datasets (see [21]) with answers collected from Amazon Mechanical Turk [9, 29] show that majority voting could even increase the errors.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "Experiments on several real datasets (see [21]) with answers collected from Amazon Mechanical Turk [9, 29] show that majority voting could even increase the errors.",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 28,
      "context" : "Experiments on several real datasets (see [21]) with answers collected from Amazon Mechanical Turk [9, 29] show that majority voting could even increase the errors.",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "On the other hand, it is usually assumed that the answers to different queries are independently erroneous (see [10]).",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 20,
      "context" : "In this case, the best known algorithms need O(n log n) queries to perform the clustering with two clusters [21].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 18,
      "context" : "While the the related notion of locally decodable source code [19, 16, 26, 6], as well as smooth compression [23, 26] have been studied, there was no nontrivial result known related to locally encodable codes in general.",
      "startOffset" : 62,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "While the the related notion of locally decodable source code [19, 16, 26, 6], as well as smooth compression [23, 26] have been studied, there was no nontrivial result known related to locally encodable codes in general.",
      "startOffset" : 62,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : "While the the related notion of locally decodable source code [19, 16, 26, 6], as well as smooth compression [23, 26] have been studied, there was no nontrivial result known related to locally encodable codes in general.",
      "startOffset" : 62,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "While the the related notion of locally decodable source code [19, 16, 26, 6], as well as smooth compression [23, 26] have been studied, there was no nontrivial result known related to locally encodable codes in general.",
      "startOffset" : 62,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : "While the the related notion of locally decodable source code [19, 16, 26, 6], as well as smooth compression [23, 26] have been studied, there was no nontrivial result known related to locally encodable codes in general.",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 25,
      "context" : "While the the related notion of locally decodable source code [19, 16, 26, 6], as well as smooth compression [23, 26] have been studied, there was no nontrivial result known related to locally encodable codes in general.",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "In many prior works, especially in the recovery algorithms of popular statistical models such as stochastic block model, it is assumed that the relative sizes of the clusters are known (see [1]).",
      "startOffset" : 190,
      "endOffset" : 193
    }, {
      "referenceID" : 20,
      "context" : "When q > 0 (erroneous oracle), it has been shown that a total number of O(γnk log n) queries are sufficient [21], where γ is the ratio of the sizes of the largest and smallest clusters.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 18,
      "context" : "This terminology is analogous to the recently developed literature on locally decodable source coding [19, 16].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "This terminology is analogous to the recently developed literature on locally decodable source coding [19, 16].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 22,
      "context" : "Another closely related notion is that of ‘smooth compression’, where each source bit contributes to at most ∆ compressed bits [23].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 22,
      "context" : "Indeed, in [23], the notion of locally encodable source coding is also present where it was called robust compression.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 16,
      "context" : "We use an old result by Massey [17] to establish this limitation.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 6,
      "context" : "Using standard rate-distortion theory [7], it can be seen that, if the queries could involve an arbitrary number of elements then with m queries it is possible to have a (1− δ̃(m/n))-good scheme where δ̃(γ) ≡ h−1(h(p) − γ).",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "This follows from a classical result of Ancheta and Massey [17] on the limitation of linear codes as rate-distortion codes.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "If we randomly choose ‘same cluster’ queries and then resort to maximum likelihood decoding (note that, for AND queries, we present a simple decoding) then O(n log n) queries are still required even if we allow for δ proportion of incorrect labels (follows from [11]).",
      "startOffset" : 262,
      "endOffset" : 266
    }, {
      "referenceID" : 2,
      "context" : "5 in [3], it is evident that Z = EZ = nδ almost always.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 23,
      "context" : "For recovery with ‘same cluster’ queries, we have used the popular spectral clustering algorithm with normalized cuts [24].",
      "startOffset" : 118,
      "endOffset" : 122
    } ],
    "year" : 2017,
    "abstractText" : "Source coding is the canonical problem of data compression in information theory. In a locally encodable source coding, each compressed bit depends on only few bits of the input. In this paper, we show that a recently popular model of semisupervised clustering is equivalent to locally encodable source coding. In this model, the task is to perform multiclass labeling of unlabeled elements. At the beginning, we can ask in parallel a set of simple queries to an oracle who provides (possibly erroneous) binary answers to the queries. The queries cannot involve more than two (or a fixed constant number ∆ of) elements. Now the labeling of all the elements (or clustering) must be performed based on the (noisy) query answers. The goal is to recover all the correct labelings while minimizing the number of such queries. The equivalence to locally encodable source codes leads us to find lower bounds on the number of queries required in variety of scenarios. We are also able to show fundamental limitations of pairwise ‘same cluster’ queries and propose pairwise AND queries, that provably performs better in many situations.",
    "creator" : null
  }
}