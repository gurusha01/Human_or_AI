{
  "name" : "f1981e4bd8a0d6d8462016d2fc6276b3.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Unsupervised Sequence Classification using Sequential Output Statistics",
    "authors" : [ "Yu Liu", "Jianshu Chen", "Li Deng" ],
    "emails" : [ "jianshuc@microsoft.com", "Li.Deng@citadel.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Unsupervised learning is one of the most challenging problems in machine learning. It is often formulated as the modeling of how the world works without requiring a huge amount of human labeling effort, e.g. [8]. To reach this grand goal, it is necessary to first solve a sub-goal of unsupervised learning with high practical value; that is, learning to predict output labels from input data without requiring costly labeled data. Toward this end, we study in this paper the learning of a sequence classifier without labels by using sequential output statistics. The problem is highly valuable since the sequential output statistics, such as language models, could be obtained independently of the input data and thus with no labeling cost.\nThe problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12]. When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10]. In both ways, the above unsupervised methods played an auxiliary role in helping supervised learning when it is applied to prediction tasks.\nRecently, various solutions have been proposed to address the input-to-output prediction problem without using labeled training data, all without demonstrated successes [11, 30, 7]. Similar to this work, the authors in [7] proposed an unsupervised cost that also exploits the sequence prior of the output samples to train classifiers. The power of such a strong prior in the form of language\n⇤All the three authors contributed equally to the paper. †The work was done while Yu Liu and Li Deng were at Microsoft Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nmodels in unsupervised learning was also demonstrated in earlier studies in [21, 3]. However, these earlier methods did not perform well in practical prediction tasks with real-world data without using additional strong generative models. Possible reasons are inappropriately formulated cost functions and inappropriate choices of optimization methods. For example, it was shown in [7] that optimizing the highly non-convex unsupervised cost function could easily get stuck in trivial solutions, although adding a special regularization mitigated the problem somewhat.\nThe solution provided in this paper fundamentally improves these prior works in [11, 30, 7] in following aspects. First, we propose a novel cost function for unsupervised learning, and find that it has a desired coverage-seeking property that makes the learning algorithm less inclined to be stuck in trivial solutions than the cost function in [7]. Second, we develop a special empirical formulation of this cost function that avoids the need for a strong generative model as in [30, 11].3 Third, although the proposed cost function is more difficult to optimize in its functional form, we develop a stochastic primal-dual gradient (SPDG) algorithm to effectively solve problem. Our analysis of SPDG demonstrates how it is able to reduce the high barriers in the cost function by transforming it into a primal-dual domain. Finally and most importantly, we demonstrate the new cost function and the associated SPDG optimization algorithm work well in two real-world classification tasks. In the rest of the paper, we proceed to demonstrate these points and discuss related works along the way."
    }, {
      "heading" : "2 Empirical-ODM: An unsupervised learning cost for sequence classifiers",
      "text" : "In this section, we extend the earlier work of [30] and propose an unsupervised learning cost named Empirical Output Distribution Match (Empirical-ODM) for training classifiers without labeled data. We first formulate the unsupervised learning problem with sequential output structures. Then, we introduce the Empirical-ODM cost and discuss its important properties that are closely related to unsupervised learning."
    }, {
      "heading" : "2.1 Problem formulation",
      "text" : "We consider the problem of learning a sequence classifier that predicts an output sequence (y1, . . . , yT0) from an input sequence (x1, . . . , xT0) without using labeled data, where T0 denotes the length of the sequence. Specifically, the learning algorithm does not have access to a labeled training set D\nXY , {(xn1 , . . . , xn Tn ), (y n 1 , . . . , y n Tn ) : n = 1, . . . ,M}, where T n denotes the length of the n-th sequence. Instead, what is available is a collection of input sequences, denoted as D\nX , {(xn1 , . . . , xn Tn ) : n = 1, . . . ,M}. In addition, we assume that the sequential output statistics (or sequence prior), in the form of an N -gram probability, are available:\npLM(i1, . . . , iN ) , pLM(yn t N+1 = i1, . . . , y n t = i N )\nwhere i1, . . . , iN 2 {1, . . . , C} and the subscript “LM” stands for language model. Our objective is to train the sequence classifier by just using D\nX and pLM(·). Note that the sequence prior pLM(·), in the form of language models, is a type of structure commonly found in natural language data, which can be learned from a large amount of text data freely available without labeling cost. For example, in optical character recognition (OCR) tasks, yn\nt could be an English character and xn t is the input image containing this character. We can estimate an N -gram character-level language model pLM(·) from a separate text corpus. Therefore, our learning algorithm will work in a fully unsupervised manner, without any human labeling cost. In our experiment section, we will demonstrate the effectiveness of our method on such a real OCR task. Other potential applications include speech recognition, machine translation, and image/video captioning.\nIn this paper, we focus on the sequence classifier in the form of p ✓ (y n\nt |xn t ) that is, it computes the posterior probability p\n✓\n(y\nn\nt |xn t ) only based on the current input sample xn t in the sequence. Furthermore, we restrict our choice of p\n✓\n(y\nn\nt |xn t ) to be linear classifiers4 and focus our attention on designing and understanding unsupervised learning costs and methods for label-free prediction. In\n3The work [11] only proposed a conceptual idea of using generative models to integrate the output structure and the output-to-input structure for unsupervised learning in speech recognition. Specifically, the generative models are built from the domain knowledge of speech waveform generation mechanism. No mathematical formulation or successful experimental results are provided in [11].\n4 p\n✓ (yn t = i|xn t\n) = e w T i x n t /\nP C\nj=1 e w\nT j x n t , where the model parameter is ✓ , {w\ni\n2 Rd, i = 1, . . . , C}.\nfact, as we will show in later sections, even with linear models, the unsupervised learning problem is still highly nontrivial and the cost function is also highly non-convex. And we emphasize that developing a successful unsupervised learning approach for linear classifiers, as we do in this paper, provides important insights and is an important first step towards more advanced nonlinear models (e.g., deep neural networks). We expect that, in future work, the insights obtained here could help us generalize our techniques to nonlinear models.\nA recent work that shares the same motivations as our work is [29], which also recognizes the high cost of obtaining labeled data and seeks label-free prediction. Different from our setting, they exploit domain knowledge from laws of physics in computer vision applications, whereas our approach exploits sequential statistics in the natural language outputs. Finally, our problem is fundamentally different from the sequence transduction method in [15], although it also exploits language models for sequence prediction. Specifically, the method in [15] is a fully supervised learning in that it requires supervision at the sequence level; that is, for each input sequence, a corresponding output sequence (of possibly different length) is provided as a label. The use of language model in [15] only serves the purpose of regularization in the sequence-level supervised learning. In stark contrast, the unsupervised learning we propose does not require supervision at any level including specifically the sequence level; we do not need the sequence labels but only the prior distribution pLM(·) of the output sequences."
    }, {
      "heading" : "2.2 The Empirical-ODM",
      "text" : "We now introduce an unsupervised learning cost that exploits the sequence structure in pLM(·). It is mainly inspired by the approach to breaking the Caesar cipher, one of the simplest forms of encryption [23]. Caesar cipher is a substitution cipher where each letter in the original message is replaced with a letter corresponding to a certain number of letters up or down in the alphabet. For example, the letter “D” is replaced by the letter “A”, the letter “E” is replaced by the letter “B”, and so on. In this way, the original message that was readable ends up being less understandable. The amount of this shifting is also known to the intended receiver of the message, who can decode the message by shifting back each letter in the encrypted message. However, Caesar cipher could also be broken by an unintended receiver (not knowing the shift) when it analyzes the frequencies of the letters in the encrypted messages and matches them up with the letter distribution of the original text [4, pp.9-11]. More formally, let y\nt\n= f(x\nt ) denote a function that maps each encrypted letter x t into an original letter y\nt . And let pLM(i) , pLM(yt = i) denote the prior letter distribution of the original message, estimated from a regular text corpus. When f(·) is constructed in a way that all mapped letters {y\nt\n: y\nt\n= f(x\nt ), t = 1, . . . , T} have the same distribution as the prior pLM(i), it is able to break the Caesar cipher and recover the original letters at the mapping outputs.\nInspired by the above approach, the posterior probability p ✓ (y n\nt |xn t ) in our classification problem can be interpreted as a stochastic mapping, which maps each input vector xn\nt (the “encrypted letter”) into an output vector yn\nt\n(the “original letter”) with probability p ✓ (y n\nt |xn t ). Then in a samplewise manner, each input sequence (xn1 , . . . , xn Tn ) is stochastically mapped into an output sequence (yn1 , . . . , yn Tn ). We move a step further than the above approach by requiring that the distribution of the N -grams among all the mapped output sequences are close to the prior N -gram distribution pLM(i1, . . . , iN ). With this motivation, we propose to learn the classifier p\n✓\n(y\nt |x t ) by minimizing the cross entropy between the prior distribution and the expected N -gram frequency of the output sequences:\nmin\n✓\n⇢ J (✓) ,\nX\ni1,...,iN\npLM(i1, . . . , iN ) ln p ✓ (i1, . . . , iN )\n(1)\nwhere p ✓ (i1, . . . , iN ) denotes the expected frequency of a given N -gram (i1, . . . , iN ) among all the output sequences. In Appendix B of the supplementary material, we derive its expression as\np\n✓\n(i1, . . . , iN ) , 1\nT\nMX\nn=1\nTnX\nt=1\nN 1Y\nk=0\np\n✓\n(y\nn t k = iN k|xnt k) (2)\nwhere T , T1 + · · ·+ TM is the total number of samples in all sequences. Note that minimizing the cross entropy in (1) is also equivalent to minimizing the Kullback-Leibler (KL) divergence between the two distributions since they only differ by a constant term, P pLM ln pLM. Therefore, the cost function (1) seeks to estimate ✓ by matching the two output distributions, where the expected N -gram\ndistribution in (2) is an empirical average over all the samples in the training set. For this reason, we name the cost (1) as Empirical Output Distribution Match (Empirical-ODM) cost.\nIn [30], the authors proposed to minimize an output distribution match (ODM) cost, defined as the KL-divergence between the prior output distribution and the marginalized output distribution, D(pLM(y)||p✓(y)), where p✓(y) , R p ✓ (y|x)p(x)dx. However, evaluating p ✓\n(y) requires integrating over the input space using a generative model p(x). Due to the lack of such a generative model, they were not able to optimize this proposed ODM cost. Instead, alternative approaches such as Dual autoencoders and GANs were proposed as heuristics. Their results were not successful without using a few labeled data. Our proposed Empirical-ODM cost is different from the ODM cost in [30] in three key aspects. (i) We do not need any labeled data for training. (ii) We exploit sequence structure of output statistics, i.e., in our case y = (y1, . . . , yN ) (N -gram) whereas in [30] y = yt (unigram, i.e., no sequence structure). This is crucial in developing a working unsupervised learning algorithm. The change from unigram to N -gram allows us to explicitly exploit the sequence structures at the output, which makes the technique from non-working to working (see Table 2 in Section 4). It might also explain why the method in [30] failed as it does not exploit the sequence structure. (iii) We replace the marginalized distribution p\n✓ (y) by the expected N -gram frequency in (2). This is critical in that it allows us to directly minimize the divergence between two output distributions without the need for a generative model, which [30] could not do. In fact, we can further show that p\n✓ (i1, . . . , iN ) is an empirical approximation of p✓(y) with y = (y1, . . . , yN ) (see Appendix B.2 of the supplementary material). In this way, our cost (1) can be understood as an N -gram and empirical version of the ODM cost except for an additive constant, i.e., y is replaced by y = (y1, . . . , yN ) and p\n✓\n(y) is replaced by its empirical approximation."
    }, {
      "heading" : "2.3 Coverage-seeking versus mode-seeking",
      "text" : "We now discuss an important property of the proposed Empirical-ODM cost (1) by comparing it with the cost proposed in [7]. We show that the Empirical-ODM cost has a coverage-seeking property, which makes it more suitable for unsupervised learning than the mode-seeking cost in [7].\nIn [7], the authors proposed the expected negative log-likelihood as the unsupervised learning cost function that exploits the output sequential statistics. The intuition was to maximize the aggregated log-likelihood of all the output sequences assumed to be generated by the stochastic mapping p\n✓\n(y\nn\nt |xn t ). We show in Appendix A of the supplementary material that their cost is equivalent to\nX\ni1,...,iN 1\nX\niN\np\n✓\n(i1, . . . , iN ) ln pLM(iN |iN 1, . . . , i1) (3)\nwhere pLM(iN |iN 1, . . . , i1) , p(yn t = i N |yn t 1 = iN 1, . . . , y n t N+1 = i1), and the summations are over all possible values of i1, . . . , iN 2 {1, . . . , C}. In contrast, we can rewrite our cost (1) as\nX\ni1,...,iN 1\npLM(i1, . . . , iN 1) · X\niN\npLM(iN |iN 1, . . . , i1) ln p ✓ (i1, . . . , iN ) (4)\nwhere we used the chain rule of conditional probabilities. Note that both costs (3) and (4) are in a cross entropy form. However, a key difference is that the positions of the distributions p\n✓ (·) and pLM(·) are swapped. We show that the cost in the form of (3) proposed in [7] is a mode-seeking divergence between two distributions, while by swapping p\n✓ (·) and pLM(·), our cost in (4) becomes a coverage-seeking divergence (see [25] for a detailed discussion on divergences with these two different behaviors). To understand this, we consider the following two situations:\n• If pLM(iN |iN 1, . . . , i1)! 0 and p ✓ (i1, . . . , iN ) > 0 for a certain (i1, . . . , iN ), the cross entropy in (3) goes to +1 and the cross entropy in (4) approaches zero.\n• If pLM(iN |iN 1, . . . , i1) > 0 and p ✓ (i1, . . . , iN )! 0 for a certain (i1, . . . , iN ), the cross entropy in (3) approaches zero and the cross entropy in (4) goes to +1.\nTherefore, the cost function (3) will heavily penalize the classifier if it predicts an output that is believed to be less probable by the prior distribution pLM(·), and it will not penalize the classifier when it does not predict an output that pLM(·) believes to be probable. That is, the classifier is encouraged to predict a single output mode with high probability in pLM(·), a behavior called “mode-seeking” in [25]. This probably explains the phenomena observed in [7]: the training process easily converges to\na trivial solution of predicting the same output that has the largest probability in pLM(·). In contrast, the cost (4) will heavily penalize the classifier if it does not predict the output for which pLM(·) is positive, and will penalize less if it predicts outputs for which pLM(·) is zero. That is, this cost will encourage p\n✓ (y|x) to cover as much of pLM(·) as possible, a behavior called “coverage-seeking” in [25]. Therefore, training the classifier using (4) will make it less inclined to learn trivial solutions than that in [7] since it will be heavily penalized. We will verify this fact in our experiment section 4. In addition, the coverage-seeking property could make the learning less sensitive to the sparseness of language models (i.e., pLM is zero for some N -grams) since the cost will not penalize these N -grams. In summary, our proposed cost (1) is more suitable for unsupervised learning than that in [7]."
    }, {
      "heading" : "2.4 The difficulties of optimizing J (✓)",
      "text" : "However, there are two main challenges of optimizing the Empirical-ODM cost J (✓) in (1). The first one is that the sample average (over the entire training data set) in the expression of p\n✓ (·) (see (2)) is inside the logarithmic loss, which is different from traditional machine learning problems where the average is outside loss functions (e.g., P t f t\n(✓)). This functional form prevents us from applying stochastic gradient descent (SGD) to minimize (1) as the stochastic gradients would be intrinsically biased (see Appendix C for a detailed discussion and see section 4 for the experiment results). The second challenge is that the cost function J (✓) is highly non-convex even with linear classifiers. To see this, we visualize the profile of the cost function J (✓) (restricted to a two-dimensional sub-space) around the supervised solution in Figure 1.56 We observe that there are local optimal solutions and there are high barriers between the local and global optimal solutions. Therefore, besides the difficulty of having the sample average inside the logarithmic loss, minimizing this cost function directly will be difficult since crossing the high barriers to reach the global optimal solution would be hard if not properly initialized."
    }, {
      "heading" : "3 The Stochastic Primal-Dual Gradient (SPDG) Algorithm",
      "text" : "To address the first difficulty in Section 2.4, we transform the original cost (1) into an equivalent min-max problem in order to bring the sample average out of the logarithmic loss. Then, we could obtain unbiased stochastic gradients to solve the problem. To this end, we first introduce the concept of convex conjugate functions. For a given convex function f(u), its convex conjugate function f?(⌫) is defined as f?(⌫) , sup\nu\n(⌫\nT u f(u)) [6, pp.90-95], where u and ⌫ are called primal and dual variables, respectively. For a scalar function f(u) = lnu, its conjugate function can be calculated as f?(⌫) = 1 ln( ⌫) with ⌫ < 0. Furthermore, it holds that f(u) = sup\n⌫\n(u\nT ⌫ f?(⌫)), by\n5The approach to visualizing the profile is explained with more detail in Appendix F. More slices and a video of the profiles from many angles can be found in the supplementary material.\n6Note that the supervised solution (red dot) coincides with the global optimal solution of J (✓). The intuition for this is that the classifier trained by supervised learning should also produce output N -gram distribution that is close to the prior marginal output N -gram distribution given by pLM(·).\nAlgorithm 1 Stochastic Primal-Dual Gradient Method 1: Input data: D\nX = {(xn1 , . . . , xn Tn ) : n = 1, . . . ,M} and pLM(i1, . . . , iN ). 2: Initialize ✓ and V where the elements of V are negative 3: repeat 4: Randomly sample a mini-batch of B subsequences of length N from all the sequences in the\ntraining set D X , i.e., B = {(xnm tm N+1, . . . , x nm tm )}B m=1.\n5: Compute the stochastic gradients for each subsequence in the mini-batch and average them\n✓ = 1\nB\nBX\nm=1\n@L nm tm\n@✓\n, V = 1\nB\nBX\nm=1\n@L nm tm\n@V\n+ @\n@V\nX\ni1...iN\npLM(i1,. . ., iN ) ln( ⌫i1,...,iN)\n6: Update ✓ and V according to ✓ ✓ µ ✓ ✓ and V V + µ v V . 7: until convergence or a certain stopping condition is met\nwhich we have lnu = max ⌫ (u⌫+1+ ln( ⌫)).7 Substituting it into (1), the original minimization problem becomes the following equivalent min-max problem:\nmin\n✓\nmax\n{⌫i1,...,iN<0}\n⇢ L(✓, V ) , 1\nT\nMX\nn=1\nTnX\nt=1\nL\nn\nt\n(✓, V ) +\nX\ni1,...,iN\npLM(i1, . . . , iN ) ln( ⌫i1,...,iN )\n(5)\nwhere V , {⌫ i1,...,iN } is a collection of all the dual variables ⌫i1,...,iN , and Lnt (✓, V ) is the t-th component function in the n-th sequence, defined as\nL\nn\nt\n(✓, V ) , X\ni1,...,iN\npLM(i1, . . . , iN )⌫i1,...,iN\nN 1Y\nk=0\np\n✓\n(y\nn t k= iN k|xnt k)\nIn the equivalent min-max problem (5), we find the optimal solution (✓?, V ?) by minimizing L with respect to the primal variable ✓ and maximizing L with respect to the dual variable V . The obtained optimal solution to (5), (✓?, V ?), is called the saddle point of L [6]. Once it is obtained, we only keep ✓?, which is also the optimal solution to (1) and thus the model parameter.\nWe further note that the equivalent min-max problem (5) is now in a form that sums over T = T1 + · · ·+ TM component functions Ln\nt (✓, V ). Therefore, the empirical average has been brought out of the logarithmic loss and we are ready to apply stochastic gradient methods. Specifically, we minimize L with respect to the primal variable ✓ by stochastic gradient descent and maximize L with respect to the dual variable V by stochastic gradient ascent. Therefore, we name the algorithm stochastic primal-dual gradient (SPDG) method (see its details in Algorithm 1). We implement the SPDG algorithm in TensorFlow, which automatically computes the stochastic gradients.8 Finally, the constraint on dual variables ⌫\ni1,...,iN are automatically enforced by the inherent log-barrier, ln( ⌫\ni1,...,iN ), in (5) [6]. Therefore, we do not need a separate method to enforce the constraint.\nWe now show that the above min-max (primal-dual) reformulation also alleviates the second difficulty discussed in Section 2.4. Similar to the case of J (✓), we examine the profile of L(✓, V ) in (5) (restricted to a two-dimensional sub-space) around the optimal (supervised) solution in Figure 2a (see Appendix F for the visualization details). Comparing Figure 2a to Figure 1, we observe that the profile of L(✓, V ) is smoother than that of J (✓) and the barrier is significantly lower. To further compare J (✓) and L(✓, V ), we plot in Figure 2b the values of J (✓) and L(✓, V ) along the same line of ✓? +\np (✓1 ✓?) for different p. It shows that the barrier of L(✓, V ) along the primal direction is lower than that in J (✓). These observations imply that the reformulated min-max problem (5) is better conditioned than the original problem (1), which further justifies the use of SPDG method.\n7The supremum is attainable and is thus replaced by maximum. 8The code will be released soon."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental setup",
      "text" : "We evaluate our unsupervised learning scheme described in earlier secitons using two classification tasks, unsupervised character-level OCR and unsupervised English Spelling Correction (Spell-Corr). In both tasks, there is no label provided during training. Hence, they are both unsupervised.\nFor the OCR task, we obtain our dataset from a public database UWIII English Document Image Database [27], which contains images for each line of text with its corresponding groudtruth. We first use Tesseract [19] to segment the image for each line of text into characters tiles and assign each tile with one character. We verify the segmentation result by training a simple neural network classifier on the segmented results and achieve 0.9% error rate on the test set. Then, we select sentence segments that are longer than 100 and contain only lowercase English characters and common punctuations (space, comma, and period). As a result, we have a vocabulary of size 29 and we obtain 1,175 sentence segments including 153,221 characters for our OCR task. To represent images, we extract VGG19 features with dim = 4096, and project them into 200-dimension vectors using Principal Component Analysis. We train the language models (LM) pLM(·) to provide the required output sequence statistics from both in-domain and out-of-domain data sources. The out-of-domain data sources are completely different databases, including three different language partitions (CNA, NYT, XIN) in the English Gigaword database [26].\nIn Spell-Corr task, we learn to correct the spelling from a mis-spelled text. From the AFP partition of the Gigaword database, we select 500 sentence segments into our Spell-Corr dataset. We select sentences that are longer than 100 and contain only English characters and common punctuations, resulting in a total of 83,567 characters. The mis-spelled texts are generated by substitution simulations and are treated as our inputs. The objective of this task is to recover the original text."
    }, {
      "heading" : "4.2 Results: Comparing optimization algorithms",
      "text" : "In the first set of experiments, we aim to evaluate the effectiveness of the SPDG method as described in Section 3, which is designed for optimizing the Empirical-ODM cost in Section 2. The analysis provided in Sections 2 and 3 sheds insight to why SPDG is superior to the method in [7] and to the standard stochastic gradient descent (SGD) method. The coverage-seeking behavior of the proposed Empirical-ODM cost helps avoid trivial solutions, and the simultaneous optimization of primal-dual variables reduces the barriers in the highly non-convex profile of the cost function. Furthermore, we do not include the methods from [30] because their approaches could not achieve satisfactory results without a few labeled data, while we only consider fully unsupervised learning setting. In addition, the methods in [30] are not optimizing the ODM cost and do not exploit the output sequential statistics.\nTable 1 provides strong experimental evidence demonstrating the substantially greater effectiveness of the primal-dual method over the SGD and the method in [7] on both tasks. All these results are obtained by training the models until converge. Let us examine the results on the OCR in detail. First, the SPGD on the unsupervised cost function achieves 9.21% error rate, much lower than the error rates of any of mini-batch SGD runs, where the size of the mini-batches ranges from 10 to 10,000. Note that, larger mini-batch sizes produce lower errors here because it becomes closer to full-batch gradient and thus lower bias in SGD. On the other hand, when the mini-batch size is as small as 10, the high error rate of 83.09% is close to a guess by majority rule — predicting the character (space) that has a largest proportion in the train set, i.e., 25, 499/153, 221 = 83.37%. Furthermore, the method from [7] does not perform well no matter how we tune the hyperparameters for the generative regularization. Finally and perhaps most interestingly, with no labels provided in the training, the classification errors produced by our method are only about twice compared with supervised learning (4.63% shown in Table 1). This clearly demonstrates that the unsupervised learning scheme proposed in this paper is an effective one. For the Spelling Correction data set (see the third column in Table 1), we observe rather consistent results with the OCR data set."
    }, {
      "heading" : "4.3 Results: Comparing orders of language modeling",
      "text" : "In the second set of experiments, we examine to what extent the use of sequential statistics (e.g. 2- and 3-gram LMs) can do better than the uni-gram LM (no sequential information) in unsupervised learning. The unsupervised prediction results are shown in Table 2, using different data sources to estimate N-gram LM parameters. Consistent across all four ways of estimating reliable N-gram LMs, we observe significantly lower error rates when the unsupervised learning exploits 2-gram and 3-gram LM as sequential statistics compared with exploiting the prior with no sequential statistics (i.e. 1-gram). In three of four cases, exploiting a 3-gram LM gives better results than a 2-gram LM. Furthermore, the comparable error rate associated with 3-gram using out-of-domain output character data (10.17% in Table 2) to that using in-domain output character data (9.59% in Table 1) indicates that the effectiveness of the unsupervised learning paradigm presented in this paper is robust to the quality of the LM acting as the sequential prior."
    }, {
      "heading" : "5 Conclusions and future work",
      "text" : "In this paper, we study the problem of learning a sequence classifier without the need for labeled training data. The practical benefit of such unsupervised learning is tremendous. For example, in large scale speech recognition systems, the currently dominant supervised learning methods typically require a few thousand hours of training data, where each utterance in the acoustic form needs to be labeled by humans. Although there are millions of hours of natural speech data available for training, labeling all of them for supervised learning is less feasible. To make effective use of such\nhuge amounts of acoustic data, the practical unsupervised learning approach discussed in this paper would be called for. Other potential applications such as machine translation, image and video captioning could also benefit from our paradigm. This is mainly because of their common natural language output structure, from which we could exploit the sequential structures for learning the classifier without labels. For other (non-natural-language) applications where there is also a sequential output strucutre, our proposed approach could be applicable in a similar manner. Furthermore, our proposed Empirical-ODM cost function significantly improves over the one in [7] by emphasizing the coverage-seeking behavior. Although the new cost function has a functional form that is more difficult to optimize, a novel SPDG algorithm is developed to effectively address the problem. An analysis of profiles of the cost functions sheds insight to why SPDG works well and why previous methods failed. Finally, we demonstrate in two datasets that our unsupervised learning method is highly effective, producing only about twice errors as fully supervised learning, which no previous unsupervised learning could produce without additional steps of supervised learning. While the current work is restricted to linear classifiers, we intend to generalize the approach to nonlinear models (e.g., deep neural nets [16]) in our future work. We also plan to extend our current method from exploiting N-gram LM to exploiting the currently state-of-the-art neural-LM. Finally, one challenge that remains to be addressed is the scaling of the current method to large vocabulary and high-order LM (i.e., large C and N ). In this case, the summation over all (i1, . . . , iN ) in (5) becomes computationally expensive. A potential solution is to parameterize the dual variable ⌫\ni1,...,iN by a recurrent neural network and approximate the sum using beamsearch, which we leave as a future work."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank all the anonymous reviewers for their constructive feedback."
    } ],
    "references" : [ {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Greedy layer-wise training of deep networks",
      "author" : [ "Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2007
    }, {
      "title" : "Unsupervised transcription of historical documents",
      "author" : [ "Taylor Berg-Kirkpatrick", "Greg Durrett", "Dan Klein" ],
      "venue" : "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Y. Ng", "Michael I. Jordan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Unsupervised learning of predictors from unpaired input-output samples",
      "author" : [ "Jianshu Chen", "Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "A path to unsupervised learning through adversarial networks",
      "author" : [ "Soumith Chintala", "Yann LeCun" ],
      "venue" : "In https://code.facebook.com/posts/1587249151575490/a-path-to-unsupervisedlearning-through-adversarial-networks/,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing",
      "author" : [ "George E Dahl", "Dong Yu", "Li Deng", "Alex Acero" ],
      "venue" : "IEEE Transactions on,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Semi-supervised sequence learning",
      "author" : [ "Andrew M Dai", "Quoc V Le" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Deep learning for speech and language processing",
      "author" : [ "Li Deng" ],
      "venue" : "In Tutorial at Interspeech Conf,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow" ],
      "venue" : "In Tutorial at NIPS, http://www.cs.toronto.edu/",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Deep Learning, by MIT Press. 2016",
      "author" : [ "Ian Goodfellow", "Yoshua Bengio", "Aaron Courville" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Sequence transduction with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1211.3711,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-Rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath", "B. Kingsbury" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2006
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "Geoffrey E Hinton", "Ruslan R Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Tesseract: An open-source optical character recognition",
      "author" : [ "Anthony Kay" ],
      "venue" : "engine. Linux Journal,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Unsupervised analysis for decipherment problems",
      "author" : [ "Kevin Knight", "Anish Nair", "Nishit Rathod", "Kenji Yamada" ],
      "venue" : "In Proceedings of the COLING/ACL,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Building high-level features using large scale unsupervised learning",
      "author" : [ "Quoc Le", "Marc’Aurelio Ranzato", "Rajat Monga", "Matthieu Devin", "Kai Chen", "Greg Corrado", "Jeff Dean", "Andrew Ng" ],
      "venue" : "In International Conference in Machine Learning,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Cryptology: From caesar ciphers to public-key cryptosystems",
      "author" : [ "Dennis Luciano", "Gordon Prichett" ],
      "venue" : "The College Mathematics Journal,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1987
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Divergence measures and message passing",
      "author" : [ "Tom Minka" ],
      "venue" : "Technical report, Technical report, Microsoft Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2005
    }, {
      "title" : "English gigaword fourth edition ldc2009t13",
      "author" : [ "Robert et al Parker" ],
      "venue" : "Philadelphia: Linguistic Data Consortium,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2009
    }, {
      "title" : "Parallel distributed processing: Explorations in the microstructure of cognition, vol. 1. chapter Information Processing in Dynamical Systems: Foundations of Harmony Theory, pages 194–281",
      "author" : [ "P. Smolensky" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1986
    }, {
      "title" : "Label-free supervision of neural networks with physics and domain knowledge",
      "author" : [ "Russell Stewart", "Stefano Ermon" ],
      "venue" : "In Proceedings of AAAI,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2017
    }, {
      "title" : "Towards principled unsupervised learning",
      "author" : [ "Ilya Sutskever", "Rafal Jozefowicz", "Karol Gregor", "Danilo Rezende", "Tim Lillicrap", "Oriol Vinyals" ],
      "venue" : "arXiv preprint arXiv:1511.06440,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].",
      "startOffset" : 194,
      "endOffset" : 232
    }, {
      "referenceID" : 24,
      "context" : "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].",
      "startOffset" : 194,
      "endOffset" : 232
    }, {
      "referenceID" : 15,
      "context" : "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].",
      "startOffset" : 194,
      "endOffset" : 232
    }, {
      "referenceID" : 14,
      "context" : "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].",
      "startOffset" : 194,
      "endOffset" : 232
    }, {
      "referenceID" : 3,
      "context" : "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].",
      "startOffset" : 194,
      "endOffset" : 232
    }, {
      "referenceID" : 0,
      "context" : "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].",
      "startOffset" : 194,
      "endOffset" : 232
    }, {
      "referenceID" : 27,
      "context" : "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].",
      "startOffset" : 194,
      "endOffset" : 232
    }, {
      "referenceID" : 17,
      "context" : "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].",
      "startOffset" : 194,
      "endOffset" : 232
    }, {
      "referenceID" : 11,
      "context" : "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].",
      "startOffset" : 194,
      "endOffset" : 232
    }, {
      "referenceID" : 9,
      "context" : "The problem we consider here is different from most studies on unsupervised learning, which concern automatic discovery of inherent regularities of the input data to learn their representations [13, 28, 18, 17, 5, 1, 31, 20, 14, 12].",
      "startOffset" : 194,
      "endOffset" : 232
    }, {
      "referenceID" : 19,
      "context" : "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].",
      "startOffset" : 210,
      "endOffset" : 228
    }, {
      "referenceID" : 15,
      "context" : "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].",
      "startOffset" : 210,
      "endOffset" : 228
    }, {
      "referenceID" : 1,
      "context" : "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].",
      "startOffset" : 210,
      "endOffset" : 228
    }, {
      "referenceID" : 21,
      "context" : "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].",
      "startOffset" : 210,
      "endOffset" : 228
    }, {
      "referenceID" : 7,
      "context" : "When these methods are applied in prediction tasks, either the learned representations are used as feature vectors [22] or the learned unsupervised models are used to initialize a supervised learning algorithm [9, 18, 2, 24, 10].",
      "startOffset" : 210,
      "endOffset" : 228
    }, {
      "referenceID" : 8,
      "context" : "Recently, various solutions have been proposed to address the input-to-output prediction problem without using labeled training data, all without demonstrated successes [11, 30, 7].",
      "startOffset" : 169,
      "endOffset" : 180
    }, {
      "referenceID" : 26,
      "context" : "Recently, various solutions have been proposed to address the input-to-output prediction problem without using labeled training data, all without demonstrated successes [11, 30, 7].",
      "startOffset" : 169,
      "endOffset" : 180
    }, {
      "referenceID" : 4,
      "context" : "Recently, various solutions have been proposed to address the input-to-output prediction problem without using labeled training data, all without demonstrated successes [11, 30, 7].",
      "startOffset" : 169,
      "endOffset" : 180
    }, {
      "referenceID" : 4,
      "context" : "Similar to this work, the authors in [7] proposed an unsupervised cost that also exploits the sequence prior of the output samples to train classifiers.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : "models in unsupervised learning was also demonstrated in earlier studies in [21, 3].",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "models in unsupervised learning was also demonstrated in earlier studies in [21, 3].",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "For example, it was shown in [7] that optimizing the highly non-convex unsupervised cost function could easily get stuck in trivial solutions, although adding a special regularization mitigated the problem somewhat.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "The solution provided in this paper fundamentally improves these prior works in [11, 30, 7] in following aspects.",
      "startOffset" : 80,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "The solution provided in this paper fundamentally improves these prior works in [11, 30, 7] in following aspects.",
      "startOffset" : 80,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "The solution provided in this paper fundamentally improves these prior works in [11, 30, 7] in following aspects.",
      "startOffset" : 80,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "First, we propose a novel cost function for unsupervised learning, and find that it has a desired coverage-seeking property that makes the learning algorithm less inclined to be stuck in trivial solutions than the cost function in [7].",
      "startOffset" : 231,
      "endOffset" : 234
    }, {
      "referenceID" : 26,
      "context" : "Second, we develop a special empirical formulation of this cost function that avoids the need for a strong generative model as in [30, 11].",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "Second, we develop a special empirical formulation of this cost function that avoids the need for a strong generative model as in [30, 11].",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 26,
      "context" : "2 Empirical-ODM: An unsupervised learning cost for sequence classifiers In this section, we extend the earlier work of [30] and propose an unsupervised learning cost named Empirical Output Distribution Match (Empirical-ODM) for training classifiers without labeled data.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "In (3)The work [11] only proposed a conceptual idea of using generative models to integrate the output structure and the output-to-input structure for unsupervised learning in speech recognition.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "No mathematical formulation or successful experimental results are provided in [11].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 25,
      "context" : "A recent work that shares the same motivations as our work is [29], which also recognizes the high cost of obtaining labeled data and seeks label-free prediction.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "Finally, our problem is fundamentally different from the sequence transduction method in [15], although it also exploits language models for sequence prediction.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "Specifically, the method in [15] is a fully supervised learning in that it requires supervision at the sequence level; that is, for each input sequence, a corresponding output sequence (of possibly different length) is provided as a label.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "The use of language model in [15] only serves the purpose of regularization in the sequence-level supervised learning.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 20,
      "context" : "It is mainly inspired by the approach to breaking the Caesar cipher, one of the simplest forms of encryption [23].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 26,
      "context" : "In [30], the authors proposed to minimize an output distribution match (ODM) cost, defined as the KL-divergence between the prior output distribution and the marginalized output distribution, D(pLM(y)||p✓(y)), where p✓(y) , R",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : "Our proposed Empirical-ODM cost is different from the ODM cost in [30] in three key aspects.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 26,
      "context" : ", yN ) (N -gram) whereas in [30] y = yt (unigram, i.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : "It might also explain why the method in [30] failed as it does not exploit the sequence structure.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 26,
      "context" : "This is critical in that it allows us to directly minimize the divergence between two output distributions without the need for a generative model, which [30] could not do.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "3 Coverage-seeking versus mode-seeking We now discuss an important property of the proposed Empirical-ODM cost (1) by comparing it with the cost proposed in [7].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "We show that the Empirical-ODM cost has a coverage-seeking property, which makes it more suitable for unsupervised learning than the mode-seeking cost in [7].",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "In [7], the authors proposed the expected negative log-likelihood as the unsupervised learning cost function that exploits the output sequential statistics.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "We show that the cost in the form of (3) proposed in [7] is a mode-seeking divergence between two distributions, while by swapping p",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : "✓ (·) and pLM(·), our cost in (4) becomes a coverage-seeking divergence (see [25] for a detailed discussion on divergences with these two different behaviors).",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "That is, the classifier is encouraged to predict a single output mode with high probability in pLM(·), a behavior called “mode-seeking” in [25].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "This probably explains the phenomena observed in [7]: the training process easily converges to 4",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 22,
      "context" : "✓ (y|x) to cover as much of pLM(·) as possible, a behavior called “coverage-seeking” in [25].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : "Therefore, training the classifier using (4) will make it less inclined to learn trivial solutions than that in [7] since it will be heavily penalized.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "In summary, our proposed cost (1) is more suitable for unsupervised learning than that in [7].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 16,
      "context" : "We first use Tesseract [19] to segment the image for each line of text into characters tiles and assign each tile with one character.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 23,
      "context" : "The out-of-domain data sources are completely different databases, including three different language partitions (CNA, NYT, XIN) in the English Gigaword database [26].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "The analysis provided in Sections 2 and 3 sheds insight to why SPDG is superior to the method in [7] and to the standard stochastic gradient descent (SGD) method.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 26,
      "context" : "Furthermore, we do not include the methods from [30] because their approaches could not achieve satisfactory results without a few labeled data, while we only consider fully unsupervised learning setting.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 26,
      "context" : "In addition, the methods in [30] are not optimizing the ODM cost and do not exploit the output sequential statistics.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "Table 1 provides strong experimental evidence demonstrating the substantially greater effectiveness of the primal-dual method over the SGD and the method in [7] on both tasks.",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, the method from [7] does not perform well no matter how we tune the hyperparameters for the generative regularization.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "Data sets SPDG (Ours) Method from [7] SGD h10i SGD h100i SGD h1ki SGD h10ki Supervised Learning Majority Guess OCR 9.",
      "startOffset" : 34,
      "endOffset" : 37
    } ],
    "year" : 2017,
    "abstractText" : "We consider learning a sequence classifier without labeled data by using sequential output statistics. The problem is highly valuable since obtaining labels in training data is often costly, while the sequential output statistics (e.g., language models) could be obtained independently of input data and thus with low or no cost. To address the problem, we propose an unsupervised learning cost function and study its properties. We show that, compared to earlier works, it is less inclined to be stuck in trivial solutions and avoids the need for a strong generative model. Although it is harder to optimize in its functional form, a stochastic primal-dual gradient method is developed to effectively solve the problem. Experiment results on real-world datasets demonstrate that the new unsupervised learning method gives drastically lower errors than other baseline methods. Specifically, it reaches test errors about twice of those obtained by fully supervised learning.",
    "creator" : null
  }
}