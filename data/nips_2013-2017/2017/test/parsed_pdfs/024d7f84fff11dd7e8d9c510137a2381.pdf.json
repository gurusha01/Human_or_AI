{
  "name" : "024d7f84fff11dd7e8d9c510137a2381.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Accelerated consensus via Min-Sum Splitting",
    "authors" : [ "Patrick Rebeschini" ],
    "emails" : [ "patrick.rebeschini@stats.ox.ac.uk", "sekhar.tatikonda@yale.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Min-Sum is a local message-passing algorithm designed to distributedly optimize an objective function that can be written as a sum of component functions, each of which depends on a subset of the decision variables. Due to its simplicity, Min-Sum has emerged as canonical protocol to address large scale problems in a variety of domains, including signal processing, statistics, and machine learning. For problems supported on tree graphs, the Min-Sum algorithm corresponds to dynamic programming and is guaranteed to converge to the problem solution. For arbitrary graphs, the ordinary Min-Sum algorithm may fail to converge, or it may converge to something different than the problem solution [28]. In the case of strictly convex objective functions, there are known sufficient conditions to guarantee the convergence and correctness of the algorithm. The most general condition requires the Hessian of the objective function to be scaled diagonally dominant [28, 25]. While the Min-Sum scheme can be applied to optimization problems with constraints, by incorporating the constraints into the objective function as hard barriers, the known sufficient conditions do not apply in this case.\nIn [34], a generalization of the traditional Min-Sum scheme has been proposed, based on a reparametrization of the original objective function. This algorithm is called Splitting, as it can be derived by creating equivalent graph representations for the objective function by “splitting” the nodes of the original graph. In the case of unconstrained problems with quadratic objective functions, where Min-Sum is also known as Gaussian Belief Propagation, the algorithm with splitting has been shown to yield convergence in settings where the ordinary Min-Sum does not converge [35]. To date, a theoretical investigation of the rates of convergence of Min-Sum Splitting has not been established.\nIn this paper we establish rates of convergence for the Min-Sum Splitting algorithm applied to solve the consensus problem, which can be formulated as an equality-constrained problem in optimization. The basic version of the consensus problem is the network averaging problem. In this setting, each node in a graph is assigned a real number, and the goal is to design a distributed protocol that allows the nodes to iteratively exchange information with their neighbors so to arrive at consensus on the average across the network. Early work include [42, 41]. The design of distributed algorithms to solve the averaging problem has received a lot of attention recently, as consensus represents a widely-used primitive to compute aggregate statistics in a variety of fields. Applications include, for instance, estimation problems in sensor networks, distributed tracking and localization, multi-agents coordination, and distributed inference [20, 21, 9, 19]. Consensus is typically combined with some\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nform of local optimization over a peer-to-peer network, as in the case of iterative subgradient methods [29, 40, 17, 10, 6, 16, 39]. In large-scale machine learning, consensus is used as a tool to distribute the minimization of a loss function over a large dataset into a network of processors that can exchange and aggregate information, and only have access to a subset of the data [31, 11, 26, 3].\nClassical algorithms to solve the network averaging problem involve linear dynamical systems supported on the nodes of the graph. Even when the coefficients that control the dynamics are optimized, these methods are known to suffer from a “diffusive” rate of convergence, which corresponds to the rate of convergence to stationarity exhibited by the “diffusion” random walk naturally associated to a graph [44, 2]. This rate is optimal for graphs with good expansion properties, such as complete graphs or expanders. In this case the convergence time, i.e., the number of iterations required to reach a prescribed level of error accuracy ε > 0 in the `2 norm relative to the initial condition, scales independently of the dimension of the problem, as Θ(log 1/ε). For graphs with geometry this rate is suboptimal [7], and it does not yield a convergence time that matches the lower bound Ω(D log 1/ε), where D is the graph diameter [37, 36]. For example, in both cycle graphs and in grid-like topologies the number of iterations scale like Θ(D2 log 1/ε) (if n is the number of nodes, D ∼ n in a cycle and D ∼ √ n in a two-dimensional torus). Θ(D2 log 1/ε) is also the convergence time exhibited in random geometric graphs, which represent the relevant topologies for many applications in sensor networks [9]. In [7] it was established that for a class of graphs with geometry (polynomial growth or finite doubling dimension), the mixing time of any reversible Markov chain scales at least like D2, embodying the fact that symmetric walks on these graphs take D2 steps to travel distances of orderD.\nMin-Sum schemes to solve the consensus problem have been previously investigated in [27]. The authors show that the ordinary Min-Sum algorithm does not converge in graphs with cycles. They investigate a modified version of it that uses a soft barrier function to incorporate the equality constrains into the objective function. In the case of d-regular graphs, upon a proper choice of initial conditions, the authors show that the algorithm they propose reduces to a linear process supported on the directed edges of the graph, and they characterize the convergence time of the algorithm in terms of the Cesàro mixing time of a Markov chain defined on the set of directed edges of the original graph. In the case of cycle graphs (i.e., d = 2), they prove that the mixing time scales like O(D), which yields the convergence time O(D/ε log 1/ε). See Theorem 4 and Theorem 5 in [27]. In the case of (d/2)-dimensional tori (D ∼ n2/d), they conjecture that the mixing time is Θ(D2(d−1)/d), but do not present bounds for the convergence time. See Conjecture 1 in [27]. For other graph topologies, they leave the mixing time (and convergence time) achieved by their method as an open question.\nIn this paper we show that the Min-Sum scheme based on splitting yields convergence to the consensus solution, and we analytically establish rates of convergence for any graph topology. First, we show that a certain parametrization of the Min-Sum protocol for consensus yields a linear message-passing update for any graph and for any choice of initial conditions. Second, we show that the introduction of the splitting parameters is not only fundamental to guarantee the convergence and correctness of the Min-Sum scheme in the consensus problem, but that proper tuning of these parameters yields accelerated (i.e., “subdiffusive”) asymptotic rates of convergence. We establish a square-root improvement for the asymptotic convergence time over diffusive methods, which allows Min-Sum Splitting to scale like O(D log(D/ε)) for cycles and tori. Our results show that Min-Sum schemes are competitive and get close to the optimal rate O(D log(1/ε)) recently established for some algorithms based on Nesterov’s acceleration [30, 36]. The main tool used for the analysis involves the construction of an auxiliary linear process supported on the nodes of the original graph to track the evolution of the Min-Sum Splitting algorithm, which is instead supported on the directed edges. This construction allows us to relate the convergence time of the Min-Sum scheme to the spectral gap of the matrix describing the dynamics of the auxiliary process, which is easier to analyze than the matrix describing the dynamics on the edges as in [27].\nIn the literature, overcoming the suboptimal convergence rate of classical algorithms for network averaging consensus has motivated the design of several accelerated methods. Two main lines of research have been developed, and seem to have evolved independently of each others: one involves lifted Markov chains techniques, see [37] for a review, the other involves accelerated first order methods in convex optimization, see [13] for a review. Another contribution of this paper is to show that Min-Sum Splitting bears similarities with both types of accelerated methods. On the one hand, Min-Sum can be seen as a process on a lifted space, which is the space of directed edges in the original graph. Here, splitting is seen to introduce a directionality in the message exchange of the ordinary Min-Sum protocol that is analogous to the directionality introduced in non-reversible\nrandom walks on lifted graphs to achieve faster convergence to stationarity. The advantage of the Min-Sum algorithm over lifted Markov chain methods is that no lifted graph needs to be constructed. On the other hand, the directionality induced on the edges by splitting translates into a memory term for the auxiliary algorithm running on the nodes. This memory term, which allows nodes to remember previous values and incorporate them into the next update, directly relates the Min-Sum Splitting algorithm to accelerated multi-step first order methods in convex optimization. In particular, we show that a proper choice of the splitting parameters recovers the same matrix that support the evolution of shift-register methods used in numerical analysis for linear solvers, and, as a consequence, we recover the same accelerated rate of convergence for consensus [45, 4, 24].\nTo summarize, the main contributions of this paper are:\n1. First connection of Min-Sum schemes with lifted Markov chains techniques and multi-step methods in convex optimization.\n2. First proof of how the directionality embedded in Belief Propagation protocols can be tuned and exploited to accelerate the convergence rate towards the problem solution.\n3. First analysis of convergence rates for Min-Sum Splitting. New proof technique based on the introduction of an auxiliary process to track the evolution of the algorithm on the nodes.\n4. Design of a Min-Sum protocol for the consensus problem that achieves better convergence rates than the ones established (and conjectured) for the Min-Sum method in [27].\nOur results motivate further studies to generalize the acceleration due to splittings to other problems.\nThe paper is organized as follows. In Section 2 we introduce the Min-Sum Splitting algorithm in its general form. In Section 3 we describe the consensus problem and review the classical diffusive algorithms. In Section 4 we review the main accelerated methods that have been proposed in the literature. In Section 5 we specialize the Min-Sum Splitting algorithm to the consensus problem, and show that a proper parametrization yields a linear exchange of messages supported on the directed edges of the graph. In Section 6 we derive the auxiliary message-passing algorithm that allows us to track the evolution of the Min-Sum Splitting algorithm via a linear process with memory supported on the nodes of the graph. In Section 7 we state Theorem 1, which shows that a proper choice of the tuning parameters recovers the rates of shift-registers. Proofs are given in the supplementary material."
    }, {
      "heading" : "2 The Min-Sum Splitting algorithm",
      "text" : "The Min-Sum algorithm is a distributed routine to optimize a cost function that is the sum of components supported on a given graph structure. Given a simple graph G = (V,E) with n := |V | vertices and m := |E| edges, let us assume that we are given a set of functions φv : R→ R ∪ {∞}, for each v ∈ V , and φvw = φwv : R × R → R ∪ {∞}, for each {v, w} ∈ E, and that we want to solve the following problem over the decision variables x = (xv)v∈V ∈ RV :\nminimize ∑ v∈V φv(xv) + ∑ {v,w}∈E φvw(xv, xw). (1)\nThe Min-Sum algorithm describes an iterative exchange of messages—which are functions of the decision variables—associated to each directed edge in G. Let E := {(v, w) ∈ V ×V : {v, w} ∈ E} be the set of directed edges associated to the undirected edges in E (each edge in E corresponds to two edges in E). In this work we consider the synchronous implementation of the Min-Sum algorithm where at any given time step s, each directed edge (v, w) ∈ E supports two messages, ξ̂svw, µ̂ s vw : R→ R ∪ {∞}. Messages are computed iteratively. Given an initial choice of messages µ̂0 = (µ̂0vw)(v,w)∈E , the Min-Sum scheme that we investigate in this paper is given in Algorithm 1. Henceforth, for each v ∈ V , let N (v) := {w ∈ V : {v, w} ∈ E} denote the neighbors of node v. The formulation of the Min-Sum scheme given in Algorithm 1, which we refer to as Min-Sum Splitting, was introduced in [34]. This formulation admits as tuning parameters the real number δ ∈ R and the symmetric matrix Γ = (Γvw)v,w∈V ∈ RV×V . Without loss of generality, we assume that the sparsity of Γ respects the structure of the graph G, in the sense that if {v, w} 6∈ E then Γvw = 0 (note that Algorithm 1 only involves summations with respect to nearest neighbors in the graph). The choice of δ = 1 and Γ = A, where A is the adjacency matrix defined as Avw := 1 if {v, w} ∈ E and Avw := 0 otherwise, yields the ordinary Min-Sum algorithm. For\nAlgorithm 1: Min-Sum Splitting Input: Messages µ̂0 = (µ̂0vw)(v,w)∈E ; parameters δ ∈ R and Γ ∈ RV×V symmetric; time t ≥ 1. for s ∈ {1, . . . , t} do\nξ̂swv = φv/δ − µ̂s−1wv + ∑ z∈N (v) Γzvµ̂ s−1 zv , (w, v) ∈ E ;\nµ̂swv = minz∈R{φvw( · , z)/Γvw + (δ − 1)ξ̂swv + δξ̂svw(z)}, (w, v) ∈ E ; µtv = φv + δ ∑ w∈N (v) Γwvµ̂ t wv, v ∈ V ; Output: xtv = arg minz∈R µtv(z), v ∈ V .\nan arbitrary choice of strictly positive integer parameters, Algorithm 1 can be seen to correspond to the ordinary Min-Sum algorithm applied to a new formulation of the original problem, where an equivalent objective function is obtained from the original one in (1) by splitting each term φvw into Γvw ∈ N \\ {0} terms, and each term φv into δ ∈ N \\ {0} terms. Namely, minimize∑ v∈V ∑δ k=1 φ k v(xv) + ∑ {v,w}∈E ∑Γvw k=1 φ k vw(xv, xw), with φ k v := φv/δ and φ k vw := φvw/Γvw. 1 Hence the reason for the name “splitting” algorithm. Despite this interpretation, Algorithm 1 is defined for any real choice of parameters δ and Γ.\nIn this paper we investigate the convergence behavior of the Min-Sum Splitting algorithm for some choices of δ and Γ, in the case of the consensus problem that we define in the next section."
    }, {
      "heading" : "3 The consensus problem and standard diffusive algorithms",
      "text" : "Given a simple graph G = (V,E) with n := |V | nodes, for each v ∈ V let φv : R→ R ∪ {∞} be a given function. The consensus problem is defined as follows:\nminimize ∑ v∈V φv(xv) subject to xv = xw, {v, w} ∈ E. (2)\nWe interpret G as a communication graph where each node represents an agent, and each edge represent a communication channel between neighbor agents. Each agent v is given the function φv , and agents collaborate by iteratively exchanging information with their neighbors in G with the goal to eventually arrive to the solution of problem (2). The consensus problem amounts to designing distributed algorithms to solve problem (2) that respect the communication constraints encoded by G.\nA classical setting investigated in the literature is the least-square case yielding the network averaging problem, where for a given b ∈ RV we have2 φv(z) := 12z\n2 − bvz and the solution of problem (2) is b̄ := 1n ∑ v∈V bv. In this setup, each agent v ∈ V is given a number bv, and agents want to exchange information with their neighbors according to a protocol that allows each of them to eventually reach consensus on the average b̄ across the entire network. Classical algorithms to solve this problem involve a linear exchange of information of the form xt = Wxt−1 with x0 = b, for a given matrix W ∈ RV×V that respects the topology of the graph G (i.e., Wvw 6= 0 only if {v, w} ∈ E or v = w), so that W t → 11T /n for t → ∞, where 1 is the all ones vector. This linear iteration allows for a distributed exchange of information among agents, as at any iteration each agent v ∈ V only receives information from his/her neighbors N (v) via the update: xtv = Wvvx t−1 v + ∑ w∈N (v)Wvwx t−1 w . The original literature on this problem investigates the case where the matrix W has non-negative coefficients and represents the transition matrix of a random walk on the nodes of the graph G, so that Wvw is interpreted as the probability that a random walk at node v visits node w in the next time step. A popular choice is given by the Metropolis-Hastings method [37], which involved the doubly-stochastic matrix WMH defined as WMHvw := 1/(2dmax) if {v, w} ∈ E, WMHvw := 1− dv/(2dmax) if w = v, and WMHvw := 0 otherwise, where dv := |N (v)| is the degree of node v, and dmax := maxv∈V dv is the maximum degree of the graph G.\n1As mentioned in [34], one can also consider a more general formulation of the splitting algorithm with δ → (δv)v∈V ∈ R (possibly also with time-varying parameters). The current choice of the algorithm is motivated by the fact that in the present case the output of the algorithm can be tracked by analyzing a linear system on the nodes of the graph, as we will show in Section 5.\n2In the literature, the classical choice is φv(z) := 12 ∑ v∈V (z − bv) 2, which yields the same results as the quadratic function that we define in the main text, as constant terms in the objective function do not alter the optimal point of the problem but only the optimal value of the objective function.\nIn [44], necessary and sufficient conditions are given for a generic matrixW to satisfyW t → 11T /n, namely, 1TW = 1T , W1 = 1, and ρ(W − 11T /n) < 1, where ρ(M) denotes the spectral radius of a given matrix M . The authors show that the problem of choosing the optimal symmetric matrix W that minimizes ρ(W − 11T /n) = ‖W − 11T /n‖— where ‖M‖ denotes the spectral norm of a matrix M that coincides with ρ(M) if M is symmetric — is a convex problem and it can be cast as a semi-definite program. Typically, the optimal matrix involves negative coefficients, hence departing from the random walk interpretation. However, even the optimal choice of symmetric matrix is shown to yield a diffusive rate of convergence, which is already attained by the matrix WMH [7]. This rate corresponds to the speed of convergence to stationarity achieved by the diffusion random walk, defined as the Markov chain with transition matrix diag(d)−1A, where diag(d) ∈ RV×V is the degree matrix, i.e., diagonal with diag(d)vv := dv, and A ∈ RV×V is the adjacency matrix, i.e., symmetric with Avw := 1 if {v, w} ∈ E, and Avw := 0 otherwise. For instance, the condition ‖W − 11T /n‖t ≤ ε, where ‖ · ‖ is the `2 norm, yields a convergence time that scales like t ∼ Θ(D2 log(1/ε)) in cycle graphs and tori [33], where D is the graph diameter. The authors in [7] established that for a class of graphs with geometry (polynomial growth or finite doubling dimension) the mixing time of any reversible Markov chain scales at least like D2, and it is achieved by Metropolis-Hastings [37]."
    }, {
      "heading" : "4 Accelerated algorithms",
      "text" : "To overcome the diffusive behavior typical of classical consensus algorithms, two main types of approaches have been investigated in the literature, which seem to have been developed independently.\nThe first approach involves the construction of a lifted graph Ĝ = (V̂ , Ê) and of a linear system supported on the nodes of it, of the form x̂t = Ŵ x̂t−1, where Ŵ ∈ RV̂×V̂ is the transition matrix of a non-reversible Markov chain on the nodes of Ĝ. This approach has its origins in the work of [8] and [5], where it was observed for the first time that certain non-reversible Markov chains on properly-constructed lifted graphs yield better mixing times than reversible chains on the original graphs. For some simple graph topologies, such as cycle graphs and two-dimensional grids, the construction of the optimal lifted graphs is well-understood already from the works in [8, 5]. A general theory of lifting in the context of Gossip algorithms has been investigated in [18, 37]. However, this construction incurs additional overhead, which yield non-optimal computational complexity, even for cycle graphs and two-dimensional grids. Typically, lifted random walks on arbitrary graph topologies are constructed on a one-by-one case, exploiting the specifics of the graph at hand. This is the case, for instance, for random geometric graphs [22, 23]. The key property that allows non-reversible lifted Markov chains to achieve subdiffusive rates is the introduction of a directionality in the process to break the diffusive nature of reversible chains. The strength of the directionality depends on global properties of the original graph, such as the number of nodes [8, 5] or the diameter [37]. See Figure 1.\nThe second approach involves designing linear updates that are supported on the original graph G and keep track of a longer history of previous iterates. This approach relies on the fact that the original consensus update xt = Wxt−1 can be interpreted as a primal-dual gradient ascent method to solve problem (2) with a quadratic objective function [32]. This allows the implementation of accelerated\ngradient methods. To the best of our knowledge, this idea was first introduced in [14], and since then it has been investigated in many other papers. We refer to [13, 24], and references in there, for a review and comparison of multi-step accelerated methods for consensus. The simplest multi-step extension of gradient methods is Polyak’s “heavy ball,” which involves adding a “momentum” term to the standard update and yields a primal iterate of the form xt = Wxt−1 + γ(xt−1 − xt−2). Another popular multi-step method involves Nesterov’s acceleration, and yields xt = (1 + γ)Wxt−1 − γWxt−2. Aligned with the idea of adding a momentum term is the idea of adding a shift register term, which yields xt = (1 + γ)Wxt−1 − γxt−2. For our purposes, we note that these methods can be written as(\nxt\nxt−1\n) = K ( xt−1\nxt−2\n) , (3)\nfor a certain matrix K ∈ R2n×2n. As in the case of lifted Markov chains techniques, also multi-step methods are able to achieve accelerated rates by exploiting some form of global information: the choice of the parameter γ that yields subdiffusive rates depends on the eigenvalues of W . Remark 1. Beyond lifted Markov chains techniques and accelerated first order methods, many other algorithms have been proposed to solve the consensus problem. The literature is vast. As we focus on Min-Sum schemes, an exhaustive literature review on consensus is beyond the scope of our work. Of particular interest for our results is the distributed ADMM approach [3, 43, 38]. Recently in [12], for a class of unconstrained problems with quadratic objective functions, it has been shown that message-passing ADMM schemes can be interpreted as lifting of gradient descent techniques. This prompts for further investigation to connect Min-Sum, ADMM, and accelerated first order methods.\nIn the next two sections we show that Min-Sum Splitting bears similarities with both types of accelerated methods described above. On the one hand, in Section 5 we show that the estimates xtv’s of Algorithm 1 applied to the network averaging problem can be interpreted as the result of a linear process supported on a lifted space, i.e., the space E of directed edges associated to the undirected edges of G. On the other hand, in Section 6 we show that the estimates xtv’s can be seen as the result of a linear multi-step process supported on the nodes of G, which can be written as in (3). Later on, in Section 7 and Section 8, we will see that the similarities just described go beyond the structure of the processes, and they extend to the acceleration mechanism itself. In particular, the choice of splitting parameters that yields subdiffusive convergence rates, matching the asymptotic rates of shift register methods, is also shown to depend on global information about G."
    }, {
      "heading" : "5 Min-Sum Splitting for consensus",
      "text" : "We apply Min-Sum Splitting to solve network averaging. We show that in this case the messagepassing protocol is a linear exchange of parameters associated to the directed edges in E . Given δ ∈ R and Γ ∈ RV×V symmetric, let ĥ(δ) ∈ RE be the vector defined as ĥ(δ)wv := bw + (1− 1/δ)bv , and let K̂(δ,Γ) ∈ RE×E be matrix defined as\nK̂(δ,Γ)wv,zu :=  δΓzw if u = w, z ∈ N (w) \\ {v}, δ(Γvw − 1) if u = w, z = v, (δ − 1)Γzv if u = v, z ∈ N (v) \\ {w}, (δ − 1)(Γwv − 1) if u = v, z = w, 0 otherwise.\n(4)\nConsider Algorithm 2 with initial conditions R̂0 = (R̂0vw)(v,w)∈E ∈ RE , r̂0 = (r̂0vw)(v,w)∈E ∈ RE .\nAlgorithm 2: Min-Sum Splitting, consensus problem, quadratic case\nInput: R̂0, r̂0 ∈ RE ; δ ∈ R, Γ ∈ RV×V symmetric; K̂(δ,Γ) defined in (5); t ≥ 1. for s ∈ {1, . . . , t} do\nR̂s = (2− 1/δ)1 + K̂(δ,Γ)R̂s−1; r̂s = ĥ(δ) + K̂(δ,Γ)r̂s−1;\nOutput: xtv := bv+δ\n∑ w∈N(v) Γwv r̂ t wv\n1+δ ∑\nw∈N(v) ΓwvR̂ t wv\n, v ∈ V .\nProposition 1. Let δ ∈ R and Γ ∈ RV×V symmetric be given. Consider Algorithm 1 applied to problem (2) with φv(z) := 12z 2−bvz and with quadratic initial messages: µ̂0vw(z) = 12 R̂ 0 vwz\n2−r̂0vwz, for some R̂0vw > 0 and r̂ 0 vw ∈ R. Then, the messages will remain quadratic, i.e., µ̂svw(z) = 12 R̂ s vwz\n2− r̂svwz for any s ≥ 1, and the parameters evolve as in Algorithm 2. If 1 + δ ∑ w∈N (v) ΓwvR̂ t wv > 0 for any v ∈ V and t ≥ 1, then the output of Algorithm 2 coincides with the output of Algorithm 1."
    }, {
      "heading" : "6 Auxiliary message-passing scheme",
      "text" : "We show that the output of Algorithm 2 can be tracked by a new message-passing scheme that corresponds to a multi-step linear exchange of parameters associated to the nodes of G. This auxiliary algorithm represents the main tool to establish convergence rates for the Min-Sum Splitting protocol, i.e., Theorem 1 below. The intuition behind the auxiliary process is that while Algorithm 1 (hence, Algorithm 2) involves an exchange of messages supported on the directed edges E , the computation of the estimates xtv’s only involve the belief functions µ t v’s, which are supported on the nodes of G. Due to the simple nature of the pairwise equality constraints in the consensus problem, in the present case a reparametrization allows to track the output of Min-Sum via an algorithm that directly updates the belief functions on the nodes of the graph, which yields Algorithm 3.\nGiven δ ∈ R and Γ ∈ Rn×n symmetric, define the matrix K(δ,Γ) ∈ R2n×2n as\nK(δ,Γ) := ( (1− δ)I − (1− δ)diag(Γ1) + δΓ δI δI − δdiag(Γ1) + (1− δ)Γ (1− δ)I ) , (5)\nwhere I ∈ RV×V is the identity matrix and diag(Γ1) ∈ RV×V is diagonal with (diag(Γ1))vv = (Γ1)v = ∑ w∈N (v) Γvw. Consider Algorithm 3 with initial conditions R 0, r0, Q0, q0 ∈ RV .\nAlgorithm 3: Auxiliary message-passing Input: R0, r0, Q0, q0 ∈ RV ; δ ∈ R, Γ ∈ RV×V symmetric; K(δ,Γ) defined in (5); t ≥ 1. for s ∈ {1, . . . , t} do(\nrs qs\n) = K(δ,Γ) ( rs−1\nqs−1\n) ;\n( Rs\nQs\n) = K(δ,Γ) ( Rs−1\nQs−1\n) ;\nOutput: xtv := rtv/Rtv, v ∈ V .\nProposition 2. Let δ ∈ R and Γ ∈ RV×V symmetric be given. The output of Algorithm 2 with initial conditions R̂0, r̂0 ∈ RE is the output of Algorithm 3 with R0v := 1 + δ ∑ w∈N (v) ΓwvR̂ 0 wv, Q 0 v :=\n1− δ ∑ w∈N (v) ΓwvR̂ 0 wv , r 0 v := bv + δ ∑ w∈N (v) Γwv r̂ 0 wv , and q 0 v := bv − δ ∑ w∈N (v) Γvw r̂ 0 vw.\nProposition 2 shows that upon proper initialization, the outputs of Algorithm 2 and Algorithm 3 are equivalent. Hence, Algorithm 3 represents a tool to investigate the convergence behavior of the Min-Sum Splitting algorithm. Analytically, the advantage of the formulation given in Algorithm 3 over the one given in Algorithm 2 is that the former involves two coupled systems of n equations whose convergence behavior can explicitly be linked to the spectral properties of the n× n matrix Γ, as we will see in Theorem 1 below. On the contrary, the linear system of 2m equations in Algorithm 2 does not seem to exhibit an immediate link to the spectral properties of Γ. In this respect, we note that the previous paper that investigated Min-Sum schemes for consensus, i.e., [27], characterized the convergence rate of the algorithm under consideration — albeit only in the case of d-regular graphs, and upon initializing the quadratic terms to the fix point — in terms of the spectral gap of a matrix that controls a linear system of 2m equations. However, the authors only list results on the behavior of this spectral gap in the case of cycle graphs, i.e., d = 2, and present a conjecture for 2d-tori."
    }, {
      "heading" : "7 Accelerated convergence rates for Min-Sum Splitting",
      "text" : "We investigate the convergence behavior of the Min-Sum Splitting algorithm to solve problem (2) with quadratic objective functions. Henceforth, without loss of generality, let b ∈ RV be given with 0 < bv < 1 for each v ∈ V , and let φv(z) := 12z 2 − bvz. Define b̄ := ∑ v∈V bv/n.\nRecall from [27] that the ordinary Min-Sum algorithm (i.e., Algorithm 2 with δ = 1 and Γ = A, where A is the adjacency matrix of the graph G) does not converge if the graph G has a cycle.\nWe now show that a proper choice of the tuning parameters allows Min-Sum Splitting to converge to the problem solution in a subdiffusive way. The proof of this result, which is contained in the supplementary material, relies on the use of the auxiliary method defined in Algorithm 3 to track the evolution of the Min-Sum Splitting scheme. Here, recall that ‖x‖ denotes the `2 norm of a given vector x, ‖M‖ denotes the `2 matrix norm of the given matrix M , and ρ(M) its spectral radius. Theorem 1. Let W ∈ RV×V be a symmetric matrix with W1 = 1 and ρW := ρ(W − 11T /n) < 1. Let δ = 1 and Γ = γW , with γ = 2/(1 + √ 1− ρ2W ). Let xt be the output at time t of Algorithm 2 with initial conditions R̂0 = r̂0 = 0. Define\nK :=\n( γW I\n(1− γ)I 0\n) , K∞ :=\n1\n(2− γ)n\n( 11T 11T (1− γ)11T (1− γ)11T ) . (6)\nThen, for any v ∈ V we have limt→∞ xtv = b̄ and ‖xt − b̄1‖ ≤ 4 √ 2n 2−γ ‖(K −K\n∞)t‖. The asymptotic rate of convergence is given by\nρK := ρ(K −K∞) = limt→∞ ‖(K −K∞)t‖1/t = √ (1− √ 1−ρ2W )/(1+ √ 1−ρ2W ) < ρW < 1,\nwhich satisfies 12 √ 1/(1− ρW ) ≤ 1/(1− ρK) ≤ √ 1/(1− ρW ).\nTheorem 1 shows that the choice of splitting parameters δ = 1 and Γ = γW , where γ and W are defined as in the statement of the theorem, allows the Min-Sum Splitting scheme to achieve the asymptotic rate of convergence that is given by the second largest eigenvalue in magnitude of the matrix K defined in (6), i.e., the quantity ρK . The matrix K is the same matrix that describes shift-register methods for consensus [45, 4, 24]. In fact, the proof of Theorem 1 relies on the spectral analysis previously established for shift-registers, which can be traced back to [15]. See also [13, 24]. Following [27], let us consider the absolute measure of error given by ‖xt − b̄1‖/ √ n (recall that we assume 0 < bv < 1 so that ‖b‖ ≤ √ n). From Theorem 1 it follows that, asymptotically, we have\n‖xt − b̄1‖/ √ n . 4 √ 2ρtK/(2− γ). If we define the asymptotic convergence time as the minimum\ntime t so that, asymptotically, ‖xt− b̄1‖/ √ n . ε, then the Min-Sum Splitting scheme investigated in Theorem 1 has an asymptotic convergence time that isO(1/(1−ρK) log{[1/(1−ρK)]/ε}). Given the last bound in Theorem 1, this result achieves (modulo logarithmic terms) a square-root improvement over the convergence time of diffusive methods, which scale like Θ(1/(1− ρW ) log 1/ε). For cycle graphs and, more generally, for higher-dimensional tori — where 1/(1 − ρW ) is Θ(D2) so that 1/(1−ρK) is Θ(D) [33, 1] — the convergence time isO(D logD/ε), whereD is the graph diameter. As prescribed by Theorem 1, the choice of γ that makes the Min-Sum scheme achieve a subdiffusive rate depends on global properties of the graph G. Namely, γ depends on the quantity ρW , the second largest eigenvalue in magnitude of the matrix W . This fact connects the acceleration mechanism induced by splitting in the Min-Sum scheme to the acceleration mechanism of lifted Markov chains techniques (see Figure 1) and multi-step first order methods, as described in Section 4.\nIt remains to be investigated how choices of splitting parameters different than the ones investigated in Theorem 1 affect the convergence behavior of the Min-Sum Splitting algorithm."
    }, {
      "heading" : "8 Conclusions",
      "text" : "The Min-Sum Splitting algorithm has been previously observed to yield convergence in settings where the ordinary Min-Sum protocol does not converge [35]. In this paper we proved that the introduction of splitting parameters is not only fundamental to guarantee the convergence of the Min-Sum scheme applied to the consensus problem, but that proper tuning of these parameters yields accelerated convergence rates. As prescribed by Theorem 1, the choice of splitting parameters that yields subdiffusive rates involves global type of information, via the spectral gap of a matrix associated to the original graph (see the choice of γ in Theorem 1). The acceleration mechanism exploited by Min-Sum Splitting is analogous to the acceleration mechanism exploited by lifted Markov chain techniques — where the transition matrix of the lifted random walks is typically chosen to depend on the total number of nodes in the graph [8, 5] or on its diameter [37] (global pieces of information) — and to the acceleration mechanism exploited by multi-step gradient methods — where the momentum/shift-register term is chosen as a function of the eigenvalues of a matrix supported on the original graph [13] (again, a global information). Prior to our results, this connection seems to have not been established in the literature. Our findings motivate further studies to generalize the acceleration due to splittings to other problem instances, beyond consensus."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was partially supported by the NSF under Grant EECS-1609484."
    } ],
    "references" : [ {
      "title" : "Reversible markov chains and random walks on graphs",
      "author" : [ "David Aldous", "James Allen Fill" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Randomized gossip algorithms",
      "author" : [ "S. Boyd", "A. Ghosh", "B. Prabhakar", "D. Shah" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein" ],
      "venue" : "Found. Trends Mach. Learn.,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Accelerated gossip algorithms for distributed computation",
      "author" : [ "Ming Cao", "Daniel A. Spielman", "Edmund M. Yeh" ],
      "venue" : "Proc. 44th Ann. Allerton Conf. Commun.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Lifting markov chains to speed up mixing",
      "author" : [ "Fang Chen", "László Lovász", "Igor Pak" ],
      "venue" : "In Proceedings of the Thirty-first Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : "Diffusion adaptation strategies for distributed optimization and learning over networks",
      "author" : [ "J. Chen", "A.H. Sayed" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Moderate growth and random walk on finite groups",
      "author" : [ "P. Diaconis", "L. Saloff-Coste" ],
      "venue" : "Geometric & Functional Analysis GAFA,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1994
    }, {
      "title" : "Analysis of a nonreversible markov chain sampler",
      "author" : [ "Persi Diaconis", "Susan Holmes", "Radford M. Neal" ],
      "venue" : "The Annals of Applied Probability,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "Gossip algorithms for distributed signal processing",
      "author" : [ "A.G. Dimakis", "S. Kar", "J.M.F. Moura", "M.G. Rabbat", "A. Scaglione" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Dual averaging for distributed optimization: Convergence analysis and network scaling",
      "author" : [ "John C. Duchi", "Alekh Agarwal", "Martin J. Wainwright" ],
      "venue" : "IEEE Trans. Automat. Contr.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Consensus-based distributed support vector machines",
      "author" : [ "Pedro A. Forero", "Alfonso Cano", "Georgios B. Giannakis" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Markov chain lifting and distributed admm",
      "author" : [ "G. França", "J. Bento" ],
      "venue" : "IEEE Signal Processing Letters,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2017
    }, {
      "title" : "Multi-step gradient methods for networked optimization",
      "author" : [ "E. Ghadimi", "I. Shames", "M. Johansson" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "First and second order diffusive methods for rapid, coarse, distributed load balancing (extended abstract)",
      "author" : [ "Bhaskar Ghosh", "S. Muthukrishnan", "Martin H. Schultz" ],
      "venue" : "In Proceedings of the Eighth Annual ACM Symposium on Parallel Algorithms and Architectures,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1996
    }, {
      "title" : "Chebyshev semi-iterative methods, successive overrelaxation iterative methods, and second order richardson iterative methods",
      "author" : [ "Gene H. Golub", "Richard S. Varga" ],
      "venue" : "Numer. Math.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1961
    }, {
      "title" : "Fast distributed gradient methods",
      "author" : [ "D. Jakovetić", "J. Xavier", "J.M.F. Moura" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "A randomized incremental subgradient method for distributed optimization in networked systems",
      "author" : [ "Björn Johansson", "Maben Rabi", "Mikael Johansson" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Distributed averaging via lifted markov chains",
      "author" : [ "K. Jung", "D. Shah", "J. Shin" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Topology for distributed inference on graphs",
      "author" : [ "S. Kar", "S. Aldosari", "J.M.F. Moura" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2008
    }, {
      "title" : "Distributed Sensor Networks: A Multiagent Perspective",
      "author" : [ "V. Lesser", "C. Ortiz", "M. Tambe", "editors" ],
      "venue" : "(Edited book),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2003
    }, {
      "title" : "Detection, classification, and tracking of targets",
      "author" : [ "Dan Li", "K.D. Wong", "Yu Hen Hu", "A.M. Sayeed" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2002
    }, {
      "title" : "Accelerating distributed consensus via lifting markov chains",
      "author" : [ "W. Li", "H. Dai" ],
      "venue" : "IEEE International Symposium on Information Theory,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Location-aided fast distributed consensus in wireless networks",
      "author" : [ "W. Li", "H. Dai", "Y. Zhang" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Analysis of accelerated gossip",
      "author" : [ "Ji Liu", "Brian D.O. Anderson", "Ming Cao", "A. Stephen Morse" ],
      "venue" : "algorithms. Automatica,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Walk-sums and belief propagation in gaussian graphical models",
      "author" : [ "Dmitry M. Malioutov", "Jason K. Johnson", "Alan S. Willsky" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    }, {
      "title" : "Distributed sparse linear regression",
      "author" : [ "G. Mateos", "J.A. Bazerque", "G.B. Giannakis" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "Consensus propagation",
      "author" : [ "C.C. Moallemi", "B. Van Roy" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2006
    }, {
      "title" : "Convergence of min-sum message-passing for convex optimization",
      "author" : [ "Ciamac C. Moallemi", "Benjamin Van Roy" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "Distributed subgradient methods for multi-agent optimization",
      "author" : [ "A. Nedic", "A. Ozdaglar" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2009
    }, {
      "title" : "Linear Time Average Consensus on Fixed Graphs and Implications for Decentralized Optimization and Multi-Agent Control",
      "author" : [ "A. Olshevsky" ],
      "venue" : "ArXiv e-prints",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "A collaborative training algorithm for distributed learning",
      "author" : [ "J.B. Predd", "S.R. Kulkarni", "H.V. Poor" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "Generalized consensus computation in networked systems with erasure links",
      "author" : [ "M.G. Rabbat", "R.D. Nowak", "J.A. Bucklew" ],
      "venue" : "In IEEE 6th Workshop on Signal Processing Advances in Wireless Communications,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2005
    }, {
      "title" : "Bounding fastest mixing",
      "author" : [ "Sébastien Roch" ],
      "venue" : "Electron. Commun. Probab.,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2005
    }, {
      "title" : "Message-passing algorithms: Reparameterizations and splittings",
      "author" : [ "N. Ruozzi", "S. Tatikonda" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2013
    }, {
      "title" : "Message-passing algorithms for quadratic minimization",
      "author" : [ "Nicholas Ruozzi", "Sekhar Tatikonda" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2013
    }, {
      "title" : "Optimal algorithms for smooth and strongly convex distributed optimization in networks",
      "author" : [ "Kevin Scaman", "Francis Bach", "Sébastien Bubeck", "Yin Tat Lee", "Laurent Massoulié" ],
      "venue" : "In Proceedings of the 34th International Conference on Machine Learning,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2017
    }, {
      "title" : "On the linear convergence of the ADMM in decentralized consensus optimization",
      "author" : [ "W. Shi", "Q. Ling", "K. Yuan", "G. Wu", "W. Yin" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2014
    }, {
      "title" : "Extra: An exact first-order algorithm for decentralized consensus optimization",
      "author" : [ "Wei Shi", "Qing Ling", "Gang Wu", "Wotao Yin" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2015
    }, {
      "title" : "Distributed stochastic subgradient projection algorithms for convex optimization",
      "author" : [ "S. Sundhar Ram", "A. Nedić", "V.V. Veeravalli" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2010
    }, {
      "title" : "Distributed asynchronous deterministic and stochastic gradient optimization algorithms",
      "author" : [ "J. Tsitsiklis", "D. Bertsekas", "M. Athans" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 1986
    }, {
      "title" : "Problems in Decentralized Decision Making and Computation",
      "author" : [ "John N. Tsitsiklis" ],
      "venue" : "PhD thesis, Department of EECS,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 1984
    }, {
      "title" : "Distributed alternating direction method of multipliers",
      "author" : [ "E. Wei", "A. Ozdaglar" ],
      "venue" : "IEEE 51st IEEE Conference on Decision and Control (CDC),",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2012
    }, {
      "title" : "Fast linear iterations for distributed averaging",
      "author" : [ "Lin Xiao", "Stephen Boyd" ],
      "venue" : "Systems & Control Letters,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2004
    }, {
      "title" : "Second-degree iterative methods for the solution of large linear systems",
      "author" : [ "David M Young" ],
      "venue" : "Journal of Approximation Theory,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 1972
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "For arbitrary graphs, the ordinary Min-Sum algorithm may fail to converge, or it may converge to something different than the problem solution [28].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 27,
      "context" : "The most general condition requires the Hessian of the objective function to be scaled diagonally dominant [28, 25].",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 24,
      "context" : "The most general condition requires the Hessian of the objective function to be scaled diagonally dominant [28, 25].",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 33,
      "context" : "In [34], a generalization of the traditional Min-Sum scheme has been proposed, based on a reparametrization of the original objective function.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 34,
      "context" : "In the case of unconstrained problems with quadratic objective functions, where Min-Sum is also known as Gaussian Belief Propagation, the algorithm with splitting has been shown to yield convergence in settings where the ordinary Min-Sum does not converge [35].",
      "startOffset" : 256,
      "endOffset" : 260
    }, {
      "referenceID" : 19,
      "context" : "Applications include, for instance, estimation problems in sensor networks, distributed tracking and localization, multi-agents coordination, and distributed inference [20, 21, 9, 19].",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 20,
      "context" : "Applications include, for instance, estimation problems in sensor networks, distributed tracking and localization, multi-agents coordination, and distributed inference [20, 21, 9, 19].",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "Applications include, for instance, estimation problems in sensor networks, distributed tracking and localization, multi-agents coordination, and distributed inference [20, 21, 9, 19].",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 18,
      "context" : "Applications include, for instance, estimation problems in sensor networks, distributed tracking and localization, multi-agents coordination, and distributed inference [20, 21, 9, 19].",
      "startOffset" : 168,
      "endOffset" : 183
    }, {
      "referenceID" : 28,
      "context" : "form of local optimization over a peer-to-peer network, as in the case of iterative subgradient methods [29, 40, 17, 10, 6, 16, 39].",
      "startOffset" : 104,
      "endOffset" : 131
    }, {
      "referenceID" : 38,
      "context" : "form of local optimization over a peer-to-peer network, as in the case of iterative subgradient methods [29, 40, 17, 10, 6, 16, 39].",
      "startOffset" : 104,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "form of local optimization over a peer-to-peer network, as in the case of iterative subgradient methods [29, 40, 17, 10, 6, 16, 39].",
      "startOffset" : 104,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "form of local optimization over a peer-to-peer network, as in the case of iterative subgradient methods [29, 40, 17, 10, 6, 16, 39].",
      "startOffset" : 104,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "form of local optimization over a peer-to-peer network, as in the case of iterative subgradient methods [29, 40, 17, 10, 6, 16, 39].",
      "startOffset" : 104,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : "form of local optimization over a peer-to-peer network, as in the case of iterative subgradient methods [29, 40, 17, 10, 6, 16, 39].",
      "startOffset" : 104,
      "endOffset" : 131
    }, {
      "referenceID" : 37,
      "context" : "form of local optimization over a peer-to-peer network, as in the case of iterative subgradient methods [29, 40, 17, 10, 6, 16, 39].",
      "startOffset" : 104,
      "endOffset" : 131
    }, {
      "referenceID" : 30,
      "context" : "In large-scale machine learning, consensus is used as a tool to distribute the minimization of a loss function over a large dataset into a network of processors that can exchange and aggregate information, and only have access to a subset of the data [31, 11, 26, 3].",
      "startOffset" : 251,
      "endOffset" : 266
    }, {
      "referenceID" : 10,
      "context" : "In large-scale machine learning, consensus is used as a tool to distribute the minimization of a loss function over a large dataset into a network of processors that can exchange and aggregate information, and only have access to a subset of the data [31, 11, 26, 3].",
      "startOffset" : 251,
      "endOffset" : 266
    }, {
      "referenceID" : 25,
      "context" : "In large-scale machine learning, consensus is used as a tool to distribute the minimization of a loss function over a large dataset into a network of processors that can exchange and aggregate information, and only have access to a subset of the data [31, 11, 26, 3].",
      "startOffset" : 251,
      "endOffset" : 266
    }, {
      "referenceID" : 2,
      "context" : "In large-scale machine learning, consensus is used as a tool to distribute the minimization of a loss function over a large dataset into a network of processors that can exchange and aggregate information, and only have access to a subset of the data [31, 11, 26, 3].",
      "startOffset" : 251,
      "endOffset" : 266
    }, {
      "referenceID" : 42,
      "context" : "Even when the coefficients that control the dynamics are optimized, these methods are known to suffer from a “diffusive” rate of convergence, which corresponds to the rate of convergence to stationarity exhibited by the “diffusion” random walk naturally associated to a graph [44, 2].",
      "startOffset" : 276,
      "endOffset" : 283
    }, {
      "referenceID" : 1,
      "context" : "Even when the coefficients that control the dynamics are optimized, these methods are known to suffer from a “diffusive” rate of convergence, which corresponds to the rate of convergence to stationarity exhibited by the “diffusion” random walk naturally associated to a graph [44, 2].",
      "startOffset" : 276,
      "endOffset" : 283
    }, {
      "referenceID" : 6,
      "context" : "For graphs with geometry this rate is suboptimal [7], and it does not yield a convergence time that matches the lower bound Ω(D log 1/ε), where D is the graph diameter [37, 36].",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 35,
      "context" : "For graphs with geometry this rate is suboptimal [7], and it does not yield a convergence time that matches the lower bound Ω(D log 1/ε), where D is the graph diameter [37, 36].",
      "startOffset" : 168,
      "endOffset" : 176
    }, {
      "referenceID" : 8,
      "context" : "Θ(D(2) log 1/ε) is also the convergence time exhibited in random geometric graphs, which represent the relevant topologies for many applications in sensor networks [9].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 6,
      "context" : "In [7] it was established that for a class of graphs with geometry (polynomial growth or finite doubling dimension), the mixing time of any reversible Markov chain scales at least like D(2), embodying the fact that symmetric walks on these graphs take D(2) steps to travel distances of orderD.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 26,
      "context" : "Min-Sum schemes to solve the consensus problem have been previously investigated in [27].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 29,
      "context" : "Our results show that Min-Sum schemes are competitive and get close to the optimal rate O(D log(1/ε)) recently established for some algorithms based on Nesterov’s acceleration [30, 36].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 35,
      "context" : "Our results show that Min-Sum schemes are competitive and get close to the optimal rate O(D log(1/ε)) recently established for some algorithms based on Nesterov’s acceleration [30, 36].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 26,
      "context" : "This construction allows us to relate the convergence time of the Min-Sum scheme to the spectral gap of the matrix describing the dynamics of the auxiliary process, which is easier to analyze than the matrix describing the dynamics on the edges as in [27].",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 12,
      "context" : "Two main lines of research have been developed, and seem to have evolved independently of each others: one involves lifted Markov chains techniques, see [37] for a review, the other involves accelerated first order methods in convex optimization, see [13] for a review.",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 43,
      "context" : "In particular, we show that a proper choice of the splitting parameters recovers the same matrix that support the evolution of shift-register methods used in numerical analysis for linear solvers, and, as a consequence, we recover the same accelerated rate of convergence for consensus [45, 4, 24].",
      "startOffset" : 286,
      "endOffset" : 297
    }, {
      "referenceID" : 3,
      "context" : "In particular, we show that a proper choice of the splitting parameters recovers the same matrix that support the evolution of shift-register methods used in numerical analysis for linear solvers, and, as a consequence, we recover the same accelerated rate of convergence for consensus [45, 4, 24].",
      "startOffset" : 286,
      "endOffset" : 297
    }, {
      "referenceID" : 23,
      "context" : "In particular, we show that a proper choice of the splitting parameters recovers the same matrix that support the evolution of shift-register methods used in numerical analysis for linear solvers, and, as a consequence, we recover the same accelerated rate of convergence for consensus [45, 4, 24].",
      "startOffset" : 286,
      "endOffset" : 297
    }, {
      "referenceID" : 26,
      "context" : "Design of a Min-Sum protocol for the consensus problem that achieves better convergence rates than the ones established (and conjectured) for the Min-Sum method in [27].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 33,
      "context" : "The formulation of the Min-Sum scheme given in Algorithm 1, which we refer to as Min-Sum Splitting, was introduced in [34].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 33,
      "context" : "(1)As mentioned in [34], one can also consider a more general formulation of the splitting algorithm with δ → (δv)v∈V ∈ R (possibly also with time-varying parameters).",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 42,
      "context" : "In [44], necessary and sufficient conditions are given for a generic matrixW to satisfyW t → 11 /n, namely, 1W = 1 , W1 = 1, and ρ(W − 11 /n) < 1, where ρ(M) denotes the spectral radius of a given matrix M .",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 6,
      "context" : "However, even the optimal choice of symmetric matrix is shown to yield a diffusive rate of convergence, which is already attained by the matrix W [7].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 32,
      "context" : "For instance, the condition ‖W − 11 /n‖ ≤ ε, where ‖ · ‖ is the `2 norm, yields a convergence time that scales like t ∼ Θ(D(2) log(1/ε)) in cycle graphs and tori [33], where D is the graph diameter.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "The authors in [7] established that for a class of graphs with geometry (polynomial growth or finite doubling dimension) the mixing time of any reversible Markov chain scales at least like D(2), and it is achieved by Metropolis-Hastings [37].",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 7,
      "context" : "This approach has its origins in the work of [8] and [5], where it was observed for the first time that certain non-reversible Markov chains on properly-constructed lifted graphs yield better mixing times than reversible chains on the original graphs.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : "This approach has its origins in the work of [8] and [5], where it was observed for the first time that certain non-reversible Markov chains on properly-constructed lifted graphs yield better mixing times than reversible chains on the original graphs.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "For some simple graph topologies, such as cycle graphs and two-dimensional grids, the construction of the optimal lifted graphs is well-understood already from the works in [8, 5].",
      "startOffset" : 173,
      "endOffset" : 179
    }, {
      "referenceID" : 4,
      "context" : "For some simple graph topologies, such as cycle graphs and two-dimensional grids, the construction of the optimal lifted graphs is well-understood already from the works in [8, 5].",
      "startOffset" : 173,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : "A general theory of lifting in the context of Gossip algorithms has been investigated in [18, 37].",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "This is the case, for instance, for random geometric graphs [22, 23].",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 22,
      "context" : "This is the case, for instance, for random geometric graphs [22, 23].",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "The strength of the directionality depends on global properties of the original graph, such as the number of nodes [8, 5] or the diameter [37].",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "The strength of the directionality depends on global properties of the original graph, such as the number of nodes [8, 5] or the diameter [37].",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "(b) Non-reversible Markov chain Ŵ on the nodes of the lifted graph Ĝ [8].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 42,
      "context" : "This is also the case for the optimal tuning in classical consensus schemes [44] and for the ADMM lifting in [12].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "This is also the case for the optimal tuning in classical consensus schemes [44] and for the ADMM lifting in [12].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 31,
      "context" : "This approach relies on the fact that the original consensus update x = Wxt−1 can be interpreted as a primal-dual gradient ascent method to solve problem (2) with a quadratic objective function [32].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 13,
      "context" : "To the best of our knowledge, this idea was first introduced in [14], and since then it has been investigated in many other papers.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "We refer to [13, 24], and references in there, for a review and comparison of multi-step accelerated methods for consensus.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 23,
      "context" : "We refer to [13, 24], and references in there, for a review and comparison of multi-step accelerated methods for consensus.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "Of particular interest for our results is the distributed ADMM approach [3, 43, 38].",
      "startOffset" : 72,
      "endOffset" : 83
    }, {
      "referenceID" : 41,
      "context" : "Of particular interest for our results is the distributed ADMM approach [3, 43, 38].",
      "startOffset" : 72,
      "endOffset" : 83
    }, {
      "referenceID" : 36,
      "context" : "Of particular interest for our results is the distributed ADMM approach [3, 43, 38].",
      "startOffset" : 72,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "Recently in [12], for a class of unconstrained problems with quadratic objective functions, it has been shown that message-passing ADMM schemes can be interpreted as lifting of gradient descent techniques.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 26,
      "context" : ", [27], characterized the convergence rate of the algorithm under consideration — albeit only in the case of d-regular graphs, and upon initializing the quadratic terms to the fix point — in terms of the spectral gap of a matrix that controls a linear system of 2m equations.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 26,
      "context" : "Recall from [27] that the ordinary Min-Sum algorithm (i.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 43,
      "context" : "The matrix K is the same matrix that describes shift-register methods for consensus [45, 4, 24].",
      "startOffset" : 84,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "The matrix K is the same matrix that describes shift-register methods for consensus [45, 4, 24].",
      "startOffset" : 84,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : "The matrix K is the same matrix that describes shift-register methods for consensus [45, 4, 24].",
      "startOffset" : 84,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "In fact, the proof of Theorem 1 relies on the spectral analysis previously established for shift-registers, which can be traced back to [15].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 26,
      "context" : "Following [27], let us consider the absolute measure of error given by ‖x − b̄1‖/ √ n (recall that we assume 0 < bv < 1 so that ‖b‖ ≤ √ n).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 32,
      "context" : "For cycle graphs and, more generally, for higher-dimensional tori — where 1/(1 − ρW ) is Θ(D(2)) so that 1/(1−ρK) is Θ(D) [33, 1] — the convergence time isO(D logD/ε), whereD is the graph diameter.",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "For cycle graphs and, more generally, for higher-dimensional tori — where 1/(1 − ρW ) is Θ(D(2)) so that 1/(1−ρK) is Θ(D) [33, 1] — the convergence time isO(D logD/ε), whereD is the graph diameter.",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 34,
      "context" : "The Min-Sum Splitting algorithm has been previously observed to yield convergence in settings where the ordinary Min-Sum protocol does not converge [35].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 7,
      "context" : "The acceleration mechanism exploited by Min-Sum Splitting is analogous to the acceleration mechanism exploited by lifted Markov chain techniques — where the transition matrix of the lifted random walks is typically chosen to depend on the total number of nodes in the graph [8, 5] or on its diameter [37] (global pieces of information) — and to the acceleration mechanism exploited by multi-step gradient methods — where the momentum/shift-register term is chosen as a function of the eigenvalues of a matrix supported on the original graph [13] (again, a global information).",
      "startOffset" : 274,
      "endOffset" : 280
    }, {
      "referenceID" : 4,
      "context" : "The acceleration mechanism exploited by Min-Sum Splitting is analogous to the acceleration mechanism exploited by lifted Markov chain techniques — where the transition matrix of the lifted random walks is typically chosen to depend on the total number of nodes in the graph [8, 5] or on its diameter [37] (global pieces of information) — and to the acceleration mechanism exploited by multi-step gradient methods — where the momentum/shift-register term is chosen as a function of the eigenvalues of a matrix supported on the original graph [13] (again, a global information).",
      "startOffset" : 274,
      "endOffset" : 280
    }, {
      "referenceID" : 12,
      "context" : "The acceleration mechanism exploited by Min-Sum Splitting is analogous to the acceleration mechanism exploited by lifted Markov chain techniques — where the transition matrix of the lifted random walks is typically chosen to depend on the total number of nodes in the graph [8, 5] or on its diameter [37] (global pieces of information) — and to the acceleration mechanism exploited by multi-step gradient methods — where the momentum/shift-register term is chosen as a function of the eigenvalues of a matrix supported on the original graph [13] (again, a global information).",
      "startOffset" : 541,
      "endOffset" : 545
    } ],
    "year" : 2017,
    "abstractText" : "We apply the Min-Sum message-passing protocol to solve the consensus problem in distributed optimization. We show that while the ordinary Min-Sum algorithm does not converge, a modified version of it known as Splitting yields convergence to the problem solution. We prove that a proper choice of the tuning parameters allows Min-Sum Splitting to yield subdiffusive accelerated convergence rates, matching the rates obtained by shift-register methods. The acceleration scheme embodied by Min-Sum Splitting for the consensus problem bears similarities with lifted Markov chains techniques and with multi-step first order methods in convex optimization.",
    "creator" : null
  }
}