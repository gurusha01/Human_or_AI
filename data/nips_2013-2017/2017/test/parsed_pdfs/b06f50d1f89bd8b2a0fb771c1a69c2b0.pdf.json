{
  "name" : "b06f50d1f89bd8b2a0fb771c1a69c2b0.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multi-Task Learning for Contextual Bandits",
    "authors" : [ "Aniket Anand Deshmukh", "Urun Dogan" ],
    "emails" : [ "aniketde@umich.edu", "urun.dogan@skype.net", "clayscot@umich.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A multi-armed bandit (MAB) problem is a sequential decision making problem where, at each time step, an agent chooses one of several “arms,\" and observes some reward for the choice it made. The reward for each arm is random according to a fixed distribution, and the agent’s goal is to maximize its cumulative reward [4] through a combination of exploring different arms and exploiting those arms that have yielded high rewards in the past [15, 11].\nThe contextual bandit problem is an extension of the MAB problem where there is some side information, called the context, associated to each arm [12]. Each context determines the distribution of rewards for the associated arm. The goal in contextual bandits is still to maximize the cumulative reward, but now leveraging the contexts to predict the expected reward of each arm. Contextual bandits have been employed to model various applications like news article recommendation [7], computational advertisement [9], website optimization [20] and clinical trials [19]. For example, in the case of news article recommendation, the agent must select a news article to recommend to a particular user. The arms are articles and contextual features are features derived from the article and the user. The reward is based on whether a user reads the recommended article.\nOne common approach to contextual bandits is to fix the class of policy functions (i.e., functions from contexts to arms) and try to learn the best function with time [13, 18, 16]. Most algorithms estimate rewards either separately for each arm, or have one single estimator that is applied to all arms. In\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\ncontrast, our approach is to adopt the perspective of multi-task learning (MTL). The intuition is that some arms may be similar to each other, in which case it should be possible to pool the historical data for these arms to estimate the mapping from context to rewards more rapidly. For example, in the case of news article recommendation, there may be thousands of articles, and some of those are bound to be similar to each other.\nProblem 1 Contextual Bandits for t = 1, ..., T do\nObserve context xa,t ∈ Rd for all arms a ∈ [N ], where [N ] = {1, ...N} Choose an arm at ∈ [N ] Receive a reward rat,t ∈ R Improve arm selection strategy based on new observation (xat,t, at, rat,t)\nend for\nThe contextual bandit problem is formally stated in Problem 1. The total T trial reward is defined as∑T t=1 rat,t and the optimal T trial reward as ∑T t=1 ra∗t ,t, where rat,t is reward of the selected arm at at time t and a∗t is the arm with maximum reward at trial t. The goal is to find an algorithm that minimizes the T trial regret\nR(T ) = T∑ t=1 ra∗t ,t − T∑ t=1 rat,t.\nWe focus on upper confidence bound (UCB) type algorithms for the remainder of the paper. A UCB strategy is a simple way to represent the exploration and exploitation tradeoff. For each arm, there is an upper bound on reward, comprised of two terms. The first term is a point estimate of the reward, and the second term reflects the confidence in the reward estimate. The strategy is to select the arm with maximum UCB. The second term dominates when the agent is not confident about its reward estimates, which promotes exploration. On the other hand, when all the confidence terms are small, the algorithm exploits the best arm(s) [2].\nIn the popular UCB type contextual bandits algorithm called Lin-UCB, the expected reward of an arm is modeled as a linear function of the context, E[ra,t|xa,t] = xTa,tθ∗a, where ra,t is the reward of arm a at time t and xa,t is the context of arm a at time t. To select the best arm, one estimates θa for each arm independently using the data for that particular arm [13]. In the language of multi-task learning, each arm is a task, and Lin-UCB learns each task independently.\nIn the theoretical analysis of the Lin-UCB [7] and its kernelized version Kernel-UCB [18] θa is replaced by θ, and the goal is to learn one single estimator using data from all the arms. In other words, the data from the different arms are pooled together and viewed as coming from a single task. These two approaches, independent and pooled learning, are two extremes, and reality often lies somewhere in between. In the MTL approach, we seek to pool some tasks together, while learning others independently.\nWe present an algorithm motivated by this idea and call it kernelized multi-task learning UCB (KMTL-UCB). Our main contributions are proposing a UCB type multi-task learning algorithm for contextual bandits, established a regret bound and interpreting the bound to reveal the impact of increased task similarity, introducing a technique for estimating task similarities on the fly, and demonstrating the effectiveness of our algorithm on several datasets.\nThis paper is organized as follows. Section 2 describes related work and in Section 3 we propose a UCB algorithm using multi-task learning. Regret analysis is presented in Section 4, and our experimental findings are reported in Section 5. We conclude in Section 6."
    }, {
      "heading" : "2 Related Work",
      "text" : "A UCB strategy is a common approach to quantify the exploration/exploitation tradeoff. At each time step t, and for each arm a, a UCB strategy estimates a reward r̂a,t and a one-sided confidence interval above r̂a,t with width ŵa,t. The term ucba,t = r̂a,t + ŵa,t is called the UCB index or just UCB. Then at each time step t, the algorithm chooses the arm a with the highest UCB.\nIn contextual bandits, the idea is to view learning the mapping x 7→ r as a regression problem. Lin-UCB uses a linear regression model while Kernel-UCB uses a nonlinear regression model drawn from the reproducing kernel Hilbert space (RKHS) of a symmetric and positive definite (SPD) kernel. Either of these two regression models could be applied in either the independent setting or the pooled setting. In the independent setting, the regression function for each arm is estimated separately. This was the approach adopted by Li et al. [13] with a linear model. Regret analysis for both Lin-UCB and Kernel-UCB adopted the pooled setting [7, 18]. Kernel-UCB in the independent setting has not previously been considered to our knowledge, although the algorithm would just be a kernelized version of Li et al. [13]. We will propose a methodology that extends the above four combinations of setting (independent and pooled) and regression model (linear and nonlinear). Gaussian Process UCB (GP-UCB) uses a Gaussian prior on the regression function and is a Bayesian equivalent of Kernel-UCB [16].\nThere are some contextual bandit setups that incorporate multi-task learning. In Lin-UCB with Hybrid Linear Models the estimated reward consists of two linear terms, one that is arm-specific and another that is common to all arms [13]. Gang of bandits [5] uses a graph structure (e.g., a social network) to transfer the learning from one user to other for personalized recommendation. Collaborative filtering bandits [14] is a similar technique which clusters the users based on context. Contextual Gaussian Process UCB (CGP-UCB) builds on GP-UCB and has many elements in common with our framework [10]. We defer a more detailed comparison to CGP-UCB until later."
    }, {
      "heading" : "3 KMTL-UCB",
      "text" : "We propose an alternate regression model that includes the independent and pooled settings as special cases. Our approach is inspired by work on transfer and multi-task learning in the batch setting [3, 8]. Intuitively, if two arms (tasks) are similar, we can pool the data for those arms to train better predictors for both.\nFormally, we consider regression functions of the form\nf : X̃ 7→ Y\nwhere X̃ = Z × X , and Z is what we call the task similarity space, X is the context space and Y ⊆ R is the reward space. Every context xa ∈ X is associated with an arm descriptor za ∈ Z , and we define x̃a = (za, xa) to be the augmented context. Intuitively, za is a variable that can be used to determine the similarity between different arms. Examples of Z and za will be given below.\nLet k̃ be a SPD kernel on X̃ . In this work we focus on kernels of the form k̃ ( (z, x), (z′, x′) ) = kZ(z, z ′)kX (x, x ′), (1)\nwhere kX is a SPD kernel on X , such as linear or Gaussian kernel if X = Rd, and kZ is a kernel on Z (examples given below). LetHk̃ be the RKHS of functions f : X̃ 7→ R associated to k̃. Note that a product kernel is just one option for k̃, and other forms may be worth exploring."
    }, {
      "heading" : "3.1 Upper Confidence Bound",
      "text" : "Instead of learning regression estimates for each arm separately, we effectively learn regression estimates for all arms at once by using all the available training data. Let N be the total number of distinct arms that algorithm has to choose from. Define [N ] = {1, ..., N} and let the observed contexts at time t be xa,t,∀a ∈ [N ]. Let na,t be the number of times the algorithm has selected arm a up to and including time t so that ∑N a=1 na,t = t. Define sets ta = {τ < t : aτ = a}, where aτ is the arm selected at time τ . Notice that |ta| = na,t−1 for all a. We solve the following problem at time t:\nf̂t = arg min f∈Hk̃\n1\nN N∑ a=1 1 na,t−1 ∑ τ∈ta (f(x̃a,τ )− ra,τ )2 + λ‖f‖2Hk̃ , (2)\nwhere x̃a,τ is the augmented context of arm a at time τ , and ra,τ is the reward of an arm a selected at time τ . This problem (2) is a variant of kernel ridge regression. Applying the representer theorem [17]\nthe optimal f can be expressed as f = ∑N a′=1 ∑ τ ′∈ta′\nαa′,τ ′ k̃(·, x̃a′,τ ′), which yields the solution (detailed derivation is in the supplementary material)\nf̂t(x̃) = k̃t−1(x̃) T (ηt−1K̃t−1 + λI) −1ηt−1yt−1, (3)\nwhere K̃t−1 is the (t − 1) × (t − 1) kernel matrix on the augmented data [x̃aτ ,τ ]t−1τ=1, k̃t−1(x̃) = [k̃(x̃, x̃aτ ,τ )] t−1 τ=1 is a vector of kernel evaluations between x̃ and the past data, yt−1 = [raτ ,τ ] t−1 τ=1 are all observed rewards, and ηt−1 is the (t− 1)× (t− 1) diagonal matrix ηt−1 = diag[ 1naτ,t−1 ] t−1 τ=1.\nWhen x̃ = x̃a,t, we write k̃a,t = k̃t−1(x̃a,t). With only minor modifications to the argument in Valko et al [18], we have the following: Lemma 1. Suppose the rewards [raτ ,τ ]Tτ=1 are independent random variables with means\nE[raτ ,τ |x̃aτ ,τ ] = f∗(x̃aτ ,τ ), where f∗ ∈ Hk̃ and ‖f∗‖Hk̃ ≤ c. Let α = √ log(2TN/δ) 2 and δ > 0. With probability at least 1− δT , we have that ∀a ∈ [N ]\n|f̂t(x̃a,t)− f∗(x̃a,t)| ≤ wa,t := (α+ c √ λ)sa,t (4)\nwhere sa,t = λ−1/2 √ k̃(x̃a,t, x̃a,t)− k̃Ta,t(ηt−1K̃t−1 + λI)−1ηt−1k̃a,t.\nThe result in Lemma 1 motivates the UCB\nucba,t = f̂t(x̃a,t) + wa,t\nand inspires Algorithm 1.\nAlgorithm 1 KMTL-UCB Input: β ∈ R+, for t = 1, ..., T do\nUpdate the (product) kernel matrix K̃t−1 and ηt−1 Observe context features at time t: xa,t for each a ∈ [N ]. Determine arm descriptor za for each a ∈ [N ] to get augmented context x̃a,t. for all a at time t do pa,t ← f̂t(x̃a,t) + βsa,t end for Choose arm at = arg max pa,t, observe a real valued payoff rat,t and update yt . Output: at\nend for\nBefore an arm has been selected at least once, f̂t(x̃a,t) and the second term in sa,t, i.e., k̃Ta,t(ηt−1K̃t−1 + λI) −1ηt−1k̃a,t, are taken to be 0. In that case, the algorithm only uses the first\nterm of sa,t, i.e., √ k̃(x̃a,t, x̃a,t), to form the UCB."
    }, {
      "heading" : "3.2 Choice of Task Similarity Space and Kernel",
      "text" : "To illustrate the flexibility of our framework, we present the following three options for Z and kZ :\n1. Independent: Z = {1, ..., N}, kZ(a, a′) = 1a=a′ . The augmented context for a context xa from arm a is just (a, xa).\n2. Pooled: Z = {1}, kZ ≡ 1. The augmented context for a context xa for arm a is just (1, xa). 3. Multi-Task: Z = {1, ..., N} and kZ is a PSD matrix reflecting arm/task similarities. If this\nmatrix is unknown, it can be estimated as discussed below.\nAlgorithm 1 with the first two choices specializes to the independent and pooled settings mentioned previously. In either setting, choosing a linear kernel for kX leads to Lin-UCB, while a more general kernel essentially gives rise to Kernel-UCB. We will argue that the multi-task setting facilitates learning when there is high task similarity.\nWe also introduce a fourth option for Z and kZ that allows task similarity to be estimated when it is unknown. In particular, we are inspired by the kernel transfer learning framework of Blanchard et al. [3]. Thus, we define the arm similarity space to be Z = PX , the set of all probability distributions on X . We further assume that contexts for arm a are drawn from probability measure Pa. Given a context xa for arm a, we define its augmented context to be (Pa, xa).\nTo define a kernel on Z = PX , we use the same construction described in [3], originally introduced by Steinwart and Christmann [6]. In particular, in our experiments we use a Gaussian-like kernel\nkZ(Pa, Pa′) = exp(−‖Ψ(Pa)−Ψ(Pa′)‖2/2σ2Z), (5) where Ψ(P ) = ∫ k′X (·, x)dPx is the kernel mean embedding of a distribution P . This embedding is defined by yet another SPD kernel k′X on X , which could be different from the kX used to define k̃. We may estimate Ψ(Pa) via Ψ(P̂a) = 1na,t−1 ∑ τ∈ta k ′ X (·, xaτ ,τ ), which leads to an estimate of kZ ."
    }, {
      "heading" : "4 Theoretical Analysis",
      "text" : "To simplify the analysis we consider a modified version of the original problem 2:\nf̂t = arg min f∈Hk̃\n1\nN N∑ a=1 ∑ τ∈ta (f(x̃a,τ )− ra,τ )2 + λ‖f‖2Hk̃ . (6)\nIn particular, this modified problem omits the terms 1na,t−1 as they obscure the analysis. In practice, these terms should be incorporated.\nIn this case sa,t = λ−1/2 √ k̃(x̃a,t, x̃a,t)− k̃Ta,t(K̃t−1 + λI)−1k̃a,t. Under this assumption KernelUCB is exactly KMTL-UCB with kZ ≡ 1. On the other hand, KMTL-UCB can be viewed as a special case of Kernel-UCB on the augmented context space X̃ . Thus, the regret analysis of Kernel-UCB applies to KMTL-UCB, but it does not reveal the potential gains of multi-task learning. We present an interpretable regret bound that reveals the benefits of MTL. We also establish a lower bound on the UCB width that decreases as task similarity increases (presented in the supplementary file)."
    }, {
      "heading" : "4.1 Analysis of SupKMTL-UCB",
      "text" : "It is not trivial to analyze algorithm 1 because the reward at time t is dependent on the past rewards. We follow the same strategy originally proposed in [1] and used in [7, 18] which uses SupKMTL-UCB as a master algorithm, and BaseKMTL-UCB (which is called by SupKMTL-UCB) to get estimates of reward and width. SupKMTL-UCB builds mutually exclusive subsets of [T ] such that rewards in any subset are independent. This guarantees that the independence assumption of Lemma 1 is satisfied. We describe these algorithms in a supplementary section because of space constraints. Theorem 1. Assume that ra,t ∈ [0, 1],∀a ∈ [N ], T ≥ 1, ‖f∗‖Hk̃ ≤ c, k̃(x̃, x̃) ≤ ck̃,∀x̃ ∈ X̃ and the task similarity matrix KZ is known. With probability at least 1− δ, SupKMTL-UCB satisfies\nR(T ) ≤ 2 √ T + 10 (√√√√ log (2TN(log(T ) + 1)/δ) 2 + c √ λ )√ 2m log g([T ]) √ T dlog(T )e\n= O (√ T log(g([T ])) )\nwhere g([T ]) = det(K̃T+1+λI) λT+1 and m = max(1, ck̃λ ).\nNote that this theorem assumes that task similarity is known. In the experiments for real datasets using the approach discussed in subsection 3.2 we estimate the task similarity from the available data."
    }, {
      "heading" : "4.2 Interpretation of Regret Bound",
      "text" : "The following theorems help us interpret the regret bound by looking at\ng([T ]) = det(K̃T+1 + λI)\nλT+1 = T+1∏ t=1 (λt + λ) λ ,\nwhere, λ1 ≥ λ2 ≥ · · · ≥ λT+1 are the eigenvalues of the kernel matrix K̃T+1. As mentioned above, the regret bound of Kernel-UCB applies to our method, and we are able to recover this bound as a corollary of Theorem 1. In the case of Kernel-UCB K̃t = KXt ,∀t ∈ [T ] as all arm estimators are assumed to be the same. We define the effective rank of K̃T+1 in the same way as [18] defines the effective dimension of the kernel feature space.\nDefinition 1. The effective rank of K̃T+1 is defined to be r := min{j : jλ log T ≥ ∑T+1 i=j+1 λi}.\nIn the following result, the notation Õ hides logarithmic terms. Corollary 1. log(g([T ])) ≤ r log ( 2T 2(T+1)ck̃+rλ−rλ log T\nrλ\n) , and therefore R(T ) = Õ( √ rT )\nHowever, beyond recovering a known bound, Theorem 1 can also be interpreted to reveal the potential gains of multi-task learning. To interpret the regret bound in Theorem 1, we make a further assumption that after time t, na,t = tN for all a ∈ [N ]. For simplicity define nt = na,t. Let ( ) denote the Hadamard product, (⊗) denote the Kronecker product and 1n ∈ Rn be the vector of ones. Let KXt = [kX (xaτ ,τ , xaτ′ ,τ ′)] t τ,τ ′=1 be the t × t kernel matrix on contexts, KZt = [kZ(zaτ , zaτ′ )] t τ,τ ′=1 be the associated t × t kernel matrix based on arm similarity, and KZ = [kZ(za, za)] N a=1 be the N × N arm/task similarity matrix between N arms, where xaτ ,τ is the observed context and zaτ is the associated arm descriptor. Using eqn. (1), we can write K̃t = KZt KXt . We rearrange the sequence of xaτ ,τ to get [xa,τ ]Na=1,τ=(t+1)a such that elements (a−1)nt to ant belong to arm a. Define K̃rt ,KrXt andK r Zt\nto be the rearranged kernel matrices based on the re-ordered set [xa,τ ]Na=1,τ=(t+1)a . Notice that we can write K̃ r t = (KZ ⊗ 1nt1Tnt) K r Xt and the eigenvalues λ(K̃t) and λ(K̃rt ) are equal. To summarize, we have\nK̃t = KZt KXt λ(K̃t) = λ ( (KZ ⊗ 1nt1Tnt) K r Xt ) . (7)\nTheorem 2. Let the rank of matrix KXT+1 be rx and the rank of matrix KZ be rz . Then log(g([T ])) ≤ rzrx log ( (T+1)ck̃+λ\nλ ) This means that when the rank of the task similarity matrix is low, which reflects a high degree of inter-task similarity, the regret bound is tighter. For comparison, note that when all tasks are independent, rz = N and when all tasks are the same (pooled), then rz = 1. In the case of LinUCB [7] where all arm estimators are assumed to be the same and kX is a linear kernel, the regret bound in Theorem 1 evaluates to Õ( √ dT ), where d is the dimension of the context space. In the original Lin-UCB algorithm [13] where all arm estimators are different, the regret bound would be Õ( √ NdT ).\nWe can further comment on g([T ]) when all distinct tasks (arms) are similar to each other with task similarity equal to µ. Thus define KZ(µ) := (1− µ)IN + µ1N1TN and K̃rt (µ) = (KZ(µ) ⊗ 1nt1 T nt) K r Xt . Theorem 3. Let gµ([T ]) = det(K̃rT+1(µ)+λI) λT+1 . If µ1 ≤ µ2 then gµ1([T ]) ≥ gµ2([T ]).\nThis shows that when there is more task similarity, the regret bound is tighter."
    }, {
      "heading" : "4.3 Comparison with CGP-UCB",
      "text" : "CGP-UCB transfers the learning from one task to another by leveraging additional known taskspecific context variables [10], similar in spirit to KTML-UCB. Indeed, with slight modifications, KMTL-UCB can be viewed as a frequentist analogue of CGP-UCB, and similarly CGP-UCB could be modified to address our setting. Furthermore, the term g([T ]) appearing in our regret bound is equivalent to an information gain term used to analyze CGP-UCB. In the agnostic case of CGPUCB where there is no assumption of a Gaussian prior on decision functions, their regret bound is O(log(g([T ])) √ T ), while their regret bound matches ours when they adopt a GP prior on f∗. Thus, our primary contributions with respect to CGP-UCB are to provide a tighter regret bound in agnostic case, and a technique for estimating task similarity which is critical for real-world applications."
    }, {
      "heading" : "5 Experiments",
      "text" : "We test our algorithm on synthetic data and some multi-class classification datasets. In the case of multi-class datasets, the number of arms N is the number of classes and the reward is 1 if we predict the correct class, otherwise it is 0. We separate the data into two parts - validation set and test set. We use all Gaussian kernels and pre-select the bandwidth of kernels using five fold cross-validation on a holdout validation set and we use β = 0.1 for all experiments. Then we run the algorithm on the test set 10 times (with different sequences of streaming data) and report the mean regret. For the synthetic data, we compare Kernel-UCB in the independent setting (Kernel-UCB-Ind) and pooled setting (Kernel-UCB-Pool), KMTL-UCB with known task similarity, and KMTL-UCB-Est which estimates task similarity on the fly. For the real datasets in the multi-class classification setting, we compare Kernel-UCB-Ind and KMTL-UCB-Est. In this case, the pooled setting is not valid because xa,t is the same for all arms (only za differs) and KMTL-UCB is not valid because the task similarity matrix is unknown. We also report the confidence intervals for these results in the supplementary material."
    }, {
      "heading" : "5.1 Synthetic News Article Data",
      "text" : "Suppose an agent has access to a pool of articles and their context features. The agent then sees a user along with his/her features for which it needs to recommend an article. Based on user features and article features the algorithm gets a combined context xa,t. The user context xu,t ∈ R2,∀t is randomly drawn from an ellipse centered at (0, 0) with major axis length 1 and minor axis length 0.5. Let xu,t[:, 1] be the minor axis and xu,t[:, 2] be the major axis. Article context xart,t is any angle θ ∈ [0, π2 ]. To get the overall summary xa,t of user and article the user context xu,t is rotated with xart,t.\nRewards for each article are defined based on the minor axis ra,t = ( 1.0− (xu,t[:, 1]− aN + 0.5) 2 ) .\nFigure 1 shows one such example for 4 different arms. The color code describes the reward, the two axes show the information about user context, and theta is the article context. We take N = 5. For KMTL-UCB, we use a Gaussian kernel on xart,t to get the task similarity.\nThe results of this experiment are shown in Figure 1. As one can see, Kernel-UCB-Pool performs the worst. That means for this setting combining all the data and learning a single estimator is not efficient. KMTL-UCB beats the other methods in all 10 runs, and Kernel-UCB-Ind and KMTL-UCB-Est perform equally well."
    }, {
      "heading" : "5.2 Multi-class Datasets",
      "text" : "In the case of multi-class classification, each class is an arm and the features of an example for which the algorithm needs to recommend a class are the contexts. We consider the following datasets: Digits (N = 10, d = 64), Letter (N = 26, d = 16), MNIST (N = 10, d = 780 ), Pendigits (N = 10, d = 16), Segment (N = 7, d = 19) and USPS (N = 10, d = 256). Empirical mean regrets are shown in Figure 2. KMTL-UCB-Est performs the best in three of the datasets and performs equally well in the other three datasets. Figure 3 shows the estimated task similarity (re-ordered\nto reveal block structure) and one can see the effect of the estimated task similarity matrix on the empirical regret in Figure 2. For the Digits, Segment and MNIST datasets, there is significant inter-task similarity. For Digits and Segment datasets, KMTL-UCB-Est is the best in all 10 runs of the experiment while for MNIST, KMTL-UCB-Est is better for all but 1 run."
    }, {
      "heading" : "6 Conclusions and future work",
      "text" : "We present a multi-task learning framework in the contextual bandit setting and describe a way to estimate task similarity when it is not given. We give theoretical analysis, interpret the regret bound, and support the theoretical analysis with extensive experiments. In the supplementary material we establish a lower bound on the UCB width, and argue that it decreases as task similarity increases.\nOur proposal to estimate the task similarity matrix using the arm similarity space Z = PX can be extended in different ways. For example, we could also incorporate previously observed rewards into Z . This would alleviate a potential problem with our approach, namely, that some contexts may have been selected when they did not yield a high reward. Additionally, by estimating the task similarity matrix, we are estimating arm-specific information. In the case of multiclass classification, kZ reflects information that represents the various classes. A natural extension is to incorporate methods for representation learning into the MTL bandit setting."
    } ],
    "references" : [ {
      "title" : "Using confidence bounds for exploitation-exploration trade-offs",
      "author" : [ "P. Auer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Generalizing from several related classification tasks to a new unlabeled sample",
      "author" : [ "G. Blanchard", "G. Lee", "C. Scott" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "A gang of bandits",
      "author" : [ "N. Cesa-Bianchi", "C. Gentile", "G. Zappella" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Universal kernels on non-standard input spaces",
      "author" : [ "A. Christmann", "I. Steinwart" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Regularized multi–task learning",
      "author" : [ "T. Evgeniou", "M. Pontil" ],
      "venue" : "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "Non-stochastic bandit slate problems",
      "author" : [ "S. Kale", "L. Reyzin", "R.E. Schapire" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Contextual gaussian process bandit optimization",
      "author" : [ "A. Krause", "C.S. Ong" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Algorithms for multi-armed bandit problems",
      "author" : [ "V. Kuleshov", "D. Precup" ],
      "venue" : "arXiv preprint arXiv:1402.6028,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "The epoch-greedy algorithm for multi-armed bandits with side information",
      "author" : [ "J. Langford", "T. Zhang" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "A contextual-bandit approach to personalized news article recommendation",
      "author" : [ "L. Li", "W. Chu", "J. Langford", "R.E. Schapire" ],
      "venue" : "In Proceedings of the 19th international conference on World wide web,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Collaborative filtering bandits",
      "author" : [ "S. Li", "A. Karatzoglou", "C. Gentile" ],
      "venue" : "In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "H. Robbins" ],
      "venue" : "In Herbert Robbins Selected Papers,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1985
    }, {
      "title" : "Gaussian process optimization in the bandit setting: No regret and experimental design",
      "author" : [ "N. Srinivas", "A. Krause", "M. Seeger", "S.M. Kakade" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Support vector machines",
      "author" : [ "I. Steinwart", "A. Christmann" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Finite-time analysis of kernelised contextual bandits",
      "author" : [ "M. Valko", "N. Korda", "R. Munos", "I. Flaounas", "N. Cristianini" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Multi-armed bandit models for the optimal design of clinical trials: benefits and challenges",
      "author" : [ "S.S. Villar", "J. Bowden", "J. Wason" ],
      "venue" : "Statistical science: a review journal of the Institute of Mathematical Statistics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Bandit algorithms for website",
      "author" : [ "J. White" ],
      "venue" : "optimization. \" O’Reilly Media, Inc.\",",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The reward for each arm is random according to a fixed distribution, and the agent’s goal is to maximize its cumulative reward [4] through a combination of exploring different arms and exploiting those arms that have yielded high rewards in the past [15, 11].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 13,
      "context" : "The reward for each arm is random according to a fixed distribution, and the agent’s goal is to maximize its cumulative reward [4] through a combination of exploring different arms and exploiting those arms that have yielded high rewards in the past [15, 11].",
      "startOffset" : 250,
      "endOffset" : 258
    }, {
      "referenceID" : 9,
      "context" : "The reward for each arm is random according to a fixed distribution, and the agent’s goal is to maximize its cumulative reward [4] through a combination of exploring different arms and exploiting those arms that have yielded high rewards in the past [15, 11].",
      "startOffset" : 250,
      "endOffset" : 258
    }, {
      "referenceID" : 10,
      "context" : "The contextual bandit problem is an extension of the MAB problem where there is some side information, called the context, associated to each arm [12].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "Contextual bandits have been employed to model various applications like news article recommendation [7], computational advertisement [9], website optimization [20] and clinical trials [19].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 18,
      "context" : "Contextual bandits have been employed to model various applications like news article recommendation [7], computational advertisement [9], website optimization [20] and clinical trials [19].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 17,
      "context" : "Contextual bandits have been employed to model various applications like news article recommendation [7], computational advertisement [9], website optimization [20] and clinical trials [19].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 11,
      "context" : ", functions from contexts to arms) and try to learn the best function with time [13, 18, 16].",
      "startOffset" : 80,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : ", functions from contexts to arms) and try to learn the best function with time [13, 18, 16].",
      "startOffset" : 80,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : ", functions from contexts to arms) and try to learn the best function with time [13, 18, 16].",
      "startOffset" : 80,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "On the other hand, when all the confidence terms are small, the algorithm exploits the best arm(s) [2].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "To select the best arm, one estimates θa for each arm independently using the data for that particular arm [13].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : "In the theoretical analysis of the Lin-UCB [7] and its kernelized version Kernel-UCB [18] θa is replaced by θ, and the goal is to learn one single estimator using data from all the arms.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 16,
      "context" : "Regret analysis for both Lin-UCB and Kernel-UCB adopted the pooled setting [7, 18].",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : "Gaussian Process UCB (GP-UCB) uses a Gaussian prior on the regression function and is a Bayesian equivalent of Kernel-UCB [16].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 11,
      "context" : "In Lin-UCB with Hybrid Linear Models the estimated reward consists of two linear terms, one that is arm-specific and another that is common to all arms [13].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 4,
      "context" : "Gang of bandits [5] uses a graph structure (e.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 12,
      "context" : "Collaborative filtering bandits [14] is a similar technique which clusters the users based on context.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "Contextual Gaussian Process UCB (CGP-UCB) builds on GP-UCB and has many elements in common with our framework [10].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Our approach is inspired by work on transfer and multi-task learning in the batch setting [3, 8].",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "Our approach is inspired by work on transfer and multi-task learning in the batch setting [3, 8].",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "With only minor modifications to the argument in Valko et al [18], we have the following: Lemma 1.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "To define a kernel on Z = PX , we use the same construction described in [3], originally introduced by Steinwart and Christmann [6].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "To define a kernel on Z = PX , we use the same construction described in [3], originally introduced by Steinwart and Christmann [6].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "We follow the same strategy originally proposed in [1] and used in [7, 18] which uses SupKMTL-UCB as a master algorithm, and BaseKMTL-UCB (which is called by SupKMTL-UCB) to get estimates of reward and width.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 16,
      "context" : "We follow the same strategy originally proposed in [1] and used in [7, 18] which uses SupKMTL-UCB as a master algorithm, and BaseKMTL-UCB (which is called by SupKMTL-UCB) to get estimates of reward and width.",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "We define the effective rank of K̃T+1 in the same way as [18] defines the effective dimension of the kernel feature space.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 11,
      "context" : "In the original Lin-UCB algorithm [13] where all arm estimators are different, the regret bound would be Õ( √ NdT ).",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "CGP-UCB transfers the learning from one task to another by leveraging additional known taskspecific context variables [10], similar in spirit to KTML-UCB.",
      "startOffset" : 118,
      "endOffset" : 122
    } ],
    "year" : 2017,
    "abstractText" : "Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent’s ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm’s performance on several data sets.",
    "creator" : null
  }
}