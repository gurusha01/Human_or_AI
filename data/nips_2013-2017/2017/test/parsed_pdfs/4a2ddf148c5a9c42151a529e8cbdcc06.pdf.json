{
  "name" : "4a2ddf148c5a9c42151a529e8cbdcc06.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery",
    "authors" : [ "Jie Shen" ],
    "emails" : [ "js2007@rutgers.edu", "pingli@stat.rutgers.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "This paper is concerned with the problem of recovering an arbitrary sparse signal from a set of its (compressed) measurements. We say that a signal x̄ ∈ Rd is s-sparse if there are no more than s non-zeros in x̄. This problem, together with its many variants, have found a variety of successful applications in compressed sensing, machine learning and statistics. Of particular interest is the setting where x̄ is the true signal and only a small number of linear measurements are given, referred to as compressed sensing. Such instance has been exhaustively studied in the last decade, along with a large body of elegant work devoted to efficient algorithms including ℓ1-based convex optimization and hard thresholding based greedy pursuits [7, 6, 15, 8, 3, 5, 11]. Another quintessential example is the sparsity-constrained minimization program recently considered in machine learning [30, 2, 14, 22], for which the goal is to efficiently learn the global sparse minimizer x̄ from a set of training data. Though in most cases, the underlying signal can be categorized into either of the two classes, we note that it could also be other object such as the parameter of logistic regression [19]. Hence, for a unified analysis, this paper copes with an arbitrary sparse signal and the results to be established quickly apply to the special instances above.\nIt is also worth mentioning that while one can characterize the performance of an algorithm and can evaluate the obtained estimate from various aspects, we are specifically interested in the quality of support recovery. Recall that for sparse recovery problems, there are two prominent metrics: the ℓ2 distance and the support recovery. Theoretical results phrased in terms of the ℓ2 metric is also referred to as parameter estimation, on which most of the previous papers emphasized. Under this metric, many popular algorithms, e.g., the Lasso [24, 27] and hard thresholding based algorithms [9, 3, 15, 8, 10, 22], are guaranteed with accurate approximation up to the energy of noise. Support recovery is another important factor to evaluate an algorithm, which is also known as feature\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nselection or variable selection. As one of the earliest work, [25] offered sufficient and necessary conditions under which orthogonal matching pursuit and basis pursuit identify the support. The theory was then developed by [35, 32, 27] for the Lasso estimator and by [29] for the garrotte estimator.\nTypically, recovering the support of a target signal is more challenging than parameter estimation. For instance, [18] showed that the restricted eigenvalue condition suffices for the Lasso to produce an accurate estimate whereas in order to recover the sign pattern, a more stringent mutual incoherence condition has to be imposed [27]. However, as has been recognized, if the support is detected precisely by a method, then the solution admits the optimal statistical rate [27]. In this regard, research on support recovery continues to be a central theme in recent years [33, 34, 31, 4, 17]. Our work follows this line and studies the support recovery performance of hard thresholding based algorithms, which enjoy superior computational efficiency to the convex programs when manipulating a huge volume of data [26].\nWe note that though [31, 4] have carried out theoretical understanding for hard thresholding pursuit (HTP) [10], showing that HTP identifies the support of a signal within a few iterations, neither of them obtained the general results in this paper. To be more detailed, under the restricted isometry property (RIP) condition [6], our iteration bound holds for an arbitrary sparse signal of interest, while the results from [31, 4] hold either for the global sparse minimizer or for the true sparse signal. Using a relaxed sparsity condition, we obtain a clear iteration complexity O(sκ log κ) where κ is a proper condition number. In contrast, it is hard to quantify the bound of [31] (see Theorem 3 therein). From the algorithmic perspective, we consider a more general algorithm than HTP. In fact, we appeal to the recently proposed partial hard thresholding (PHT) operator [13] and demonstrate novel results, which in turn indicates the best known iteration complexity for HTP and orthogonal matching pursuit with replacement (OMPR) [12]. Thereby, the results in this paper considerably extend our earlier work on HTP [23]. It is also worth mentioning that, though our analysis hinges on the PHT operator, the support recovery results to be established are stronger than the results in [13] since they only showed parameter estimation of PHT. Finally, we remark that while a couple of previous work considered signals that are not exactly sparse (e.g., [4]), we in this paper focus on the sparse case. Extensions to the generic signals are left as interesting future directions.\nContribution. The contribution of this paper is summarized as follows. We study the iteration complexity of the PHT algorithm, and show that under the RIP condition or the relaxed sparsity condition (to be clarified), PHT recovers the support of an arbitrary s-sparse signal within O(sκ log κ) iterations. This strengthens the theoretical results of [13] where only parameter estimation of PHT was established. Thanks to the generality of the PHT operator, our results shed light on the support recovery performance of a family of prevalent iterative algorithms. As two extreme cases of PHT, the new results immediately apply to HTP and OMPR, and imply the best known bound.\nRoadmap. The remainder of the paper is organized as follows. We describe the problem setting, as well as the partial hard thresholding operator in Section 2, followed by the main results regarding the iteration complexity. In Section 3, we sketch the proof of the main results and list some useful lemmas which might be of independent interest. Numerical results are illustrated in Section 4 and Section 5 concludes the paper and poses several interesting future work. The detailed proof of our theoretical results is deferred to the appendix (see the supplementary file).\nNotation. We collect the notation that is involved in this paper. The upper-right letter C and its subscript variants (e.g., C1) are used to denote absolute constants whose values may change from appearance to appearance. For a vector x ∈ Rd, its ℓ2 norm is denoted by ‖x‖. The support set of x is denoted by supp (x) which indexes the non-zeros in x. With a slight abuse, supp (x, k) is the set of indices for the k largest (in magnitude) elements. Ties are broken lexicographically. We interchangeably write ‖x‖0 or |supp (x)| to signify the cardinality of supp (x). We will also consider a vector restricted on a support set. That is, for a d-dimensional vector x and a support set T ⊂ {1, 2, . . . , d}, depending on the context, xT can either be a |T |-dimensional vector by extracting the elements belonging to T or a d-dimensional vector by setting the elements outside T to zero. The complement of a set T is denoted by T .\nWe reserve x̄ ∈ Rd for the target s-sparse signal whose support is denoted by S. The quantity x̄min > 0 is the minimum absolute element in x̄S , where we recall that x̄S ∈ Rs consists of the non-zeros of x̄. The PHT algorithm will depend on a carefully chosen function F (x). We write its gradient as ∇F (x) and we use ∇kF (x) as a shorthand of (∇F (x))supp(∇F (x),k), i.e., the top k absolute components of ∇F (x)."
    }, {
      "heading" : "2 Partial Hard Thresholding",
      "text" : "To pursue a sparse solution, hard thresholding has been broadly invoked by many popular greedy algorithms. In the present work, we are interested in the partial hard thresholding operator which sheds light upon a unified design and analysis for iterative algorithms employing this operator and the hard thresholding operator [13]. Formally, given a support set T and a freedom parameter r > 0, the PHT operator which is used to produce a k-sparse approximation to z is defined as follows:\nPHTk (z;T, r) := argmin x∈Rd\n‖x− z‖ , s.t. ‖x‖0 ≤ k, |T \\ supp (x)| ≤ r. (1)\nThe first constraint simply enforces a k-sparse solution. To gain intuition on the second one, consider that T is the support set of the last iterate of an iterative algorithm, for which |T | ≤ k. Then the second constraint ensures that the new support set differs from the previous one by at most r positions. As a special case, one may have noticed that the PHT operator reduces to the standard hard thresholding when picking the freedom parameter r ≥ k. On the other spectrum, if we look at the case where r = 1, the PHT operator yields the interesting algorithm termed orthogonal matching pursuit with replacement [12], which in general replaces one element in each iteration.\nIt has been shown in [13] that the PHT operator can be computed in an efficient manner for a general support set T and a freedom parameter r. In this paper, our major focus will be on the case |T | = k1. Then Lemma 1 of [13] indicates that PHTk (z;T, r) is given as follows:\ntop = supp ( z T , r ) , PHTk (z;T, r) = HTk ( zT∪top ) , (2)\nwhere HTk(·) is the standard hard thresholding operator that sets all but the k largest absolute components of a vector to zero.\nEquipped with the PHT operator, we are now in the position to describe a general iterative greedy algorithm, termed PHT(r) where r is the freedom parameter in (1). At the t-th iteration, the algorithm reveals the last iterate xt−1 as well as its support set St−1, and returns a new solution as follows:\nzt = xt−1 − η∇F (xt−1), yt = PHTk ( zt;St−1, r ) , St = supp ( yt ) , xt = argmin x∈Rd F (x), s.t. supp (x) ⊂ St.\nAbove, we note that η > 0 is a step size and F (x) is a proxy function which should be carefully chosen (to be clarified later). Typically, the sparsity parameter k equals s, the sparsity of the target signal x̄. In this paper, we consider a more general choice of k which leads to novel results. For further clarity, several comments on F (x) are in order.\nFirst, one may have observed that in the context of sparsity-constrained minimization, the proxy function F (x) used above is chosen as the objective function [30, 14]. In that scenario, the target signal is a global optimum and PHT(r) proceeds as projected gradient descent. Nevertheless, recall that our goal is to estimate an arbitrary signal x̄. It is not realistic to look for a function F (x) such that our target happens to be its global minimizer. The remedy we will offer is characterizing a deterministic condition between x̄ and ∇F (x̄) which is analogous to the signal-to-noise ratio condition, so that any function F (x) fulfilling that condition suffices. In this light, we find that F (x) behaves more like a proxy that guides the algorithm to a given target. Remarkably, our analysis also encompasses the situation considered in [30, 14].\nSecond, though it is not being made explicitly, one should think of F (x) as containing the measurements or the training data. Consider, for example, recovering x̄ from y = Ax̄ where A is a design matrix and y is the response (both are known). A natural way would be running the PHT(r) algorithm with F (x) = ‖y −Ax‖2. One may also think of the logistic regression model where y is a binary vector (label), A is a collection of training data (feature), and F (x) is the logistic loss evaluated on the training samples.\nWith the above clarification, we are ready to make assumptions on F (x). It turns out that two properties of F (x) are vital for our analysis: restricted strong convexity and restricted smoothness. These two conditions were proposed by [16] and have been standard in the literature [34, 1, 14, 22].\n1Our results actually hold for |T | ≤ k. But we observe that the size of T we will consider is usually equal to k. Hence, for ease of exposition, we take |T | = k. This is also the case considered in [12].\nDefinition 1. We say that a differentiable function F (x) satisfies the property of restricted strong convexity (RSC) at sparsity level s with parameter ρ−s > 0 if for all x, x ′ ∈ Rd with ‖x− x′‖0 ≤ s,\nF (x)− F (x′)− 〈∇F (x′),x− x′〉 ≥ ρ − s\n2 ‖x− x′‖2 .\nLikewise, we say that F (x) satisfies the property of restricted smoothness (RSS) at sparsity level s with parameter ρ+s > 0 if for all x, x ′ ∈ Rd with ‖x− x′‖0 ≤ s, it holds that\nF (x)− F (x′)− 〈∇F (x′),x− x′〉 ≤ ρ + s\n2 ‖x− x′‖2 .\nWe call κs = ρ + s /ρ − s as the condition number of the problem, since it is essentially identical to the condition number of the Hessian matrix of F (x) restricted on s-sparse directions."
    }, {
      "heading" : "2.1 Deterministic Analysis",
      "text" : "The following proposition shows that under very mild conditions, PHT(r) either terminates or recovers the support of an arbitrary s-sparse signal x̄ using at most O(sκ2s log κ2s) iterations. Proposition 2. Consider the PHT(r) algorithm with k = s. Suppose that F (x) is ρ−2s-RSC and ρ + 2sRSS, and the step size η ∈ (0, 1/ρ+2s). Let κ := ρ+2s/ρ−2s. Then PHT(r) either terminates or recovers the support of x̄ within O(sκ log κ) iterations provided that x̄min ≥ 4 √ 2+2 √ κ\nρ− 2s\n‖∇2sF (x̄)‖.\nA few remarks are in order. First, we remind the reader that under the conditions stated above, it is not guaranteed that PHT(r) succeeds. We say that PHT(r) fails if it terminates at some time stamp t but St 6= S. This indeed happens if, for example, we feed it with a bad initial point and pick a very small step size. In particular, if x0min > η ∥ ∥∇F (x0) ∥ ∥\n∞, then the algorithm makes no progress. The crux to remedy this issue is imposing a lower bound on η or looking at more coordinates in each iteration, which is the theme below. However, the proposition is still useful because it asserts that as far as we make sure that PHT(r) runs long enough (i.e., O(sκ log κ) iterations), it recovers the support of an arbitrary sparse signal. We also note that neither the RIP condition nor a relaxed sparsity is assumed in this proposition.\nThe x̄min-condition above is natural, which can be viewed as a generalization of the well-known signal-to-noise ratio (SNR) condition. This follows by considering the noisy compressed sensing problem, where y = Ax̄+ e and F (x) = ‖y −Ax‖2. Here, the vector e is some noise. Now the RSC and RSS imply for any 2s-sparse x\nρ−2s ‖x‖2 ≤ ‖Ax‖2 ≤ ρ+2s ‖x‖2 . Hence\n‖∇2sF (x̄)‖ = ∥ ∥ ∥ (A⊤e)2s ∥ ∥ ∥ = Θ(‖e‖)\nIn fact, the x̄min-condition has been used many times to establish support recovery. See, for example, [31, 4, 23].\nIn the following, we strengthen Prop. 2 by considering the RIP condition which requires a wellbounded condition number, i.e., κ ≤ O(1). Theorem 3. Consider the PHT(r) algorithm with k = s. Suppose that F (x) is ρ−2s+r-RSC and ρ+2s+r-RSS. Let κ := ρ + 2s+r/ρ − 2s+r be the condition number which is smaller than 1 + 1/( √ 2 + ν)\nwhere ν = √ s− r + 2. Pick the step size η = η′/ρ+2s+r such that κ − 1√2+ν < η\n′ ≤ 1. Then PHT(r) recovers the support of x̄ within\ntmax =\n(\nlog κ\nlog(1/β) +\nlog( √ 2/(1− λ))\nlog(1/β) + 2\n)\n‖x̄‖0\niterations, provided that for some constant λ ∈ (0, 1)\nx̄min ≥ 2ν + 6\nλρ−2s+r ‖∇s+rF (x̄)‖ .\nAbove, β = ( √ 2 + ν)(κ− η′) ∈ (0, 1).\nWe remark several aspects of the theorem. The most important part is that Theorem 3 offers the theoretical justification that PHT(r) always recovers the support. This is achieved by imposing an RIP condition (i.e., bounding the condition number from the above) and using a proper step size.\nWe also make the iteration bound explicit, in order to examine the parameter dependency. First, we note that tmax scales approximately linearly with λ. This conforms the intuition because a small λ actually indicates a large signal-to-noise ratio, and hence easy to distinguish the support of interest from the noise. The freedom parameter r is mainly encoded in the coefficient β through the quantity ν. Observe that when increasing the scalar r, we have a small β, and hence fewer iterations. This is not surprising since a large value of r grants the algorithm more freedom to look at the current iterate. Indeed, in the best case, PHT(s) is able to recover the support in O(1) iterations while PHT(1) has to take O(s) steps. However, if we investigate the conditions, we find that we need a stronger RSC/RSS condition to afford a large freedom parameter.\nIt is also interesting to contrast Theorem 3 to [31, 4], which independently built state-of-the-art support recovery results for HTP. As has been mentioned, [31] made use of the optimality of the target signal, which is a restricted setting compared to our result. Their iteration bound (see Theorem 1 therein), though provides an appealing insight, does not have a clear parameter dependence on the natural parameters of the problem (e.g., sparsity and condition number). [4] developed O(‖x̄‖0) iteration complexity for compressed sensing. Again, they confined to a special signal whereas we carry out a generalization that allows us to analyze a family of algorithms.\nThough the RIP condition has been ubiquitous in the literature, many researchers point out that it is not realistic in practical applications [18, 20, 21]. This is true for large-scale machine learning problems, where the condition number may grow with the sample size (hence one cannot upper bound it with a constant). A clever solution was first (to our knowledge) suggested by [14], where they showed that using the sparsity parameter k = O(κ2s) guarantees convergence of projected gradient descent. The idea was recently employed by [22, 31] to show an RIP-free condition for sparse recovery, though in a technically different way. The following theorem borrows this elegant idea to prove RIP-free results for PHT(r). Theorem 4. Consider the PHT(r) algorithm. Suppose that F (x) is ρ−2k-RSC and ρ + 2k-RSS. Let κ := ρ+2k/ρ − 2k be the condition number. Further pick k ≥ s + ( 1 + 4 η2(ρ−\n2k )2\n)\nmin{s, r} where η ∈ (0, 1/ρ+2k). Then the support of x̄ is included in the iterate of PHT(r) within\ntmax =\n(\n3 logκ\nlog(1/µ) + 2 log(2/(1− λ)) log(1/µ) + 2 ) ‖x̄‖0\niterations, provided that for some constant λ ∈ (0, 1),\nx̄min ≥ √ κ+ 3\nλρ−2k ‖∇k+sF (x̄)‖ .\nAbove, we have µ = 1− ηρ − 2k (1−ηρ+ 2k )\n2 .\nWe discuss the salient features of Theorem 4 compared to Prop. 2 and Theorem 3. First, note that we can pick η = 1/(2ρ+2k) in the above theorem, which results in µ = O(1 − 1/κ). So the iteration complexity is essentially given by O(sκ log κ) that is similar to the one in Prop. 2. However, in Theorem 4, the sparsity parameter k is set to be O(s+ κ2 min{s, r}) which guarantees support inclusion. We pose an open question of whether the x̄min-condition might be refined, in that it currently scales with √ κ which is stringent for ill-conditioned problems. Another important consequence implied by the theorem is that the sparsity parameter k actually depends on the minimum of s and r. Consider r = 1 which corresponds to the OMPR algorithm. Then k = O(s+ κ2) suffices. In contrast, previous work of [14, 31, 22, 23] only obtained theoretical result for k = O(κ2s), owing to a restricted problem setting. We also note that even in the original OMPR paper [12] and its latest version [13], such an RIP-free condition was not established."
    }, {
      "heading" : "2.2 Statistical Results",
      "text" : "Until now, all of our theoretical results are phrased in terms of deterministic conditions (i.e., RSC, RSS and x̄min). It is known that these conditions can be satisfied by prevalent statistical models\nsuch as linear regression and logistic regression. Here, we give detailed statistical results for sparse linear regression, and we refer the reader to [1, 14, 22, 23] for other applications.\nConsider the sparse linear regression model yi = 〈ai, x̄〉+ ei, 1 ≤ i ≤ N, where ai are drawn i.i.d. from a sub-gaussian distribution with zero mean and covarianceΣ ∈ Rd×d and ei are drawn i.i.d. from N (0, ω2). We presume that the diagonal elements of Σ are properly scaled, i.e., Σjj ≤ 1 for 1 ≤ j ≤ d. Let A = (a⊤1 ; . . . ;a⊤N ) and y = (y1; . . . ; yN). Our goal is to recover x̄ from the knowledge of A and y. To this end, we may choose F (x) = 12 ‖y −Ax‖ 2 . Let σmin(Σ) and σmax(Σ) be the smallest and the largest singulars of Σ, respectively. Then it is known that with high probability, F (x) satisfies the RSC and RSS properties at the sparsity level K with parameters\nρ−K = σmin(Σ)− C1 · K log d\nN , ρ+K = σmax(Σ) + C2 ·\nK log d\nN , (3)\nrespectively. It is also known that with high probability, the following holds:\n‖∇KF (x̄)‖ ≤ 2ω √ K log d\nN . (4)\nSee [1] for a detailed discussion. With these probabilistic arguments on hand, we investigate the sufficient conditions under which the preceding deterministic results hold.\nFor Prop. 2, recall that the sparsity level of RSC and RSS is 2s. Hence, if we pick the sample size N = q · 2C1s log d/σmin(Σ) for some q > 1, then\n4 √ 2 + 2 √ κ2s\nρ−2s ‖∇2sF (x̄)‖ ≤ 4ω\n2 √ 2 + √\nσmax(Σ) σmin(Σ)\n· √\n1+C2/qC1 1−1/q\n(1− 1/q) √ qC1σmin(Σ) .\nThe right-hand side is monotonically decreasing with q, which indicates that as soon as we pick q large enough, it becomes smaller than x̄min. To be more concrete, consider that the covariance matrix Σ is the identity matrix for which σmin(Σ) = σmax(Σ) = 1. Now suppose that q ≥ 2, which gives an upper bound\n4 √ 2 + 2 √ κ2s\nρ−2s ‖∇2sF (x̄)‖ ≤\n8ω(2 √ 2 + √\n2 + C2/C1)√ qC1 .\nThus, in order to fulfill the x̄min-condition in Prop. 2, it suffices to pick\nq = max\n{\n2,\n( 8ω(2 √ 2 + √\n2 + C2/C1)√ C1x̄min\n)2}\n.\nFor Theorem 3, it essentially asks for a well-conditioned design matrix at the sparsity level 2s+ r. Note that (3) implies κ2s+r ≥ σmax(Σ)/σmin(Σ), which in return requires a well-conditioned covariance matrix. Thus, to guarantee that κ2s+r ≤ 1 + ǫ for some ǫ > 0, it suffices to choose Σ such that σmax(Σ)/σmin(Σ) < 1 + ǫ and pick N = q · C1(2s+ r) log d/σmin(Σ) with\nq = 1 + ǫ +C−11 C2σmax(Σ)/σmin(Σ)\n1 + ǫ− σmax(Σ)/σmin(Σ) .\nFinally, Theorem 4 asserts support inclusion by expanding the support size of the iterates. Suppose that η = 1/(2ρ+2k), which results in k ≥ s+(16κ22k+1)min{r, s}. Given that the condition number κ2k is always greater than 1, we can pick k ≥ s + 20κ22k min{r, s}. At a first sight, this seems to be weird in that k depends on the condition number κ2k which itself relies on the choice of k. In the following, we present concrete sample complexity showing that this condition can be met. We will focus on two extreme cases: r = 1 and r = s.\nFor r = 1, we require k ≥ s+ 20κ22k. Let us pick N = 4C1k log d/σmin(Σ). In this way, we obtain ρ−2k = 1 2σmin(Σ) and ρ + 2k ≤ (1 + C22C1 )σmax(Σ). It then follows that the condition number of the design matrix κ2k ≤ (2 + C2C1 )σmax(Σ)/σmin(Σ). Consequently, we can set the parameter\nk = s+ 20\n((\n2 + C2 C1\n)\nσmax(Σ)\nσmin(Σ)\n)2\n.\nNote that the above quantities depend only on the covariance matrix. Again, if Σ is the identity matrix, the sample complexity is O(s log d).\nFor r = s, likewise k ≥ 20κ22ks suffices. Following the deduction above, we get\nk = 20\n((\n2 + C2 C1\n)\nσmax(Σ)\nσmin(Σ)\n)2\ns."
    }, {
      "heading" : "3 Proof Sketch",
      "text" : "We sketch the proof and list some useful lemmas which might be of independent interest. The high-level proof technique follows from the recent work of [4] which performs an RIP analysis for compressed sensing. But for our purpose, we have to deal with the freedom parameter r as well as the RIP-free condition. We also need to generalize the arguments in [4] to show support recovery results for arbitrary sparse signals. Indeed, we prove the following lemma which is crucial for our analysis. Below we assume without loss of generality that the elements in x̄ are in descending order according to the magnitude. Lemma 5. Consider the PHT(r) algorithm. Assume that F (x) is ρ−2k-RSC and ρ + 2k-RSS. Further assume that the sequence of {xt}t≥0 satisfies ∥\n∥xt − x̄ ∥ ∥ ≤ α · βt ∥ ∥x0 − x̄ ∥\n∥+ ψ1, (5) ∥ ∥xt − x̄ ∥ ∥ ≤ γ ∥\n∥x̄St ∥ ∥+ ψ2, (6)\nfor positive α, ψ1, γ, ψ2 and 0 < β < 1. Suppose that at the n-th iteration (n ≥ 0), Sn contains the indices of top p (in magnitude) elements of x̄. Then, for any integer 1 ≤ q ≤ s − p, there exists an integer ∆ ≥ 1 determined by\n√ 2 |x̄p+q | > αγ · β∆−1 ∥ ∥x̄{p+1,...,s} ∥ ∥+Ψ,\nwhere\nΨ = αψ2 + ψ1 + 1\nρ−2k ‖∇2F (x̄)‖ ,\nsuch that Sn+∆ contains the indices of top p + q elements of x̄ provided that Ψ ≤ √ 2λx̄min for some constant λ ∈ (0, 1).\nWe isolate this lemma here since we find it inspiring and general. The lemma states that under proper conditions, as far as one can show that the sequence satisfies (5) and (6), then after a few iterations, PHT(r) captures more correct indices in the iterate. Note that the condition (5) states that the sequence should contract with a geometric rate, and the condition (6) follows immediately from the fully corrective step (i.e., minimizing F (x) over the new support set).\nThe next theorem concludes that under the conditions of Lemma 5, the total iteration complexity for support recovery is proportional to the sparsity of the underlying signal.\nTheorem 6. Assume same conditions as in Lemma 5. Then PHT(r) successfully identifies the support of x̄ using (\nlog 2 2 log(1/β) + log(αγ/(1−λ)) log(1/β) + 2\n)\n‖x̄‖0 number of iterations.\nThe detailed proofs of these two results are given in the appendix. Armed with them, it remains to show that PHT(r) satisfies the condition (5) under different settings.\nProof Sketch for Prop. 2. We start with comparing F (ztSt) and F (x t−1). For the sake, we record several important properties. First, due to the fully corrective step, the support set of ∇F (xt−1) is orthogonal to St−1. That means for any subset Ω ⊂ St−1, ztΩ = xt−1Ω and for any set Ω ⊂ St−1, ztΩ = −η∇ΩF (xt−1). We also note that due to the PHT operator, any element of ztSt\\St−1 is not smaller than that of ztSt−1\\St . These critical facts together with the RSS condition result in\nF (xt)− F (xt−1) ≤ F (ztSt)− F (xt−1) ≤ −η(1 − ηρ+2s) ∥ ∥ ∥ ( ∇F (xt−1) )\nSt\\St−1\n∥ ∥ ∥ 2 .\nSince St \\ St−1 consists of the top elements of ∇F (xt−1), we can show that ∥\n∥ ∥\n( ∇F (xt−1) )\nSt\\St−1\n∥ ∥ ∥ 2 ≥ 2ρ − 2s ∣ ∣St \\ St−1 ∣ ∣\n|St \\ St−1|+ |S \\ St−1| ( F (xt−1)− F (x̄) ) .\nUsing the argument of Prop. 21, we establish the linear convergence of the iterates, i.e., the condition (5). The result then follows.\nProof Sketch for Theorem 3. To prove this theorem, we present a more careful analysis on the problem structure. In particular, let T = supp ( ∇F (xt−1, r) )\n, J t = St−1∪T , and consider the elements of ∇F (xt−1). Since T contains the largest elements, any element outside T is smaller than those of T . Then we may compare the elements of ∇F (xt−1) on S \\ T and S \\ T . Though they have different number of components, we can show the relationship between the averaged energy:\n1 |T \\ S| ∥ ∥ ∥ ( ∇F (xt−1) ) T\\S ∥ ∥ ∥ 2 ≥ 1|S \\ T | ∥ ∥ ∥ ( ∇F (xt−1) ) S\\T ∥ ∥ ∥ 2 .\nUsing this equation followed by some standard relaxation, we can bound ∥ ∥x̄Jt ∥ ∥ in terms of ∥ ∥xt−1 − x̄ ∥ ∥ as follows.\nLemma 7. Assume that F (x) satisfies the properties of RSC and RSS at sparsity level k + s + r. Let ρ− := ρ−k+s+r and ρ + := ρ+k+s+r . Consider the support set J t = St−1 ∪ supp ( ∇F (xt−1), r ) . We have for any 0 < θ ≤ 1/ρ+, ∥\n∥x̄Jt ∥ ∥ ≤ ν(1− θρ−) ∥ ∥xt−1 − x̄ ∥\n∥+ ν\nρ− ‖∇s+rF (x̄)‖ ,\nwhere ν = √ s− r + 2. In particular, picking θ = 1/ρ+ gives\n∥ ∥x̄Jt ∥\n∥ ≤ ν ( 1− 1 κ ) ∥ ∥xt−1 − x̄ ∥ ∥+ ν ρ− ‖∇s+rF (x̄)‖ .\nNote that the lemma also applies to the two-stage thresholding algorithms (e.g., CoSaMP [15]) whose first step is expanding the support set.\nOn the other hand, we also know that ∥\n∥ ∥ ztJt\\St\n∥ ∥ ∥ ≤ ∥ ∥\n∥ ztJt\\S\n∥ ∥ ∥ .\nThis is because J t \\St contains the r smallest elements of ztJt . It then follows that ∥ ∥x̄Jt\\St ∥ ∥ can be upper bounded by ∥ ∥xt−1 − x̄ ∥ ∥. Finally, we note that St = (J t \\ St) ∪ J t. Hence, (5) follows.\nProof Sketch for Theorem 4. The proof idea of Theorem 4 is inspired by [31], though we give a tighter and a more general analysis. We first observe that St \\ St−1 contains larger elements than St−1 \\ St, due to PHT. This enables us to show that\nF (xt)− F (xt−1) ≤ −1− ηρ + 2k\n2η\n∥ ∥ztSt − xt−1 ∥ ∥\n2 ≤ −1− ηρ + 2k\n2η\n∥ ∥ ∥ ( ∇F (xt−1) )\nSt\\St−1\n∥ ∥ ∥ 2 .\nThen we prove the claim ∥\n∥ ∥\n( ∇F (xt−1) )\nSt\\St−1\n∥ ∥ ∥ 2\n≥ ρ−2k ( F (xt−1)− F (x̄) ) .\nTo this end, we consider whether r is larger than s. If r ≥ s, then it is possible that ∣ ∣St \\ St−1 ∣ ∣ ≥ s. In this case, using the RSC condition and the PHT property, we can show that\n∥ ∥ ∥ ( ∇F (xt−1) )\nSt\\St−1\n∥ ∥ ∥ 2 ≥ ∥ ∥ ∥ ( ∇F (xt−1) )\nS\\St−1\n∥ ∥ ∥ 2\n≥ ρ−2k ( F (xt−1)− F (x̄) ) .\nIf ∣ ∣St \\ St−1 ∣ ∣ < s ≤ r, then the above does not hold. But we may partition the set S \\ St−1 as a union of T1 = S \\ (St ∪ St−1) and T2 = (St \\ St−1) ∩ S, and show that the ℓ2-norm of F (xt−1) on T1 is smaller than that on T2 if k = s+ κ 2s. In addition, the RSC condition gives ρ−2k 4 ∥ ∥x̄− xt−1 ∥ ∥ 2 ≤ F (x̄)− F (xt−1) + 1 ρ−2k ∥ ∥ ∥ ( ∇F (xt−1) ) T1 ∥ ∥ ∥ 2 + 1 ρ−2k ∥ ∥ ∥ ( ∇F (xt−1) ) St\\St−1 ∥ ∥ ∥ 2 . Since T2 ⊂ St \\ St−1, it implies the desired bound by rearranging the terms. The case r < s follows in a reminiscent way. The proof is complete."
    }, {
      "heading" : "4 Simulation",
      "text" : "We complement our theoretical results by performing numerical experiments in this section. In particular, we are interested in two aspects: first, the number of iterations required to identify the support of an s-sparse signal; second, the tradeoff between the iteration number and percentage of success resulted from different choices of the freedom parameter r.\nWe consider the compressed sensing model y = Ax̄+0.01e, where the dimension d = 200 and the entries of A and e are i.i.d. normal variables. Given a sparsity level s, we first uniformly choose the support of x̄, and assign values to the non-zeros with i.i.d. normals. There are two configurations: the sparsity s and the sample size N . Given s and N , we independently generate 100 signals and test PHT(r) on them. We say PHT(r) succeeds in a trial if it returns an iterate with correct support within 10 thousands iterations. Otherwise we mark the trial as failure. Iteration numbers to be reported are counted only on those success trials. The step size η is fixed to be the unit, though one can tune it using cross-validation for better performance.\nTo study how the iteration number scales with the sparsity in practice, we fix N = 200 and tune s from 1 to 100. We test different freedom parameter r on these signals. The results are shown in the leftmost figure in Figure 1. As our theory predicted, we observe that within O(s) iterations, PHT(r) precisely identifies the true support. In the second subfigure, we plot the percentage of success against the sparsity. It appears that PHT(r) lays on top of each other. This is possibly because we used a sufficiently large sample size.\nNext, we fix s = 10 and vary N from 1 to 200. Surprisingly, from the rightmost figure, we do not observe performance degrade using a large freedom parameter. So we conjecture that the x̄mincondition we established can be refined.\nFigure 1 also illustrates an interesting phenomenon: after a particular threshold, say r = 5, PHT(r) does not significantly reduces the iteration number by increasing r. This cannot be explained by our theorems in the paper. We leave it as a promising research direction."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we have presented a principled analysis on a family of hard thresholding algorithms. To facilitate our analysis, we appealed to the recently proposed partial hard thresholding operator. We have shown that under the RIP condition or the relaxed sparsity condition, the PHT(r) algorithm recovers the support of an arbitrary sparse signal x̄ within O(‖x̄‖0 κ log κ) iterations, provided that a generalized signal-to-noise ratio condition is satisfied. On account of our unified analysis, we have established the best known bound for HTP and OMPR. We have also illustrated that the simulation results agree with our finding that the iteration number is proportional to the sparsity.\nThere are several interesting future directions. First, it would be interesting to examine if we can close the logarithmic factor log κ in the iteration bound. Second, it is also useful to study RIP-free conditions for two-stage PHT algorithms such as CoSaMP. Finally, we pose the open question of whether one can improve the √ κ factor in the x̄min-condition.\nAcknowledgements. The work is supported in part by NSF-Bigdata-1419210 and NSF-III1360971. We thank the anonymous reviewers for valuable comments."
    } ],
    "references" : [ {
      "title" : "Fast global convergence of gradient methods for high-dimensional statistical recovery",
      "author" : [ "A. Agarwal", "S. Negahban", "M.J. Wainwright" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Greedy sparsity-constrained optimization",
      "author" : [ "S. Bahmani", "B. Raj", "P.T. Boufounos" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Iterative hard thresholding for compressed sensing",
      "author" : [ "T. Blumensath", "M.E. Davies" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Hard thresholding pursuit algorithms: number of iterations",
      "author" : [ "J.-L. Bouchot", "S. Foucart", "P. Hitczenko" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Orthogonal matching pursuit for sparse signal recovery with noise",
      "author" : [ "T.T. Cai", "L. Wang" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Decoding by linear programming",
      "author" : [ "E.J. Candès", "T. Tao" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "Atomic decomposition by basis pursuit",
      "author" : [ "S.S. Chen", "D.L. Donoho", "M.A. Saunders" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1998
    }, {
      "title" : "Subspace pursuit for compressive sensing signal reconstruction",
      "author" : [ "W. Dai", "O. Milenkovic" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint",
      "author" : [ "I. Daubechies", "M. Defrise", "C.D. Mol" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    }, {
      "title" : "Hard thresholding pursuit: An algorithm for compressive sensing",
      "author" : [ "S. Foucart" ],
      "venue" : "SIAM Journal on Numerical Analysis,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "A Mathematical Introduction to Compressive Sensing",
      "author" : [ "S. Foucart", "H. Rauhut" ],
      "venue" : "Applied and Numerical Harmonic Analysis. Birkhäuser,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Orthogonal matching pursuit with replacement",
      "author" : [ "P. Jain", "A. Tewari", "I.S. Dhillon" ],
      "venue" : "In Proceedings of the 25th Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Partial hard thresholding",
      "author" : [ "P. Jain", "A. Tewari", "I.S. Dhillon" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2017
    }, {
      "title" : "On iterative hard thresholding methods for high-dimensional Mestimation",
      "author" : [ "P. Jain", "A. Tewari", "P. Kar" ],
      "venue" : "In Proceedings of the 28th Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples",
      "author" : [ "D. Needell", "J.A. Tropp" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "A unified framework for highdimensional analysis of M -estimators with decomposable regularizers",
      "author" : [ "S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu" ],
      "venue" : "In Proceedings of the 23rd Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Sparse recovery via differential inclusions",
      "author" : [ "S. Osher", "F. Ruan", "J. Xiong", "Y. Yao", "W. Yin" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Simultaneous analysis of lasso and dantzig selector",
      "author" : [ "Y.R. Peter J. Bickel", "A.B. Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach",
      "author" : [ "Y. Plan", "R. Vershynin" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Restricted eigenvalue properties for correlated gaussian designs",
      "author" : [ "G. Raskutti", "M.J. Wainwright", "B. Yu" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Reconstruction from anisotropic random measurements",
      "author" : [ "M. Rudelson", "S. Zhou" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "A tight bound of hard thresholding",
      "author" : [ "J. Shen", "P. Li" ],
      "venue" : "arXiv preprint arXiv:1605.01656,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "On the iteration complexity of support recovery via hard thresholding pursuit",
      "author" : [ "J. Shen", "P. Li" ],
      "venue" : "In Proceedings of the 34th International Conference on Machine Learning,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2017
    }, {
      "title" : "Regression shrinkage and selection via the Lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Methodological),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1996
    }, {
      "title" : "Greed is good: algorithmic results for sparse approximation",
      "author" : [ "J.A. Tropp" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2004
    }, {
      "title" : "Computational methods for sparse solution of linear inverse problems",
      "author" : [ "J.A. Tropp", "S.J. Wright" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "Sharp thresholds for high-dimensional and noisy sparsity recovery using l1-constrained quadratic programming (Lasso)",
      "author" : [ "M.J. Wainwright" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2009
    }, {
      "title" : "Recovery of sparse signals via generalized orthogonal matching pursuit: A new analysis",
      "author" : [ "J. Wang", "S. Kwon", "P. Li", "B. Shim" ],
      "venue" : "IEEE Trans. Signal Processing,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "On the non-negative garrotte estimator",
      "author" : [ "M. Yuan", "Y. Lin" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2007
    }, {
      "title" : "Gradient hard thresholding pursuit for sparsity-constrained optimization",
      "author" : [ "X.-T. Yuan", "P. Li", "T. Zhang" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "Exact recovery of hard thresholding pursuit",
      "author" : [ "X.-T. Yuan", "P. Li", "T. Zhang" ],
      "venue" : "In Proceedings of the 30th Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2016
    }, {
      "title" : "On the consistency of feature selection using greedy least squares regression",
      "author" : [ "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2009
    }, {
      "title" : "Some sharp performance bounds for least squares regression with  L1 regularization",
      "author" : [ "T. Zhang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2009
    }, {
      "title" : "Sparse recovery with orthogonal matching pursuit under RIP",
      "author" : [ "T. Zhang" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2011
    }, {
      "title" : "On model selection consistency of lasso",
      "author" : [ "P. Zhao", "B. Yu" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Such instance has been exhaustively studied in the last decade, along with a large body of elegant work devoted to efficient algorithms including l1-based convex optimization and hard thresholding based greedy pursuits [7, 6, 15, 8, 3, 5, 11].",
      "startOffset" : 219,
      "endOffset" : 242
    }, {
      "referenceID" : 5,
      "context" : "Such instance has been exhaustively studied in the last decade, along with a large body of elegant work devoted to efficient algorithms including l1-based convex optimization and hard thresholding based greedy pursuits [7, 6, 15, 8, 3, 5, 11].",
      "startOffset" : 219,
      "endOffset" : 242
    }, {
      "referenceID" : 14,
      "context" : "Such instance has been exhaustively studied in the last decade, along with a large body of elegant work devoted to efficient algorithms including l1-based convex optimization and hard thresholding based greedy pursuits [7, 6, 15, 8, 3, 5, 11].",
      "startOffset" : 219,
      "endOffset" : 242
    }, {
      "referenceID" : 7,
      "context" : "Such instance has been exhaustively studied in the last decade, along with a large body of elegant work devoted to efficient algorithms including l1-based convex optimization and hard thresholding based greedy pursuits [7, 6, 15, 8, 3, 5, 11].",
      "startOffset" : 219,
      "endOffset" : 242
    }, {
      "referenceID" : 2,
      "context" : "Such instance has been exhaustively studied in the last decade, along with a large body of elegant work devoted to efficient algorithms including l1-based convex optimization and hard thresholding based greedy pursuits [7, 6, 15, 8, 3, 5, 11].",
      "startOffset" : 219,
      "endOffset" : 242
    }, {
      "referenceID" : 4,
      "context" : "Such instance has been exhaustively studied in the last decade, along with a large body of elegant work devoted to efficient algorithms including l1-based convex optimization and hard thresholding based greedy pursuits [7, 6, 15, 8, 3, 5, 11].",
      "startOffset" : 219,
      "endOffset" : 242
    }, {
      "referenceID" : 10,
      "context" : "Such instance has been exhaustively studied in the last decade, along with a large body of elegant work devoted to efficient algorithms including l1-based convex optimization and hard thresholding based greedy pursuits [7, 6, 15, 8, 3, 5, 11].",
      "startOffset" : 219,
      "endOffset" : 242
    }, {
      "referenceID" : 29,
      "context" : "Another quintessential example is the sparsity-constrained minimization program recently considered in machine learning [30, 2, 14, 22], for which the goal is to efficiently learn the global sparse minimizer x̄ from a set of training data.",
      "startOffset" : 120,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "Another quintessential example is the sparsity-constrained minimization program recently considered in machine learning [30, 2, 14, 22], for which the goal is to efficiently learn the global sparse minimizer x̄ from a set of training data.",
      "startOffset" : 120,
      "endOffset" : 135
    }, {
      "referenceID" : 13,
      "context" : "Another quintessential example is the sparsity-constrained minimization program recently considered in machine learning [30, 2, 14, 22], for which the goal is to efficiently learn the global sparse minimizer x̄ from a set of training data.",
      "startOffset" : 120,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "Another quintessential example is the sparsity-constrained minimization program recently considered in machine learning [30, 2, 14, 22], for which the goal is to efficiently learn the global sparse minimizer x̄ from a set of training data.",
      "startOffset" : 120,
      "endOffset" : 135
    }, {
      "referenceID" : 18,
      "context" : "Though in most cases, the underlying signal can be categorized into either of the two classes, we note that it could also be other object such as the parameter of logistic regression [19].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 23,
      "context" : ", the Lasso [24, 27] and hard thresholding based algorithms [9, 3, 15, 8, 10, 22], are guaranteed with accurate approximation up to the energy of noise.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 26,
      "context" : ", the Lasso [24, 27] and hard thresholding based algorithms [9, 3, 15, 8, 10, 22], are guaranteed with accurate approximation up to the energy of noise.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : ", the Lasso [24, 27] and hard thresholding based algorithms [9, 3, 15, 8, 10, 22], are guaranteed with accurate approximation up to the energy of noise.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : ", the Lasso [24, 27] and hard thresholding based algorithms [9, 3, 15, 8, 10, 22], are guaranteed with accurate approximation up to the energy of noise.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : ", the Lasso [24, 27] and hard thresholding based algorithms [9, 3, 15, 8, 10, 22], are guaranteed with accurate approximation up to the energy of noise.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : ", the Lasso [24, 27] and hard thresholding based algorithms [9, 3, 15, 8, 10, 22], are guaranteed with accurate approximation up to the energy of noise.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : ", the Lasso [24, 27] and hard thresholding based algorithms [9, 3, 15, 8, 10, 22], are guaranteed with accurate approximation up to the energy of noise.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : ", the Lasso [24, 27] and hard thresholding based algorithms [9, 3, 15, 8, 10, 22], are guaranteed with accurate approximation up to the energy of noise.",
      "startOffset" : 60,
      "endOffset" : 81
    }, {
      "referenceID" : 24,
      "context" : "As one of the earliest work, [25] offered sufficient and necessary conditions under which orthogonal matching pursuit and basis pursuit identify the support.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 34,
      "context" : "The theory was then developed by [35, 32, 27] for the Lasso estimator and by [29] for the garrotte estimator.",
      "startOffset" : 33,
      "endOffset" : 45
    }, {
      "referenceID" : 31,
      "context" : "The theory was then developed by [35, 32, 27] for the Lasso estimator and by [29] for the garrotte estimator.",
      "startOffset" : 33,
      "endOffset" : 45
    }, {
      "referenceID" : 26,
      "context" : "The theory was then developed by [35, 32, 27] for the Lasso estimator and by [29] for the garrotte estimator.",
      "startOffset" : 33,
      "endOffset" : 45
    }, {
      "referenceID" : 28,
      "context" : "The theory was then developed by [35, 32, 27] for the Lasso estimator and by [29] for the garrotte estimator.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "For instance, [18] showed that the restricted eigenvalue condition suffices for the Lasso to produce an accurate estimate whereas in order to recover the sign pattern, a more stringent mutual incoherence condition has to be imposed [27].",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 26,
      "context" : "For instance, [18] showed that the restricted eigenvalue condition suffices for the Lasso to produce an accurate estimate whereas in order to recover the sign pattern, a more stringent mutual incoherence condition has to be imposed [27].",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 26,
      "context" : "However, as has been recognized, if the support is detected precisely by a method, then the solution admits the optimal statistical rate [27].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 32,
      "context" : "In this regard, research on support recovery continues to be a central theme in recent years [33, 34, 31, 4, 17].",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 33,
      "context" : "In this regard, research on support recovery continues to be a central theme in recent years [33, 34, 31, 4, 17].",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 30,
      "context" : "In this regard, research on support recovery continues to be a central theme in recent years [33, 34, 31, 4, 17].",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "In this regard, research on support recovery continues to be a central theme in recent years [33, 34, 31, 4, 17].",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 16,
      "context" : "In this regard, research on support recovery continues to be a central theme in recent years [33, 34, 31, 4, 17].",
      "startOffset" : 93,
      "endOffset" : 112
    }, {
      "referenceID" : 25,
      "context" : "Our work follows this line and studies the support recovery performance of hard thresholding based algorithms, which enjoy superior computational efficiency to the convex programs when manipulating a huge volume of data [26].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 30,
      "context" : "We note that though [31, 4] have carried out theoretical understanding for hard thresholding pursuit (HTP) [10], showing that HTP identifies the support of a signal within a few iterations, neither of them obtained the general results in this paper.",
      "startOffset" : 20,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "We note that though [31, 4] have carried out theoretical understanding for hard thresholding pursuit (HTP) [10], showing that HTP identifies the support of a signal within a few iterations, neither of them obtained the general results in this paper.",
      "startOffset" : 20,
      "endOffset" : 27
    }, {
      "referenceID" : 9,
      "context" : "We note that though [31, 4] have carried out theoretical understanding for hard thresholding pursuit (HTP) [10], showing that HTP identifies the support of a signal within a few iterations, neither of them obtained the general results in this paper.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "To be more detailed, under the restricted isometry property (RIP) condition [6], our iteration bound holds for an arbitrary sparse signal of interest, while the results from [31, 4] hold either for the global sparse minimizer or for the true sparse signal.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 30,
      "context" : "To be more detailed, under the restricted isometry property (RIP) condition [6], our iteration bound holds for an arbitrary sparse signal of interest, while the results from [31, 4] hold either for the global sparse minimizer or for the true sparse signal.",
      "startOffset" : 174,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "To be more detailed, under the restricted isometry property (RIP) condition [6], our iteration bound holds for an arbitrary sparse signal of interest, while the results from [31, 4] hold either for the global sparse minimizer or for the true sparse signal.",
      "startOffset" : 174,
      "endOffset" : 181
    }, {
      "referenceID" : 30,
      "context" : "In contrast, it is hard to quantify the bound of [31] (see Theorem 3 therein).",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "In fact, we appeal to the recently proposed partial hard thresholding (PHT) operator [13] and demonstrate novel results, which in turn indicates the best known iteration complexity for HTP and orthogonal matching pursuit with replacement (OMPR) [12].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "In fact, we appeal to the recently proposed partial hard thresholding (PHT) operator [13] and demonstrate novel results, which in turn indicates the best known iteration complexity for HTP and orthogonal matching pursuit with replacement (OMPR) [12].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 22,
      "context" : "Thereby, the results in this paper considerably extend our earlier work on HTP [23].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 12,
      "context" : "It is also worth mentioning that, though our analysis hinges on the PHT operator, the support recovery results to be established are stronger than the results in [13] since they only showed parameter estimation of PHT.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 3,
      "context" : ", [4]), we in this paper focus on the sparse case.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 12,
      "context" : "This strengthens the theoretical results of [13] where only parameter estimation of PHT was established.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : "In the present work, we are interested in the partial hard thresholding operator which sheds light upon a unified design and analysis for iterative algorithms employing this operator and the hard thresholding operator [13].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 11,
      "context" : "On the other spectrum, if we look at the case where r = 1, the PHT operator yields the interesting algorithm termed orthogonal matching pursuit with replacement [12], which in general replaces one element in each iteration.",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 12,
      "context" : "It has been shown in [13] that the PHT operator can be computed in an efficient manner for a general support set T and a freedom parameter r.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "Then Lemma 1 of [13] indicates that PHTk (z;T, r) is given as follows: top = supp (",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 29,
      "context" : "First, one may have observed that in the context of sparsity-constrained minimization, the proxy function F (x) used above is chosen as the objective function [30, 14].",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 13,
      "context" : "First, one may have observed that in the context of sparsity-constrained minimization, the proxy function F (x) used above is chosen as the objective function [30, 14].",
      "startOffset" : 159,
      "endOffset" : 167
    }, {
      "referenceID" : 29,
      "context" : "Remarkably, our analysis also encompasses the situation considered in [30, 14].",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 13,
      "context" : "Remarkably, our analysis also encompasses the situation considered in [30, 14].",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 15,
      "context" : "These two conditions were proposed by [16] and have been standard in the literature [34, 1, 14, 22].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 33,
      "context" : "These two conditions were proposed by [16] and have been standard in the literature [34, 1, 14, 22].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "These two conditions were proposed by [16] and have been standard in the literature [34, 1, 14, 22].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "These two conditions were proposed by [16] and have been standard in the literature [34, 1, 14, 22].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 21,
      "context" : "These two conditions were proposed by [16] and have been standard in the literature [34, 1, 14, 22].",
      "startOffset" : 84,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "This is also the case considered in [12].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 30,
      "context" : "It is also interesting to contrast Theorem 3 to [31, 4], which independently built state-of-the-art support recovery results for HTP.",
      "startOffset" : 48,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "It is also interesting to contrast Theorem 3 to [31, 4], which independently built state-of-the-art support recovery results for HTP.",
      "startOffset" : 48,
      "endOffset" : 55
    }, {
      "referenceID" : 30,
      "context" : "As has been mentioned, [31] made use of the optimality of the target signal, which is a restricted setting compared to our result.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "[4] developed O(‖x̄‖0) iteration complexity for compressed sensing.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "Though the RIP condition has been ubiquitous in the literature, many researchers point out that it is not realistic in practical applications [18, 20, 21].",
      "startOffset" : 142,
      "endOffset" : 154
    }, {
      "referenceID" : 19,
      "context" : "Though the RIP condition has been ubiquitous in the literature, many researchers point out that it is not realistic in practical applications [18, 20, 21].",
      "startOffset" : 142,
      "endOffset" : 154
    }, {
      "referenceID" : 20,
      "context" : "Though the RIP condition has been ubiquitous in the literature, many researchers point out that it is not realistic in practical applications [18, 20, 21].",
      "startOffset" : 142,
      "endOffset" : 154
    }, {
      "referenceID" : 13,
      "context" : "A clever solution was first (to our knowledge) suggested by [14], where they showed that using the sparsity parameter k = O(κ(2)s) guarantees convergence of projected gradient descent.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 21,
      "context" : "The idea was recently employed by [22, 31] to show an RIP-free condition for sparse recovery, though in a technically different way.",
      "startOffset" : 34,
      "endOffset" : 42
    }, {
      "referenceID" : 30,
      "context" : "The idea was recently employed by [22, 31] to show an RIP-free condition for sparse recovery, though in a technically different way.",
      "startOffset" : 34,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "In contrast, previous work of [14, 31, 22, 23] only obtained theoretical result for k = O(κ(2)s), owing to a restricted problem setting.",
      "startOffset" : 30,
      "endOffset" : 46
    }, {
      "referenceID" : 30,
      "context" : "In contrast, previous work of [14, 31, 22, 23] only obtained theoretical result for k = O(κ(2)s), owing to a restricted problem setting.",
      "startOffset" : 30,
      "endOffset" : 46
    }, {
      "referenceID" : 21,
      "context" : "In contrast, previous work of [14, 31, 22, 23] only obtained theoretical result for k = O(κ(2)s), owing to a restricted problem setting.",
      "startOffset" : 30,
      "endOffset" : 46
    }, {
      "referenceID" : 22,
      "context" : "In contrast, previous work of [14, 31, 22, 23] only obtained theoretical result for k = O(κ(2)s), owing to a restricted problem setting.",
      "startOffset" : 30,
      "endOffset" : 46
    }, {
      "referenceID" : 11,
      "context" : "We also note that even in the original OMPR paper [12] and its latest version [13], such an RIP-free condition was not established.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "We also note that even in the original OMPR paper [12] and its latest version [13], such an RIP-free condition was not established.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "Here, we give detailed statistical results for sparse linear regression, and we refer the reader to [1, 14, 22, 23] for other applications.",
      "startOffset" : 100,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "Here, we give detailed statistical results for sparse linear regression, and we refer the reader to [1, 14, 22, 23] for other applications.",
      "startOffset" : 100,
      "endOffset" : 115
    }, {
      "referenceID" : 21,
      "context" : "Here, we give detailed statistical results for sparse linear regression, and we refer the reader to [1, 14, 22, 23] for other applications.",
      "startOffset" : 100,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : "Here, we give detailed statistical results for sparse linear regression, and we refer the reader to [1, 14, 22, 23] for other applications.",
      "startOffset" : 100,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "(4) See [1] for a detailed discussion.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 3,
      "context" : "The high-level proof technique follows from the recent work of [4] which performs an RIP analysis for compressed sensing.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "We also need to generalize the arguments in [4] to show support recovery results for arbitrary sparse signals.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : ", CoSaMP [15]) whose first step is expanding the support set.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 30,
      "context" : "The proof idea of Theorem 4 is inspired by [31], though we give a tighter and a more general analysis.",
      "startOffset" : 43,
      "endOffset" : 47
    } ],
    "year" : 2017,
    "abstractText" : "In machine learning and compressed sensing, it is of central importance to understand when a tractable algorithm recovers the support of a sparse signal from its compressed measurements. In this paper, we present a principled analysis on the support recovery performance for a family of hard thresholding algorithms. To this end, we appeal to the partial hard thresholding (PHT) operator proposed recently by Jain et al. [IEEE Trans. Information Theory, 2017]. We show that under proper conditions, PHT recovers an arbitrary s-sparse signal within O(sκ log κ) iterations where κ is an appropriate condition number. Specifying the PHT operator, we obtain the best known results for hard thresholding pursuit and orthogonal matching pursuit with replacement. Experiments on the simulated data complement our theoretical findings and also illustrate the effectiveness of PHT.",
    "creator" : null
  }
}