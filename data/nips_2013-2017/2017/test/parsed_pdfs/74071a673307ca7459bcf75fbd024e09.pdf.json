{
  "name" : "74071a673307ca7459bcf75fbd024e09.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Screening Rule for `1-Regularized Ising Model Estimation",
    "authors" : [ "Zhaobin Kuang", "Sinong Geng", "David Page" ],
    "emails" : [ "zkuang@wisc.edu,", "sgeng2@wisc.edu,", "page@biostat.wisc.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "While the field of statistical learning with sparsity [Hastie et al., 2015] has been steadily rising to prominence ever since the introduction of the lasso (least absolute shrinkage and selection operator) at the end of the last century [Tibshirani, 1996], it was not until the recent decade that various screening rules debuted to further equip the ever-evolving optimization arsenals for some of the most fundamental problems in sparse learning such as `1-regularized generalized linear models (GLMs, Friedman et al. 2010) and inverse covariance matrix estimation [Friedman et al., 2008]. Screening rules, usually in the form of an analytic formula or an optimization procedure that is extremely fast to solve, can accelerate learning drastically by leveraging the inherent sparsity of many high-dimensional problems. Generally speaking, screening rules can identify a significant portion of the zero components of an optimal solution beforehand at the cost of minimal computational overhead, and hence substantially reduce the dimension of the parameterization, which makes possible efficient computation for large-scale sparse learning problems.\nPioneered by Ghaoui et al. 2010, various screening rules have emerged to speed up learning for generative models (e.g. Gaussian graphical models) as well as for discriminative models (e.g. GLMs), and for continuous variables (e.g. lasso) as well as for discrete variables (e.g. logistic regression, support vector machines). Table 1 summarizes some of the iconic work in the literature, where, to the best of our knowledge, screening rules for generative models with discrete variables are still notably absent.\nContrasted with this notable absence is the ever stronger craving in the big data era for scaling up the learning of generative models with discrete variables, especially in a blockwise structure identification setting. For example, in gene mutation analysis [Wan et al., 2015, 2016], among tens of thousands of sparse binary variables representing mutations of genes, we are interested in identifying a handful of mutated genes that are connected into various blocks and exert synergistic effects on the cancer. While a sparse Ising model is a desirable choice, for such an application the scalability of the model could fail due to the innate NP-hardness [Karger and Srebro, 2001] of inference, and hence maximum likelihood learning, owing to the partition function. To date, even with modern\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\napproximation techniques, a typical application with sparse discrete graphical models usually involves only hundreds of variables [Viallon et al., 2014, Barber et al., 2015, Vuffray et al., 2016].\nBetween the need for the scalability of high-dimensional Ising models and the absence of screening rules that are deemed crucial to accelerated and scalable learning, we have a technical gap to bridge: can we identify screening rules that can speed up the learning of `1-regularized Ising models? The major contribution of this paper is to give an affirmative answer to this question. Specifically, we show the following.\n• The screening rule is a simple closed-form formula that is a necessary and sufficient condition for exact blockwise structure recovery of the solution with a given regularization parameter. Upon the identification of blockwise structures, different blocks of variables can be considered as different Ising models and can be solved separately. The various blocks can even be solved in parallel to attain further efficiency. Empirical results on both simulated and real-world datasets demonstrate the tremendous efficiency, scalability, and insights gained from the introduction of the screening rule. Efficient learning of `1-regularized Ising models from thousands of variables on a single machine is hence readily attainable.\n• As an initial attempt to fill in the vacancy illustrated in Table 1, our work is instructive to further exploration of screening rules for other graphical models with discrete random variables, and to combining screening rules with various optimization methods to facilitate better learning. Furthermore, compared with its Gaussian counterpart, where screening rules are available (Table 1) and learning is scalable [Hsieh et al., 2013], the proposed screening rule is especially valuable and desperately needed to address the more challenging learning problem of sparse Ising models.\nWe defer all the proofs in the paper to the supplement and focus on providing intuition and interpretation of the technical results in the paper."
    }, {
      "heading" : "2 Notation and Background",
      "text" : ""
    }, {
      "heading" : "2.1 Ising Models",
      "text" : "Let X = [X1, X2, · · · , Xp]> be a p × 1 binary random vector, with Xi ∈ {−1, 1}, and i ∈ {1, 2, · · · , p} , V . Let there be a dataset X with n independent and identically distributed samples ofX , denoted as X = { x(1), x(2), · · · , x(n) } . Here, x(k) is a p×1 vector of assignments that realizes\nX , where k ∈ {1, 2, · · · , n}. We further use x(k)i to denote the ith component of the kth sample in the dataset. Let θ ∈ Θ be a p× p symmetric matrix whose diagonal entries are zeros. An Ising model [Wan et al., 2016] with the parameterization θ is:\nPθ(x) = 1\nZ(θ) exp p−1∑ i=1 p∑ j>i θijxixj  , (1) where θij represents the component of θ at the ith row and the jth column, and xi and xj represent the ith and the jth components of x, respectively. Z(θ) is a normalization constant, partition function, that ensures the probabilities sum up to one. The partition function is given as Z(θ) =∑ x∈{−1,1}p exp (∑p−1 i=1 ∑p j>i θijxixj ) . Note that for ease of presentation, we consider Ising models with only pairwise interaction/potential here. Generalization to Ising models with unary potentials is given in Section 6."
    }, {
      "heading" : "2.2 Graphical Interpretation",
      "text" : "With the notion of the probability given by an Ising model in (1), estimating an `1-regularized Ising model is defined as finding θ̂, the penalized maximum likelihood estimator (MLE) under the lasso penalty:\nθ̂ = arg max θ\n1\nn n∑ k=1 log Pθ ( x(k) ) − λ 2 ‖θ‖1\n= arg min θ − 1 n n∑ k=1 p−1∑ i=1 p∑ j>i θijx (k) i x (k) j +A(θ) + λ 2 ‖θ‖1.\n(2)\nHere, A(θ) = logZ(θ) is the log-partition function; ‖θ‖1 = ∑p i=1 ∑p j=1|θij | is the lasso penalty that encourages a sparse parameterization. λ ≥ 0 is a given regularization parameter. Using λ2 is suggestive of the symmetry of θ so that λ2 ‖θ‖1 = λ ∑p−1 i=1 ∑p j>i|θij |, which echoes the summations in the negative log-likelihood function. Note that θ corresponds to the adjacency matrix constructed by the p components of X as nodes, and θij 6= 0 indicates that there is an edge between Xi and Xj . We further denote a partition of V into L blocks as {C1, C2, · · · , CL}, where Cl, Cl′ ⊆ V , Cl ∩ Cl′ = ∅, ⋃L l=1 Cl = V , l 6= l′, and for all l, l′ ∈ {1, 2, · · · , L}. Without loss of generality, we assume that the nodes in different blocks are ordered such that if i ∈ Cl, j ∈ Cl′ , and l < l′, then i < j."
    }, {
      "heading" : "2.3 Blockwise Solutions",
      "text" : "We introduce the definition of a blockwise parameterization: Definition 1. We call θ blockwise with respect to the partition {C1, C2, · · · , CL} if ∀l and l′ ∈ {1, 2, · · · , L}, where l 6= l′, and ∀i ∈ Cl, ∀j ∈ Cl′ , we have θij = 0.\nWhen θ is blockwise, we can represent θ in a block diagonal fashion:\nθ = diag (θ1, θ2, · · · , θL) , (3)\nwhere θ1, θ2, · · · , and θL are symmetric matrices that correspond to C1, C2, · · · , andCL, respectively. Note that if we can identify the blockwise structure of θ̂ in advance, we can solve each block independently (See A.1). Since the size of each block could be much smaller than the size of the original problem, each block could be much easier to learn compared with the original problem. Therefore, efficient identification of blockwise structure could lead to substantial speedup in learning."
    }, {
      "heading" : "3 The Screening Rule",
      "text" : ""
    }, {
      "heading" : "3.1 Main Results",
      "text" : "The preparation in Section 2 leads to the discovery of the following strikingly simple screening rule presented in Theorem 1. Theorem 1. Let a partition of V, {C1, C2, · · · , CL}, be given. Let the dataset X ={ x(1), x(2), · · · , x(n) } be given. Define EXXiXj = 1n ∑n k=1 x (k) i x (k) j . A necessary and sufficient condition for θ̂ to be blockwise with respect to the given partition is that\n|EXXiXj | ≤ λ, (4)\nfor all l and l′ ∈ {1, 2, · · · , L}, where l 6= l′, and for all i ∈ Cl, j ∈ Cl′ .\nIn terms of exact blockwise structure identification, Theorem 1 provides a foolproof (necessary and sufficient) and yet easily checkable result by comparing the absolute second empirical moments |EXXiXj |’s with the regularization parameter λ. We also notice the remarkable similarity between the proposed screening rule and the screening rule for Gaussian graphical model blockwise structure identification in Witten et al. 2011, Mazumder and Hastie 2012. In the Gaussian case, the screening rule can be attained by simply replacing the second empirical moment matrix in (4) with the sample\nAlgorithm 1 Blockwise Minimization 1: Input: dataset X, regularization parameter λ. 2: Output: θ̂. 3: ∀i, j ∈ V such that j > i, compute the second empirical moments EXXiXj’s . 4: Identify the partition {C1, C2, · · · , CL} using the second empirical moments from the previous\nstep and according to Witten et al. [2011], Mazumder and Hastie [2012]. 5: ∀l ∈ L, perform blockwise optimization over Cl for θ̂l. 6: Ensemble θ̂l’s according to (3) for θ̂. 7: Return θ̂.\ncovariance matrix. While the exact solution in the Gaussian case can be computed in polynomial time, estimating an Ising model via maximum likelihood in general is NP-hard . However, as a consequence of applying the screening rule, the blockwise structure of an `1-regularized Ising model can be determined as easily as the blockwise structure of a Gaussian graphical model, despite the fact that within each block, exact learning of a sparse Ising model could still be challenging.\nFurthermore, the screening rule also provides us a principal approach to leverage sparsity for the gain of efficiency: by increasing λ, the nodes of the Ising model will be shattered into smaller and smaller blocks, according to the screening rule. Solving many Ising models with small blocks of variables is amenable to both estimation algorithm and parallelism."
    }, {
      "heading" : "3.2 Regularization Parameters",
      "text" : "The screening rule also leads to a significant implication to the range of regularization parameters in which θ̂ 6= 0. Specifically, we have the following theorem. Theorem 2. Let the dataset X = { x(1), x(2), · · · , x(n) } be given, and let λ = λmax represent the\nsmallest regularization parameter such that θ̂ = 0 in (2). Then λmax = maxi,j∈V,i 6=j |EXXiXj | ≤ 1.\nWith λmax, one can decide the range of regularization parameters, [0, λmax], that generates graphs with nonempty edge sets, which is an important first step for pathwise optimization algorithms (a.k.a. homotopy algorithms) that learn the solutions to the problem under a range of λ’s. Furthermore, the fact that λmax ≤ 1 for any given dataset X suggests that comparison across different networks generated by different datasets is comprehensible. Finally, in Section 4, λmax will also help to establish the connection between the screening rule for exact learning and some of the popular inexact (alternative) learning algorithms in the literature."
    }, {
      "heading" : "3.3 Fully Disconnected Nodes",
      "text" : "Another consequence of the screening rule is the necessary and sufficient condition that determines the regularization parameter with which a node is fully disconnected from the remaining nodes: Corollary 1. Let the dataset X = { x(1), x(2), · · · , x(n) } be given. Xi is fully disconnected from\nthe remaining nodes in θ̂, where i ∈ V (i.e., θ̂ij = θ̂ji = 0, ∀j ∈ V \\ {i}), if and only if λ ≥ maxj∈V \\{i}|EXXiXj |.\nIn high-dimensional exploratory data analysis, it is usually the case that most of the variables are fully disconnected [Danaher et al., 2014, Wan et al., 2016]. In this scenario, Corollary 1 provides a regularization parameter threshold with which we can identify exactly the subset of fully disconnected nodes. Since we can choose a threshold large enough to make any nodes fully disconnected, we can discard a significant portion of the variables efficiently and flexibly at will with exact optimization guarantees due to Corollary 1. By discarding the large portion of fully disconnected variables, the learning algorithm can focus on only a moderate number of connected variables, which potentially results in a substantial efficiency gain."
    }, {
      "heading" : "3.4 Blockwise Minimization",
      "text" : "We conclude this section by providing the blockwise minimization algorithm in Algorithm 1 due to the screening rule. Note that both the second empirical moments and the partition of V in the\nalgorithm can be computed in O(p2) operations [Witten et al., 2011, Mazumder and Hastie, 2012]. On the contrary, the complexity of the exact optimization of a block of variables grows exponentially with respect to the maximal clique size of that block. Therefore, by encouraging enough sparsity, the blockwise minimization due to the screening rule can provide remarkable speedup by not only shrinking the size of the blocks in general but also potentially reducing the size of cliques within each block via eliminating enough edges."
    }, {
      "heading" : "4 Applications to Inexact (Alternative) Methods",
      "text" : "We now discuss the interplay between the screening rule and two popular inexact (alternative) estimation methods: node-wise (NW) logistic regression [Wainwright et al., 2006, Ravikumar et al., 2010] and the pseudolikelihood (PL) method [Höfling and Tibshirani, 2009]. In what follows, we use θ̂NW and θ̂PL to denote the solutions given by the node-wise logistic regression method and the pseudolikelihood method, respectively. NW can be considered as an asymmetric pseudolikelihood method (i.e., ∃i,j ∈ V such that i 6= j and θ̂NWij 6= θ̂NWji ), while PL is a pseudolikelihood method that is similar to NW but imposes additional symmetric constraints on the parameterization (i.e., ∀i,j ∈ V where i 6= j, we have θ̂PLij = θ̂PLji ). Our incorporation of the screening rule to the inexact methods is straightforward: after using the screening rule to identify different blocks in the solution, we use inexact methods to solve each block for the solution. As shown in Section 3, when combined with exact optimization, the screening rule is foolproof for blockwise structure identification. However, in general, when combined with inexact methods, the proposed screening rule is not foolproof any more because the screening rule is derived from the exact problem in (2) instead of the approximate problems such as NW and PL. We provide a toy example in A.6 to illustrate mistakes made by the screening rule when combined with inexact methods. Nonetheless, as we will show in this section, NW and PL are deeply connected to the screening rule, and when given a large enough regularization parameter, the application of the screening rule to NW and PL can be lossless in practice (see Section 5). Therefore, when applied to NW and PL, the proposed screening rule can be considered as a strong rule (i.e., a rule that is not foolproof but barely makes mistakes) and an optimal solution can be safeguarded by adjusting the screened solution to optimality based on the KKT conditions of the inexact problem [Tibshirani et al., 2012]."
    }, {
      "heading" : "4.1 Node-wise (NW) Logistic Regression and the Pseudolikelihood (PL) Method",
      "text" : "In NW, for each i ∈ V , we consider the conditional probability of Xi upon X\\i, where X\\i = {Xt | t ∈ V \\ {i}}. This is equivalent to solving p `1-regularized logistic regression problems separately, i.e., ∀i ∈ V :\nθ̂NW\\i = arg min θ\\i\n1\nn n∑ k=1 [ −y(k)i η (k) \\i + log ( 1 + exp ( η (k) \\i ))] + λ ∥∥θ\\i∥∥1 , (5) where η(k)\\i = θ > \\i(2x (k) \\i ), y (k) i = 1 represents a successful event x (k) i = 1, y (k) i = 0 represents an unsuccessful event x(k)i = −1, and\nθ\\i = [ θi1 θi2 · · · θi(i−1) θi(i+1) · · · θip ]> ,\nx (k) \\i =\n[ x\n(k) i1 x (k) i2 · · · x (k) i(i−1) x (k) i(i+1) · · · x (k) ip\n]> .\nNote that θ̂NW constructed from θ̂NW\\i ’s is asymmetric, and ad hoc post processing techniques are used to generate a symmetric estimation such as setting each pair of elements from θ̂NW in symmetric positions to the one with a larger (or smaller) absolute value.\nOn the other hand, PL can be considered as solving all p `1-regularized logistic regression problems in (5) jointly with symmetric constraints over the parameterization [Geng et al., 2017]:\nθ̂PL = arg min θ∈Θ\n1\nn n∑ k=1 p∑ i=1 [ −y(k)i ξ (k) i + log ( 1 + exp ( ξ (k) i ))] + λ 2 ‖θ‖1 , (6)\nwhere ξ(k)i = ∑ j∈V \\{i} 2θmin{i,j},max{i,j}x (k) j .That is to say, if i < j, then θmin{i,j},max{i,j} = θij ; if i > j, then θmin{i,j},max{i,j} = θji. Recall that Θ in (6) defined in Section 2.1 represents a space of symmetric matrices whose diagonal entries are zeros."
    }, {
      "heading" : "4.2 Regularization Parameters in NW and PL",
      "text" : "Since the blockwise structure of a solution is given by the screening rule under a fixed regularization parameter, the ranges of regularization parameters under which NW and PL can return nonzero solutions need to be linked to the range [0, λmax] in the exact problem. Theorem 3 and Theorem 4 establish such relationships for NW and PL, respectively.\nTheorem 3. Let the dataset X = { x(1), x(2), · · · , x(n) } be given, and let λ = λNWmax represent the smallest regularization parameter such that θ̂NW\\i = 0 in (5), ∀i ∈ V . Then λ NW max = λmax.\nTheorem 4. Let the dataset X = { x(1), x(2), · · · , x(n) } be given, and let λ = λPLmax represent the smallest regularization parameter such that θ̂PL = 0 in (6), then λPLmax = 2λmax.\nLet λ be the regularization parameter used in the exact problem. A strategy is to set the corresponding λNW = λ when using NW and λPL = 2λ when using PL, based on the range of regularization parameters given in Theorem 3 and Theorem 4 for NW and PL. Since the magnitude of the regularization parameter is suggestive of the magnitude of the gradient of the unregulated objective, the proposed strategy leverages that the magnitudes of the gradients of the unregulated objectives for NW and PL are roughly the same as, and roughly twice as large as, that of the unregulated exact objective, respectively.\nThis observation has been made in the literature of binary pairwise Markov networks [Höfling and Tibshirani, 2009, Viallon et al., 2014]. Here, by Theorem 3 and Theorem 4, we demonstrate that this relationship is exactly true if the optimal parameterization is zero. Höfling and Tibshirani 2009 even further exploits this observation in PL for exact optimization. Their procedure can be viewed as iteratively solving adjusted PL problems regularized by λPL = 2λ in order to obtain an exact solution regularized by λ. The close quantitative correspondence between the derivatives of the inexact objectives and that of the exact objective also provides insights into why combing the screening rule with inexact methods does not lose much in practice."
    }, {
      "heading" : "4.3 Preservation for Fully Disconnectedness",
      "text" : "While the screening rule is not foolproof when combined with NW and PL, it turns out that in terms of identifying fully disconnected nodes, the necessary and sufficient condition in Corollary 1 can be preserved when applying NW with caution, as shown in the following.\nTheorem 5. Let the dataset X = { x(1), x(2), · · · , x(n) } be given. Let θ̂NWmin ∈ Θ denote a symmetric matrix derived from θ̂NW by setting each pair of elements from θ̂NW in symmetric positions to the one with a smaller absolute value. A sufficient condition for Xi to be fully disconnected from the remaining nodes in θ̂NWmin, where i ∈ V , is that λNW ≥ maxj∈V \\{i}|EXXiXj |. Furthermore, when θ̂NW\\i = 0, the sufficient condition is also necessary.\nIn practice, the utility of Theorem 5 is to provide us a lower bound for λ above which we can fully disconnect Xi (sufficiency). Moreover, if θ̂NW\\i = 0 also happens to be true, which is easily verifiable, we can conclude that such a lower bound is tight (necessity)."
    }, {
      "heading" : "5 Experiments",
      "text" : "Experiments are conducted on both synthetic data and real world data. We will focus on efficiency in Section 5.1 and discuss support recovery performance in Section 5.2. We consider three synthetic networks (Table 2) with 20, 35, and 50 blocks of 20-node, 35-node, and 50-node subnetworks, respectively. To demonstrate the estimation of networks with unbalanced-size subnetworks, we also consider a 46-block network with power law degree distributed subnetworks of sizes ranging from 5 to 50. Within each network, the subnetwork is generated according to a power law degree distribution, which mimics the structure of a biological network and is believed to be more challenging to recover\ncompared with other less complicated structures [Chen and Sharp, 2004, Peng et al., 2009, Danaher et al., 2014]. Each edge of each network is associated with a weight first sampled from a standard normal distribution, and then increased or decreased by 0.2 to further deviate from zero. For each network, 1600 samples are generated via Gibbs sampling within each subnetwork. Experiments on exact optimization are reported in B.2."
    }, {
      "heading" : "5.1 Pathwise Optimization",
      "text" : "Pathwise optimization aims to compute solutions over a range of different λ’s. Formally, we denote the set of λ’s used in (2) as Λ = {λ1, λ2, · · · , λτ}, and without loss of generality, we assume that λ1 < λ2 < · · · < λτ . The introduction of the screening rule provides us insightful heuristics for the determination of Λ. We start by choosing a λ1 that reflects the sparse blockwise structural assumption on the data. To achieve sparsity and avoid densely connected structures, we assume that the number of edges in the ground truth network is O(p). This assumption coincides with networks generated according to a power law degree distribution and hence is a faithful representation of the prior knowledge stemming from many biological problems. As a heuristic, we relax and apply the screening rule in (4) on each of the ( p 2 ) second empirical moments and choose λ1 such that the number of the absolute second empirical moments that are greater than λ1 is about p log p. Given a λ1 chosen this way, one can check how many blocks θ̂(λ1) has by the screening rule. To encourage blockwise structures, we magnify λ1 via λ1 ← 1.05λ1 until the current θ̂(λ1) has more than one block. We then choose λτ such that the number of absolute second empirical moments that are greater than λτ is about p. In our experiments, we use an evenly spaced Λ with τ = 25.\nTo estimate the networks in Table 2, we implement both NW and PL with and without screening using glmnet [Friedman et al., 2010] in R as a building block for logistic regression according to Ravikumar et al. 2010 and Geng et al. 2017. To generate a symmetric parameterization for NW, we set each pair of elements from θNW in symmetric positions to the element with a larger absolute value. Given Λ, we screen only at λ1 to identify various blocks. Each block is then solved separately in a pathwise fashion under Λ without further screening. The rationale of performing only one screening is that starting from a λ1 chosen in the aforementioned way has provided us a sparse blockwise structure that sets a significant portion of the parameterization to zeros; further screening over larger λ’s hence does not necessarily offer more efficiency gain.\nFigure 1 summarizes the runtime of pathwise optimization on the four synthetic networks in Table 2. The experiments are conducted on a PowerEdge R720 server with two Intel(R) Xeon(R) E5-2620 CPUs and 128GB RAM. As many as 24 threads can be run in parallel. For robustness, each runtime reported is the median runtime over five trials. When the sample size is less than 1600, each trial uses a subset of samples (subsamples) that are randomly drawn from the original datasets without replacement. As illustrated in Figure 1, the efficiency gain due to the screening rule is self-evident. Both NW and PL benefit substantially from the application of the screening rule. The speedup is more apparent with the increase of sample size as well as the increase of the dimension of the data. In our experiments, we observe that even with arguably the state-of-the-art implementation [Geng et al.,\nindx #blk #nd/blk TL#nd 1 20 20 400 2 35 35 1225 3 50 50 2500 4 46 5-50 1265\nTable 2: Summary of the four synthetic networks used in the experiments. indx represents the index of each network. #blk represents the number of blocks each network has. #nd/blk represents the number of nodes each block has. TL#nd represents the total number of nodes each network has.\n2017], PL without screening still has a significantly larger memory footprint compared with that of NW. Therefore, the experiments for PL without screening are not fully conducted in Figure 1b,1c, and 1d for networks with thousands of nodes. On the contrary, PL with the screening rule has a comparable memory footprint with that of NW. Furthermore, as shown in Figure 1, after applying the screening rule, PL also has a similar runtime with NW. This phenomenon demonstrates the utility of the screening rule for effectively reducing the memory footprint of PL, making PL readily available for large-scale problems."
    }, {
      "heading" : "5.2 Model Selection",
      "text" : "Our next experiment performs model selection by choosing an appropriate λ from the regularization parameter set Λ. We leverage the Stability Approach to Regularization Selection (StARS, Liu et al. 2010) for this task. In a nutshell, StARS learns a set of various models, denoted asM, over Λ using many subsamples that are drawn randomly from the original dataset without replacement. It then picks a λ∗ ∈ Λ that strikes the best balance between network sparsity and edge selection stability among the models inM. After the determination of λ∗, it is used on the entire original dataset to learn a model with which we compare the ground truth model and calculate its support recovery Area Under Curve (AUC). Implementation details of model selection are provided in B.1.\nIn Figure 2, we summarize the experimental results of model selection, where 24 subsamples are used for pathwise optimization in parallel to constructM. In Figure 2a, NW with and without screening achieve the same high AUC values over all four networks, while the application of the screening rule to NW provides roughly a 2x speedup, according to Figure 2b. The same AUC value shared by the two variants of NW is due to the same λ∗ chosen by the model selection procedure. Even more importantly, it is also because that under the same λ∗, the screening rule is able to perfectly identify the blockwise structure of the parameterization.\nDue to high memory cost, the model selection for PL without screening (green bars in Figure 2) is omitted in some networks. To control the memory footprint, the model selection for PL with screening (golden bars in Figure 2) also needs to be carried out meticulously by avoiding small λ’s in Λ that correspond to dense structures inM during estimation from subsamples. While avoiding dense structures makes PL with screening the fastest among all (Figure 2b), it comes at the cost of delivering the least accurate (though still reasonably effective) support recovery performance (Figure 2a). To improve the accuracy of this approach, we also leverage the connection between NW and PL by substituting 2λ∗NW for the resultant regularization parameter from model selection of PL, where λ∗NW is the regularization parameter selected for NW. This strategy results in better performance in support recovery (purple bars in Figure 2a)."
    }, {
      "heading" : "5.3 Real World Data",
      "text" : "Our real world data experiment applies NW with and without screening to a real world gene mutation dataset collected from 178 lung squamous cell carcinoma samples [Weinstein et al., 2013]. Each sample contains 13,665 binary variables representing the mutation statuses of various genes. For ease\nof interpretation, we keep genes whose mutation rates are at least 10% across all samples, yielding a subset of 145 genes in total. We use the model selection procedure introduced in Section 5.2 to determine a λ∗NW with which we learn the gene mutation network whose connected components are shown in Figure 3. For model selection, other than the configuration in B.1, we choose τ = 25. 384 trials are run in parallel using all 24 threads. We also choose λ1 such that about 2p log(p) absolute second empirical moments are greater than λ1. We choose λτ such that about 0.25p absolute second empirical moments are greater than λτ .\nIn our experiment, NW with and without screening select the same λ∗NW, and generate the same network. Since the dataset in question has a lower dimension and a smaller sample size compared with the synthetic data, NW without screening is adequately efficient. Nonetheless, with screening NW is still roughly 20% faster. This phenomenon once again indicates that in practice the screening rule can perfectly identify the blockwise sparsity pattern in the parameterization and deliver a significant efficiency gain. The genes in red in Figure 3 represent (lung) cancer and other disease related genes, which are scattered across the seven subnetworks discovered by the algorithm. In our experiment, we also notice that all the weights on the edges are positive. This is consistent with the biological belief that associated genes tend to mutate together to cause cancer."
    }, {
      "heading" : "6 Generalization",
      "text" : "With unary potentials, the `1-regularized MLE for the Ising model is defined as:\nθ̂ = arg min θ − 1 n n∑ k=1  p∑ i=1 θiix (k) i + p−1∑ i=1 p∑ j>i θijx (k) i x (k) j +A(θ) + λ 2 ‖θ‖1,off, (7)\nwhere ‖θ‖1,off = ∑p i=1 ∑p j 6=i|θij |. Note that the unary potentials are not penalized, which is a common practice [Wainwright et al., 2006, Höfling and Tibshirani, 2009, Ravikumar et al., 2010, Viallon et al., 2014] to ensure a hierarchical parameterization. The screening rule here is to replace (4) in Theorem 3 with: |EXXiXj − EXXiEXXj | ≤ λ. (8) Exhaustive justification, interpretation, and experiments are provided in Supplement C."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have proposed a screening rule for `1-regularized Ising model estimation. The simple closed-form screening rule is a necessary and sufficient condition for exact blockwise structural identification. Experimental results suggest that the proposed screening rule can provide drastic speedups for learning when combined with various optimization algorithms. Future directions include deriving screening rules for more general undirected graphical models [Liu et al., 2012, 2014b,a, Liu, 2014, Liu et al., 2016], and deriving screening rules for other inexact optimization algorithms [Liu and Page, 2013]. Further theoretical justifications regarding the conditions upon which the screening rule can be combined with inexact algorithms to recover block structures losslessly are also desirable.\nAcknowledgment: The authors would like to gratefully acknowledge the NIH BD2K Initiative grant U54 AI117924 and the NIGMS grant 2RO1 GM097618."
    } ],
    "references" : [ {
      "title" : "d’Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data",
      "author" : [ "O. Banerjee", "L.E. Ghaoui" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Banerjee et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2008
    }, {
      "title" : "High-dimensional ising model selection with bayesian information criteria",
      "author" : [ "R.F. Barber", "M. Drton" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "Barber and Drton,? \\Q2015\\E",
      "shortCiteRegEx" : "Barber and Drton",
      "year" : 2015
    }, {
      "title" : "Content-rich biological network constructed by mining pubmed abstracts",
      "author" : [ "H. Chen", "B.M. Sharp" ],
      "venue" : "BMC Bioinformatics,",
      "citeRegEx" : "Chen and Sharp.,? \\Q2004\\E",
      "shortCiteRegEx" : "Chen and Sharp.",
      "year" : 2004
    }, {
      "title" : "The joint graphical lasso for inverse covariance estimation across multiple classes",
      "author" : [ "P. Danaher", "P. Wang", "D.M. Witten" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Danaher et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Danaher et al\\.",
      "year" : 2014
    }, {
      "title" : "Mind the duality gap: safer rules for the lasso",
      "author" : [ "O. Fercoq", "A. Gramfort", "J. Salmon" ],
      "venue" : "In Proceedings of The 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Fercoq et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fercoq et al\\.",
      "year" : 2015
    }, {
      "title" : "Sparse inverse covariance estimation with the graphical lasso",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Friedman et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2008
    }, {
      "title" : "Regularization paths for generalized linear models via coordinate descent",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Journal of Statistical Software,",
      "citeRegEx" : "Friedman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2010
    }, {
      "title" : "An efficient pseudo-likelihood method for sparse binary pairwise Markov network estimation",
      "author" : [ "S. Geng", "Z. Kuang", "D. Page" ],
      "venue" : "arXiv Preprint,",
      "citeRegEx" : "Geng et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Geng et al\\.",
      "year" : 2017
    }, {
      "title" : "Safe feature elimination for the lasso and sparse supervised learning problems",
      "author" : [ "L.E. Ghaoui", "V. Viallon", "T. Rabbani" ],
      "venue" : "arXiv Preprint,",
      "citeRegEx" : "Ghaoui et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ghaoui et al\\.",
      "year" : 2010
    }, {
      "title" : "Statistical learning with sparsity: the lasso and generalizations",
      "author" : [ "T. Hastie", "R. Tibshirani", "M. Wainwright" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2015
    }, {
      "title" : "Estimation of sparse binary pairwise Markov networks using pseudolikelihoods",
      "author" : [ "H. Höfling", "R. Tibshirani" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Höfling and Tibshirani.,? \\Q2009\\E",
      "shortCiteRegEx" : "Höfling and Tibshirani.",
      "year" : 2009
    }, {
      "title" : "Multi-task learning of gaussian graphical models",
      "author" : [ "J. Honorio", "D. Samaras" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "Honorio and Samaras.,? \\Q2010\\E",
      "shortCiteRegEx" : "Honorio and Samaras.",
      "year" : 2010
    }, {
      "title" : "Big & quic: Sparse inverse covariance estimation for a million variables",
      "author" : [ "C.-J. Hsieh", "M.A. Sustik", "I.S. Dhillon", "P.K. Ravikumar", "R. Poldrack" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hsieh et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hsieh et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning Markov networks: Maximum bounded tree-width graphs",
      "author" : [ "D. Karger", "N. Srebro" ],
      "venue" : "In Proceedings of the Twelfth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Karger and Srebro.,? \\Q2001\\E",
      "shortCiteRegEx" : "Karger and Srebro.",
      "year" : 2001
    }, {
      "title" : "Probabilistic graphical models: principles and techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Koller and Friedman.,? \\Q2009\\E",
      "shortCiteRegEx" : "Koller and Friedman.",
      "year" : 2009
    }, {
      "title" : "Ensembles of lasso screening rules",
      "author" : [ "S. Lee", "N. Gornitz", "E.P. Xing", "D. Heckerman", "C. Lippert" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Lee et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2017
    }, {
      "title" : "Stability approach to regularization selection (stars) for high dimensional graphical models",
      "author" : [ "H. Liu", "K. Roeder", "L. Wasserman" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Liu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2010
    }, {
      "title" : "Statistical Methods for Genome-wide Association Studies and Personalized Medicine",
      "author" : [ "J. Liu" ],
      "venue" : "PhD thesis, The University of Wisconsin-Madison,",
      "citeRegEx" : "Liu.,? \\Q2014\\E",
      "shortCiteRegEx" : "Liu.",
      "year" : 2014
    }, {
      "title" : "Structure learning of undirected graphical models with contrastive divergence",
      "author" : [ "J. Liu", "D. Page" ],
      "venue" : "ICML 2013 Workshop on Structured Learning: Inferring Graphs from Structured and Unstructured Inputs,",
      "citeRegEx" : "Liu and Page.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liu and Page.",
      "year" : 2013
    }, {
      "title" : "Graphical-model based multiple testing under dependence, with applications to genome-wide association studies",
      "author" : [ "J. Liu", "P. Peissig", "C. Zhang", "E. Burnside", "C. McCarty", "D. Page" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Liu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2012
    }, {
      "title" : "Safe screening with variational inequalities and its application to lasso",
      "author" : [ "J. Liu", "Z. Zhao", "J. Wang", "J. Ye" ],
      "venue" : "arXiv Preprint arXiv:1307.7577,",
      "citeRegEx" : "Liu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning heterogeneous hidden Markov random fields",
      "author" : [ "J. Liu", "C. Zhang", "E. Burnside", "D. Page" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "Liu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2014
    }, {
      "title" : "Multiple testing under dependence via semiparametric graphical models",
      "author" : [ "J. Liu", "C. Zhang", "E. Burnside", "D. Page" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "Liu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2014
    }, {
      "title" : "Multiple testing under dependence via graphical models",
      "author" : [ "J. Liu", "C. Zhang", "D. Page" ],
      "venue" : "The Annals of Applied Statistics,",
      "citeRegEx" : "Liu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses",
      "author" : [ "P.-L. Loh", "M.J. Wainwright" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Loh and Wainwright,? \\Q2012\\E",
      "shortCiteRegEx" : "Loh and Wainwright",
      "year" : 2012
    }, {
      "title" : "Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses",
      "author" : [ "P.-L. Loh", "M.J. Wainwright" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Loh and Wainwright,? \\Q2013\\E",
      "shortCiteRegEx" : "Loh and Wainwright",
      "year" : 2013
    }, {
      "title" : "Sure screening for gaussian graphical models",
      "author" : [ "S. Luo", "R. Song", "D. Witten" ],
      "venue" : "arXiv Preprint arXiv:1407.7819,",
      "citeRegEx" : "Luo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2014
    }, {
      "title" : "Exact covariance thresholding into connected components for large-scale graphical lasso",
      "author" : [ "R. Mazumder", "T. Hastie" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Mazumder and Hastie.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mazumder and Hastie.",
      "year" : 2012
    }, {
      "title" : "Gap safe screening rules for sparse multi-task and multi-class models",
      "author" : [ "E. Ndiaye", "O. Fercoq", "A. Gramfort", "J. Salmon" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Ndiaye et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ndiaye et al\\.",
      "year" : 2015
    }, {
      "title" : "Lecture notes in machine learning 10-725/statistics 36-725-convex optimization (fall",
      "author" : [ "J. Pena", "R. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Pena and Tibshirani.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pena and Tibshirani.",
      "year" : 2016
    }, {
      "title" : "Partial correlation estimation by joint sparse regression models",
      "author" : [ "J. Peng", "P. Wang", "N. Zhou", "J. Zhu" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Peng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2009
    }, {
      "title" : "High-dimensional ising model selection using l1-regularized logistic regression",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Ravikumar et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ravikumar et al\\.",
      "year" : 2010
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Tibshirani.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani.",
      "year" : 1996
    }, {
      "title" : "Strong rules for discarding predictors in lasso-type problems",
      "author" : [ "R. Tibshirani", "J. Bien", "J. Friedman", "T. Hastie", "N. Simon", "J. Taylor", "R.J. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Tibshirani et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tibshirani et al\\.",
      "year" : 2012
    }, {
      "title" : "Tissue-based map of the human",
      "author" : [ "M. Uhlén", "L. Fagerberg", "B.M. Hallström", "C. Lindskog", "P. Oksvold", "A. Mardinoglu", "Å. Sivertsson", "C. Kampf", "E. Sjöstedt", "A. Asplund" ],
      "venue" : "proteome. Science,",
      "citeRegEx" : "Uhlén et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Uhlén et al\\.",
      "year" : 2015
    }, {
      "title" : "Empirical comparison study of approximate methods for structure selection in binary graphical models",
      "author" : [ "V. Viallon", "O. Banerjee", "E. Jougla", "G. Rey", "J. Coste" ],
      "venue" : "Biometrical Journal,",
      "citeRegEx" : "Viallon et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Viallon et al\\.",
      "year" : 2014
    }, {
      "title" : "Interaction screening: Efficient and sampleoptimal learning of ising models",
      "author" : [ "M. Vuffray", "S. Misra", "A. Lokhov", "M. Chertkov" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Vuffray et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Vuffray et al\\.",
      "year" : 2016
    }, {
      "title" : "High-dimensional graphical model selection using l1-regularized logistic regression",
      "author" : [ "M.J. Wainwright", "J.D. Lafferty", "P.K. Ravikumar" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Wainwright et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Wainwright et al\\.",
      "year" : 2006
    }, {
      "title" : "Tcga2stat: simple tcga data access for integrated statistical analysis in r",
      "author" : [ "Y.-W. Wan", "G.I. Allen", "Z. Liu" ],
      "venue" : "Bioinformatics, page btv677,",
      "citeRegEx" : "Wan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2015
    }, {
      "title" : "Xmrf: an r package to fit Markov networks to high-throughput genetics data",
      "author" : [ "Y.-W. Wan", "G.I. Allen", "Y. Baker", "E. Yang", "P. Ravikumar", "M. Anderson", "Z. Liu" ],
      "venue" : "BMC Systems Biology,",
      "citeRegEx" : "Wan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2016
    }, {
      "title" : "Lasso screening rules via dual polytope projection",
      "author" : [ "J. Wang", "J. Zhou", "P. Wonka", "J. Ye" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Wang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "A safe screening rule for sparse logistic regression",
      "author" : [ "J. Wang", "J. Zhou", "J. Liu", "P. Wonka", "J. Ye" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Wang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "The cancer genome atlas pan-cancer analysis project",
      "author" : [ "J.N. Weinstein", "E.A. Collisson", "G.B. Mills", "K.R.M. Shaw", "B.A. Ozenberger", "K. Ellrott", "I. Shmulevich", "C. Sander", "J.M. Stuart", "C.G.A.R. Network" ],
      "venue" : "Nature Genetics,",
      "citeRegEx" : "Weinstein et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Weinstein et al\\.",
      "year" : 2013
    }, {
      "title" : "New insights and faster computations for the graphical lasso",
      "author" : [ "D.M. Witten", "J.H. Friedman", "N. Simon" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "Witten et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Witten et al\\.",
      "year" : 2011
    }, {
      "title" : "Screening tests for lasso problems",
      "author" : [ "Z.J. Xiang", "Y. Wang", "P.J. Ramadge" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Xiang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xiang et al\\.",
      "year" : 2016
    }, {
      "title" : "Fused multiple graphical lasso",
      "author" : [ "S. Yang", "Z. Lu", "X. Shen", "P. Wonka", "J. Ye" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "While the field of statistical learning with sparsity [Hastie et al., 2015] has been steadily rising to prominence ever since the introduction of the lasso (least absolute shrinkage and selection operator) at the end of the last century [Tibshirani, 1996], it was not until the recent decade that various screening rules debuted to further equip the ever-evolving optimization arsenals for some of the most fundamental problems in sparse learning such as `1-regularized generalized linear models (GLMs, Friedman et al.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 32,
      "context" : ", 2015] has been steadily rising to prominence ever since the introduction of the lasso (least absolute shrinkage and selection operator) at the end of the last century [Tibshirani, 1996], it was not until the recent decade that various screening rules debuted to further equip the ever-evolving optimization arsenals for some of the most fundamental problems in sparse learning such as `1-regularized generalized linear models (GLMs, Friedman et al.",
      "startOffset" : 169,
      "endOffset" : 187
    }, {
      "referenceID" : 5,
      "context" : "2010) and inverse covariance matrix estimation [Friedman et al., 2008].",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "While a sparse Ising model is a desirable choice, for such an application the scalability of the model could fail due to the innate NP-hardness [Karger and Srebro, 2001] of inference, and hence maximum likelihood learning, owing to the partition function.",
      "startOffset" : 144,
      "endOffset" : 169
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, compared with its Gaussian counterpart, where screening rules are available (Table 1) and learning is scalable [Hsieh et al., 2013], the proposed screening rule is especially valuable and desperately needed to address the more challenging learning problem of sparse Ising models.",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 39,
      "context" : "An Ising model [Wan et al., 2016] with the parameterization θ is:",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : ", 2010] and the pseudolikelihood (PL) method [Höfling and Tibshirani, 2009].",
      "startOffset" : 45,
      "endOffset" : 75
    }, {
      "referenceID" : 33,
      "context" : ", a rule that is not foolproof but barely makes mistakes) and an optimal solution can be safeguarded by adjusting the screened solution to optimality based on the KKT conditions of the inexact problem [Tibshirani et al., 2012].",
      "startOffset" : 201,
      "endOffset" : 226
    }, {
      "referenceID" : 7,
      "context" : "On the other hand, PL can be considered as solving all p `1-regularized logistic regression problems in (5) jointly with symmetric constraints over the parameterization [Geng et al., 2017]:",
      "startOffset" : 169,
      "endOffset" : 188
    }, {
      "referenceID" : 6,
      "context" : "To estimate the networks in Table 2, we implement both NW and PL with and without screening using glmnet [Friedman et al., 2010] in R as a building block for logistic regression according to Ravikumar et al.",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 42,
      "context" : "Our real world data experiment applies NW with and without screening to a real world gene mutation dataset collected from 178 lung squamous cell carcinoma samples [Weinstein et al., 2013].",
      "startOffset" : 163,
      "endOffset" : 187
    }, {
      "referenceID" : 34,
      "context" : "Genes in red are (lung) cancer and other disease related genes [Uhlén et al., 2015].",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 38,
      "context" : "Mutation data are extracted via the TCGA2STAT package [Wan et al., 2015] in R and the figure is rendered by Cytoscape.",
      "startOffset" : 54,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : ", 2016], and deriving screening rules for other inexact optimization algorithms [Liu and Page, 2013].",
      "startOffset" : 80,
      "endOffset" : 100
    } ],
    "year" : 2017,
    "abstractText" : "We discover a screening rule for `1-regularized Ising model estimation. The simple closed-form screening rule is a necessary and sufficient condition for exactly recovering the blockwise structure of a solution under any given regularization parameters. With enough sparsity, the screening rule can be combined with various optimization procedures to deliver solutions efficiently in practice. The screening rule is especially suitable for large-scale exploratory data analysis, where the number of variables in the dataset can be thousands while we are only interested in the relationship among a handful of variables within moderate-size clusters for interpretability. Experimental results on various datasets demonstrate the efficiency and insights gained from the introduction of the screening rule.",
    "creator" : null
  }
}