{
  "name" : "70efdf2ec9b086079795c442636b55fb.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning",
    "authors" : [ "Zhen He", "Shaobing Gao", "Liang Xiao", "Daxue Liu", "Hangen He", "David Barber" ],
    "emails" : [ "<gaoshaobing@scu.edu.cn>", "<hezhen.cs@gmail.com>." ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider the time-series prediction task of producing a desired output yt at each timestep t∈{1, . . . , T} given an observed input sequence x1:t = {x1,x2, · · · ,xt}, where xt ∈ RR and yt∈RS are vectors1. The Recurrent Neural Network (RNN) [17, 43] is a powerful model that learns how to use a hidden state vector ht ∈RM to encapsulate the relevant features of the entire input history x1:t up to timestep t. Let hcatt−1∈RR+M be the concatenation of the current input xt and the previous hidden state ht−1:\nhcatt−1 = [xt,ht−1] (1)\nThe update of the hidden state ht is defined as:\nat = h cat t−1W h + bh (2) ht = φ(at) (3)\nwhere W h∈R(R+M)×M is the weight, bh∈RM the bias, at∈RM the hidden activation, and φ(·) the element-wise tanh function. Finally, the output yt at timestep t is generated by:\nyt = ϕ(htW y + by) (4)\nwhereW y∈RM×S and by ∈ RS , and ϕ(·) can be any differentiable function, depending on the task. However, this vanilla RNN has difficulties in modeling long-range dependencies due to the vanishing/exploding gradient problem [4]. Long Short-Term Memories (LSTMs) [19, 24] alleviate\n∗Corresponding authors: Shaobing Gao <gaoshaobing@scu.edu.cn> and Zhen He <hezhen.cs@gmail.com>. 1Vectors are assumed to be in row form throughout this paper.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nthese problems by employing memory cells to preserve information for longer, and adopting gating mechanisms to modulate the information flow. Given the success of the LSTM in sequence modeling, it is natural to consider how to increase the complexity of the model and thereby increase the set of tasks for which the LSTM can be profitably applied.\nWe consider the capacity of a network to consist of two components: the width (the amount of information handled in parallel) and the depth (the number of computation steps) [5]. A naive way to widen the LSTM is to increase the number of units in a hidden layer; however, the parameter number scales quadratically with the number of units. To deepen the LSTM, the popular Stacked LSTM (sLSTM) stacks multiple LSTM layers [20]; however, runtime is proportional to the number of layers and information from the input is potentially lost (due to gradient vanishing/explosion) as it propagates vertically through the layers.\nIn this paper, we introduce a way to both widen and deepen the LSTM whilst keeping the parameter number and runtime largely unchanged. In summary, we make the following contributions: (a) We tensorize RNN hidden state vectors into higher-dimensional tensors which allow more flexible\nparameter sharing and can be widened more efficiently without additional parameters. (b) Based on (a), we merge RNN deep computations into its temporal computations so that the\nnetwork can be deepened with little additional runtime, resulting in a Tensorized RNN (tRNN). (c) We extend the tRNN to an LSTM, namely the Tensorized LSTM (tLSTM), which integrates a\nnovel memory cell convolution to help to prevent the vanishing/exploding gradients."
    }, {
      "heading" : "2 Method",
      "text" : ""
    }, {
      "heading" : "2.1 Tensorizing Hidden States",
      "text" : "It can be seen from (2) that in an RNN, the parameter number scales quadratically with the size of the hidden state. A popular way to limit the parameter number when widening the network is to organize parameters as higher-dimensional tensors which can be factorized into lower-rank sub-tensors that contain significantly fewer elements [6, 15, 18, 26, 32, 39, 46, 47, 51], which is is known as tensor factorization. This implicitly widens the network since the hidden state vectors are in fact broadcast to interact with the tensorized parameters. Another common way to reduce the parameter number is to share a small set of parameters across different locations in the hidden state, similar to Convolutional Neural Networks (CNNs) [34, 35].\nWe adopt parameter sharing to cutdown the parameter number for RNNs, since compared with factorization, it has the following advantages: (i) scalability, i.e., the number of shared parameters can be set independent of the hidden state size, and (ii) separability, i.e., the information flow can be carefully managed by controlling the receptive field, allowing one to shift RNN deep computations to the temporal domain (see Sec. 2.2). We also explicitly tensorize the RNN hidden state vectors, since compared with vectors, tensors have a better: (i) flexibility, i.e., one can specify which dimensions to share parameters and then can just increase the size of those dimensions without introducing additional parameters, and (ii) efficiency, i.e., with higher-dimensional tensors, the network can be widened faster w.r.t. its depth when fixing the parameter number (see Sec. 2.3).\nFor ease of exposition, we first consider 2D tensors (matrices): we tensorize the hidden state ht∈RM to becomeHt∈RP×M , where P is the tensor size, and M the channel size. We locally-connect the first dimension ofHt in order to share parameters, and fully-connect the second dimension ofHt to allow global interactions. This is analogous to the CNN which fully-connects one dimension (e.g., the RGB channel for input images) to globally fuse different feature planes. Also, if one compares Ht to the hidden state of a Stacked RNN (sRNN) (see Fig. 1(a)), then P is akin to the number of stacked hidden layers, and M the size of each hidden layer. We start to describe our model based on 2D tensors, and finally show how to strengthen the model with higher-dimensional tensors."
    }, {
      "heading" : "2.2 Merging Deep Computations",
      "text" : "Since an RNN is already deep in its temporal direction, we can deepen an input-to-output computation by associating the input xt with a (delayed) future output. In doing this, we need to ensure that the output yt is separable, i.e., not influenced by any future input xt′ (t′ > t). Thus, we concatenate the projection of xt to the top of the previous hidden state Ht−1, then gradually shift the input\ninformation down when the temporal computation proceeds, and finally generate yt from the bottom ofHt+L−1, where L−1 is the number of delayed timesteps for computations of depth L. An example with L=3 is shown in Fig. 1(b). This is in fact a skewed sRNN as used in [1] (also similar to [48]). However, our method does not need to change the network structure and also allows different kinds of interactions as long as the output is separable, e.g, one can increase the local connections and use feedback (see Fig. 1(c)), which can be beneficial for sRNNs [10]. In order to share parameters, we updateHt using a convolution with a learnable kernel. In this manner we increase the complexity of the input-to-output mapping (by delaying outputs) and limit parameter growth (by sharing transition parameters using convolutions).\nTo describe the resulting tRNN model, letHcatt−1∈R(P+1)×M be the concatenated hidden state, and p∈Z+ the location at a tensor. The channel vector hcatt−1,p∈RM at location p ofHcatt−1 is defined as:\nhcatt−1,p =\n{ xtW\nx + bx if p = 1 ht−1,p−1 if p > 1\n(5)\nwhereW x ∈ RR×M and bx ∈ RM . Then, the update of tensorHt is implemented via a convolution:\nAt =H cat t−1 ~ {W h, bh} (6)\nHt = φ(At) (7)\nwhereW h∈RK×Mi×Mo is the kernel weight of size K, with M i=M input channels and Mo=M output channels, bh ∈RMo is the kernel bias, At ∈RP×M o\nis the hidden activation, and ~ is the convolution operator (see Appendix A.1 for a more detailed definition). Since the kernel convolves across different hidden layers, we call it the cross-layer convolution. The kernel enables interaction, both bottom-up and top-down across layers. Finally, we generate yt from the channel vector ht+L−1,P ∈RM which is located at the bottom ofHt+L−1:\nyt = ϕ(ht+L−1,PW y + by) (8)\nwhereW y∈RM×S and by∈RS . To guarantee that the receptive field of yt only covers the current and previous inputs x1:t (see Fig. 1(c)), L, P , and K should satisfy the constraint:\nL = ⌈ 2P K −K mod 2 ⌉ (9)\nwhere d·e is the ceil operation. For the derivation of (9), please see Appendix B. We call the model defined in (5)-(8) the Tensorized RNN (tRNN). The model can be widened by increasing the tensor size P , whilst the parameter number remains fixed (thanks to the convolution). Also, unlike the sRNN of runtime complexity O(TL), tRNN breaks down the runtime complexity to O(T+L), which means either increasing the sequence length T or the network depth L would not significantly increase the runtime."
    }, {
      "heading" : "2.3 Extending to LSTMs",
      "text" : "To allow the tRNN to capture long-range temporal dependencies, one can straightforwardly extend it to an LSTM by replacing the tRNN tensor update equations of (6)-(7) as follows:\n[Agt ,A i t,A f t ,A o t ] =H cat t−1 ~ {W h, bh} (10)\n[Gt, It,Ft,Ot] = [φ(A g t ), σ(A i t), σ(A f t ), σ(A o t )] (11)\nCt = Gt It +Ct−1 Ft (12) Ht = φ(Ct) Ot (13)\nwhere the kernel {W h, bh} is of size K, with M i=M input channels and Mo=4M output channels, Agt ,A i t,A f t ,A o t ∈RP×M are activations for the new content Gt, input gate It, forget gate Ft, and output gate Ot, respectively, σ(·) is the element-wise sigmoid function, is the element-wise multiplication, and Ct∈RP×M is the memory cell. However, since in (12) the previous memory cell Ct−1 is only gated along the temporal direction (see Fig. 1(d)), long-range dependencies from the input to output might be lost when the tensor size P becomes large.\nMemory Cell Convolution. To capture long-range dependencies from multiple directions, we additionally introduce a novel memory cell convolution, by which the memory cells can have a larger receptive field (see Fig. 1(e)). We also dynamically generate this convolution kernel so that it is both time- and location-dependent, allowing for flexible control over long-range dependencies from different directions. This results in our tLSTM tensor update equations:\n[Agt ,A i t,A f t ,A o t ,A q t ] =H cat t−1 ~ {W h, bh} (14)\n[Gt, It,Ft,Ot,Qt] = [φ(A g t ), σ(A i t), σ(A f t ), σ(A o t ), ς(A q t )] (15) W ct (p) = reshape (qt,p, [K, 1, 1]) (16) Cconvt−1 = Ct−1 ~W c t (p) (17) Ct = Gt It +Cconvt−1 Ft (18) Ht = φ(Ct) Ot (19)\nwhere, in contrast to (10)-(13), the kernel {W h, bh} has additional 〈K〉 output channels2 to generate the activation Aqt ∈RP×〈K〉 for the dynamic kernel bankQt∈RP×〈K〉, qt,p∈R〈K〉 is the vectorized adaptive kernel at the location p of Qt, and W ct (p)∈RK×1×1 is the dynamic kernel of size K with a single input/output channel, which is reshaped from qt,p (see Fig. 2(a) for an illustration). In (17), each channel of the previous memory cell Ct−1 is convolved with W ct (p) whose values vary with p, forming a memory cell convolution (see Appendix A.2 for a more detailed definition), which produces a convolved memory cell Cconvt−1 ∈RP×M . Note that in (15) we employ a softmax function ς(·) to normalize the channel dimension ofQt, which, similar to [37], can stabilize the value of memory cells and help to prevent the vanishing/exploding gradients (see Appendix C for details).\nThe idea of dynamically generating network weights has been used in many works [6, 14, 15, 23, 44, 46], where in [14] location-\ndependent convolutional kernels are also dynamically generated to improve CNNs. In contrast to these works, we focus on broadening the receptive field of tLSTM memory cells. Whilst the flexibility is retained, fewer parameters are required to generate the kernel since the kernel is shared by different memory cell channels.\nChannel Normalization. To improve training, we adapt Layer Normalization (LN) [3] to our tLSTM. Similar to the observation in [3] that LN does not work well in CNNs where channel vectors at different locations have very different statistics, we find that LN is also unsuitable for tLSTM where lower level information is near the input while higher level information is near the output. We\n2The operator 〈·〉 returns the cumulative product of all elements in the input variable.\ntherefore normalize the channel vectors at different locations with their own statistics, forming a Channel Normalization (CN), with its operator CN(·):\nCN(Z;Γ,B) = Ẑ Γ +B (20)\nwhere Z, Ẑ,Γ,B ∈ RP×Mz are the original tensor, normalized tensor, gain parameter, and bias parameter, respectively. The mz-th channel of Z, i.e. zmz ∈RP , is normalized element-wisely:\nẑmz = (zmz − zµ)/zσ (21)\nwhere zµ, zσ∈RP are the mean and standard deviation along the channel dimension of Z, respectively, and ẑmz ∈RP is the mz-th channel of Ẑ. Note that the number of parameters introduced by CN/LN can be neglected as it is very small compared to the number of other parameters in the model.\nUsing Higher-Dimensional Tensors. One can observe from (9) that when fixing the kernel size K, the tensor size P of a 2D tLSTM grows linearly w.r.t. its depth L. How can we expand the tensor volume more rapidly so that the network can be widened more efficiently? We can achieve this goal by leveraging higher-dimensional tensors. Based on previous definitions for 2D tLSTMs, we replace the 2D tensors with D-dimensional (D>2) tensors, obtainingHt,Ct∈RP1×P2×...×PD−1×M with the tensor size P=[P1, P2, . . . , PD−1]. Since the hidden states are no longer matrices, we concatenate the projection of xt to one corner ofHt−1, and thus (5) is extended as:\nhcatt−1,p =  xtW\nx + bx if pd = 1 for d = 1, 2, . . . , D − 1 ht−1,p−1 if pd > 1 for d = 1, 2, . . . , D − 1 0 otherwise\n(22)\nwhere hcatt−1,p ∈RM is the channel vector at location p ∈ ZD−1+ of the concatenated hidden state Hcatt−1∈R(P1+1)×(P2+1)×...×(PD−1+1)×M . For the tensor update, the convolution kernelW h andW ct (·) also increase their dimensionality with kernel size K = [K1,K2, . . . ,KD−1]. Note that W ct (·) is reshaped from the vector, as illustrated in Fig. 2(b). Correspondingly, we generate the output yt from the opposite corner ofHt+L−1, and therefore (8) is modified as:\nyt = ϕ(ht+L−1,PW y + by) (23)\nFor convenience, we set Pd = P and Kd = K for d = 1, 2, . . . , D − 1 so that all dimensions of P and K can satisfy (9) with the same depth L. In addition, CN still normalizes the channel dimension of tensors."
    }, {
      "heading" : "3 Experiments",
      "text" : "We evaluate tLSTM on five challenging sequence learning tasks under different configurations: (a) sLSTM (baseline): our implementation of sLSTM [21] with parameters shared across all layers. (b) 2D tLSTM: the standard 2D tLSTM, as defined in (14)-(19). (c) 2D tLSTM–M: removing (–) memory (M) cell convolutions from (b), as defined in (10)-(13). (d) 2D tLSTM–F: removing (–) feedback (F) connections from (b). (e) 3D tLSTM: tensorizing (b) into 3D tLSTM. (f) 3D tLSTM+LN: applying (+) LN [3] to (e). (g) 3D tLSTM+CN: applying (+) CN to (e), as defined in (20). To compare different configurations, we also use L to denote the number of layers of a sLSTM, and M to denote the hidden size of each sLSTM layer. We set the kernel size K to 2 for 2D tLSTM–F and 3 for other tLSTMs, in which case we have L=P according to (9).\nFor each configuration, we fix the parameter number and increase the tensor size to see if the performance of tLSTM can be boosted without increasing the parameter number. We also investigate how the runtime is affected by the depth, where the runtime is measured by the average GPU milliseconds spent by a forward and backward pass over one timestep of a single example. Next, we compare tLSTM against the state-of-the-art methods to evaluate its ability. Finally, we visualize the internal working mechanism of tLSTM. Please see Appendix D for training details.\n3.1 Wikipedia Language Modeling\nThe Hutter Prize Wikipedia dataset [25] consists of 100 million characters taken from 205 different characters including alphabets, XML markups and special symbols. We model the dataset at the character-level, and try to predict the next character of the input sequence.\nWe fix the parameter number to 10M, corresponding to channel sizes M of 1120 for sLSTM and 2D tLSTM–F, 901 for other 2D tLSTMs, and 522 for 3D tLSTMs. All configurations are evaluated with depths L=1, 2, 3, 4. We use Bits-per-character (BPC) to measure the model performance.\nResults are shown in Fig. 3. When L ≤ 2, sLSTM and 2D tLSTM–F outperform other models because of a larger M . With L increasing, the performances of sLSTM and 2D tLSTM–M improve but become saturated when L≥3, while tLSTMs with memory cell convolutions improve with increasing L and finally outperform both sLSTM and 2D tLSTM–M. When L= 4, 2D tLSTM–F is surpassed by 2D tLSTM, which is in turn surpassed by 3D tLSTM. The performance of 3D tLSTM+LN benefits from LN only when L ≤ 2. However, 3D tLSTM+CN consistently improves 3D tLSTM with different L.\nWhilst the runtime of sLSTM is almost proportional to L, it is nearly constant in each tLSTM configuration and largely independent of L.\nWe compare a larger model, i.e. a 3D tLSTM+CN with L=6 and M= 1200, to the state-of-the-art methods on the test set, as reported in Table 1. Our model achieves 1.264 BPC with 50.1M parameters, and is competitive to the best performing methods [38, 54] with similar parameter numbers.\n3.2 Algorithmic Tasks\n(a) Addition: The task is to sum two 15-digit integers. The network first reads two integers with one digit per timestep, and then predicts the summation. We follow the processing of [30], where a symbol ‘-’ is used to delimit the integers as well as pad the input/target sequence. A 3-digit integer addition task is of the form:\nInput: - 1 2 3 - 9 0 0 - - - - - Target: - - - - - - - - 1 0 2 3 -\n(b) Memorization: The goal of this task is to memorize a sequence of 20 random symbols. Similar to the addition task, we use 65 different\nsymbols. A 5-symbol memorization task is of the form:\nInput: - a b c c b - - - - - - Target: - - - - - - a b c c b -\nWe evaluate all configurations with L=1, 4, 7, 10 on both tasks, where M is 400 for addition and 100 for memorization. The performance is measured by the symbol prediction accuracy.\nFig. 4 show the results. In both tasks, large L degrades the performances of sLSTM and 2D tLSTM– M. In contrast, the performance of 2D tLSTM–F steadily improves with L increasing, and is further enhanced by using feedback connections, higher-dimensional tensors, and CN, while LN helps only when L=1. Note that in both tasks, the correct solution can be found (when 100% test accuracy is achieved) due to the repetitive nature of the task. In our experiment, we also observe that for the addition task, 3D tLSTM+CN with L=7 outperforms other configurations and finds the solution with only 298K training samples, while for the memorization task, 3D tLSTM+CN with L=10 beats others configurations and achieves perfect memorization after seeing 54K training samples. Also, unlike in sLSTM, the runtime of all tLSTMs is largely unaffected by L.\nWe further compare the best performing configurations to the state-of-the-art methods for both tasks (see Table 2). Our models solve both tasks significantly faster (i.e., using fewer training samples) than other models, achieving the new state-of-the-art results."
    }, {
      "heading" : "3.3 MNIST Image Classification",
      "text" : "The MNIST dataset [35] consists of 50000/10000/10000 handwritten digit images of size 28×28 for training/validation/test. We have two tasks on this dataset:\n(a) Sequential MNIST: The goal is to classify the digit after sequentially reading the pixels in a scanline order [33]. It is therefore a 784 timestep sequence learning task where a single output is produced at the last timestep; the task requires very long range dependencies in the sequence.\n(b) Sequential Permuted MNIST: We permute the original image pixels in a fixed random order as in [2], resulting in a permuted MNIST\n(pMNIST) problem that has even longer range dependencies across pixels and is harder.\nIn both tasks, all configurations are evaluated with M=100 and L=1, 3, 5. The model performance is measured by the classification accuracy.\nResults are shown in Fig. 5. sLSTM and 2D tLSTM–M no longer benefit from the increased depth when L = 5. Both increasing the depth and tensorization boost the performance of 2D tLSTM. However, removing feedback connections from 2D tLSTM seems not to affect the performance. On the other hand, CN enhances the 3D tLSTM and when L≥ 3 it outperforms LN. 3D tLSTM+CN with L=5 achieves the highest performances in both tasks, with a validation accuracy of 99.1% for MNIST and 95.6% for pMNIST. The runtime of tLSTMs is negligibly affected by L, and all tLSTMs become faster than sLSTM when L=5.\nWe also compare the configurations of the highest test accuracies to the state-of-the-art methods (see Table 3). For sequential MNIST, our 3D tLSTM+CN with L=3 performs as well as the state-of-the-art Dilated GRU model [8], with a test accuracy of 99.2%. For the sequential pMNIST, our 3D tLSTM+CN with L= 5 has a test accuracy of 95.7%, which is close to the state-of-the-art of 96.7% produced by the Dilated CNN [40] in [8]."
    }, {
      "heading" : "3.4 Analysis",
      "text" : "The experimental results of different model configurations on different tasks suggest that the performance of tLSTMs can be improved by increasing the tensor size and network depth, requiring no additional parameters and little additional runtime. As the network gets wider and deeper, we found that the memory cell convolution mechanism is crucial to maintain improvement in performance. Also, we found that feedback connections are useful for tasks of sequential output (e.g., our Wikipedia and algorithmic tasks). Moreover, tLSTM can be further strengthened via tensorization or CN.\nIt is also intriguing to examine the internal working mechanism of tLSTM. Thus, we visualize the memory cell which gives insight into how information is routed. For each task, the best performing tLSTM is run on a random example. We record the channel mean (the mean over channels, e.g., it is of size P×P for 3D tLSTMs) of the memory cell at each timestep, and visualize the diagonal values of the channel mean from location pin=[1, 1] (near the input) to pout=[P, P ] (near the output).\nVisualization results in Fig. 6 reveal the distinct behaviors of tLSTM when dealing with different tasks: (i) Wikipedia: the input can be carried to the output location with less modification if it is sufficient to determine the next character, and vice versa; (ii) addition: the first integer is gradually encoded into memories and then interacts (performs addition) with the second integer, producing the sum; (iii) memorization: the network behaves like a shift register that continues to move the input symbol to the output location at the correct timestep; (iv) sequential MNIST: the network is more sensitive to the pixel value change (representing the contour, or topology of the digit) and can gradually accumulate evidence for the final prediction; (v) sequential pMNIST: the network is sensitive to high value pixels (representing the foreground digit), and we conjecture that this is because the permutation destroys the topology of the digit, making each high value pixel potentially important.\nFrom Fig. 6 we can also observe common phenomena for all tasks: (i) at each timestep, the values at different tensor locations are markedly different, implying that wider (larger) tensors can encode more information, with less effort to compress it; (ii) from the input to the output, the values become increasingly distinct and are shifted by time, revealing that deep computations are indeed performed together with temporal computations, with long-range dependencies carried by memory cells."
    }, {
      "heading" : "4 Related Work",
      "text" : "Convolutional LSTMs. Convolutional LSTMs (cLSTMs) are proposed to parallelize the computation of LSTMs when the input at each timestep is structured (see Fig. 7(a)), e.g., a vector array [48], a vector matrix [41, 42, 50, 52], or a vector tensor [9, 45]. Unlike cLSTMs, tLSTM aims to increase the capacity of LSTMs when the input at each timestep is non-structured, i.e., a single vector, and is advantageous over cLSTMs in that: (i) it performs the convolution across different hidden layers whose structure is independent of the input structure, and integrates information bottom-up and top-down; while cLSTM performs the convolution within each hidden layer whose structure is coupled with the input structure, thus will fall back to the vanilla LSTM if the input at each timestep is a single vector; (ii) it can be widened efficiently without additional parameters by increasing the tensor size; while cLSTM can be widened by increasing the kernel size or kernel channel, which significantly increases the number of parameters; (iii) it can be deepened with little additional runtime by delaying the output; while cLSTM can be deepened by using more hidden layers, which significantly increases the runtime; (iv) it captures long-range dependencies from multiple directions through the memory cell convolution; while cLSTM struggles to capture long-range dependencies from multiple directions since memory cells are only gated along one direction.\nDeep LSTMs. Deep LSTMs (dLSTMs) extend sLSTMs by making them deeper (see Fig. 7(b)-(d)). To keep the parameter number small and ease training, Graves [22], Kalchbrenner et al. [30], Mujika et al. [38], Zilly et al. [54] apply another RNN/LSTM along the depth direction of dLSTMs, which, however, multiplies the runtime. Though there are implementations to accelerate the deep computation [1, 16], they generally aim at simple architectures such sLSTMs. Compared with dLSTMs, tLSTM performs the deep computation with little additional runtime, and employs a cross-layer convolution to enable the feedback mechanism. Moreover, the capacity of tLSTM can be increased more efficiently by using higher-dimensional tensors, whereas in dLSTM all hidden layers as a whole only equal to a 2D tensor (i.e., a stack of hidden vectors), the dimensionality of which is fixed.\nOther Parallelization Methods. Some methods [7, 8, 28, 29, 36, 40] parallelize the temporal computation of the sequence (e.g., use the temporal convolution, as in Fig. 7(e)) during training, in which case full input/target sequences are accessible. However, during the online inference when the input presents sequentially, temporal computations can no longer be parallelized and will be blocked by deep computations of each timestep, making these methods potentially unsuitable for real-time applications that demand a high sampling/output frequency. Unlike these methods, tLSTM can speed up not only training but also online inference for many tasks since it performs the deep computation by the temporal computation, which is also human-like: we convert each signal to an action and meanwhile receive new signals in a non-blocking way. Note that for the online inference of tasks that use the previous output yt−1 for the current input xt (e.g., autoregressive sequence generation), tLSTM cannot parallel the deep computation since it requires to delay L−1 timesteps to get yt−1."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We introduced the Tensorized LSTM, which employs tensors to share parameters and utilizes the temporal computation to perform the deep computation for sequential tasks. We validated our model on a variety of tasks, showing its potential over other popular approaches."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by the NSFC grant 91220301, the Alan Turing Institute under the EPSRC grant EP/N510129/1, and the China Scholarship Council."
    } ],
    "references" : [ {
      "title" : "Optimizing performance of recurrent neural networks on gpus",
      "author" : [ "Jeremy Appleyard", "Tomas Kocisky", "Phil Blunsom" ],
      "venue" : "arXiv preprint arXiv:1604.01946,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Unitary evolution recurrent neural networks",
      "author" : [ "Martin Arjovsky", "Amar Shah", "Yoshua Bengio" ],
      "venue" : "In ICML, 2016",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Yoshua Bengio", "Patrice Simard", "Paolo Frasconi" ],
      "venue" : "IEEE TNN,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1994
    }, {
      "title" : "Learning deep architectures for ai",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "Foundations and trends R  © in Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Learning feed-forward one-shot learners",
      "author" : [ "Luca Bertinetto", "João F Henriques", "Jack Valmadre", "Philip Torr", "Andrea Vedaldi" ],
      "venue" : "In NIPS, 2016",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Quasi-recurrent neural networks",
      "author" : [ "James Bradbury", "Stephen Merity", "Caiming Xiong", "Richard Socher" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2017
    }, {
      "title" : "Dilated recurrent neural networks",
      "author" : [ "Shiyu Chang", "Yang Zhang", "Wei Han", "Mo Yu", "Xiaoxiao Guo", "Wei Tan", "Xiaodong Cui", "Michael Witbrock", "Mark Hasegawa-Johnson", "Thomas Huang" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2017
    }, {
      "title" : "Combining fully convolutional and recurrent neural networks for 3d biomedical image segmentation",
      "author" : [ "Jianxu Chen", "Lin Yang", "Yizhe Zhang", "Mark Alber", "Danny Z Chen" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Gated feedback recurrent neural networks",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In ICML,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Hierarchical multiscale recurrent neural networks",
      "author" : [ "Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2017
    }, {
      "title" : "Torch7: A matlab-like environment for machine learning",
      "author" : [ "Ronan Collobert", "Koray Kavukcuoglu", "Clément Farabet" ],
      "venue" : "In NIPS Workshop,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Recurrent batch normalization",
      "author" : [ "Tim Cooijmans", "Nicolas Ballas", "César Laurent", "Aaron Courville" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2017
    }, {
      "title" : "Dynamic filter networks",
      "author" : [ "Bert De Brabandere", "Xu Jia", "Tinne Tuytelaars", "Luc Van Gool" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Predicting parameters in deep learning",
      "author" : [ "Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Persistent rnns: Stashing recurrent weights on-chip",
      "author" : [ "Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh" ],
      "venue" : "In ICML,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L Elman" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1990
    }, {
      "title" : "Ultimate tensorization: compressing convolutional and fc layers alike",
      "author" : [ "Timur Garipov", "Dmitry Podoprikhin", "Alexander Novikov", "Dmitry Vetrov" ],
      "venue" : "In NIPS Workshop,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Learning to forget: Continual prediction with lstm",
      "author" : [ "Felix A Gers", "Jürgen Schmidhuber", "Fred Cummins" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2000
    }, {
      "title" : "Speech recognition with deep recurrent neural networks",
      "author" : [ "Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1308.0850,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Adaptive computation time for recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1603.08983,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1997
    }, {
      "title" : "The human knowledge compression contest",
      "author" : [ "Marcus Hutter" ],
      "venue" : "URL http://prize.hutter1.net,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Modeling compositionality with multiplicative recurrent neural networks",
      "author" : [ "Ozan Irsoy", "Claire Cardie" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    }, {
      "title" : "An empirical exploration of recurrent network architectures",
      "author" : [ "Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever" ],
      "venue" : "In ICML,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Can active memory replace attention",
      "author" : [ "Łukasz Kaiser", "Samy Bengio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "Neural gpus learn algorithms",
      "author" : [ "Łukasz Kaiser", "Ilya Sutskever" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Grid long short-term memory",
      "author" : [ "Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "Multiplicative lstm for sequence modelling",
      "author" : [ "Ben Krause", "Liang Lu", "Iain Murray", "Steve Renals" ],
      "venue" : "In ICLR Workshop,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2017
    }, {
      "title" : "A simple way to initialize recurrent networks of rectified linear units",
      "author" : [ "Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton" ],
      "venue" : "arXiv preprint arXiv:1504.00941,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "Backpropagation applied to handwritten zip code recognition",
      "author" : [ "Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1989
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1998
    }, {
      "title" : "Training rnns as fast as cnns",
      "author" : [ "Tao Lei", "Yu Zhang" ],
      "venue" : "arXiv preprint arXiv:1709.02755,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2017
    }, {
      "title" : "Cells in multidimensional recurrent neural networks",
      "author" : [ "Gundram Leifert", "Tobias Strauß", "Tobias Grüning", "Welf Wustlich", "Roger Labahn" ],
      "venue" : "JMLR, 17(1):3313–3349,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Fast-slow recurrent neural networks",
      "author" : [ "Asier Mujika", "Florian Meier", "Angelika Steger" ],
      "venue" : "In NIPS, 2017",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2017
    }, {
      "title" : "Tensorizing neural networks",
      "author" : [ "Alexander Novikov", "Dmitrii Podoprikhin", "Anton Osokin", "Dmitry P Vetrov" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2015
    }, {
      "title" : "Wavenet: A generative model for raw audio",
      "author" : [ "Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu" ],
      "venue" : "arXiv preprint arXiv:1609.03499,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2016
    }, {
      "title" : "Spatio-temporal video autoencoder with differentiable memory",
      "author" : [ "Viorica Patraucean", "Ankur Handa", "Roberto Cipolla" ],
      "venue" : "In ICLR Workshop,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2016
    }, {
      "title" : "Recurrent instance segmentation",
      "author" : [ "Bernardino Romera-Paredes", "Philip Hilaire Sean Torr" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2016
    }, {
      "title" : "Learning representations by backpropagating",
      "author" : [ "David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams" ],
      "venue" : "errors. Nature,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 1986
    }, {
      "title" : "Learning to control fast-weight memories: An alternative to dynamic recurrent networks",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 1992
    }, {
      "title" : "Parallel multi-dimensional lstm, with application to fast biomedical volumetric image segmentation",
      "author" : [ "Marijn F Stollenga", "Wonmin Byeon", "Marcus Liwicki", "Juergen Schmidhuber" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2015
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "Ilya Sutskever", "James Martens", "Geoffrey E Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2011
    }, {
      "title" : "Factored conditional restricted boltzmann machines for modeling motion style",
      "author" : [ "Graham W Taylor", "Geoffrey E Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2009
    }, {
      "title" : "Pixel recurrent neural networks",
      "author" : [ "Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu" ],
      "venue" : "In ICML, 2016",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2016
    }, {
      "title" : "Full-capacity unitary recurrent neural networks",
      "author" : [ "Scott Wisdom", "Thomas Powers", "John Hershey", "Jonathan Le Roux", "Les Atlas" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2016
    }, {
      "title" : "Deep recurrent convolutional networks for video-based person re-identification: An end-to-end approach",
      "author" : [ "Lin Wu", "Chunhua Shen", "Anton van den Hengel" ],
      "venue" : "arXiv preprint arXiv:1606.01609,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2016
    }, {
      "title" : "On multiplicative integration with recurrent neural networks",
      "author" : [ "Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2016
    }, {
      "title" : "Convolutional lstm network: A machine learning approach for precipitation nowcasting",
      "author" : [ "SHI Xingjian", "Zhourong Chen", "Hao Wang", "Dit-Yan Yeung", "Wai-kin Wong", "Wang-chun Woo" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2015
    }, {
      "title" : "Architectural complexity measures of recurrent neural networks",
      "author" : [ "Saizheng Zhang", "Yuhuai Wu", "Tong Che", "Zhouhan Lin", "Roland Memisevic", "Ruslan R Salakhutdinov", "Yoshua Bengio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2016
    }, {
      "title" : "Recurrent highway networks",
      "author" : [ "Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutník", "Jürgen Schmidhuber" ],
      "venue" : "In ICML, 2017",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "The Recurrent Neural Network (RNN) [17, 43] is a powerful model that learns how to use a hidden state vector ht ∈R to encapsulate the relevant features of the entire input history x1:t up to timestep t.",
      "startOffset" : 35,
      "endOffset" : 43
    }, {
      "referenceID" : 40,
      "context" : "The Recurrent Neural Network (RNN) [17, 43] is a powerful model that learns how to use a hidden state vector ht ∈R to encapsulate the relevant features of the entire input history x1:t up to timestep t.",
      "startOffset" : 35,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "However, this vanilla RNN has difficulties in modeling long-range dependencies due to the vanishing/exploding gradient problem [4].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 17,
      "context" : "Long Short-Term Memories (LSTMs) [19, 24] alleviate",
      "startOffset" : 33,
      "endOffset" : 41
    }, {
      "referenceID" : 21,
      "context" : "Long Short-Term Memories (LSTMs) [19, 24] alleviate",
      "startOffset" : 33,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "We consider the capacity of a network to consist of two components: the width (the amount of information handled in parallel) and the depth (the number of computation steps) [5].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 18,
      "context" : "To deepen the LSTM, the popular Stacked LSTM (sLSTM) stacks multiple LSTM layers [20]; however, runtime is proportional to the number of layers and information from the input is potentially lost (due to gradient vanishing/explosion) as it propagates vertically through the layers.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "A popular way to limit the parameter number when widening the network is to organize parameters as higher-dimensional tensors which can be factorized into lower-rank sub-tensors that contain significantly fewer elements [6, 15, 18, 26, 32, 39, 46, 47, 51], which is is known as tensor factorization.",
      "startOffset" : 220,
      "endOffset" : 255
    }, {
      "referenceID" : 13,
      "context" : "A popular way to limit the parameter number when widening the network is to organize parameters as higher-dimensional tensors which can be factorized into lower-rank sub-tensors that contain significantly fewer elements [6, 15, 18, 26, 32, 39, 46, 47, 51], which is is known as tensor factorization.",
      "startOffset" : 220,
      "endOffset" : 255
    }, {
      "referenceID" : 16,
      "context" : "A popular way to limit the parameter number when widening the network is to organize parameters as higher-dimensional tensors which can be factorized into lower-rank sub-tensors that contain significantly fewer elements [6, 15, 18, 26, 32, 39, 46, 47, 51], which is is known as tensor factorization.",
      "startOffset" : 220,
      "endOffset" : 255
    }, {
      "referenceID" : 23,
      "context" : "A popular way to limit the parameter number when widening the network is to organize parameters as higher-dimensional tensors which can be factorized into lower-rank sub-tensors that contain significantly fewer elements [6, 15, 18, 26, 32, 39, 46, 47, 51], which is is known as tensor factorization.",
      "startOffset" : 220,
      "endOffset" : 255
    }, {
      "referenceID" : 29,
      "context" : "A popular way to limit the parameter number when widening the network is to organize parameters as higher-dimensional tensors which can be factorized into lower-rank sub-tensors that contain significantly fewer elements [6, 15, 18, 26, 32, 39, 46, 47, 51], which is is known as tensor factorization.",
      "startOffset" : 220,
      "endOffset" : 255
    }, {
      "referenceID" : 36,
      "context" : "A popular way to limit the parameter number when widening the network is to organize parameters as higher-dimensional tensors which can be factorized into lower-rank sub-tensors that contain significantly fewer elements [6, 15, 18, 26, 32, 39, 46, 47, 51], which is is known as tensor factorization.",
      "startOffset" : 220,
      "endOffset" : 255
    }, {
      "referenceID" : 43,
      "context" : "A popular way to limit the parameter number when widening the network is to organize parameters as higher-dimensional tensors which can be factorized into lower-rank sub-tensors that contain significantly fewer elements [6, 15, 18, 26, 32, 39, 46, 47, 51], which is is known as tensor factorization.",
      "startOffset" : 220,
      "endOffset" : 255
    }, {
      "referenceID" : 44,
      "context" : "A popular way to limit the parameter number when widening the network is to organize parameters as higher-dimensional tensors which can be factorized into lower-rank sub-tensors that contain significantly fewer elements [6, 15, 18, 26, 32, 39, 46, 47, 51], which is is known as tensor factorization.",
      "startOffset" : 220,
      "endOffset" : 255
    }, {
      "referenceID" : 48,
      "context" : "A popular way to limit the parameter number when widening the network is to organize parameters as higher-dimensional tensors which can be factorized into lower-rank sub-tensors that contain significantly fewer elements [6, 15, 18, 26, 32, 39, 46, 47, 51], which is is known as tensor factorization.",
      "startOffset" : 220,
      "endOffset" : 255
    }, {
      "referenceID" : 31,
      "context" : "Another common way to reduce the parameter number is to share a small set of parameters across different locations in the hidden state, similar to Convolutional Neural Networks (CNNs) [34, 35].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 32,
      "context" : "Another common way to reduce the parameter number is to share a small set of parameters across different locations in the hidden state, similar to Convolutional Neural Networks (CNNs) [34, 35].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 0,
      "context" : "This is in fact a skewed sRNN as used in [1] (also similar to [48]).",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 45,
      "context" : "This is in fact a skewed sRNN as used in [1] (also similar to [48]).",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "1(c)), which can be beneficial for sRNNs [10].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 34,
      "context" : "Note that in (15) we employ a softmax function ς(·) to normalize the channel dimension ofQt, which, similar to [37], can stabilize the value of memory cells and help to prevent the vanishing/exploding gradients (see Appendix C for details).",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "The idea of dynamically generating network weights has been used in many works [6, 14, 15, 23, 44, 46], where in [14] locationdependent convolutional kernels are also dynamically generated to improve CNNs.",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "The idea of dynamically generating network weights has been used in many works [6, 14, 15, 23, 44, 46], where in [14] locationdependent convolutional kernels are also dynamically generated to improve CNNs.",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "The idea of dynamically generating network weights has been used in many works [6, 14, 15, 23, 44, 46], where in [14] locationdependent convolutional kernels are also dynamically generated to improve CNNs.",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 41,
      "context" : "The idea of dynamically generating network weights has been used in many works [6, 14, 15, 23, 44, 46], where in [14] locationdependent convolutional kernels are also dynamically generated to improve CNNs.",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 43,
      "context" : "The idea of dynamically generating network weights has been used in many works [6, 14, 15, 23, 44, 46], where in [14] locationdependent convolutional kernels are also dynamically generated to improve CNNs.",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "The idea of dynamically generating network weights has been used in many works [6, 14, 15, 23, 44, 46], where in [14] locationdependent convolutional kernels are also dynamically generated to improve CNNs.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : "We evaluate tLSTM on five challenging sequence learning tasks under different configurations: (a) sLSTM (baseline): our implementation of sLSTM [21] with parameters shared across all layers.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 22,
      "context" : "The Hutter Prize Wikipedia dataset [25] consists of 100 million characters taken from 205 different characters including alphabets, XML markups and special symbols.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 35,
      "context" : "1M parameters, and is competitive to the best performing methods [38, 54] with similar parameter numbers.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 51,
      "context" : "1M parameters, and is competitive to the best performing methods [38, 54] with similar parameter numbers.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 27,
      "context" : "We follow the processing of [30], where a symbol ‘-’ is used to delimit the integers as well as pad the input/target sequence.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 19,
      "context" : "Stacked LSTM [21] 51% 5M >50% 900K Grid LSTM [30] >99% 550K >99% 150K",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 27,
      "context" : "Stacked LSTM [21] 51% 5M >50% 900K Grid LSTM [30] >99% 550K >99% 150K",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 32,
      "context" : "The MNIST dataset [35] consists of 50000/10000/10000 handwritten digit images of size 28×28 for training/validation/test.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 30,
      "context" : "(a) Sequential MNIST: The goal is to classify the digit after sequentially reading the pixels in a scanline order [33].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "(b) Sequential Permuted MNIST: We permute the original image pixels in a fixed random order as in [2], resulting in a permuted MNIST (pMNIST) problem that has even longer range dependencies across pixels and is harder.",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "For sequential MNIST, our 3D tLSTM+CN with L=3 performs as well as the state-of-the-art Dilated GRU model [8], with a test accuracy of 99.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 37,
      "context" : "7% produced by the Dilated CNN [40] in [8].",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 6,
      "context" : "7% produced by the Dilated CNN [40] in [8].",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : ", it is of size P×P for 3D tLSTMs) of the memory cell at each timestep, and visualize the diagonal values of the channel mean from location p=[1, 1] (near the input) to p=[P, P ] (near the output).",
      "startOffset" : 142,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : ", it is of size P×P for 3D tLSTMs) of the memory cell at each timestep, and visualize the diagonal values of the channel mean from location p=[1, 1] (near the input) to p=[P, P ] (near the output).",
      "startOffset" : 142,
      "endOffset" : 148
    }, {
      "referenceID" : 45,
      "context" : "(a) A single layer cLSTM [48] with vector array input.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "(e) A 3-layer QRNN [7] with kernel size 2, where costly computations are done by temporal convolution.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 45,
      "context" : ", a vector array [48], a vector matrix [41, 42, 50, 52], or a vector tensor [9, 45].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 38,
      "context" : ", a vector array [48], a vector matrix [41, 42, 50, 52], or a vector tensor [9, 45].",
      "startOffset" : 39,
      "endOffset" : 55
    }, {
      "referenceID" : 39,
      "context" : ", a vector array [48], a vector matrix [41, 42, 50, 52], or a vector tensor [9, 45].",
      "startOffset" : 39,
      "endOffset" : 55
    }, {
      "referenceID" : 47,
      "context" : ", a vector array [48], a vector matrix [41, 42, 50, 52], or a vector tensor [9, 45].",
      "startOffset" : 39,
      "endOffset" : 55
    }, {
      "referenceID" : 49,
      "context" : ", a vector array [48], a vector matrix [41, 42, 50, 52], or a vector tensor [9, 45].",
      "startOffset" : 39,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : ", a vector array [48], a vector matrix [41, 42, 50, 52], or a vector tensor [9, 45].",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 42,
      "context" : ", a vector array [48], a vector matrix [41, 42, 50, 52], or a vector tensor [9, 45].",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "To keep the parameter number small and ease training, Graves [22], Kalchbrenner et al.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 51,
      "context" : "[54] apply another RNN/LSTM along the depth direction of dLSTMs, which, however, multiplies the runtime.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Though there are implementations to accelerate the deep computation [1, 16], they generally aim at simple architectures such sLSTMs.",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "Though there are implementations to accelerate the deep computation [1, 16], they generally aim at simple architectures such sLSTMs.",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 5,
      "context" : "Some methods [7, 8, 28, 29, 36, 40] parallelize the temporal computation of the sequence (e.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 6,
      "context" : "Some methods [7, 8, 28, 29, 36, 40] parallelize the temporal computation of the sequence (e.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : "Some methods [7, 8, 28, 29, 36, 40] parallelize the temporal computation of the sequence (e.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 26,
      "context" : "Some methods [7, 8, 28, 29, 36, 40] parallelize the temporal computation of the sequence (e.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 33,
      "context" : "Some methods [7, 8, 28, 29, 36, 40] parallelize the temporal computation of the sequence (e.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 37,
      "context" : "Some methods [7, 8, 28, 29, 36, 40] parallelize the temporal computation of the sequence (e.",
      "startOffset" : 13,
      "endOffset" : 35
    } ],
    "year" : 2017,
    "abstractText" : "Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, usually the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensors and updated via a cross-layer convolution. By increasing the tensor size, the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor; by delaying the output, the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. Experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.",
    "creator" : null
  }
}