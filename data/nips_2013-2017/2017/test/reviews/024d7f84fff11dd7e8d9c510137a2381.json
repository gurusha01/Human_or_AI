{"title": "Accelerated consensus via Min-Sum Splitting", "abstract": "We apply the Min-Sum message-passing protocol to solve the consensus problem in distributed optimization. We show that while the ordinary Min-Sum algorithm does not converge, a modified version of it known as Splitting yields convergence to the problem solution. We prove that a proper choice of the tuning parameters allows Min-Sum Splitting to yield subdiffusive accelerated convergence rates, matching the rates obtained by shift-register methods. The acceleration scheme embodied by Min-Sum Splitting for the consensus problem bears similarities with lifted Markov chains techniques and with multi-step first order methods in convex optimization.", "id": "024d7f84fff11dd7e8d9c510137a2381", "authors": ["Patrick Rebeschini", "Sekhar C. Tatikonda"], "conference": "NIPS2017", "accepted": true, "reviews": [{"comments": "This paper applies an accelerated variant of the min-sum algorithm, called min-sum splitting, to the distributed consensus problem. The paper is very well written, with the contribution clearly placed in the context of the state of the art in the topic. To the best of my knowledge (although I am not an expert on the topic), the results are novel and constitute a qualitative advance. In particular, the paper presents a novel connection between min-sum algorithms and lifted Markov chain techniques.\n\nThere is a detail which is not clear in the presentation. In page 4, when describing the equivalent objective function that is minimized by the min-sum algorithm to yield the min-sum splitting scheme, the authors write: \"...splitting each term $\\phi_{vw}$ into $\\Gamma_{vw}$ terms, and each term $\\phi_v$ into $\\delta$ terms,...\" However, it is not clear what this means, since $\\delta$ and $\\Gamma_{vw}$, as introduced on the previous page are real numbers.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper studies the convergence rate of a so-called min-sum splitting method on the average consensus problem. In general he paper reads fine but the improvement of the result seems not impressive. Detailed comments are as follows.\n\n(1) It writes that ``This rate is optimal for graphs with good expansion properties, such as the complete graph. In this case the convergence time, i.e., the number of iterations required to reach a prescribed level of error accuracy in the\u2026 of the dimension of the problem, as\u2026\u2019\u2019. \nFor complete graphs, the linear rate is 0 because everyone converges to the average in 1 step. Also complete graphs are too special to be representative.\nSo for which general category of graphs the complexity does not depend on the dimension (number of nodes)? Which general category of graphs is considered as good?\n\n(2) In this paragraph (same as comment 1), the literature review should include \u2018\u2019Linear Time Average Consensus on Fixed Graphs and Implications for Decentralized Optimization and Multi-Agent Control\u2019\u2019 by Olshevsky. Its convergence rate should be reported properly (more explanation will be given in comment 8). The reference mentioned here has reached a rather competitive or ever better bound compared the result of the submission. \n\n(3) At the top of page 2, for consensus optimization, important references like \n``On the\u00a0Linear Convergence\u00a0of the\u00a0ADMM\u00a0in Decentralized Consensus Optimization\u2019\u2019 by Shi, Ling, Kun, Wu, and Yin,\n``Optimal algorithms for smooth and strongly convex distributed optimization in networks\u2019\u2019 by Scaman, Bach, Bubeck, Lee, Massouli\u00e9\nshould be cited. Also the authors should report the state-of-the-art algorithms for consensus optimization and their corresponding (linear) convergence rates. \n\n(4) When discussing lifted graph and Markov chain, this paper ignored a very related paper ``Markov Chain Lifting and Distributed ADMM\u2019\u2019 by Franca and Bento.\n\n(5) The content of the the last paragraph of page 5 is a long known fact. Should refer to ``Generalized consensus computation in networked systems with erasure links\u2019\u2019 by Rabbat, Nowak, and Bucklew. In the sequel, the connection between those variants and Heavy ball/Nesterov/Polyak is known to the field.\n\n(6) There are many important references regarding consensus optimization the authors have ignored. For example, \n``Extra: An\u00a0exact first-order\u00a0algorithm for decentralized consensus optimization\u2019\u2019 by Shi, Ling, Wu, and Yin.\n``Fast distributed gradient methods\u2019\u2019 by Jakovetic,\u00a0J Xavier, and Moura.\n\n(7) Proposition 3 seems to be trivial and is a supplementary contribution. \n\n(8) The rate has reached by this paper, D log(D/eps), does not seem to have a significant improvement on the rate D log(1/eps) that has been reached by Linear Time Average Consensus on Fixed Graphs and Implications for Decentralized Optimization and Multi-Agent Control (see comment 2). Especially in the worst case scenario (holds for all graphs), D~n, the bound is even worse than that has been achieved in ``Linear Time Average Consensus\u2026.\u2019\u2019.\n\n(9) The paper``Linear Time Average Consensus\u2026\u2019\u2019 improves the bound through Nesterov\u2019s acceleration. The reviewer suspects that the so-called ``Auxiliary message-passing scheme\u2019\u2019 proposed by the authors is again a Nestov\u2019s acceleration applied to min-sum algorithm. This is fine but the analysis is done for consensus which boils down to analyzing a linear system and is supposed to be not hard. The contribution of the paper becomes not clear given such situation.\n\n(10) The tiny improvement may come from a careful handle on the spectral gap of graphs. Eventually the worst case bound is still O(n) because O(n)=O(D) for the set of all graphs with n nodes.\n\n(11) Line 243 of page 6. The graph is simple but the author is using directed edges. This is confusing.\n\n(12) Typo at line 220 of page 6. Laplacian\u2014> Lagrangian.\n\nAfter rebuttal:\n\nThe reviewer is satisfied with the authors' response. But the evaluation score from this reviewer stays the same.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In this paper, the authors present an accelerated variant of the Min-Sum message-passing protocol for solving consensus problems in distributed optimization. The authors use the reparametrization techniques proposed in [Ruozzi and Tatikonda, 2013] and establish rates of convergence for the Min-Sum Splitting algorithm for solving consensus problems with quadratic objective functions. The main tool used for the analysis is the construction of an auxiliary linear process that tracks the evolution of the Min-Sum Splitting algorithm.\n\nThe main contributions of the paper can be summarized as follows: (i) provide analysis for the Min-Sum splitting algorithm using a new proof technique based on the introduction of an auxiliary process, (ii) design a Min-Sum protocol for consensus problems that achieves better convergence than previously established results, and (iii) show the connection between the proposed method, and lifted Markov chains and multi-step methods in convex optimization.\n\nThe motivation and contributions of the paper are clear. The paper is well written and easy to follow, however, it does contain several typos and grammatical mistakes (listed below). The proofs of Propositions 1 and 2, and Theorem 1 appear to be correct.\n\nTypos and Grammatical errors:\n- Line 34: \u201c\u2026with theirs neighbors\u2026\u201d -> \u201c\u2026with their neighbors\u2026\u201d\n- Line 174: \u201cdouble-stochastic\u201d -> \u201cdoubly-stochastic\u201d\n- Line 183: \u201c\u2026 can be casted as\u2026\u201d -> \u201c\u2026 can be cast as\u2026\u201d\n- Line 192: \u201c\u2026class of graph with\u2026\u201d -> \u201c\u2026class of graphs with\u2026\u201d\n- Line 197: \u201c\u2026which seems to\u2026\u201d -> \u201c\u2026which seem to\u2026\u201d\n- Line 206: \u201c\u2026additional overheads\u2026\u201d -> \u201c\u2026additional overhead\u2026\u201d\n- Line 225: \u201c\u2026pugging\u2026\u201d -> \u201c\u2026plugging\u2026\u201d\n- Line 238: \u201c\u2026are seen to\u2026\u201d -> \u201c\u2026are able to\u2026\u201d\n- Line 240: \u201c\u2026both type of\u2026\u201d -> \u201c\u2026both types of\u2026\u201d\n- Line 248: \u201c\u2026also seen to\u2026\u201d -> \u201c\u2026also shown to\u2026\u201d\n- Line 279-280: \u201c\u2026to convergence to\u2026\u201d -> \u201c\u2026to converge to\u2026\u201d\n- Line 300: \u201c\u2026,which scales like\u2026\u201d -> \u201c\u2026,which scale like\u2026\u201d\n- Line 302: \u201c\u2026for the cycle,\u2026\u201d -> \u201c\u2026for cycle graphs,\u2026\u201d\n\nOther minor comments:\n- Lines 220 and 221: Do you mean \u201cLagrangian\u201d and \u201cLagrange multipliers\u201d instead of \u201cLaplacian\u201d and \u201cLaplace multipliers\u201d?\n- The authors present 3 algorithms, and the quantities involved are not always explained or described. For example, what is R_{vw} and r_{vw} in Algorithm 2? Also, in Algorithm 2, the quantities \\hat{R}^0 and \\hat{r}^0 do not appear to be initialized. Moreover, since the auxiliary linear process is key to the analysis and the central idea of the paper, the authors show clearly state which variables correspond to this in Algorithm 3.\n\nThe paper also appears to be missing several references. More specifically:\n- Lines 41 and 43: (Sub)gradient methods for consensus optimization. There are several more references that could be included:\n-- Bertsekas and Tsitsiklis, Parallel and distributed computation: numerical methods, 1989\n-- Sundhar Ram Srinivasan et. al., Incremental stochastic subgradient algorithms for convex optimization, 2009\n-- Wei Shi, Extra: An exact first-order algorithm for decentralized consensus optimization, 2015\n(and, of course, many more)\n- Line 170: \u201cThe original literature\u2026\u201d\n- Line 229: work by Polyak (Heavy-ball)\n- Line 232: work by Nesterov\n\nIt would be interesting and useful if the authors could answer/comment and address in the paper the following:\n- Although the paper is a theoretical paper, the authors should comment on the practicality of the method, and when such a method should be used as opposed to other distributed methods for consensus optimization. \n- What are the limitations of the Min-Sum Splitting method? \n- What is the intuition behind using the auxiliary process in the Min-Sum Splitting method?\n- The results provided in this paper are for consensus problems with quadratic objective functions. Can this framework be extended to solve more general consensus problems that often arise in Machine Learning? \n- The authors should also clearly state why such an approach is of interest in the context of Machine Learning and for the Machine Learning community.\n\nIn summary, this paper is a purely theoretical paper in which the authors establish rates of convergence using a new proof technique and show the connections between their method and well-established methods in the literature. Overall, the ideas presented in this paper are interesting, however, the practicality of the method and intuition behind the results are missing, as well as some justification for the importance of this result for the Machine Learning community.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
