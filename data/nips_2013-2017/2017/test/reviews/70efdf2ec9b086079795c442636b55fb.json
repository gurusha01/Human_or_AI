{"title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence Learning", "abstract": "Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, usually the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensors and updated via a cross-layer convolution. By increasing the tensor size, the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor; by delaying the output, the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. Experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.", "id": "70efdf2ec9b086079795c442636b55fb", "authors": ["Zhen He", "Shaobing Gao", "Liang Xiao", "Daxue Liu", "Hangen He", "David Barber"], "conference": "NIPS2017", "accepted": true, "reviews": [{"comments": "The paper explores a new way to share parameters in an RNN by using convolution inside an LSTM cell for an unstructured input sequence, and using tensors as convolution kernels and feature maps. The method also adds depth to the model by delaying the output target for a specified number of steps.\n\nThe idea is quite interesting, and is novel as far as I can tell. The authors provide clear formulation (although a rather complicated one) and provide some experimental results. \nOn a real world dataset (wikipedia LM) the method seems very close to SOA, with about half the parameters.\n\nThe problems I see with this approach are:\n- I find it hard to believe that meaningful high dimensional feature maps can be created for most problems, thus scaling to high dimensional tensors is questionable (The authors only try up to dimension 3)\n- Using \u201cdepth in time\u201d introduces a delay and is not suitable for streaming applications (e.g. speech)\n- For high dimensional tensors the number of hyper parameters can become quite large.\n\nMinor nit:\n- Line 242, it says \u201cFig.3\u201d and should be \u201cTable 3\u201d", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper introduces a tensorized version of LSTM that allows for implicitly adding depth and width to the network while controlling the computational runtime.\nThe paper is clearly written, the contribution is interesting and the experimental validation is convincing.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes Tensorized LSTMs for efficient sequence learning. It represents hidden layers as tensors, and employs cross-layer memory cell convolution for efficiency and effectiveness. The model is clearly formulated. Experimental results show the utility of the proposed method.\n\nAlthough the paper is well written, I still have some questions/confusion as follows. I would re-consider my final decision if the authors address these points in rebuttal.\n\n1. My biggest confusion comes from Sec 2.1, when the authors describe how to widen the network with convolution (lines 65-73). As mentioned in text, \"P is akin to the number of stacked hidden layers\", and the model \"locally-connects\" along the P direction to share parameters. I think it is a strategy to deepen the network instead of widening it, since increasing P (the number of hidden layers) won't incur more parameters in the convolution. Similarly, as mentioned in lines 103-104, tRNN can be \"widened without additional parameters by increasing the tensor size P\". It does not make sense, as increasing P is conceptually equivalent to increasing the number of hidden layers in sRNN. This is to deepen the network, not to widen it.\n\n2. The authors claim to deepen the network with delayed outputs (Sec 2.2). They use the parameter L to set up the network depth. However, as shown in Eq. 9, L is determined by P and K, meaning that we can not really set up the network depth as a free parameter. I guess in practice, we would always pre-set P and K before experiments, and then derive L from Eq. 9. It seems over-claimed in lines 6-10, which reads like \"we could freely set up the width and depth of the network\".\n\n3. The authors claims that the proposed memory cell convolution is able to prevent gradient vanishing/exploding (line 36). This is not verified theoretically or empirically. The words \"gradient vanishing/exploding\" are even not mentioned in the later text.\n\n4. In the experiments, the authors compared tLSTM variants in the following dimentions: tensor shape (2D or 3D), normalization (no normalization, LN, CN), memory cell convolution (yes or no), and feedback connections (yes or no). There are 2x3x2x2=24 combinations in total. Why just pick up the six combinations in lines 166-171? I understand it become messy when comparing too many methods, but there are some interesting variants like 2D tLSTM+CN. Also, it might help to split the experiments in groups, like one for normalization strategy, one for memory cell convolution, one for feedback connections, etc.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
