{"title": "Multi-Task Learning for Contextual Bandits", "abstract": "Contextual bandits are a form of multi-armed bandit in which the agent has access to predictive side information (known as the context) for each arm at each time step, and have been used to model personalized news recommendation, ad placement, and other applications. In this work, we propose a multi-task learning framework for contextual bandit problems. Like multi-task learning in the batch setting, the goal is to leverage similarities in contexts for different arms so as to improve the agent's ability to predict rewards from contexts. We propose an upper confidence bound-based multi-task learning algorithm for contextual bandits, establish a corresponding regret bound, and interpret this bound to quantify the advantages of learning in the presence of high task (arm) similarity. We also describe an effective scheme for estimating task similarity from data, and demonstrate our algorithm's performance on several data sets.", "id": "b06f50d1f89bd8b2a0fb771c1a69c2b0", "authors": ["Aniket Anand Deshmukh", "Urun Dogan", "Clay Scott"], "conference": "NIPS2017", "accepted": true, "reviews": [{"comments": "Summary.\n\nThe paper is about contextual bandits with N arms. In each round the learner observes a context x_{ti} for each arm, chooses\nan arm to pull and receives reward r_{ti}.\n\nThe question is what structure to impose on the rewards. The authors note that\n\nE[r_{ti}] = < x_{ti}, theta > is a common choice, as is \nE[r_{ti}] = < x_{ti}, theta_i >\n\nThe former allows for faster learning, but has less capacity while the latter has more capacity and slower learning. The natural question\naddressed in this paper concerns the middle ground, which is simultaneously generalized by kernelization.\n\nThe main idea is to augment the context space so the learner observes (z_{ti}, x_{ti}) where z_{ti} lies in some other space Z.\nThen a kernel can be defined on this augmented space that measures similarity between contexts and determines the degree of sharing\nbetween the arms.\n\nContribution.\n\nThe main contribution as far as I can tell is the idea to augment the context space in this way. The regret analysis\nemploys the usual techniques.\n\nNovelty. \n\nThere is something new here, but not much. Unless I am somehow mistaken the analysis of Valko et al. should apply directly to the augmented\ncontexts with more-or-less the same guarantees. So the new idea is really the augmentation. \n\nImpact.\n\nIt's hard to tell what will be the impact of this paper. From a theoretical perspective there is not much new. The practical experiments\nare definitely appreciated, but not overwhelming. Eg., what about linear Thompson sampling? \n\nCorrectness. \n\nI only skimmed the proofs in the supplementary material, but the bound passes plausibility tests.\n\nOverall.\n\nThis seems like a borderline paper to me. I would increase my score if the authors can argue convincingly that the theoretical results are really doing\nmore than the analysis in the cited paper of Valko et al.\n\nOther comments.\n\n- On L186 you remark that under \"further assumption that after time t, n_{a,t} = t/N\". But this assumption is completely unjustified, so how meaningful\n are conclusions drawn from it?\n\n- It's a pity that the analysis was not possible for Algorithm 1. I presume the sup-\"blah blah\" algorithms don't really work? It could be useful to \n show their regret relative to Algorithm 1 in one of the figures. Someone needs to address these issues at some point (but I know this is a tricky problem).\n\n\nMinors.\n\n- I would capitalize A_t and other random variables.\n- L49: \"one estimate\" -> \"one estimates\".\n- L98: \"z_a\" -> \"z_a \\in \\mathcal Z\".\n- L110: The notation t_a as a set that depends on t and a is just odd.\n- L114: Why all the primes?\n- There must be some noise assumption on the rewards (or bounded, or whatever). Did I miss it?\n- L135: augmented context here is also (a, x_a).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper introduces a multitask bandit learning approach. It takes after two existing contributions: Valko et al. 2013 on kernelised contextual bandits, Evgueniou and Pontil, 2004 on regularized multitask learning. The authors of the present paper provide a way to estimate the similarities between the tasks if it is not given, which is essential for real-world data.\n\nPros of the paper:\n- the problem of learning multitask contextual bandits is of importance for many practical problems (e.g. recommendation);\n- the mathematical anaylsis, as far as I have checked, is correct;\n- results from numerical simulations are convincing.\n\nCons:\n- I would point out that the paper provides an incremental contribution and/or that the novelty is not well sold. For instance, it seems a lot of the work provided here is similar to the work of Valko et al 2013. What if that work is implemented with multitask kernels ? Would the resulting algorithm be very different from that proposed in the present paper ?\n- there is the question of the computational complexity induced by the growing kernel matrix K_{t-1}: something should be said here.\n- there is the frustrating proof fo the regret that mentions two algorithms SupKMTL-UCB and BaseKMTL-UCB that are only given in the supplementary material: the authors should at least provide the main lines of these algorithms in the main text. Otherwise, Theorem 4.1 cannot be understood.\n\nAll in all, the paper addresses an interesting problem. However, there is some drawbacks regarding 1) the incrementality of the contribution, 2) some algorithmic points (e.g. growing kernel matrix) and 3) the presentation of Theorem 4.1.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
