{"title": "Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery", "abstract": "In machine learning and compressed sensing, it is of central importance to understand when a tractable algorithm recovers the support of a sparse signal from its compressed measurements. In this paper, we present a principled analysis on the support recovery performance for a family of hard thresholding algorithms. To this end, we appeal to the partial hard thresholding (PHT) operator proposed recently by Jain et al. [IEEE Trans. Information Theory, 2017]. We show that under proper conditions, PHT recovers an arbitrary \"s\"-sparse signal within O(s kappa log kappa) iterations where \"kappa\" is an appropriate condition number. Specifying the PHT operator, we obtain the best known result for hard thresholding pursuit and orthogonal matching pursuit with replacement. Experiments on the simulated data complement our theoretical findings and also illustrate the effectiveness of PHT compared to other popular recovery methods.", "id": "4a2ddf148c5a9c42151a529e8cbdcc06", "authors": ["Jie Shen", "Ping Li"], "conference": "NIPS2017", "accepted": true, "reviews": [{"comments": "Summary. This paper studies support recovery of the partial hard thresholding algorithm, and have derived, under certain conditions, the iteration complexity of the partial hard thresholding algorithm.\n\nQuality. There seems no explicit description about the condition under which the PHT(r) algorithm terminates, which makes the statement of Proposition 1 difficult to understand.\nIt would have been better if concrete arguments on specific problem examples were given. They are presented only in a very brief and abstract manner in the last paragraph of Section 2. In particular, if nonzeros in \\bar{x} are distributed according to a distribution without discontinuity at x=0, then x_min should scale as O(1/n), so that it should become difficult to satisfy the x_min condition when n is large.\n\nCOMMENT AFTER REBUTTAL: In the above, I should have written that x_min should scale as O(1/d). I am pretty sorry for my careless mistake.\n\nClarity. I think that the description of the simulations is so brief that one cannot relate the simulation setups with the theoretical results. More concretely, the function F(x) adopted in the simulations should be explicitly stated, as well as the condition numbers and other parameters appearing in the theoretical results. Of interest also would be whether the 10,000 iterations are sufficient with regard to the theoretical guarantees. Also, nonzeros of the signals are generated as Gaussians, so that x_min values vary from trial to trial, as well as the s values specified.\nLemma 4 and Theorem 5 in the main text appear in the supplementary material as Lemma 19 and Theorem 20 without explicit statement that they are the same.\n\nOriginality. I think that this work would be moderately original, in that it seems that it has extended the existing arguments on support recovery via hard thresholding to partial hard thresholding, which would certainly be non-trivial.\n\nSignificance. Since the partial hard thresholding includes the conventional hard thresholding as well as what is called the orthogonal matching pursuit with replacement as special cases, the theoretical support-recovery guarantees for the partial hard thresholding algorithm presented here should be of significance. \n\nMinor points:\n\nLine 16: ha(ve -> s) found\nLine 56: in turn indicate(s)\nLine 76: (Lowercase -> A lowercase) letter(s)\nLine 132: restricted (strongly) smooth\nLine 196: The(n) the support\nLine 287: i.i.d. (standard) normal variables.\nLine 304: significantly reduce(s)", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Review prior to rebuttal:\n\nThe paper provides analytical results on partial hard thresholding for sparse signal recovery that are based on RIP and on an alternative condition (RSC and RSS) and that is applicable to arbitrary signals.\n\nThe description of previous results is confusing at times. First, [4] has more generic results than indicated in this paper in line 51. Remark 7 provides a result for arbitrary signals, while Theorems 5 and 6 consider all sufficiently sparse signals. Second, [13] provides a guarantee for sparse signal recovery (Theorem 4), and so it is not clear what the authors mean by \u201cparameter estimation only\u201d on line 59. If exact recovery is achieved, it is obvious that the support is estimated correctly too. Third, it is also not clear why Theorem 2 is described as an RIP-based guarantee - how does RIP connect to the condition number, in particular since RIP is a property that can go beyond specific matrix constructions?\n\nThe simulation restricts itself to s-sparse signals, and so it is not clear that the authors are testing the aspects of the results that go beyond those available in existing work. It is also not clear how the results test the dependence on the condition number of the matrix.\n\nMinor comments follow.\nTitle: \u201cA Towards\u201d is not grammatically correct.\nLine 128: the names of the properties are grammatically incorrect: convex -> convexity, smooth -> smoothness? or add \u201cproperty\u201d at the end of each?\nLine 136: calling M/m \u201cthe condition number of the problem\u201d can be confusing, since this terminology is already used in linear algebra. Is there a relationship between these two quantities?\nLine 140: there is no termination condition given in the algorithm. Does this mean when the gradient in the first step is equal to zero? When the support St does not change in consecutive iterations?\nLine 185: the paper refers to a comparison of the analysis of [4] being \u201cconfined to a special signal\u201d versus the manuscript\u2019s \u201cgeneralization\u2026 [to] a family of algorithms\u201d. It is not clear how these two things compare to one another.\nLine 301: degrade -> degradation\nLine 304: reduces -> reduce\n\nResponse to rebuttal: The authors have addressed some of the points above (in particular relevant to the description of prior work), but the impact of the contribution appears to be \"borderline\" for acceptance. Given that the paper is acceptable if the authors implement the changes described in the rebuttal, I have set my score to \"marginally above threshold\".", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper analyzes the support recovery performance of the PHT algrorithm of Jain et al. While in Jain et al.'s paper the main focus (at least with respect to the PHT family) was estimation error, the current paper aims to extend this to support recovery performance. \n\nThe technical contribution of the paper appears sound to me. However, its not entirely clear how different the techniques are to those used in [4]. I am willing to revise my rating if the authors can make this more clear. \n\nThe presentation of the paper needs improvement. \n- There are several typos throughout. A thorough pass needs to be made. \n- The title of the paper is grammatically incorrect -- perhaps the authors meant \"Towards a Unified Analysis of Support Recovery\". This phrase \"a towards unified analysis\" is used in other places in the manuscript, and its incorrect. \n- In the abstract the authors use \\kappa and say that it is THE condition number, while it is unclear for a while which condition number it is. This should be replaced by something like \"where \\kappa is an approporiate condition number\", since the authors probably dont want to introduce the formal definition in the abstract.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
