{
  "name" : "7fe1f8abaad094e0b5cb1b01d712f708.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Safe and Nested Subgame Solving for Imperfect-Information Games",
    "authors" : [ "Noam Brown", "Tuomas Sandholm" ],
    "emails" : [ "noamb@cs.cmu.edu", "sandholm@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Imperfect-information games model strategic settings that have hidden information. They have a myriad of applications including negotiation, auctions, cybersecurity, and physical security.\nIn perfect-information games, determining the optimal strategy at a decision point only requires knowledge of the game tree’s current node and the remaining game tree beyond that node (the subgame rooted at that node). This fact has been leveraged by nearly every AI for perfect-information games, including AIs that defeated top humans in chess [7] and Go [29]. In checkers, the ability to decompose the game into smaller independent subgames was even used to solve the entire game [27]. However, it is not possible to determine a subgame’s optimal strategy in an imperfect-information game using only knowledge of that subgame, because the game tree’s exact node is typically unknown. Instead, the optimal strategy may depend on the value an opponent could have received in some other, unreached subgame. Although this is counter-intuitive, we provide a demonstration in Section 2.\nRather than rely on subgame decomposition, past approaches for imperfect-information games typically solved the game as a whole upfront. For example, heads-up limit Texas hold’em, a relatively simple form of poker with 1013 decision points, was essentially solved without decomposition [2]. However, this approach cannot extend to larger games, such as heads-up no-limit Texas hold’em—the primary benchmark in imperfect-information game solving—which has 10161 decision points [16].\nThe standard approach to computing strategies in such large games is to first generate an abstraction of the game, which is a smaller version of the game that retains as much as possible the strategic characteristics of the original game [24, 26, 25]. For example, a continuous action space might be discretized. This abstract game is solved and its solution is used when playing the full game by mapping states in the full game to states in the abstract game. We refer to the solution of an abstraction (or more generally any approximate solution to a game) as a blueprint strategy.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nIn heavily abstracted games, a blueprint strategy may be far from the true solution. Subgame solving attempts to improve upon the blueprint strategy by solving in real time a more fine-grained abstraction for an encountered subgame, while fitting its solution within the overarching blueprint strategy."
    }, {
      "heading" : "2 Coin Toss",
      "text" : "In this section we provide intuition for why an imperfect-information subgame cannot be solved in isolation. We demonstrate this in a simple game we call Coin Toss, shown in Figure 1a, which will be used as a running example throughout the paper.\nCoin Toss is played between players P1 and P2. The figure shows rewards only for P1; P2 always receives the negation of P1’s reward. A coin is flipped and lands either Heads or Tails with equal probability, but only P1 sees the outcome. P1 then chooses between actions “Sell” and “Play.” The Sell action leads to a subgame whose details are not important, but the expected value (EV) of choosing the Sell action will be important. (For simplicity, one can equivalently assume in this section that Sell leads to an immediate terminal reward, where the value depends on whether the coin landed Heads or Tails). If the coin lands Heads, it is considered lucky and P1 receives an EV of $0.50 for choosing Sell. On the other hand, if the coin lands Tails, it is considered unlucky and P1 receives an EV of −$0.50 for action Sell. (That is, P1 must on average pay $0.50 to get rid of the coin). If P1 instead chooses Play, then P2 may guess how the coin landed. If P2 guesses correctly, then P1 receives a reward of −$1. If P2 guesses incorrectly, then P1 receives $1. P2 may also forfeit, which should never be chosen but will be relevant in later sections. We wish to determine the optimal strategy for P2 in the subgame S that occurs after P1 chooses Play, shown in Figure 1a.\nWere P2 to always guess Heads, P1 would receive $0.50 for choosing Sell when the coin lands Heads, and $1 for Play when it lands Tails. This would result in an average of $0.75 for P1. Alternatively, were P2 to always guess Tails, P1 would receive $1 for choosing Play when the coin lands Heads, and−$0.50 for choosing Sell when it lands Tails. This would result in an average reward of $0.25 for P1. However, P2 would do even better by guessing Heads with 25% probability and Tails with 75% probability. In that case, P1 could only receive $0.50 (on average) by choosing Play when the coin lands Heads—the same value received for choosing Sell. Similarly, P1 could only receive −$0.50 by choosing Play when the coin lands Tails, which is the same value received for choosing Sell. This would yield an average reward of $0 for P1. It is easy to see that this is the best P2 can do, because P1 can average $0 by always choosing Sell. Therefore, choosing Heads with 25% probability and Tails with 75% probability is an optimal strategy for P2 in the “Play” subgame.\nNow suppose the coin is considered lucky if it lands Tails and unlucky if it lands Heads. That is, the expected reward for selling the coin when it lands Heads is now −$0.50 and when it lands Tails is now $0.50. It is easy to see that P2’s optimal strategy for the “Play” subgame is now to guess Heads with 75% probability and Tails with 25% probability. This shows that a player’s optimal strategy in a subgame can depend on the strategies and outcomes in other parts of the game. Thus, one cannot solve a subgame using information about that subgame alone. This is the central challenge of imperfect-information games as opposed to perfect-information games."
    }, {
      "heading" : "3 Notation and Background",
      "text" : "In a two-player zero-sum extensive-form game there are two players, P = {1, 2}. H is the set of all possible nodes, represented as a sequence of actions. A(h) is the actions available in a node and P (h) ∈ P ∪ c is the player who acts at that node, where c denotes chance. Chance plays an action a ∈ A(h) with a fixed probability. If action a ∈ A(h) leads from h to h′, then we write h · a = h′. If a sequence of actions leads from h to h′, then we write h @ h′. The set of nodes Z ⊆ H are terminal nodes. For each player i ∈ P , there is a payoff function ui : Z → < where u1 = −u2. Imperfect information is represented by information sets (infosets). Every node h ∈ H belongs to exactly one infoset for each player. For any infoset Ii, nodes h, h′ ∈ Ii are indistinguishable to player i. Thus the same player must act at all the nodes in an infoset, and the same actions must be available. Let P (Ii) and A(Ii) be such that all h ∈ Ii, P (Ii) = P (h) and A(Ii) = A(h). A strategy σi(Ii) is a probability vector over A(Ii) for infosets where P (Ii) = i. The probability of action a is denoted by σi(Ii, a). For all h ∈ Ii, σi(h) = σi(Ii). A full-game strategy σi ∈ Σi defines a strategy for each player i infoset. A strategy profile σ is a tuple of strategies, one for each player. The expected payoff for player i if all players play the strategy profile 〈σi, σ−i〉 is ui(σi, σ−i), where σ−i denotes the strategies in σ of all players other than i.\nLet πσ(h) = ∏ h′·avh σP (h′)(h\n′, a) denote the probability of reaching h if all players play according to σ. πσi (h) is the contribution of player i to this probability (that is, the probability of reaching h if chance and all players other than i always chose actions leading to h). πσ−i(h) is the contribution of all players, and chance, other than i. πσ(h, h′) is the probability of reaching h′ given that h has been reached, and 0 if h 6@ h′. This papers focuses on perfect-recall games, where a player never forgets past information. Thus, for every Ii, ∀h, h′ ∈ Ii, πσi (h) = πσi (h′). We define πσi (Ii) = πσi (h) for h ∈ Ii. Also, I ′i @ Ii if for some h′ ∈ I ′i and some h ∈ Ii, h′ @ h. Similarly, I ′i · a @ Ii if h′ · a @ h. A Nash equilibrium [22] is a strategy profile σ∗ where no player can improve by shifting to a different strategy, so σ∗ satisfies ∀i, ui(σ∗i , σ∗−i) = maxσ′i∈Σi ui(σ ′ i, σ ∗ −i). A best response BR(σ−i) is a strategy for player i that is optimal against σ−i. Formally, BR(σ−i) satisfies ui(BR(σ−i), σ−i) = maxσ′i∈Σi ui(σ ′ i, σ−i). In a two-player zero-sum game, the exploitability exp(σi) of a strategy σi is how much worse σi does against an opponent best response than a Nash equilibrium strategy would do. Formally, exploitability of σi is ui(σ∗)− ui(σi, BR(σi)), where σ∗ is a Nash equilibrium.\nThe expected value of a node h when players play according to σ is vσi (h) = ∑ z∈Z ( πσ(h, z)ui(z) ) . An infoset’s value is the weighted average of the values of the nodes in the infoset, where a node\nis weighed by the player’s belief that she is in that node. Formally, vσi (Ii) = ∑ h∈Ii\n( πσ−i(h)v σ i (h) ) ∑ h∈Ii πσ−i(h)\nand vσi (Ii, a) = ∑ h∈Ii\n( πσ−i(h)v σ i (h·a) ) ∑ h∈Ii πσ−i(h) . A counterfactual best response [21] CBR(σ−i) is a best\nresponse that also maximizes value in unreached infosets. Specifically, a counterfactual best response is a best response σi with the additional condition that if σi(Ii, a) > 0 then vσi (Ii, a) = maxa′ v σ i (Ii, a\n′). We further define counterfactual best response value CBV σ−i(Ii) as the value player i expects to achieve by playing according to CBR(σ−i), having already reached infoset Ii. Formally, CBV σ−i(Ii) = v 〈CBR(σ−i),σ−i〉 i (Ii) and CBV σ−i(Ii, a) = v 〈CBR(σ−i),σ−i〉 i (Ii, a).\nAn imperfect-information subgame, which we refer to simply as a subgame in this paper, can in most cases (but not all) be described as including all nodes which share prior public actions (that is, actions viewable to both players). In poker, for example, a subgame is uniquely defined by a sequence of bets and public board cards. Figure 1b shows the public game tree of Coin Toss. Formally, an imperfect-information subgame is a set of nodes S ⊆ H such that for all h ∈ S, if h @ h′, then h′ ∈ S, and for all h ∈ S and all i ∈ P , if h′ ∈ Ii(h) then h′ ∈ S. Define Stop as the set of earliest-reachable nodes in S. That is, h ∈ Stop if h ∈ S and h′ 6∈ S for any h′ @ h."
    }, {
      "heading" : "4 Prior Approaches to Subgame Solving",
      "text" : "This section reviews prior techniques for subgame solving in imperfect-information games, which we build upon. Throughout this section, we refer to the Coin Toss game shown in Figure 1a.\nAs discussed in Section 1, a standard approach to dealing with large imperfect-information games is to solve an abstraction of the game. The abstract solution is a (probably suboptimal) strategy profile\nin the full game. We refer to this full-game strategy profile as the blueprint. The goal of subgame solving is to improve upon the blueprint by changing the strategy only in a subgame.\nAssume that a blueprint strategy profile σ (shown in Figure 2) has already been computed for Coin Toss in which P1 chooses Play 34 of the time with Heads and 1 2 of the time with Tails, and P2 chooses Heads 12 of the time, Tails 1 4 of the time, and Forfeit 1 4 of the time after P1 chooses Play. The details of the blueprint strategy in the Sell subgame are not relevant in this section, but the EV for choosing the Sell action is relevant. We assume that if P1 chose the Sell action and played optimally thereafter, then she would receive an expected payoff of 0.5 if the coin is Heads, and −0.5 if the coin is Tails. We will attempt to improve P2’s strategy in the subgame S that follows P1 choosing Play."
    }, {
      "heading" : "4.1 Unsafe Subgame Solving",
      "text" : "We first review the most intuitive form of subgame solving, which we refer to as Unsafe subgame solving [1, 12, 13, 10]. This form of subgame solving assumes both players played according to the blueprint strategy prior to reaching the subgame. That defines a probability distribution over the nodes at the root of the subgame S, representing the probability that the true game state matches that node. A strategy for the subgame is then calculated which assumes that this distribution is correct.\nIn all subgame solving algorithms, an augmented subgame containing S and a few additional nodes is solved to determine the strategy for S. Applying Unsafe subgame solving to the blueprint strategy in Coin Toss (after P1 chooses Play) means solving the augmented subgame shown in Figure 3a.\nSpecifically, the augmented subgame consists of only an initial chance node and S. The initial chance node reaches h ∈ Stop with probability π\nσ(h)∑ h′∈Stop π σ(h′) . The augmented subgame is solved and its\nstrategy for P2 is used in S rather than the blueprint strategy.\nUnsafe subgame solving lacks theoretical solution quality guarantees and there are many situations where it performs extremely poorly. Indeed, if it were applied to the blueprint strategy of Coin Toss then P2 would always choose Heads—which P1 could exploit severely by only choosing Play with Tails. Despite the lack of theoretical guarantees and potentially bad performance, Unsafe subgame solving is simple and can sometimes produce low-exploitability strategies, as we show later.\nWe now move to discussing safe subgame-solving techniques, that is, ones that ensure that the exploitability of the strategy is no higher than that of the blueprint strategy."
    }, {
      "heading" : "4.2 Subgame Resolving",
      "text" : "In subgame Resolving [6], a safe strategy is computed for P2 in the subgame by solving the augmented subgame shown in Figure 3b, producing an equilibrium strategy σS . This augmented subgame differs from Unsafe subgame solving by giving P1 the option to “opt out” from entering S and instead receive the EV of playing optimally against P2’s blueprint strategy in S.\nSpecifically, the augmented subgame for Resolving differs from unsafe subgame solving as follows. For each htop ∈ Stop we insert a new P1 node hr, which exists only in the augmented subgame, between the initial chance node and htop. The set of these hr nodes is Sr. The initial chance node connects to each node hr ∈ Sr in proportion to the probability that player P1 could reach htop if P1 tried to do so (that is, in proportion to πσ−1(htop)). At each node hr ∈ Sr, P1 has two possible actions. Action a′S leads to htop, while action a ′ T leads to a terminal payoff that awards the value of playing optimally against P2’s blueprint strategy, which is CBV σ2(I1(htop)). In the blueprint strategy of Coin Toss, P1 choosing Play after the coin lands Heads results in an EV of 0, and 12 if the coin is Tails. Therefore, a′T leads to a terminal payoff of 0 for Heads and 1 2 for Tails. After the equilibrium strategy σS is computed in the augmented subgame, P2 plays according to the computed subgame strategy σS2 rather than the blueprint strategy when in S. The P1 strategy σ S 1 is not used.\nClearly P1 cannot do worse than always picking action a′T (which awards the highest EV P1 could achieve against P2’s blueprint). But P1 also cannot do better than always picking a′T , because P2 could simply play according to the blueprint in S, which means action a′S would give the same EV to P1 as action a′T (if P1 played optimally in S). In this way, the strategy for P2 in S is pressured to be no worse than that of the blueprint. In Coin Toss, if P2 were to always choose Heads (as was the case in Unsafe subgame solving), then P1 would always choose a′T with Heads and a ′ S with Tails.\nResolving guarantees that P2’s exploitability will be no higher than the blueprint’s (and may be better). However, it may miss opportunities for improvement. For example, if we apply Resolving to the example blueprint in Coin Toss, one solution to the augmented subgame is the blueprint itself, so P2 may choose Forfeit 25% of the time even though Heads and Tails dominate that action. Indeed, the original purpose of Resolving was not to improve upon a blueprint strategy in a subgame, but rather to compactly store it by keeping only the EV at the root of the subgame and then reconstructing the strategy in real time when needed rather than storing the whole subgame strategy.\nMaxmargin subgame solving [21], discussed in Appendix A, can improve performance by defining a margin Mσ S (I1) = CBV σ2(I1) − CBV σ S 2 (I1) for each I1 ∈ Stop and maximizing minI1∈Stop M σS (I1). Resolving only makes all margins nonnegative. However, Maxmargin does worse in practice when using estimates of equilibrium values as discussed in Appendix C."
    }, {
      "heading" : "5 Reach Subgame Solving",
      "text" : "All of the subgame-solving techniques described in Section 4 only consider the target subgame in isolation, which can lead to suboptimal strategies. For example, Maxmargin solving applied to S in Coin Toss results in P2 choosing Heads with probability 58 and Tails with 3 8 in S. This results in P1 receiving an EV of − 14 by choosing Play in the Heads state, and an EV of 1 4 in the Tails state. However, P1 could simply always choose Sell in the Heads state (earning an EV of 0.5) and Play in the Tails state and receive an EV of 38 for the entire game. In this section we introduce Reach subgame solving, an improvement to past subgame-solving techniques that considers what the opponent could have alternatively received from other subgames.1 For example, a better strategy for P2 would be to choose Heads with probability 34 and Tails with probability 1 4 . Then P1 is indifferent between choosing Sell and Play in both cases and overall receives an expected payoff of 0 for the whole game.\nHowever, that strategy is only optimal if P1 would indeed achieve an EV of 0.5 for choosing Sell in the Heads state and −0.5 in the Tails state. That would be the case if P2 played according to the blueprint in the Sell subgame (which is not shown), but in reality we would apply subgame solving to the Sell subgame if the Sell action were taken, which would change P2’s strategy there and therefore P1’s EVs. Applying subgame solving to any subgame encountered during play is equivalent to applying it to all subgames independently; ultimately, the same strategy is played in both cases. Thus, we must consider that the EVs from other subgames may differ from what the blueprint says because subgame solving would be applied to them as well.\n1Other subgame-solving methods have also considered the cost of reaching a subgame [31, 15]. However, those approaches are not correct in theory when applied in real time to any subgame reached during play.\nAs an example of this issue, consider the game shown in Figure 4 which contains two identical subgames S1 and S2 where the blueprint has P2 pick Heads and Tails with 50% probability. The Sell action leads to an EV of 0.5 from the Heads state, while Play leads to an EV of 0. If we were to solve just S1, then P2 could afford to always choose Tails in S1, thereby letting P1 achieve an EV of 1 for reaching that subgame from Heads because, due to the chance node C1, S1 is only reached with 50% probability. Thus, P1’s EV for choosing Play would be 0.5 from Heads and −0.5 from Tails, which is optimal. We can achieve this strategy in S1 by solving an augmented subgame in which the alternative payoff for Heads is 1. In that augmented subgame, P2 always choosing Tails would be a solution (though not the only solution).\nHowever, if the same reasoning were applied independently to S2 as well, then P2 might always choose Tails in both subgames and P1’s EV for choosing Play from Heads would become 1 while the EV for Sell would only be 0.5. Instead, we could allow P1 to achieve an EV of 0.5 for reaching each subgame from Heads (by setting the alternative payoff for Heads to 0.5). In that case, P1’s overall EV for choosing Play could only increase to 0.5, even if both S1 and S2 were solved independently.\nWe capture this intuition by considering for each I1 ∈ Stop all the infosets and actions I ′1 · a′ @ I1 that P1 would have taken along the path to I1. If, at some I ′1 · a′ @ I1 where P1 acted, there was a different action a∗ ∈ A(I ′1) that leads to a higher EV, then P1 would have taken a suboptimal action if they reached I1. The difference in value between a∗ and a′ is referred to as a gift. We can afford to let P1’s value for I1 increase beyond the blueprint value (and in the process lower P1’s value in some other infoset in Stop), so long as the increase to I1’s value is small enough that choosing actions leading to I1 is still suboptimal for P1. Critically, we must ensure that the increase in value is small enough even when the potential increase across all subgames is summed together, as in Figure 4.2\nA complicating factor is that gifts we assumed were present may actually not exist. For example, in Coin Toss, suppose applying subgame solving to the Sell subgame results in P1’s value for Sell from the Heads state decreasing from 0.5 to 0.25. If we independently solve the Play subgame, we have no way of knowing that P1’s value for Sell is lower than the blueprint suggested, so we may still assume there is a gift of 0.5 from the Heads state based on the blueprint. Thus, in order to guarantee a theoretical result on exploitability that is as strong as possible, we use in our theory and experiments a lower bound on what gifts could be after subgame solving was applied to all other subgames.\nFormally, let σ2 be a P2 blueprint and let σ−S2 be the P2 strategy that results from applying subgame solving independently to a set of disjoint subgames other than S. Since we do not want to compute σ−S2 in order to apply subgame solving to S, let bgσ −S 2 (I ′1, a\n′)c be a lower bound of CBV σ −S 2 (I ′1)− CBV σ −S 2 (I ′1, a ′) that does not require knowledge of σ−S2 . In our experiments we\n2In this paper and in our experiments, we allow any infoset that descends from a gift to increase by the size of the gift (e.g., in Figure 4 the gift from Heads is 0.5, so we allow P1’s value for Heads in both S1 and S2 to increase by 0.5). However, any division of the gift among subgames is acceptable so long as the potential increase across all subgames (multiplied by the probability of P1 reaching that subgame) does not exceed the original gift. For example in Figure 4 if we only apply Reach subgame solving to S1, then we could allow the Heads state in S1 to increase by 1 rather than just by 0.5. In practice, some divisions may do better than others. The division we use in this paper (applying gifts equally to all subgames) did well in practice.\nuse bgσ −S 2 (I ′1, a ′)c = maxa∈Az(I′1)∪{a′} CBV σ2(I ′1, a) − CBV σ2(I ′1, a′) where Az(I ′1) ⊆ A(I ′1) is the set of actions leading immediately to terminal nodes. Reach subgame solving modifies the augmented subgame in Resolving and Maxmargin by increasing the alternative payoff for infoset I1 ∈ Stop by ∑ I′1·a′vI1|P (I′1)=P1 bgσ −S 2 (I ′1, a ′)c. Formally, we define a reach margin as\nMσ S r (I1) = M σS (I1) + ∑ I′1·a′vI1|P (I′1)=P1 bgσ −S 2 (I ′1, a ′)c (1)\nThis margin is larger than or equal to the one for Maxmargin, because bgσ −S 2 (I ′, a′)c is nonnegative. We refer to the modified algorithms as Reach-Resolve and Reach-Maxmargin.\nUsing a lower bound on gifts is not necessary to guarantee safety. So long as we use a gift value gσ ′ (I ′1, a\n′) ≤ CBV σ2(I ′1) − CBV σ2(I ′1, a′), the resulting strategy will be safe. However, using a lower bound further guarantees a reduction to exploitability when a P1 best response reaches with positive probability an infoset I1 ∈ Stop that has positive margin, as proven in Theorem 1. In practice, it may be best to use an accurate estimate of gifts. One option is to use ĝσ −S 2 (I ′1, a\n′) = ˜CBV σ2 (I ′1)− ˜CBV σ2 (I ′1, a ′) in place of bgσ −S 2 (I ′1, a\n′)c, where ˜CBV σ2 is the closest P1 can get to the value of a counterfactual best response while P1 is constrained to playing within the abstraction that generated the blueprint. Using estimates is covered in more detail in Appendix C.\nTheorem 1 shows that when subgames are solved independently and using lower bounds on gifts, Reach-Maxmargin solving has exploitability lower than or equal to past safe techniques. The theorem statement is similar to that of Maxmargin [21], but the margins are now larger (or equal) in size. Theorem 1. Given a strategy σ2 in a two-player zero-sum game, a set of disjoint subgames S, and a strategy σS2 for each subgame S ∈ S produced via Reach-Maxmargin solving using lower bounds for gifts, let σ′2 be the strategy that plays according to σ S 2 for each subgame S ∈ S, and σ2 elsewhere. Moreover, let σ−S2 be the strategy that plays according to σ ′ 2 everywhere except for P2 nodes in S, where it instead plays according to σ2. If π BR(σ′2) 1 (I1) > 0 for some I1 ∈ Stop, then\nexp(σ′2) ≤ exp(σ−S2 )− ∑ h∈I1 π σ2 −1(h)M σS r (I1).\nSo far the described techniques have guaranteed a reduction in exploitability over the blueprint by setting the value of a′T equal to the value of P1 playing optimally to P2’s blueprint. Relaxing this guarantee by instead setting the value of a′T equal to an estimate of P1’s value when both players play optimally leads to far lower exploitability in practice. We discuss this approach in Appendix C."
    }, {
      "heading" : "6 Nested Subgame Solving",
      "text" : "As we have discussed, large games must be abstracted to reduce the game to a tractable size. This is particularly common in games with large or continuous action spaces. Typically the action space is discretized by action abstraction so that only a few actions are included in the abstraction. While we might limit ourselves to the actions we included in the abstraction, an opponent might choose actions that are not in the abstraction. In that case, the off-tree action can be mapped to an action that is in the abstraction, and the strategy from that in-abstraction action can be used. For example, in an auction game we might include a bid of $100 in our abstraction. If a player bids $101, we simply treat that as a bid of $100. This is referred to as action translation [14, 28, 8]. Action translation is the state-of-the-art prior approach to dealing with this issue. It has been used, for example, by all the leading competitors in the Annual Computer Poker Competition (ACPC).\nIn this section, we develop techniques for applying subgame solving to calculate responses to opponent off-tree actions, thereby obviating the need for action translation. That is, rather than simply treat a bid of $101 as $100, we calculate in real time a unique response to the bid of $101. This can also be done in a nested fashion in response to subsequent opponent off-tree actions. Additionally, these techniques can be used to solve finer-grained models as play progresses down the game tree.\nWe refer to the first method as the inexpensive method.3 When P1 chooses an off-tree action a, a subgame S is generated following that action such that for any infoset I1 that P1 might be in, I1 · a ∈ Stop. This subgame may itself be an abstraction. A solution σS is computed via subgame solving, and σS is combined with σ to form a new blueprint σ′ in the expanded abstraction that now includes action a. The process repeats whenever P1 again chooses an off-tree action.\n3Following our study, the AI DeepStack used a technique similar to this form of nested subgame solving [20].\nTo conduct safe subgame solving in response to off-tree action a, we could calculate CBV σ2(I1, a) by defining, via action translation, a P2 blueprint following a and best responding to it [4]. However, that could be computationally expensive and would likely perform poorly in practice because, as we show later, action translation is highly exploitable. Instead, we relax the guarantee of safety and use ˜CBV σ2 (I1) for the alternative payoff, where ˜CBV σ2\n(I1) is P1’s counterfactual best response value in I1 when constrained to playing in the blueprint abstraction (which excludes action a). In this case, exploitability depends on how well ˜CBV σ2 (I1) approximates CBV σ ∗ 2 (I1), where σ∗2 is an optimal P2 strategy (see Appendix C).4 In general, we find that only a small number of near-optimal actions need to be included in the blueprint abstraction for ˜CBV σ2 (I1) to be close to CBV σ ∗ 2 (I1). We can then approximate a near-optimal response to any opponent action, even in a continuous action space.\nThe “inexpensive” approach cannot be combined with Unsafe subgame solving because the probability of reaching an action outside of a player’s abstraction is undefined. Nevertheless, a similar approach is possible with Unsafe subgame solving (as well as all the other subgame-solving techniques) by starting the subgame solving at h rather than at h · a. In other words, if action a taken in node h is not in the abstraction, then Unsafe subgame solving is conducted in the smallest subgame containing h (and action a is added to that abstraction). This increases the size of the subgame compared to the inexpensive method because a strategy must be recomputed for every action a′ ∈ A(h) in addition to a. We therefore call this method the expensive method. We present experiments with both methods."
    }, {
      "heading" : "7 Experiments",
      "text" : "Our experiments were conducted on heads-up no-limit Texas hold’em, as well as two smaller-scale poker games we call No-Limit Flop Hold’em (NLFH) and No-Limit Turn Hold’em (NLTH). The description for these games can be found in Appendix G. For equilibrium finding, we used CFR+ [30].\nOur first experiment compares the performance of the subgame-solving techniques when applied to information abstraction (which is card abstraction in the case of poker). Specifically, we solve NLFH with no information abstraction on the preflop. On the flop, there are 1,286,792 infosets for each betting sequence; the abstraction buckets them into 200, 2,000, or 30,000 abstract ones (using a leading information abstraction algorithm [9]). We then apply subgame solving immediately after the flop community cards are dealt. We experiment with two versions of the game, one small and one large, which include only a few of the available actions in each infoset. We also experimented on abstractions of NLTH. In that case, we solve NLTH with no information abstraction on the preflop or flop. On the turn, there are 55,190,538 infosets for each betting sequence; the abstraction buckets them into 200, 2,000, or 20,000 abstract ones. We apply subgame solving immediately after the turn community card is dealt. Table 1 shows the performance of each technique when using 30,000 buckets (20,000 for NLTH). The full results are presented in Appendix E. In all our experiments, exploitability is measured in the standard units used in this field: milli big blinds per hand (mbb/h).\nEstimate and Estimate+Distributional are techniques introduced in Appendix C. We use a normal distribution in the Distributional subgame solving experiments, with standard deviation determined by the heuristic presented in Appendix C.1.\nSince subgame solving begins immediately after a chance node with an extremely high branching factor (1, 755 in NLFH), the gifts for the Reach algorithms are divided among subgames inefficiently.\n4We estimate CBV σ ∗ 2 (I1) rather than CBV σ ∗ 2 (I1, a) because CBV σ ∗ 2 (I1)− CBV σ ∗ 2 (I1, a) is a gift that\nmay be added to the alternative payoff anyway.\nMany subgames do not use the gifts at all, while others could make use of more. In the experiments we show results both for the theoretically safe splitting of gifts, as well as a more aggressive version where gifts are scaled up by the branching factor of the chance node (1, 755). This weakens the theoretical guarantees of the algorithm, but in general did better than splitting gifts in a theoretically correct manner. However, this is not universally true. Appendix F shows that in at least one case, exploitability increased when gifts were scaled up too aggressively. In all cases, using Reach subgame solving in at least the theoretical safe method led to lower exploitability.\nDespite lacking theoretical guarantees, Unsafe subgame solving did surprisingly well in most games. However, it did substantially worse in Large NLFH with 30,000 buckets. This exemplifies its variability. Among the safe methods, all of the changes we introduce show improvement over past techniques. The Reach-Estimate + Distributional algorithm generally resulted in the lowest exploitability among the various choices, and in most cases beat unsafe subgame solving.\nThe second experiment evaluates nested subgame solving, and compares it to action translation. In order to also evaluate action translation, in this experiment, we create an NLFH game that includes 3 bet sizes at every point in the game tree (0.5, 0.75, and 1.0 times the size of the pot); a player can also decide not to bet. Only one bet (i.e., no raises) is allowed on the preflop, and three bets are allowed on the flop. There is no information abstraction anywhere in the game. We also created a second, smaller abstraction of the game in which there is still no information abstraction, but the 0.75× pot bet is never available. We calculate the exploitability of one player using the smaller abstraction, while the other player uses the larger abstraction. Whenever the large-abstraction player chooses a 0.75× pot bet, the small-abstraction player generates and solves a subgame for the remainder of the game (which again does not include any subsequent 0.75× pot bets) using the nested subgame-solving techniques described above. This subgame strategy is then used as long as the large-abstraction player plays within the small abstraction, but if she chooses the 0.75× pot bet again later, then the subgame solving is used again, and so on.\nTable 2 shows that all the subgame-solving techniques substantially outperform action translation. We did not test distributional alternative payoffs in this experiment, since the calculated best response values are likely quite accurate. These results suggest that nested subgame solving is preferable to action translation (if there is sufficient time to solve the subgame).\nWe used the techniques presented in this paper to develop Libratus, an AI that competed against four top human professionals in heads-up no-limit Texas hold’em [5]. Heads-up no-limit Texas hold’em has been the primary benchmark challenge for AI in imperfect-information games. The competition involved 120,000 hands of poker and a prize pool of $200,000 split among the humans to incentivize strong play. The AI decisively defeated the human team by 147 mbb / hand, with 99.98% statistical significance. This was the first, and so far only, time an AI defeated top humans in no-limit poker."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We introduced a subgame-solving technique for imperfect-information games that has stronger theoretical guarantees and better practical performance than prior subgame-solving methods. We presented results on exploitability of both safe and unsafe subgame-solving techniques. We also introduced a method for nested subgame solving in response to the opponent’s off-tree actions, and demonstrated that this leads to dramatically better performance than the usual approach of action translation. This is, to our knowledge, the first time that exploitability of subgame-solving techniques has been measured in large games.\nFinally, we demonstrated the effectiveness of these techniques in practice in heads-up no-limit Texas Hold’em poker, the main benchmark challenge for AI in imperfect-information games. We developed the first AI to reach the milestone of defeating top humans in heads-up no-limit Texas Hold’em."
    }, {
      "heading" : "9 Acknowledgments",
      "text" : "This material is based on work supported by the National Science Foundation under grants IIS1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082, as well as XSEDE computing resources provided by the Pittsburgh Supercomputing Center. The Brains vs. AI competition was sponsored by Carnegie Mellon University, Rivers Casino, GreatPoint Ventures, Avenue4Analytics, TNG Technology Consulting, Artificial Intelligence, Intel, and Optimized Markets, Inc. We thank Kristen Gardner, Marcelo Gutierrez, Theo Gutman-Solo, Eric Jackson, Christian Kroer, Tim Reiff, and the anonymous reviewers for helpful feedback."
    } ],
    "references" : [ {
      "title" : "Approximating game-theoretic optimal strategies for fullscale poker",
      "author" : [ "Darse Billings", "Neil Burch", "Aaron Davidson", "Robert Holte", "Jonathan Schaeffer", "Terence Schauenberg", "Duane Szafron" ],
      "venue" : "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Heads-up limit hold’em poker is solved",
      "author" : [ "Michael Bowling", "Neil Burch", "Michael Johanson", "Oskari Tammelin" ],
      "venue" : "Science,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Dynamic thresholding and pruning for regret minimization",
      "author" : [ "Noam Brown", "Christian Kroer", "Tuomas Sandholm" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2017
    }, {
      "title" : "Simultaneous abstraction and equilibrium finding in games",
      "author" : [ "Noam Brown", "Tuomas Sandholm" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Superhuman AI for heads-up no-limit poker: Libratus beats top professionals",
      "author" : [ "Noam Brown", "Tuomas Sandholm" ],
      "venue" : "Science, page eaao1733,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2017
    }, {
      "title" : "Solving imperfect information games using decomposition",
      "author" : [ "Neil Burch", "Michael Johanson", "Michael Bowling" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Action translation in extensive-form games with large action spaces: axioms, paradoxes, and the pseudo-harmonic mapping",
      "author" : [ "Sam Ganzfried", "Tuomas Sandholm" ],
      "venue" : "In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Potential-aware imperfect-recall abstraction with earth mover’s distance in imperfect-information games",
      "author" : [ "Sam Ganzfried", "Tuomas Sandholm" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Endgame solving in large imperfect-information games",
      "author" : [ "Sam Ganzfried", "Tuomas Sandholm" ],
      "venue" : "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "First-order algorithm with O(ln(1/ )) convergence for -equilibrium in two-person zero-sum games",
      "author" : [ "Andrew Gilpin", "Javier Peña", "Tuomas Sandholm" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "A competitive Texas Hold’em poker player via automated abstraction and real-time equilibrium computation",
      "author" : [ "Andrew Gilpin", "Tuomas Sandholm" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Better automated abstraction techniques for imperfect information games, with application to Texas Hold’em poker",
      "author" : [ "Andrew Gilpin", "Tuomas Sandholm" ],
      "venue" : "In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "A heads-up no-limit texas hold’em poker player: discretized betting models and automatically generated equilibriumfinding programs",
      "author" : [ "Andrew Gilpin", "Tuomas Sandholm", "Troels Bjerre Sørensen" ],
      "venue" : "In Proceedings of the Seventh International Joint Conference on Autonomous Agents and Multiagent Systems-Volume",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "A time and space efficient algorithm for approximately solving large imperfect information games",
      "author" : [ "Eric Jackson" ],
      "venue" : "In AAAI Workshop on Computer Poker and Imperfect Information,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Measuring the size of large no-limit poker games",
      "author" : [ "Michael Johanson" ],
      "venue" : "Technical report, University of Alberta,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Finding optimal abstract strategies in extensive-form games",
      "author" : [ "Michael Johanson", "Nolan Bard", "Neil Burch", "Michael Bowling" ],
      "venue" : "In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Theoretical and practical advances on smoothing for extensive-form games",
      "author" : [ "Christian Kroer", "Kevin Waugh", "Fatma Kılınç-Karzan", "Tuomas Sandholm" ],
      "venue" : "In Proceedings of the ACM Conference on Economics and Computation",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2017
    }, {
      "title" : "The weighted majority algorithm",
      "author" : [ "Nick Littlestone", "M.K. Warmuth" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1994
    }, {
      "title" : "Deepstack: Expert-level artificial intelligence in heads-up no-limit poker",
      "author" : [ "Matej Moravčík", "Martin Schmid", "Neil Burch", "Viliam Lisý", "Dustin Morrill", "Nolan Bard", "Trevor Davis", "Kevin Waugh", "Michael Johanson", "Michael Bowling" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2017
    }, {
      "title" : "Refining subgames in large imperfect information games",
      "author" : [ "Matej Moravcik", "Martin Schmid", "Karel Ha", "Milan Hladik", "Stephen Gaukrodger" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Equilibrium points in n-person games",
      "author" : [ "John Nash" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1950
    }, {
      "title" : "Excessive gap technique in nonsmooth convex minimization",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "SIAM Journal of Optimization,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "The state of solving large incomplete-information games, and application to poker",
      "author" : [ "Tuomas Sandholm" ],
      "venue" : "AI Magazine,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Abstraction for solving large incomplete-information games",
      "author" : [ "Tuomas Sandholm" ],
      "venue" : "In AAAI Conference on Artificial Intelligence (AAAI), pages 4127–4131,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Probabilistic state translation in extensive games with large action sets",
      "author" : [ "David Schnizlein", "Michael Bowling", "Duane Szafron" ],
      "venue" : "In Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2009
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Solving heads-up limit texas hold’em",
      "author" : [ "Oskari Tammelin", "Neil Burch", "Michael Johanson", "Michael Bowling" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Strategy grafting in extensive games",
      "author" : [ "Kevin Waugh", "Nolan Bard", "Michael Bowling" ],
      "venue" : "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "Regret minimization in games with incomplete information",
      "author" : [ "Martin Zinkevich", "Michael Johanson", "Michael H Bowling", "Carmelo Piccione" ],
      "venue" : "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "This fact has been leveraged by nearly every AI for perfect-information games, including AIs that defeated top humans in chess [7] and Go [29].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "For example, heads-up limit Texas hold’em, a relatively simple form of poker with 10(13) decision points, was essentially solved without decomposition [2].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 14,
      "context" : "However, this approach cannot extend to larger games, such as heads-up no-limit Texas hold’em—the primary benchmark in imperfect-information game solving—which has 10(161) decision points [16].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 22,
      "context" : "The standard approach to computing strategies in such large games is to first generate an abstraction of the game, which is a smaller version of the game that retains as much as possible the strategic characteristics of the original game [24, 26, 25].",
      "startOffset" : 238,
      "endOffset" : 250
    }, {
      "referenceID" : 23,
      "context" : "The standard approach to computing strategies in such large games is to first generate an abstraction of the game, which is a smaller version of the game that retains as much as possible the strategic characteristics of the original game [24, 26, 25].",
      "startOffset" : 238,
      "endOffset" : 250
    }, {
      "referenceID" : 20,
      "context" : "A Nash equilibrium [22] is a strategy profile σ∗ where no player can improve by shifting to a different strategy, so σ∗ satisfies ∀i, ui(σ i , σ∗ −i) = maxσ′ i∈Σi ui(σ ′ i, σ ∗ −i).",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "A counterfactual best response [21] CBR(σ−i) is a best response that also maximizes value in unreached infosets.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "We first review the most intuitive form of subgame solving, which we refer to as Unsafe subgame solving [1, 12, 13, 10].",
      "startOffset" : 104,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "We first review the most intuitive form of subgame solving, which we refer to as Unsafe subgame solving [1, 12, 13, 10].",
      "startOffset" : 104,
      "endOffset" : 119
    }, {
      "referenceID" : 11,
      "context" : "We first review the most intuitive form of subgame solving, which we refer to as Unsafe subgame solving [1, 12, 13, 10].",
      "startOffset" : 104,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "We first review the most intuitive form of subgame solving, which we refer to as Unsafe subgame solving [1, 12, 13, 10].",
      "startOffset" : 104,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "2 Subgame Resolving In subgame Resolving [6], a safe strategy is computed for P2 in the subgame by solving the augmented subgame shown in Figure 3b, producing an equilibrium strategy σ .",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "Maxmargin subgame solving [21], discussed in Appendix A, can improve performance by defining a margin M S (I1) = CBV 2(I1) − CBV σ S 2 (I1) for each I1 ∈ Stop and maximizing minI1∈Stop M σ (I1).",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 27,
      "context" : "Other subgame-solving methods have also considered the cost of reaching a subgame [31, 15].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 13,
      "context" : "Other subgame-solving methods have also considered the cost of reaching a subgame [31, 15].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : "The theorem statement is similar to that of Maxmargin [21], but the margins are now larger (or equal) in size.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "This is referred to as action translation [14, 28, 8].",
      "startOffset" : 42,
      "endOffset" : 53
    }, {
      "referenceID" : 24,
      "context" : "This is referred to as action translation [14, 28, 8].",
      "startOffset" : 42,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : "This is referred to as action translation [14, 28, 8].",
      "startOffset" : 42,
      "endOffset" : 53
    }, {
      "referenceID" : 18,
      "context" : "Following our study, the AI DeepStack used a technique similar to this form of nested subgame solving [20].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "To conduct safe subgame solving in response to off-tree action a, we could calculate CBV 2(I1, a) by defining, via action translation, a P2 blueprint following a and best responding to it [4].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 26,
      "context" : "For equilibrium finding, we used CFR+ [30].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "On the flop, there are 1,286,792 infosets for each betting sequence; the abstraction buckets them into 200, 2,000, or 30,000 abstract ones (using a leading information abstraction algorithm [9]).",
      "startOffset" : 190,
      "endOffset" : 193
    }, {
      "referenceID" : 4,
      "context" : "We used the techniques presented in this paper to develop Libratus, an AI that competed against four top human professionals in heads-up no-limit Texas hold’em [5].",
      "startOffset" : 160,
      "endOffset" : 163
    } ],
    "year" : 2017,
    "abstractText" : "In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames. Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games. Nevertheless, it is possible to first approximate a solution for the whole game and then improve it in individual subgames. This is referred to as subgame solving. We introduce subgame-solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability. These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold’em poker.",
    "creator" : null
  }
}