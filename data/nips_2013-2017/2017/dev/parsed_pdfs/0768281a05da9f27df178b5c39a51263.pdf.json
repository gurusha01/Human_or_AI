{
  "name" : "0768281a05da9f27df178b5c39a51263.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deep Hyperalignment",
    "authors" : [ "Deep Hyperalignment", "Muhammad Yousefnezhad", "Daoqiang Zhang" ],
    "emails" : [ "myousefnezhad@nuaa.edu.cn", "dqzhang@nuaa.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The multi-subject fMRI analysis is a challenging problem in the human brain decoding [1–7]. On the one hand, the multi-subject analysis can verify the developed models across subjects. On the other hand, this analysis requires authentic functional and anatomical alignments among neuronal activities of different subjects, which these alignments can significantly improve the performance of the developed models [1, 4]. In fact, multi-subject fMRI images must be aligned across subjects in order to take between-subject variability into account. There are technically two main alignment methods, including anatomical alignment and functional alignment, which can work in unison. Indeed, anatomical alignment is only utilized in the majority of the fMRI studies as a preprocessing step. It is applied by aligning fMRI images based on anatomical features of standard structural MRI images, e.g. Talairach [2, 7]. However, anatomical alignment can limitedly improve the accuracy because the size, shape and anatomical location of functional loci differ across subjects [1, 2, 7]. By contrast, functional alignment explores to precisely align the fMRI images across subjects. Indeed, it has a broad range of applications in neuroscience, such as localization of the Brain’s tumor [8].\nAs the widely used functional alignment method [1–7], Hyperalignment (HA) [1] is an ‘anatomy free’ functional alignment method, which can be mathematically formulated as a multiple-set Canonical Correlation Analysis (CCA) problem [2, 3, 5]. Original HA does not work in a very high dimensional space. In order to extend HA into the real-world problems, Xu et al. developed the Regularized Hyperalignment (RHA) by utilizing an EM algorithm to iteratively seek the regularized optimum parameters [2]. Further, Chen et al. developed Singular Value Decomposition Hyperalignment (SVDHA), which firstly provides dimensionality reduction by SVD, and then HA aligns the functional responses in the reduced space [4]. In another study, Chen et al. introduced Shared Response Model (SRM), which is technically equivalent to Probabilistic CCA [5]. In addition, Guntupalli et al. developed SearchLight (SL) model, which is actually an ensemble of quasi-CCA models fits on patches of the brain images [9]. Lorbert et al. illustrated the limitation of HA methods on the linear representation of fMRI responses. They also proposed Kernel Hyperalignment (KHA) as a nonlinear alternative in an embedding space for solving the HA limitation [3]. Although KHA\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\ncan solve the nonlinearity and high-dimensionality problems, its performance is limited by the fixed employed kernel function. As another nonlinear HA method, Chen et al. recently developed Convolutional Autoencoder (CAE) for whole brain functional alignment. Indeed, this method reformulates the SRM as a multi-view autoencoder [5] and then uses the standard SL analysis [9] in order to improve the stability and robustness of the generated classification (cognitive) model [6]. Since CAE simultaneously employs SRM and SL, its time complexity is so high. In a nutshell, there are three main challenges in previous HA methods for calculating accurate functional alignments, i.e. nonlinearity [3, 6], high-dimensionality [2, 4, 5], and using a large number of subjects [6].\nAs the main contribution of this paper, we propose a novel kernel approach, which is called Deep Hyperalignment (DHA), in order to solve mentioned challenges in HA problems. Indeed, DHA employs deep network, i.e. multiple stacked layers of nonlinear transformation, as the kernel function, which is parametric and uses rank-m SVD [10] and Stochastic Gradient Descent (SGD) [13] for optimization. Consequently, DHA generates low-runtime on large datasets, and the training data is not referenced when DHA computes the functional alignment for a new subject. Further, DHA is not limited by a restricted fixed representational space because the kernel in DHA is a multi-layer neural network, which can separately implement any nonlinear function [11–13] for each subject to transfer the brain activities to a common space.\nThe proposed method is related to RHA [2] and MVLSA [10]. Indeed, the main difference between DHA and the mentioned methods lies in the deep kernel function. Further, KHA [3] is equivalent to DHA, where the proposed deep network is employed as the kernel function. In addition, DHA can be looked as a multi-set regularized DCCA [11] with stochastic optimization [13]. Finally, DHA is related to DGCCA [12], when DGCCA is reformulated for functional alignment by using regularization, and rank-m SVD [10].\nThe rest of this paper is organized as follows: In Section 2, this study briefly introduces HA method. Then, DHA is proposed in Section 3. Experimental results are reported in Section 4; and finally, this paper presents conclusion and pointed out some future works in Section 5."
    }, {
      "heading" : "2 Hyperalignment",
      "text" : "As a training set, preprocessed fMRI time series for S subjects can be denoted by X(`) = { x (`) mn } ∈\nRT×V , ` = 1:S,m = 1:T, n = 1:V , where V denotes the number of voxels, T is the number of time points in units of TRs (Time of Repetition), and x(`)mn ∈ R denotes the functional activity for the `-th subject in the m-th time point and the n-th voxel. For assuring temporal alignment, the stimuli in the training set are considered time synchronized, i.e. the m-th time point for all subjects illustrates the same simulation [2, 3]. Original HA can be defined based on Inter-Subject Correlation (ISC), which is a classical metric in order to apply functional alignment: [1-4, 7]\nmax R(i),R(j) S∑ i=1 S∑ j=i+1 ISC(X(i)R(i),X(j)R(j)) ≡ max R(i),R(j) S∑ i=1 S∑ j=i+1 tr ( (X(i)R(i))>X(j)R(j) )\ns.t. ( X(`)R(`) )> X(`)R(`) = I, ` = 1:S,\n(1)\nwhere tr() denotes the trace function, I is the identity matrix, R(`) ∈ RV×V denotes the solution for `-th subject. For avoiding overfitting, the constrains must be imposed in R(`) [2, 7]. If X(`) ∼ N (0, 1), ` = 1:S are column-wise standardized, the ISC lies in [−1,+1]. Here, the large values illustrate better alignment [2, 3]. In order to seek an optimum solution, solving (1) may not be the best approach because there is no scale to evaluate the distance between current result and the optimum (fully maximized) solution [2, 4, 7]. Instead, we can reformulate (1) as a minimization problem by using a multiple-set CCA: [1–4]\nmin R(i),R(j) S∑ i=1 S∑ j=i+1 ∥∥∥X(i)R(i) −X(j)R(j)∥∥∥2 F , s.t. ( X(`)R(`) )> X(`)R(`) = I, ` = 1:S, (2)\nwhere (2) approaches zero for an optimum result. Indeed, the main assumption in the original HA is that the R(`), ` = 1:S are noisy ‘rotations’ of a common template [1, 9]. This paper provides a detailed description of HA methods in the supplementary materials (https://sourceforge.net/ projects/myousefnezhad/files/DHA/)."
    }, {
      "heading" : "3 Deep Hyperalignment",
      "text" : "Objective function of DHA is defined as follows:\nmin θ(i),R(i) θ(j),R(j)\nS∑ i=1 S∑ j=i+1 ∥∥∥fi(X(i);θ(i))R(i) − fj(X(j);θ(j))R(j)∥∥∥2 F\ns.t. ( R(`) )>(( f` ( X(`);θ(`) ))> f` ( X(`);θ(`) ) + I ) R(`) = I, ` = 1:S,\n(3)\nwhere θ(`)= { W (`) m , b (`) m , m=2:C } denotes all parameters in `-th deep network belonged to `-th subject, R(`) ∈ RVnew×Vnew is the DHA solution for `-th subject, Vnew ≤ V denotes the number of features after transformation, the regularized parameter is a small constant, e.g. 10−8, and deep multi-layer kernel function f` ( X(`);θ(`) ) ∈ RT×Vnew is denoted as follows:\nf` ( X(`);θ(`) ) = mat ( h (`) C , T, Vnew ) , (4)\nwhere T denotes the number of time points, C ≥ 3 is number of deep network layers, mat(x,m, n):Rmn → Rm×n denotes the reshape (matricization) function, and h(`)C ∈ RTVnew is the output layer of the following multi-layer deep network:\nh(`)m = g ( W(`)m h (`) m−1 + b (`) m ) , where h(`)1 = vec ( X(`) ) and m = 2:C. (5)\nHere, g:R → R is a nonlinear function applied componentwise, vec:Rm×n → Rmn denotes the vectorization function, consequently h(`)1 = vec ( X(`) ) ∈ RTV . Notably, this paper considers both\nvec() and mat() functions are linear transformations, where X ∈ Rm×n = mat ( vec(X),m, n ) for\nany matrix X. By considering U (m) units in the m-th intermediate layer, parameters of distinctive layers of f` ( X(`);θ(`) ) are defined by following properties: W(`)C ∈ RTVnew×U (C-1) and b(`)C ∈ RTVnew for the output layer, W(`)2 ∈ RU (2)×TV and b(`)2 ∈ RU (2)\nfor the first intermediate layer, and W(`)m ∈ RU (m)×U(m-1) , b(`)m ∈ RU (m) and h(`)m ∈ RU (m)\nfor m-th intermediate layer (3 ≤ m ≤ C − 1). Since (3) must be calculated for any new subject in the testing phase, it is not computationally efficient. In other words, the transformed training data must be referenced by the current objective function for each new subject in the testing phase. Lemma 1. The equation (3) can be reformulated as follows where G ∈ RT×Vnew is the HA template:\nmin G,R(i),θ(i) S∑ i=1 ∥∥∥G− fi(X(i);θ(i))R(i)∥∥∥2 F s.t. G>G = I, where G = 1 S S∑ j=1 fj ( X(j);θ(j) ) R(j).\n(6) Proof. In a nutshell, both (3) and (6) can be rewritten as −S2tr ( G>G ) +(\nS ∑S `=1 tr (( f` ( X(`);θ(`) ) R(`) )> f` ( X(`);θ(`) ) R(`) )) . Please see supplementary materi-\nals for proof in details. Remark 1. G is called DHA template, which can be used for functional alignment in the testing phase. Remark 2. Same as previous approaches for HA problems [1–7], a DHA solution is not unique. If a DHA template G is calculated for a specific HA problem, then QG is another solution for that specific HA problem, where Q ∈ RVnew×Vnew can be any orthogonal matrix. Consequently, if two independent templates G1, G2 are trained for a specific dataset, the solutions can be mapped to each other by calculating\n∥∥G2 −QG1∥∥, where Q can be used as a coefficient for functional alignment in the first solution in order to compare its results to the second one. Indeed, G1 and G2 are located in different positions on the same contour line [5, 7]."
    }, {
      "heading" : "3.1 Optimization",
      "text" : "This section proposes an effective approach for optimizing the DHA objective function by using rank-m SVD [10] and SGD [13]. This method seeks an optimum solution for the DHA objective function (6) by using two different steps, which iteratively work in unison. By considering fixed network parameters (θ(`)), a mini-batch of neural activities is firstly aligned through the deep network. Then, back-propagation algorithm [14] is used to update the network parameters. The main challenge for solving the DHA objective function is that we cannot seek a natural extension of the correlation object to more than two random variables. Consequently, functional alignments are stacked in a S × S matrix and maximize a certain matrix norm for that matrix [10, 12]. As the first step, we consider network parameters are in an optimum state. Therefore, the mappings (R(`), ` = 1:S) and template (G) must be calculated to solve the DHA problem. In order to scale DHA approach, this paper employs the rank-m SVD [10] of the mapped neural activities as follows:\nf` ( X(`);θ(`) ) SV D = Ω(`)Σ(`) ( Ψ(`) )> , ` = 1:S (7)\nwhere Σ(`) ∈ Rm×m denotes the diagonal matrix with m-largest singular values of the mapped feature f` ( X(`);θ(`) ) , Ω(`) ∈ RT×m and Ψ(`) ∈ Rm×Vnew are respectively the corresponding left and right singular vectors. Based on (7), the projection matrix for `-th subject can be generated as follows: [10]\nP(`) = f` ( X(`);θ(`) )(( f` ( X(`);θ(`) ))> f` ( X(`);θ(`) ) + I )−1( f` ( X(`);θ(`) ))> = Ω(`) ( Σ(`) )>( Σ(`) ( Σ(`) )> + I )−1 Σ(`) ( Ω(`) )> = Ω(`)D(`) ( Ω(`)D(`) )> ,\n(8)\nwhere P(`) ∈ RT×T is symmetric and idempotent [10, 12], and diagonal matrix D(`) ∈ Rm×m is\nD(`) ( D(`) )> = ( Σ(`) )>( Σ(`) ( Σ(`) )> + I )−1 Σ(`). (9)\nFurther, the sum of projection matrices can be defined as follows, where ÃÃ> is the Cholesky decomposition [10] of A:\nA = S∑ i=1 P(i) = ÃÃ>, where Ã ∈ RT×mS = [ Ω(1)D(1) . . .Ω(S)D(S) ] . (10)\nLemma 2. Based on (10), the objective function of DHA (6) can be rewritten as follows:\nmin G,R(i),θ(i) S∑ i=1 ∥∥∥G− fi(X(i);θ(i))R(i)∥∥∥ ≡ max G ( tr ( G>AG )) . (11)\nProof. Since P(`) is idempotent, the trace form of (6) can be reformulated as maximizing the sum of projections. Please see the supplementary materials for proof in details.\nBased on Lemma 2, the first optimization step of DHA problem can be expressed as eigendecomposition of AG = GΛ, where Λ = { λ1 . . . λT } and G respectively denote the eigenvalues and eigenvectors of A. Further, the matrix G that we are interested in finding, can be calculated by the left singular vectors of Ã = GΣ̃Ψ̃>, where G>G = I [10]. This paper utilizes Incremental SVD [15] for calculating these left singular vectors. Further, DHA mapping for `-th subject is denoted as follows:\nR(`) = (( f` ( X(`);θ(`) ))> f` ( X(`);θ(`) ) + I )−1( f` ( X(`);θ(`) ))> G. (12)\nLemma 3. In order to update network parameters as the second step, the derivative of Z = ∑T `=1 λ`, which is the sum of eigenvalues of A, over the mapped neural activities of `-th subject is defined as follows:\n∂Z ∂f` ( X(`);θ(`) ) = 2R(`)G> − 2R(`)(R(`))>(f`(X(`);θ(`)))>. (13) Proof. This derivative can be solved by using the chain and product rules in the matrix derivative as well as considering ∂Z/∂A = GG> [12]. Please see the supplementary materials for proof in details.\nAlgorithm 1 Deep Hyperalignment (DHA) Input: Data X(i), i = 1:S, Regularized parameter , Number of layers C, Number of units U (m)\nfor m = 2:C, HA template Ĝ for testing phase (default ∅), Learning rate η (default 10−4 [13]). Output: DHA mappings R(`) and parameters θ(`), HA template G just from training phase Method: 01. Initialize iteration counter: m← 1 and θ(`) ∼ N (0, 1) for ` = 1:S. 02. Construct f` ( X(`);θ(`) ) based on (4) and (5) by using θ(`), C, U (m) for ` = 1:S.\n03. IF (Ĝ 6= ∅) THEN % The first step of DHA: fixed θ(`) and calculating G and R(`) ↓ 04. Generate Ã by using (8) and (10). 05. Calculate G by applying Incremental SVD [15] to Ã = GΣ̃Ψ̃>. 06. ELSE 07. G = Ĝ. 08. END IF 09. Calculate mappings R(`), ` = 1:S by using (12).\n10. Estimate error of iteration γm = ∑S i=1 ∑S j=i+1 ∥∥∥fi(X(i);θ(i))R(i) − fj(X(j);θ(j))R(j)∥∥∥2 F .\n11. IF ( (m > 3) and (γm ≥ γm−1 ≥ γm−2) ) THEN % This is the finishing condition. 12. Return calculated G, R(`), θ(`)(` = 1:S) related to (m-2)-th iteration. 13. END IF % The second step of DHA: fixed G and R(`) and updating θ(`) ↓ 14. ∇θ(`) ← backprop ( ∂Z/∂f` ( X(`);θ(`) ) , θ(`) ) by using (13) for ` = 1:S.\n15. Update θ(`) ← θ(`) − η∇θ(`) for ` = 1:S and then m← m+ 1 16. SAVE all DHA parameters related to this iteration and GO TO Line 02.\nAlgorithm 1 illustrates the DHA method for both training and testing phases. As depicted in this algorithm, (12) is just needed as the first step in the testing phase because the DHA template G is calculated for this phase based on the training samples (please see Lemma 1). As the second step in the DHA method, the networks’ parameters (θ(`)) must be updated. This paper employs the back-propagation algorithm (backprop() function) [14] as well as Lemma 3 for this step. In addition, finishing condition is defined by tackling errors in last three iterations, i.e. the average of the difference between each pair correlations of aligned functional activities across subjects (γm for last three iterations). In other words, DHA will be finished if the error rates in the last three iterations are going to be worst. Further, a structure (nonlinear function for componentwise, and numbers of layers and units) for the deep network can be selected based on the optimum-state error (γopt) generated by training samples across different structures (see Experiment Schemes in the supplementary materials).\nIn summary, this paper proposes DHA as a flexible deep kernel approach to improve the performance of functional alignment in fMRI analysis. In order to seek an efficient functional alignment, DHA uses a deep network (multiple stacked layers of nonlinear transformation) for mapping fMRI responses of each subject to an embedded space (f` : RT×V → RT×Vnew , ` = 1:S). Unlike previous methods that use a restricted fixed kernel function, mapping functions in DHA are flexible across subjects because they employ multi-layer neural networks, which can implement any nonlinear function [12]. Therefore, DHA does not suffer from disadvantages of the previous kernel approach. In order to deal with high-dimensionality (broad ROI), DHA can also apply an optional feature selection by considering Vnew < V for constructing the deep networks. The performance of the optional feature selection will be analyzed in Section 4. Finally, DHA can be scaled across a large number of subjects by using the proposed optimization algorithm, i.e. rank-m SVD, regularization, and mini-batch SGD."
    }, {
      "heading" : "4 Experiments",
      "text" : "The empirical studies are reported in this section. Like previous studies [1–7, 9], this paper employs the ν-SVM algorithms [16] for generating the classification model. Indeed, we use the binary ν-SVM for datasets with just two categories of stimuli and multi-label ν-SVM [3, 16] as the multi-class approach. All datasets are separately preprocessed by FSL 5.0.9 (https://fsl.fmrib.ox.ac.uk), i.e. slice timing, anatomical alignment, normalization, smoothing. Regions of Interests (ROI) are also denoted by employing the main reference of each dataset. In addition, leave-one-subject-out\ncross-validation is utilized for partitioning datasets to the training set and testing set. Different HA methods are employed for functional aligning and then the mapped neural activities are used to generate the classification model. The performance of the proposed method is compared with the ν-SVM algorithm as the baseline, where the features are used after anatomical alignment without applying any hyperalignment mapping. Further, performances of the standard HA [1], RHA [2], KHA [3], SVDHA [4], SRM [5], and SL [9] are reported as state-of-the-arts HA methods. In this paper, the results of HA algorithm is generated by employing Generalized CCA proposed in [10]. In addition, regularized parameters (α, β) in RHA are optimally assigned based on [2]. Further, KHA algorithm is used by the Gaussian kernel, which is evaluated as the best kernel in the original paper [3]. As another deep-learning-based alternative for functional alignment, the performance of CAE [6] is also compared with the proposed method. Like the original paper [6], this paper employs k1 = k3 = {5, 10, 15, 20, 25}, ρ = {0.1, 0.25, 0.5, 0.75, 0.9}, λ = {0.1, 1, 5, 10}. Then, aligned neural activities (by using CAE) are applied to the classification algorithm same as other HA techniques. This paper follows the CAE setup to set the same settings in the proposed method. Consequently, three hidden layers (C = 5) and the regularized parameters = {10−4, 10−6, 10−8} are employed in the DHA method. In addition, the number of units in the intermediate layers are considered U (m) = KV , where m = 2:C-1, C is the number of layers, V denotes the number of voxels andK is the number of stimulus categories in each dataset1. Further, three distinctive activation functions are employed, i.e. Sigmoid (g(x) = 1/1 + exp(−x)), Hyperbolic (g(x) = tanh(x)), and Rectified Linear Unit or ReLU (g(x) = ln(1 + exp(x))). In this paper, the optimum parameters for DHA and CAE methods are reported for each dataset. Moreover, all algorithms are implemented by Python 3 on a PC with certain specifications2 by authors in order to generate experimental results. Experiment schemes are also described in supplementary materials."
    }, {
      "heading" : "4.1 Simple Tasks Analysis",
      "text" : "This paper utilizes 5 datasets, shared by Open fMRI (https://openfmri.org), for running empirical studies of this section. Further, numbers of original and aligned features are considered\n1Although we can use any settings for DHA, we empirically figured out this setting is acceptable to seek an optimum solution. Indeed, we followed CAE setup in the network structure but used the number of categories (K) rather than a series of parameters. In the current format of DHA, we just need to set the regularized constant and the nonlinear activation function, while a wide range of parameters must be set in the CAE.\n2DEL, CPU = Intel Xeon E5-2630 v3 (8×2.4 GHz), RAM = 64GB, GPU = GeForce GTX TITAN X (12GB memory), OS = Ubuntu 16.04.3 LTS, Python = 3.6.2, Pip = 9.0.1, Numpy = 1.13.1, Scipy = 0.19.1, Scikit-Learn = 0.18.2, Theano = 0.9.0.\nequal (V = Vnew) for all HA methods. As the first dataset, ‘Mixed-gambles task’ (DS005) includes S = 48 subjects. It also contains K = 2 categories of risk tasks in the human brain, where the chance of selection is 50/50. In this dataset, the best results for CAE is generated by following parameters k1 = k3 = 20, ρ = 0.75, λ = 1 and for DHA by using = 10−8 and Hyperbolic function. In addition, ROI is defined based on the original paper [17]. As the second dataset, ‘Visual Object Recognition’ (DS105) includes S = 71 subjects. It also contains K = 8 categories of visual stimuli, i.e. gray-scale images of faces, houses, cats, bottles, scissors, shoes, chairs, and scrambles (nonsense patterns). In this dataset, the best results for CAE is generated by following parameters k1 = k3 = 25, ρ = 0.9, λ = 5 and for DHA by using = 10−6 and Sigmoid function. Please see [1, 7] for more information. As the third dataset, ‘Word and Object Processing’ (DS107) includes S = 98 subjects. It contains K = 4 categories of visual stimuli, i.e. words, objects, scrambles, consonants. In this dataset, the best results for CAE is generated by following parameters k1 = k3 = 10, ρ = 0.5, λ = 10 and for DHA by using = 10−6 and ReLU function. Please see [18] for more information. As the fourth dataset, ‘Multi-subject, multi-modal human neuroimaging dataset’ (DS117) includes MEG and fMRI images for S = 171 subjects. This paper just uses the fMRI images of this dataset. It also contains K = 2 categories of visual stimuli, i.e. human faces, and scrambles. In this dataset, the best results for CAE is generated by following parameters k1 = k3 = 20, ρ = 0.9, λ = 5 and for DHA by using = 10−8 and Sigmoid function. Please see [19] for more information. The responses of voxels in the Ventral Cortex are analyzed for these three datasets (DS105, DS107, DS117). As the last dataset, ‘Auditory and Visual Oddball EEG-fMRI’ (DS116) includes EEG signals and fMRI images for S = 102 subjects. This paper only employs the fMRI images of this dataset. It contains K = 2 categories of audio and visual stimuli, including oddball tasks. In this dataset, the best results for CAE is generated by following parameters k1 = k3 = 10, ρ = 0.75, λ = 1 and for DHA by using = 10−4 and ReLU function. In addition, ROI is defined based on the original paper [20]. This paper also provides the technical information of the employed datasets in the supplementary materials. Table 1 and 2 respectively demonstrate the classification Accuracy and Area Under the ROC Curve (AUC) in percentage (%) for the predictors. As these tables demonstrate, the performances of classification analysis without HA method are significantly low. Further, the proposed algorithm has generated better performance in comparison with other methods because it provided a better embedded space in order to align neural activities."
    }, {
      "heading" : "4.2 Complex Tasks Analysis",
      "text" : "This section uses two fMRI datasets, which are related to watching movies. The numbers of original and aligned features are considered equal (V = Vnew) for all HA methods. As the first dataset, ‘A high-resolution 7-Tesla fMRI dataset from complex natural stimulation with an audio movie’ (DS113) includes the fMRI data of S = 18 subjects, who watched ‘Forrest Gump (1994)’ movie during the experiment. This dataset provided by Open fMRI. In this dataset, the best results for CAE is generated by following parameters k1 = k3 = 25, ρ = 0.9, λ = 10 and for DHA by using = 10−8 and Sigmoid function. Please see [7] for more information. As the second dataset, S = 10 subjects watched ‘Raiders of the Lost Ark (1981)’, where whole brain volumes are 48. In this dataset, the best results for CAE is generated by following parameters k1 = k3 = 15, ρ = 0.75, λ = 1 and for DHA\n100 90 80 70 60 40\n45\n50\n55\n60\n65\nCl as\nsif ica\ntio n A\ncc ur\nac y\nThe Percentage of Selected Features SVDHA SRM CAE DHA\n(A) DS105\n100 90 80 70 60 60 62 64 66 68 70 72 74 76 78 80 82 Cl as sif ica tio n A cc ur ac y\nThe Percentage of Selected Features SVDHA SRM CAE DHA\n(B) DS107\nFigure 2: Classification by using feature selection.\nby using = 10−4 and Sigmoid function. Please see [3-5] for more information. In these two datasets, the ROI is defined in the ventral temporal cortex (VT). Figure 1 depicts the generated results, where the voxels in ROI are ranked by the method proposed in [1] based on their neurological priorities same as previous studies [1, 4, 7, 9]. Then, the experiments are repeated by using the different number of ranked voxels per hemisphere, i.e. in Forrest: [100, 200, 400, 600, 800, 1000, 1200], and in Raiders: [70, 140, 210, 280, 350, 420, 490]. In addition, the empirical studies are reported by using the first TRs = [100, 400, 800, 2000] in both datasets. Figure 1 shows that the DHA achieves superior performance to other HA algorithms."
    }, {
      "heading" : "4.3 Classification analysis by using feature selection",
      "text" : "In this section, the effect of features selection (Vnew < V ) on the performance of classification methods will be discussed by using DS105 and DS107 datasets. Here, the performance of the proposed method is compared with SVDHA [4], SRM [5], and CAE [6] as the state-of-the-art HA techniques, which can apply feature selection before generating a classification model. Here, multi-label ν-SVM [16] is used for generating the classification models after each of the mentioned methods applied on preprocessed fMRI images for functional alignment. In addition, the setup of this experiment is same as the previous sections (cross-validation, the best parameters, etc.). Figure 2 illustrates the performance of different methods by employing 100% to 60% of features. As depicted in this figure, the proposed method has generated better performance in comparison with other methods because it provides better feature representation in comparison with other techniques."
    }, {
      "heading" : "4.4 Runtime Analysis",
      "text" : "In this section, the runtime of the proposed method is compared with the previous HA methods by using DS105 and DS107 datasets. As mentioned before, all of the results in this experiment are generated by a PC with certain specifications. Figure 3 illustrates the runtime of the mentioned methods, where runtime of other methods are scaled based on the DHA (runtime of the proposed method is considered as the unit). As depicted in this figure, CAE generated the worse runtime because it concurrently employs modified versions of SRM and SL for functional alignment. Further, SL also includes high time complexity because of the ensemble approach. By considering the performance of the proposed method in the previous sections, it generates acceptable runtime. As mentioned before, the proposed method employs rank-m SVD [10] as well as Incremental SVD [15], which can significantly reduce the time complexity of the optimization procedure [10, 12]."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This paper extended a deep approach for hyperalignment methods in order to provide accurate functional alignment in multi-subject fMRI analysis. Deep Hyperalignment (DHA) can handle fMRI datasets with nonlinearity, high-dimensionality (broad ROI), and a large number of subjects. We have also illustrated how DHA can be used for post-alignment classification. DHA is parametric and uses rank-m SVD and stochastic gradient descent for optimization. Therefore, DHA generates lowruntime on large datasets, and DHA does not require the training data when the functional alignment is computed for a new subject. Further, DHA is not limited by a restricted fixed representational space because the kernel in DHA is a multi-layer neural network, which can separately implement any nonlinear function for each subject to transfer the brain activities to a common space. Experimental studies on multi-subject fMRI analysis confirm that the DHA method achieves superior performance to other state-of-the-art HA algorithms. In the future, we will plan to employ DHA for improving the performance of other techniques in fMRI analysis, e.g. Representational Similarity Analysis (RSA)."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by the National Natural Science Foundation of China (61422204, 61473149, and 61732006), and NUAA Fundamental Research Funds (NE2013105)."
    } ],
    "references" : [ {
      "title" : "Decoding neural representational spaces using multivariate pattern analysis",
      "author" : [ "J.V. Haxby", "A.C. Connolly", "J.S. Guntupalli" ],
      "venue" : "Annual Review of Neuroscience",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Regularized hyperalignment of multi-set fMRI data",
      "author" : [ "H. Xu", "A. Lorbert", "P.J. Ramadge", "J.S. Guntupalli", "J.V. Haxby" ],
      "venue" : "IEEE Statistical Signal Processing Workshop (SSP)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Kernel hyperalignment",
      "author" : [ "A. Lorbert", "P.J. Ramadge" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS). pp. 1790–179",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Joint SVD-Hyperalignment for multisubject FMRI data alignment",
      "author" : [ "P.H. Chen", "J.S. Guntupalli", "J.V. Haxby", "P.J. Ramadge" ],
      "venue" : "IEEE International Workshop on Machine Learning for Signal Processing (MLSP). pp. 1–6,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "A reduceddimension fMRI shared response model",
      "author" : [ "P.H. Chen", "J. Chen", "Y. Yeshurun", "U. Hasson", "J.V. Haxby", "P.J. Ramadge" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS). pp. 460–468,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Local Discriminant Hyperalignment for multi-subject fMRI data alignment",
      "author" : [ "M. Yousefnezhad", "Zhang D" ],
      "venue" : "34th AAAI Conference on Artificial Intelligence. pp. 59–61,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2017
    }, {
      "title" : "Functional geometry alignment and localization of brain",
      "author" : [ "G. Langs", "Y. Tie", "L. Rigolo", "A. Golby", "P. Golland" ],
      "venue" : "areas, 23th Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Multiview LSA: Representation Learning via Generalized CCA. 14th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (HLT-NAACL)",
      "author" : [ "P. Rastogi", "Van D.B", "R. Arora" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Deep Canonical Correlation Analysis",
      "author" : [ "G. Andrew", "R. Arora", "J. Bilmes", "K. Livescu" ],
      "venue" : "30th International Conference on Machine Learning (ICML)",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Learning representations by back-propagating errors",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1986
    }, {
      "title" : "Incremental Singular Value Decomposition of uncertain data with missing values",
      "author" : [ "M. Brand" ],
      "venue" : "European Conference on Computer Vision (ECCV)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2002
    }, {
      "title" : "A tutorial on support vector regression. Statistics and Computing",
      "author" : [ "A.J. Smola" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "The neural basis of loss aversion in decision-making under risk",
      "author" : [ "T.M. Sabrina", "F.R. Craig", "C. Trepel", "R.A. Poldrack" ],
      "venue" : "American Association for the Advancement of Science",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Consistency and variability in functional localisers",
      "author" : [ "K.J. Duncan", "C. Pattamadilok", "I. Knierim", "Devlin", "Joseph T" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "A multi-subject, multi-modal human neuroimaging dataset",
      "author" : [ "D.G. Wakeman", "R.N. Henson" ],
      "venue" : "Scientific Data",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Simultaneous EEG-fMRI reveals temporal evolution of coupling between supramodal cortical attention networks and the brainstem",
      "author" : [ "J.M. Walz" ],
      "venue" : "Journal of Neuroscience",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "On the other hand, this analysis requires authentic functional and anatomical alignments among neuronal activities of different subjects, which these alignments can significantly improve the performance of the developed models [1, 4].",
      "startOffset" : 227,
      "endOffset" : 233
    }, {
      "referenceID" : 3,
      "context" : "On the other hand, this analysis requires authentic functional and anatomical alignments among neuronal activities of different subjects, which these alignments can significantly improve the performance of the developed models [1, 4].",
      "startOffset" : 227,
      "endOffset" : 233
    }, {
      "referenceID" : 0,
      "context" : "However, anatomical alignment can limitedly improve the accuracy because the size, shape and anatomical location of functional loci differ across subjects [1, 2, 7].",
      "startOffset" : 155,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "However, anatomical alignment can limitedly improve the accuracy because the size, shape and anatomical location of functional loci differ across subjects [1, 2, 7].",
      "startOffset" : 155,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "However, anatomical alignment can limitedly improve the accuracy because the size, shape and anatomical location of functional loci differ across subjects [1, 2, 7].",
      "startOffset" : 155,
      "endOffset" : 164
    }, {
      "referenceID" : 6,
      "context" : "Indeed, it has a broad range of applications in neuroscience, such as localization of the Brain’s tumor [8].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "As the widely used functional alignment method [1–7], Hyperalignment (HA) [1] is an ‘anatomy free’ functional alignment method, which can be mathematically formulated as a multiple-set Canonical Correlation Analysis (CCA) problem [2, 3, 5].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "As the widely used functional alignment method [1–7], Hyperalignment (HA) [1] is an ‘anatomy free’ functional alignment method, which can be mathematically formulated as a multiple-set Canonical Correlation Analysis (CCA) problem [2, 3, 5].",
      "startOffset" : 230,
      "endOffset" : 239
    }, {
      "referenceID" : 2,
      "context" : "As the widely used functional alignment method [1–7], Hyperalignment (HA) [1] is an ‘anatomy free’ functional alignment method, which can be mathematically formulated as a multiple-set Canonical Correlation Analysis (CCA) problem [2, 3, 5].",
      "startOffset" : 230,
      "endOffset" : 239
    }, {
      "referenceID" : 4,
      "context" : "As the widely used functional alignment method [1–7], Hyperalignment (HA) [1] is an ‘anatomy free’ functional alignment method, which can be mathematically formulated as a multiple-set Canonical Correlation Analysis (CCA) problem [2, 3, 5].",
      "startOffset" : 230,
      "endOffset" : 239
    }, {
      "referenceID" : 1,
      "context" : "developed the Regularized Hyperalignment (RHA) by utilizing an EM algorithm to iteratively seek the regularized optimum parameters [2].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "developed Singular Value Decomposition Hyperalignment (SVDHA), which firstly provides dimensionality reduction by SVD, and then HA aligns the functional responses in the reduced space [4].",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 4,
      "context" : "introduced Shared Response Model (SRM), which is technically equivalent to Probabilistic CCA [5].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "They also proposed Kernel Hyperalignment (KHA) as a nonlinear alternative in an embedding space for solving the HA limitation [3].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : "Indeed, this method reformulates the SRM as a multi-view autoencoder [5] and then uses the standard SL analysis [9] in order to improve the stability and robustness of the generated classification (cognitive) model [6].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "nonlinearity [3, 6], high-dimensionality [2, 4, 5], and using a large number of subjects [6].",
      "startOffset" : 13,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "nonlinearity [3, 6], high-dimensionality [2, 4, 5], and using a large number of subjects [6].",
      "startOffset" : 41,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "nonlinearity [3, 6], high-dimensionality [2, 4, 5], and using a large number of subjects [6].",
      "startOffset" : 41,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "nonlinearity [3, 6], high-dimensionality [2, 4, 5], and using a large number of subjects [6].",
      "startOffset" : 41,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : "multiple stacked layers of nonlinear transformation, as the kernel function, which is parametric and uses rank-m SVD [10] and Stochastic Gradient Descent (SGD) [13] for optimization.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "The proposed method is related to RHA [2] and MVLSA [10].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 7,
      "context" : "The proposed method is related to RHA [2] and MVLSA [10].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "Further, KHA [3] is equivalent to DHA, where the proposed deep network is employed as the kernel function.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "In addition, DHA can be looked as a multi-set regularized DCCA [11] with stochastic optimization [13].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "Finally, DHA is related to DGCCA [12], when DGCCA is reformulated for functional alignment by using regularization, and rank-m SVD [10].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "the m-th time point for all subjects illustrates the same simulation [2, 3].",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "the m-th time point for all subjects illustrates the same simulation [2, 3].",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "Original HA can be defined based on Inter-Subject Correlation (ISC), which is a classical metric in order to apply functional alignment: [1-4, 7]",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "Original HA can be defined based on Inter-Subject Correlation (ISC), which is a classical metric in order to apply functional alignment: [1-4, 7]",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "Original HA can be defined based on Inter-Subject Correlation (ISC), which is a classical metric in order to apply functional alignment: [1-4, 7]",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "Original HA can be defined based on Inter-Subject Correlation (ISC), which is a classical metric in order to apply functional alignment: [1-4, 7]",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "Original HA can be defined based on Inter-Subject Correlation (ISC), which is a classical metric in order to apply functional alignment: [1-4, 7]",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "For avoiding overfitting, the constrains must be imposed in R [2, 7].",
      "startOffset" : 62,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "For avoiding overfitting, the constrains must be imposed in R [2, 7].",
      "startOffset" : 62,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "Here, the large values illustrate better alignment [2, 3].",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "Here, the large values illustrate better alignment [2, 3].",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "In order to seek an optimum solution, solving (1) may not be the best approach because there is no scale to evaluate the distance between current result and the optimum (fully maximized) solution [2, 4, 7].",
      "startOffset" : 196,
      "endOffset" : 205
    }, {
      "referenceID" : 3,
      "context" : "In order to seek an optimum solution, solving (1) may not be the best approach because there is no scale to evaluate the distance between current result and the optimum (fully maximized) solution [2, 4, 7].",
      "startOffset" : 196,
      "endOffset" : 205
    }, {
      "referenceID" : 5,
      "context" : "In order to seek an optimum solution, solving (1) may not be the best approach because there is no scale to evaluate the distance between current result and the optimum (fully maximized) solution [2, 4, 7].",
      "startOffset" : 196,
      "endOffset" : 205
    }, {
      "referenceID" : 0,
      "context" : "Indeed, the main assumption in the original HA is that the R, ` = 1:S are noisy ‘rotations’ of a common template [1, 9].",
      "startOffset" : 113,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "Indeed, G1 and G2 are located in different positions on the same contour line [5, 7].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "Indeed, G1 and G2 are located in different positions on the same contour line [5, 7].",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "This section proposes an effective approach for optimizing the DHA objective function by using rank-m SVD [10] and SGD [13].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "Then, back-propagation algorithm [14] is used to update the network parameters.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 7,
      "context" : "Consequently, functional alignments are stacked in a S × S matrix and maximize a certain matrix norm for that matrix [10, 12].",
      "startOffset" : 117,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "In order to scale DHA approach, this paper employs the rank-m SVD [10] of the mapped neural activities as follows:",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "Based on (7), the projection matrix for `-th subject can be generated as follows: [10]",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "where P ∈ RT×T is symmetric and idempotent [10, 12], and diagonal matrix D ∈ Rm×m is D ( D )> = ( Σ )>( Σ ( Σ )> + I )−1 Σ.",
      "startOffset" : 43,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "Further, the sum of projection matrices can be defined as follows, where ÃÃ> is the Cholesky decomposition [10] of A:",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "Further, the matrix G that we are interested in finding, can be calculated by the left singular vectors of Ã = GΣ̃Ψ̃>, where G>G = I [10].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : "This paper utilizes Incremental SVD [15] for calculating these left singular vectors.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : "Calculate G by applying Incremental SVD [15] to Ã = GΣ̃Ψ̃>.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "This paper employs the back-propagation algorithm (backprop() function) [14] as well as Lemma 3 for this step.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "Like previous studies [1–7, 9], this paper employs the ν-SVM algorithms [16] for generating the classification model.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "Indeed, we use the binary ν-SVM for datasets with just two categories of stimuli and multi-label ν-SVM [3, 16] as the multi-class approach.",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "Indeed, we use the binary ν-SVM for datasets with just two categories of stimuli and multi-label ν-SVM [3, 16] as the multi-class approach.",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "Table 1: Accuracy of HA methods in post-alignment classification by using simple task datasets ↓Algorithms, Datasets→ DS005 DS105 DS107 DS116 DS117 ν-SVM [17] 71.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 12,
      "context" : "94 Table 2: Area under the ROC curve (AUC) of different HA methods in post-alignment classification by using simple task datasets ↓Algorithms, Datasets→ DS005 DS105 DS107 DS116 DS117 ν-SVM [17] 68.",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : "Further, performances of the standard HA [1], RHA [2], KHA [3], SVDHA [4], SRM [5], and SL [9] are reported as state-of-the-arts HA methods.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Further, performances of the standard HA [1], RHA [2], KHA [3], SVDHA [4], SRM [5], and SL [9] are reported as state-of-the-arts HA methods.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "Further, performances of the standard HA [1], RHA [2], KHA [3], SVDHA [4], SRM [5], and SL [9] are reported as state-of-the-arts HA methods.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "Further, performances of the standard HA [1], RHA [2], KHA [3], SVDHA [4], SRM [5], and SL [9] are reported as state-of-the-arts HA methods.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "Further, performances of the standard HA [1], RHA [2], KHA [3], SVDHA [4], SRM [5], and SL [9] are reported as state-of-the-arts HA methods.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "In this paper, the results of HA algorithm is generated by employing Generalized CCA proposed in [10].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "In addition, regularized parameters (α, β) in RHA are optimally assigned based on [2].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "Further, KHA algorithm is used by the Gaussian kernel, which is evaluated as the best kernel in the original paper [3].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "In addition, ROI is defined based on the original paper [17].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "In addition, ROI is defined based on the original paper [20].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "Figure 1 depicts the generated results, where the voxels in ROI are ranked by the method proposed in [1] based on their neurological priorities same as previous studies [1, 4, 7, 9].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Figure 1 depicts the generated results, where the voxels in ROI are ranked by the method proposed in [1] based on their neurological priorities same as previous studies [1, 4, 7, 9].",
      "startOffset" : 169,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "Figure 1 depicts the generated results, where the voxels in ROI are ranked by the method proposed in [1] based on their neurological priorities same as previous studies [1, 4, 7, 9].",
      "startOffset" : 169,
      "endOffset" : 181
    }, {
      "referenceID" : 5,
      "context" : "Figure 1 depicts the generated results, where the voxels in ROI are ranked by the method proposed in [1] based on their neurological priorities same as previous studies [1, 4, 7, 9].",
      "startOffset" : 169,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "Here, the performance of the proposed method is compared with SVDHA [4], SRM [5], and CAE [6] as the state-of-the-art HA techniques, which can apply feature selection before generating a classification model.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "Here, the performance of the proposed method is compared with SVDHA [4], SRM [5], and CAE [6] as the state-of-the-art HA techniques, which can apply feature selection before generating a classification model.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 11,
      "context" : "Here, multi-label ν-SVM [16] is used for generating the classification models after each of the mentioned methods applied on preprocessed fMRI images for functional alignment.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "As mentioned before, the proposed method employs rank-m SVD [10] as well as Incremental SVD [15], which can significantly reduce the time complexity of the optimization procedure [10, 12].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "As mentioned before, the proposed method employs rank-m SVD [10] as well as Incremental SVD [15], which can significantly reduce the time complexity of the optimization procedure [10, 12].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "As mentioned before, the proposed method employs rank-m SVD [10] as well as Incremental SVD [15], which can significantly reduce the time complexity of the optimization procedure [10, 12].",
      "startOffset" : 179,
      "endOffset" : 187
    } ],
    "year" : 2017,
    "abstractText" : "This paper proposes Deep Hyperalignment (DHA) as a regularized, deep extension, scalable Hyperalignment (HA) method, which is well-suited for applying functional alignment to fMRI datasets with nonlinearity, high-dimensionality (broad ROI), and a large number of subjects. Unlink previous methods, DHA is not limited by a restricted fixed kernel function. Further, it uses a parametric approach, rank-m Singular Value Decomposition (SVD), and stochastic gradient descent for optimization. Therefore, DHA has a suitable time complexity for large datasets, and DHA does not require the training data when it computes the functional alignment for a new subject. Experimental studies on multi-subject fMRI analysis confirm that the DHA method achieves superior performance to other state-of-the-art HA algorithms.",
    "creator" : null
  }
}