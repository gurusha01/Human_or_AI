{
  "name" : "a34bacf839b923770b2c360eefa26748.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Predicting User Activity Level In Point Processes With Mass Transport Equation",
    "authors" : [ "Yichen Wang", "Xiaojing Ye", "Hongyuan Zha", "Le Song" ],
    "emails" : [ "yichen.wang@gatech.edu,", "xye@gsu.edu", "zha@cc.gatech.edu", "lsong@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Online social platforms, such as Facebook and Twitter, enable users to post opinions, share information, and influence peers. Recently, user-generated event data archived in fine-grained temporal resolutions are becoming increasingly available, which calls for expressive models and algorithms to understand, predict and distill knowledge from complex dynamics of these data. Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].\nA fundamental task in social networks is to predict user activity levels based on learned point process models. Mathematically, the goal is to compute E[f(N(t))], where N(·) is a given point process that is learned from user behaviors, t is a fixed future time, and f is an application-dependent function. A framework for doing this is critically important. For example, for social networking services, an accurate inference of the number of reshares of a post enables the network moderator to detect trending posts and improve its content delivery networks [13, 32]; an accurate estimate of the change of network topology (the number of new followers of a user) facilitates the moderator to identify influential users and suppress the spread of terrorist propaganda and cyber-attacks [12]; an accurate inference of the activity level (number of posts in the network) allows us to gain fundamental insight into the predictability of collective behaviors [22]. Moreover, for online merchants such as Amazon, an accurate estimate of the number of future purchases of a product helps optimizing future advertisement placements [10, 25].\nDespite the prevalence of prediction problems, an accurate prediction is very challenging for two reasons. First, the function f is arbitrary. For instance, to evaluate the homogeneity of user activities, we set f(x) = x log(x) to compute the Shannon entropy; to measure the distance between a predicted activity level and a target x⇤, we set f(x) = (x x⇤)2. However, most works [8, 9, 13, 30, 31, 32] are problem specific and only designed for the simple task with f(x) = x; hence these works are\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nnot generalizable. Second, point process models typically have intertwined stochasticity and can co-evolve over time [12, 25], e.g., in the influence propagation problem, the information diffusion over networks can change the structure of networks, which adversely influences the diffusion process [12]. However, previous works often ignore parts of the stochasticity in the intensity function [29] or make heuristic approximations [13, 32]. Hence, there is an urgent need for a method that is applicable to an arbitrary function f and keeps all the stochasticity in the process, which is largely nonexistent to date.\nWe propose HYBRID, a generic framework that provides an efficient estimator of the probability mass of point processes. Figure 1 illustrates our framework. We also make the following contributions:\n• Unifying framework. Our framework is applicable to general point processes and does not depend on specific parameterization of intensity functions. It incorporates all stochasticity in point processes and is applicable to prediction tasks with an arbitrary function f .\n• Technical challenges. We reformulate the prediction problem and design a random variable with reduced variance. To derive an analytical form of this random variable, we also propose a mass transport equation to compute the conditional probability mass of point processes. We further transform this equation to an Ordinary Differential Equation and provide a scalable algorithm.\n• Superior performance. Our framework significantly reduces the sample size to estimate the probability mass function of point processes in real-world applications. For example, to infer the number of tweeting and retweeting events of users in the co-evolution model of information diffusion and social link creation [12], our method needs 103 samples and 14.4 minutes, while Monte Carlo needs 106 samples and 27.8 hours to achieve the same relative error of 0.1."
    }, {
      "heading" : "2 Background and preliminaries",
      "text" : "Point processes. A temporal point process [1] is a random process whose realization consists of a set of discrete events {tk}, localized in time. It has been successfully applied to model user behaviors in social networks [16, 17, 19, 23, 24, 25, 28, 30]. It can be equivalently represented as a counting process N(t), which records the number of events on [0, t]. The counting process is a right continuous step function, i.e., if an event happens at t, N(t) N(t ) = 1.\nLet Ht = {tk|tk < t} be the history of events happened up to time t. An important way to characterize point processes is via the conditional intensity function (t) := (t|Ht ), a stochastic model for the time of the next event given the history. Formally, (t) is the conditional probability of observing an event in [t, t + dt) given events on [0, t), i.e., P {event in [t, t+ dt)|Ht } = E[dN(t)|Ht ] := (t)dt, where dN(t) 2 {0, 1}.\nThe intensity function is designed to capture the phenomena of interest. Some useful forms include (i) Poisson process: the intensity is a deterministic function, and (ii) Hawkes process [15]: it captures the mutual excitation phenomena between events and its intensity is parameterized as\n(t) = ⌘ + ↵ X\ntk2Ht (t tk), (1)\nwhere ⌘ > 0 is the baseline intensity; the trigging kernel (t) = exp( t) models the decay of past events’ influence over time; ↵ > 0 quantifies the strength of influence from each past event. Here, the occurrence of each historical event increases the intensity by a certain amount determined by (t) and ↵, making (t) history-dependent and a stochastic process by itself.\nMonte Carlo (MC). To compute the probability mass of a point process, MC simulates n realizations of history {Hit} using the thinning algorithm [20]. The number of events in sample i is defined as N i(t) = |Hit|. Let (x, t) := P[N(t) = x], where x 2 N, be the probability mass. Then its estimator ̂mcn (x, t) and the estimator µ̂mcn (t) for µ(t) := E[f(N(t))] are defined as ̂mcn (x, t) = 1 n P i I[N i(t) = x] and µ̂mcn (t) = 1n P i f(N i(t)). The root mean square error (RMSE) is defined as\n\"(µ̂mcn (t)) = p E[µ̂mcn (t) µ(t)]2 = p VAR[f(N(t))]/n. (2)"
    }, {
      "heading" : "3 Solution overview",
      "text" : "Given an arbitrary point process N(t) that is learned from data, existing prediction methods for computing E[f(N(t))] have three major limitations:\n• Generalizability. Most methods [8, 9, 13, 30, 31, 32] only predict E[N(t)] and are not generalizable to an arbitrary function f . Moreover, they typically rely on specific parameterizations of the intensity functions, such as the reinforced Poisson process [13] and Hawkes process [5, 32]; hence they are not applicable to general point processes.\n• Approximation and heuristics. These works also ignore parts of the stochasticity in the intensity functions [29] or make heuristic approximations to the point process [13, 32]. Hence the accuracy is limited by the approximations and heuristic corrections.\n• Large sample size. The MC method overcomes the above limitations since it has an unbiased estimator of the probability mass. However, the high stochasticity in point processes leads to a large value of VAR[f(N(t))], which requires a large number of samples to achieve a small error.\nTo address these challenges, we propose a generic framework with a novel estimator of the probability mass, which has a smaller sample size than MC. Our framework has the following key steps.\nI. New random variable. We design a random variable g(Ht ), a conditional expectation given the history. Its variance is guaranteed to be smaller than that of f(N(t)). For a fixed number of samples, the error of MC is decided by the variance of the random variable of interest, as shown in (2). Hence, to achieve the same error, applying MC to estimate the new objective EHt [g(Ht )] requires smaller number of samples compared with the procedure that directly estimates E[f(N(t))]. II. Mass transport equation. To compute g(Ht ), we derive a differential-difference equation that describes the evolutionary dynamics of the conditional probability mass P[N(t) = x|Ht ]. We further formulate this equation as an Ordinary Differential Equation, and provide a scalable algorithm."
    }, {
      "heading" : "4 Hybrid inference machine with probability mass transport",
      "text" : "In this section, we present technical details of our framework. We first design a new random variable for prediction; then we propose a mass transport equation to compute this random variable analytically. Finally, we combine the mass transport equation with the sampling scheme to compute the probability mass function of general point processes and solve prediction tasks with an arbitrary function f ."
    }, {
      "heading" : "4.1 New random variable with reduced variance",
      "text" : "We reformulate the problem and design a new random variable g(Ht ), which has a smaller variance than f(N(t)) and the same expectation. To do this, we express E[f(N(t))] as an iterated expectation\nE[f(N(t))] = EHt h EN(t)|Ht ⇥ f(N(t))|Ht ⇤i = EHt h g(Ht ) i , (3)\nwhere EHt is w.r.t. the randomness of the history and EN(t)|Ht is w.r.t. the randomness of the point process given the history. We design the random variable as a conditional expectation given the history: g(Ht ) = EN(t)|Ht [f(N(t))|Ht ]. Theorem 1 shows that it has a smaller variance.\nTheorem 1. For time t > 0 and an arbitrary function f , we have VAR[g(Ht )] < VAR[f(N(t))].\nTheorem 1 extends the Rao-Blackwell (RB) theorem [3] to point processes. RB says that if ✓̂ is an estimator of a parameter ✓ and T is a sufficient statistic for ✓; then VAR[E[✓̂|T ]] 6 VAR[✓̂], i.e., the sufficient statistic reduces uncertainty of ✓̂. However, RB is not applicable to point processes since it studies a different problem (improving the estimator of a distribution’s parameter), while we focus on the prediction problem for general point processes, which introduces two new technical challenges:\n(i) Is there a definition in point processes whose role is similar to the sufficient statistic in RB? Our first contribution shows that the history Ht contains all the necessary information in a point process and reduces the uncertainty of N(t). Hence, g(Ht ) is an improved variable for prediction. Moreover, in contrast to the RB theorem, the inequality in Theorem 1 is strict because the counting process N(t) is right-continuous in time t and not predictable [4] (a predictable process is measurable w.r.t. Ht , such as the processes that are left-continuous). Appendix C contains details on the proof.\n(ii) Is g(Ht ) computable for general point processes and an arbitrary function f? An efficient computation will enable us to estimate EHt [g(Ht )] using the sampling method. Specifically, let µ̂n(t) = 1 n P i g(H i t ) be the estimator computed from n samples; then from the definition of RMSE in (2), this estimator has smaller error than MC: \"(µ̂n(t)) < \"(µ̂mcn (t)).\nHowever, the challenge in our new formulation is that it seems very hard to compute this conditional expectation, as one typically needs another round of sampling, which is undesirable as it will increase the variance of the estimator. To address this challenge, next we propose a mass transport equation."
    }, {
      "heading" : "4.2 Transport equation for conditional probability mass function",
      "text" : "We present a novel mass transport equation that computes the conditional probability mass ̃(x, t) := P[N(t) = x|Ht ] of general point processes. With this definition, we derive an analytical expression for the conditional expectation: g(Ht ) = P x f(x)̃(x, t). The transport equation is as follows.\nTheorem 2 (Mass Transport Equation for Point Processes). Let (t) := (t|Ht ) be the conditional intensity function of the point process N(t) and ̃(x, t) := P[N(t) = x|Ht ] be its conditional probability mass function; then ̃(x, t) satisfies the following differential-difference equation:\ñt(x, t) \"\nrate of change in conditional mass\n:= @̃(x, t)\n@t =\n8 ><\n>:\n(t)̃(x, t) if x = 0 (t)̃(x, t)| {z }\nloss in mass, at rate (t) + (t)̃(x 1, t)| {z } gain in mass, at rate (t) if x = 1, 2, 3, · · · (4)\nProof sketch. For the simplicity of notation, we set the right-hand-side of (4) to be F [̃], where F is a functional operator on ̃. We also define the inner product between functions u : N ! R and v : N ! R as (u, v) := P x u(x)v(x). The main idea in our proof is to show that the equality (v, ̃t) = (v,F [̃]) holds for any test function v; then ̃t = F [̃] follows from the fundamental lemma of the calculus of variations [14]. Specifically, the proof contains two parts as follows.\nWe first prove (v, ̃t) = (B[v], ̃), where B[v] is a functional operator defined as B[v] = (v(x + 1) v(x)) (t). This equality can be proved by the property of point processes and the definition of conditional mass. Second, we show (B[v], ̃) = (v,F [̃]) using a variable substitution technique. Mathematically, this equality means B and F are adjoint operators on the function space. Combining these two equalities yields the mass transport equation. Appendix A contains details on the proof.\nMass transport dynamics. This differential-difference equation describes the time evolution of the conditional mass. Specifically, the differential term ̃t, i.e., the instantaneous rate of change in the probability mass, is equal to a first order difference equation on the right-hand-side. This difference equation is a summation of two terms: (i) the negative loss of its own probability mass ̃(x, t) at rate (t), and (ii) the positive gain of probability mass ̃(x 1, t) from last state x 1 at rate (t). Moreover, since initially no event happens with probability one, we have ̃(x, 0) = I[x = 0]. Solving this transport equation on [0, t] essentially transports the initial mass to the mass at time t.\nAlgorithm 1: CONDITIONAL MASS FUNCTION Input: Ht = {tk}Kk=1, ⌧ , set t = tK+1 Output: Conditional probability mass function ̃(t) for k = 0, · · ·K do\nConstruct (s) and Q(s) on [tk, tk+1] ; ̃(tk+1) = ODE45[̃(tk),Q(s), ⌧)] (RK Alg);\nend Set ̃(t) = ̃(tK+1)\nAlgorithm 2: HYBRID MASS TRANSPORT Input: Sample size n, time t, ⌧ Output: µ̂n(t), ̂n(x, t) Generate n samples of point process: H\ni t n i=1\n; for i = 1, · · · , n do\ñi(x, t) = COND-MASS-FUNC(Hit , ⌧); end ̂n(x, t) = 1n P i ̃ i(x, t), µ̂n(t) = P x f(x)̂n(x, t)"
    }, {
      "heading" : "4.3 Mass transport as a banded linear Ordinary Differential Equation (ODE)",
      "text" : "To efficiently solve the mass transport equation, we reformulate it as a banded linear ODE. Specifically, we set the upper bound for x to be M , and set ̃(t) to be a vector that includes the value of ̃(x, t) for each integer x: ̃(t) = (̃(0, t), ̃(1, t), · · · , ̃(M, t))>. With this representation of the conditional mass, the mass transport equation in (4) can be expressed as a simple banded linear ODE:\ñ(t)0 = Q(t)̃(t), (5)\nwhere ̃(t)0 = (̃t(0, t), · · · , ̃t(M, t))>, and the matrix Q(t) is a sparse bi-diagonal matrix with Qi,i = (t) and Qi 1,i = (t). The following equation visualizes the ODE in (5) when M = 2.\n0 @ ̃t(0, t) ̃t(1, t) ̃t(2, t) 1 A =\n(t) (t) (t)\n(t) (t)\n!0\n@ ̃(0, t) ̃(1, t) ̃(2, t)\n1\nA . (6)\nThis dynamic ODE is a compact representation of the transport equation in (4) and M decides the dimension of the ODE in (5). In theory, M can be unbounded. However, the conditional probability mass is tends to zero when M becomes large. Hence, in practice we choose a finite support {0, 1, · · · ,M} for the conditional probability mass function. To choose a proper M , we generate samples from the point process. Suppose the largest number of events in the samples is L, we set M = 2L such that it is reasonably large. Next, with the initial probability mass ̃(t0) = (1, 0, · · · , 0)>, we present an efficient algorithm to solve the ODE."
    }, {
      "heading" : "4.4 Scalable algorithm for solving the ODE",
      "text" : "We present the algorithm that transports the initial mass ̃(t0) to ̃(t) by solving the ODE.\nSince the intensity function is history-dependent and has a discrete jump when an event happens at time tk, the matrix Q(t) in the ODE is discontinuous at tk. Hence we split [0, t] into intervals [tk, tk+1]. On each interval, the intensity is continuous and we can use the classic numerical Runge-Kutta (RK) method [7] to solve the ODE. Figure 2 illustrates the overall algorithm.\nOur algorithm works as follows. First, with the initial intensity on [0, t1] and ̃(t0) as input, the RK method solves the ODE on [0, t1] and outputs ̃(t1). Since an event happens at t1, the intensity is updated on [t1, t2]. Next, with the updated intensity and ̃(t1) as the initial value, the RK method solves the ODE on [t1, t2] and outputs ̃(t2). This procedure repeats for each [tk, tk+1] until time t.\nNow we present the RK method that solves the ODE on each interval [tk, tk+1]. RK divides this interval into equally-spaced subintervals [⌧i, ⌧i+1], for i = 0, · · · , I and ⌧ = ⌧i+1 ⌧i. It then conducts linear extrapolation on each subinterval. It starts from ⌧0 = tk and uses ̃(⌧0) and the approximation of the gradient ̃(⌧0)0 to compute ̃(⌧1). Next, ̃(⌧1) is taken as the initial value and the process is repeated until ⌧I = tk+1. Appendix D contains details of this method.\nThe RK method approximates the gradient ̃(t)0 with different levels of accuracy, called states s. When s = 1, it is the Euler method, which uses the first order approximation ̃(⌧i+1) ̃(⌧i)/ ⌧ .\nWe use the ODE45 solver in MATLAB and choose the stage s = 4 for RK. Moreover, the main computation in the RK method comes from the matrix-vector product. Since the matrix Q(t) is sparse and bi-diagonal with O(M) non-zero elements, the cost for this operation is only O(M)."
    }, {
      "heading" : "4.5 Hybrid inference machine with mass transport equation",
      "text" : "With the conditional probability mass, we are now ready to express g(Ht ) in closed form and estimate EHt [g(Ht )] using the MC sampling method. We present our framework HYBRID:\n(i) Generate n samples {Hit } from a point process N(t) with a stochastic intensity (t). (ii) For each sample Hit , we compute the value of intensity function (s|H i s ), for each s 2\n[0, t]; then we solve (5) to compute the conditional probability mass ̃i(x, t). (iii) We obtain the estimator of the probability mass function (x, t) and µ(t) by taking the\naverage: ̂n(x, t) = 1n Pn i=1 ̃ i(x, t), µ̂n(t) = P x f(x)̂n(x, t)\nAlgorithm 2 summarizes the above procedure. Next, we discuss two properties of HYBRID.\nFirst, our framework efficiently uses all event information in each sample. In fact, each event tk influences the transport rate of the conditional probability mass (Figure 2). This feature is in sharp contrast to MC that only uses the information of the total number of events and neglects the differences in event times. For instance, the two samples in Figure 1(a) both have three events and MC treats them equally; hence its estimator is an indicator function ̂mcn (x, t) = I[x = 3]. However, for HYBRID, these samples have different event information and conditional probability mass functions, and our estimator in Figure 1(d) is much more informative than an indicator function.\nMoreover, our estimator for the probability mass is unbiased if we can solve the mass transport equation in (4) exactly. To prove this property, we show that the following equality holds for an arbitrary function f : (f, ) = E[f(N(t))] = EHt [g(Ht )] = (f,EHt [̃]). Then EHt [̂n] = follows from the fundamental lemma of the calculus of variations [14]. Appendix B contains detailed derivations. In practice, we choose a reasonable finite support for the conditional probability mass in order to solve the mass transport ODE in (5). Hence our estimator is nearly unbiased."
    }, {
      "heading" : "5 Applications and extensions to multi-dimensional point processes",
      "text" : "In this section, we present two real world applications, where the point process models have intertwined stochasticity and co-evolving intensity functions.\nPredicting the activeness and popularity of users in social networks. The co-evolution model [12] uses a Hawkes process Nus(t) to model information diffusion (tweets/retweets), and a survival process Aus(t) to model the dynamics of network topology (link creation process). The intensity of Nus(t) depends on the network topology Aus(t), and the intensity of Aus(t) also depends on Nus(t); hence these processes co-evolve over time. We focus on two tasks in this model: (i) inferring the activeness of a user by E[ P u Nus(t)], which is the number of tweets and retweets from user s; and (ii) inferring\nthe popularity of a user by E[ P\nu Aus(t)], which is the number of new links created to the user.\nPredicting the popularity of items in recommender systems. Recent works on recommendation systems [10, 25] use a point process Nui(t) to model user u’s sequential interaction with item i. The intensity function ui(t) denotes user’s interest to the item. As users interact with items over time, the user latent feature uu(t) and item latent feature iu(t) co-evolve over time, and are mutually dependent [25]. The intensity is parameterized as ui(t) = ⌘ui+uu(t)>ii(t), where ⌘ui is a baseline term representing the long-term preference, and the tendency for u to interact with i depends on the compatibility of their instantaneous latent features uu(t)>ii(t). With this model, we can infer an item’s popularity by evaluating E[ P u Nui(t)], which is the number of events happened to item i.\nTo solve these prediction tasks, we extend the transport equation to the multivariate case. Specifically, we create a new stochastic process x(t) = P u Nus(t) and compute its conditional mass function. Theorem 3 (Mass Transport for Multidimensional Point Processes). Let Nus(t) be the point process with intensity us(t), x(t) = PU u=1 Nus(t), and ̃(x, t) = P[x(t) = x|Ht ] be the conditional\nprobability mass of x(t); then ̃ satisfies: ̃t = P u us(t) ̃(x, t) + P u us(t) ̃(x 1, t).\nTo compute the conditional probability mass, we also solve the ODE in (5), where the diagonal and off-diagonal of Q(t) is now the negative and positive summation of intensities in all dimensions."
    }, {
      "heading" : "6 Experiments",
      "text" : "In this section, we evaluate the predictive performance of HYBRID in two real world applications in Section 5 and a synthetic dataset. We use the following metrics:\n(i) Mean Average Percentage Error (MAPE). Given a prediction time t, we compute the MAPE |µ̂n(t) µ(t)|/µ(t) between the estimated value and the ground truth.\n(ii) Rank correlation. For all users/items, we obtain two lists of ranks according to the true and estimated value of user activeness/user popularity/item popularity. The accuracy is evaluated by the Kendall-⌧ rank correlation [18] between two lists."
    }, {
      "heading" : "6.1 Experiments on real world data",
      "text" : "We show HYBRID has both accuracy and efficiency improvement in predicting the activeness and popularity of users in social networks and predicting the popularity of items in recommender systems.\nCompetitors. We use 103 samples for HYBRID and compare it with the following the state of the art. • SEISMIC [32]. It defines a self-exciting process with a post infectiousness factor. It uses the\nbranching property of Hawkes process and heuristic corrections for prediction. • RPP [13]. It adds a reinforcement coefficient to Poisson process that depicts the self-excitation\nphenomena. It sets dN(t) = (t)dt and solves a deterministic equation for prediction. • FPE [29]. It uses a deterministic function to approximate the stochastic intensity function. • MC-1E3. It is the MC sampling method with 103 samples (same as these for HYBRID), and\nMC-1E6 uses 106 samples."
    }, {
      "heading" : "6.1.1 Predicting the activeness and popularity of users in social networks",
      "text" : "We use a Twitter dataset [2] that contains 280,000 users with 550,000 tweet, retweet, and link creation events during Sep. 21 - 30, 2012. This data is previously used to validate the network co-evolution model [12]. The parameters for tweeting/retweeting processes and link creation process are learned using maximum likelihood estimation [12]. SEISMIC and RPP are not designed for the popularity prediction task since they do not consider the evolution of network topology. We use p proportion of total data as the training data to learn parameters of all methods, and the rest as test data. We make predictions for each user and report the averaged results.\nPredictive performance. Figure 3(a) shows that MAPE increases as test time increases, since the model’s stochasticity increases. HYBRID has the smallest error. Figure 3(b) shows that MAPE decreases as training data increases since model parameters are more accurate. Moreover, HYBRID is more accurate than SEISMIC and FPE with only 60% of training data, while these works need 80%. Thus, we make accurate predictions by observing users in the early stage. This feature is important for network moderators to identify malicious users and suppress the propagation undesired content.\nMoreover, the consistent performance improvement shows two messages: (i) considering all the randomness is important. HYBRID is 2⇥ more accurate than SEISMIC and FPE because HYBRID naturally considers all the stochasticity, but SEISMIC, FPE, and RPP need heuristics or approximations that discard parts of the stochasticity; (ii) sampling efficiently is important. To consider all the stochasticity, we need to use the sampling scheme, and HYBRID has a much smaller sample size. Specifically, HYBRID uses the same 103 samples, but has 4⇥ error reduction compared with MC-1E3. MC-1E6 has a similar predictive performance as HYBRID, but needs 103⇥ more samples.\nScalability. How does the reduction in sample size improve the speed? Figure 5(a) shows that as the error decreases from 0.5 to 0.1, MC has higher computation cost, since it needs much more samples than HYBRID to achieve the same error. We include the plots of HYBRID in (c). In particular, to achieve the error of 0.1, MC needs 106 samples in 27.8 hours, but HYBRID only needs 14.4 minutes with 103 samples. We use the machine with 16 cores, 2.4 GHz Intel Core i5 CPU and 64 GB memory.\nRank correlation. We rank all users according to the predicted level of activeness and level of popularity separately. Figure 6(a,b) show that HYBRID performs the best with the accuracy around 80%, and it consistently identifies around 30% items more correctly than FPE on both tasks."
    }, {
      "heading" : "6.1.2 Predicting the popularity of items in recommender systems",
      "text" : "In the recommendation system setting, we use two datasets from [25]. The IPTV dataset contains 7,100 users’ watching history of 436 TV programs in 11 months, with around 2M events. The Reddit dataset contains online discussions of 1,000 users in 1,403 groups, with 10,000 discussion events. The predictive and scalability performance are consistent with the application in social networks. Figure 4 shows that HYBRID is 15% more accurate than FPE and 20% than SEISMIC. Figure 5 also shows that HYBRID needs much smaller amount of computation time than MC-1E6. To achieve the error of 0.1, it takes 9.8 minutes for HYBRID and 7.5 hours for MC-1E6. Figure 6(c,d) show that HYBRID achieves the rank correlation accuracy of 77%, with 20% improvement over FPE."
    }, {
      "heading" : "6.2 Experiments on synthetic data",
      "text" : "We compare HYBRID with MC in two aspects: (i) the significance of the reduction in the error and sample size, and (ii) estimators of the probability mass function. We study a Hawkes process and set the parameters of its intensity function as ⌘ = 1.2, and ↵ = 0.5. We fix the prediction time to be t = 30. The ground truth is computed with 108 samples from MC simulations.\nError vs. number of samples. In four tasks with different f , Figure 7 shows that given the same number of samples, HYBRID has a smaller error. Moreover, to achieve the same error, HYBRID needs 100⇥ less samples than MC. In particular, to achieve the error of 0.01, (a) shows HYBRID needs 103 and MC needs 105 samples; (b) shows HYBRID needs 104 and MC needs 106 samples.\nProbability mass functions. We compare our estimator of the probability mass with MC. Figure 8(a,b) show that our estimator is much smoother than MC, because our estimator is the average of conditional probability mass functions, which are computed by solving the mass transport equation. Moreover, our estimator centers around 85, which is the ground truth of E[N(t)], while that of MC centers around 80. Hence HYBRID is more accurate. We also plot two conditional mass functions in (c,d). The average of 1000 conditional mass functions yields (a). Thus, this averaging procedure in HYBRID adjusts the shape of the estimated probability mass. On the contrary, given one sample, the estimator in MC is just an indicator function and cannot capture the shape of the probability mass."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have proposed HYBRID, a generic framework with a new formulation of the prediction problem in point processes and a novel mass transport equation. This equation efficiently uses the event information to update the transport rate and compute the conditional mass function. Moreover, HYBRID is applicable to general point processes and prediction tasks with an arbitrary function f . Hence it can take any point process models as input, and the predictive performance of our framework can be further improved with the advancement of point process models. Experiments on real world and synthetic data demonstrate that HYBRID outperforms the state of the art both in terms of accuracy and efficiency. There are many interesting lines for future research. For example, HYBRID can be generalized to marked point processes [4], where a mark is observed along with the timing of each event.\nAcknowledgements. This project was supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF CNS-1704701, ONR N00014-15-1-2340, DMS-1620342, CMMI-1745382, IIS-1639792, IIS-1717916, NVIDIA, Intel ISTC and Amazon AWS."
    } ],
    "references" : [ {
      "title" : "Survival and event history analysis: a process point of view",
      "author" : [ "O. Aalen", "O. Borgan", "H. Gjessing" ],
      "venue" : "Springer",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Co-evolutionary dynamics in social networks: A case study of twitter",
      "author" : [ "D. Antoniades", "C. Dovrolis" ],
      "venue" : "arXiv preprint arXiv:1309.6001",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Conditional expectation and unbiased sequential estimation",
      "author" : [ "D. Blackwell" ],
      "venue" : "The Annals of Mathematical Statistics, pages 105–110",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1947
    }, {
      "title" : "Hawkes process: Fast calibration",
      "author" : [ "J. Da Fonseca", "R. Zaatour" ],
      "venue" : "application to trade clustering, and diffusive limit. Journal of Futures Markets, 34(6):548–579",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep coevolutionary network: Embedding user and item features for recommendation",
      "author" : [ "H. Dai", "Y. Wang", "R. Trivedi", "L. Song" ],
      "venue" : "arXiv preprint arXiv:1609.03675",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A family of embedded runge-kutta formulae",
      "author" : [ "J.R. Dormand", "P.J. Prince" ],
      "venue" : "Journal of computational and applied mathematics, 6(1):19–26",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Scalable influence estimation in continuoustime diffusion networks",
      "author" : [ "N. Du", "L. Song", "M. Gomez-Rodriguez", "H. Zha" ],
      "venue" : "NIPS",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning networks of heterogeneous influence",
      "author" : [ "N. Du", "L. Song", "A.J. Smola", "M. Yuan" ],
      "venue" : "NIPS",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Time sensitive recommendation from recurrent user activities",
      "author" : [ "N. Du", "Y. Wang", "N. He", "L. Song" ],
      "venue" : "NIPS, pages 3492–3500",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Real analysis and probability",
      "author" : [ "R.M. Dudley" ],
      "venue" : "Cambridge University Press, Cambridge, UK",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Coevolve: A joint point process model for information diffusion and network co-evolution",
      "author" : [ "M. Farajtabar", "Y. Wang", "M. Gomez-Rodriguez", "S. Li", "H. Zha", "L. Song" ],
      "venue" : "NIPS, pages 1954–1962",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Modeling and predicting retweeting dynamics on microblogging platforms",
      "author" : [ "S. Gao", "J. Ma", "Z. Chen" ],
      "venue" : "WSDM",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "et al",
      "author" : [ "I.M. Gelfand", "R.A. Silverman" ],
      "venue" : "Calculus of variations. Courier Corporation",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Spectra of some self-exciting and mutually exciting point processes",
      "author" : [ "A.G. Hawkes" ],
      "venue" : "Biometrika, 58(1):83–90",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Fast and simple optimization for poisson likelihood models",
      "author" : [ "N. He", "Z. Harchaoui", "Y. Wang", "L. Song" ],
      "venue" : "arXiv preprint arXiv:1608.01264",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Hawkestopic: A joint model for network inference and topic modeling from text-based cascades",
      "author" : [ "X. He", "T. Rekatsinas", "J. Foulds", "L. Getoor", "Y. Liu" ],
      "venue" : "ICML, pages 871–880",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A new measure of rank correlation",
      "author" : [ "M.G. Kendall" ],
      "venue" : "Biometrika, 30(1/2):81–93",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1938
    }, {
      "title" : "A multitask point process predictive model",
      "author" : [ "W. Lian", "R. Henao", "V. Rao", "J.E. Lucas", "L. Carin" ],
      "venue" : "ICML, pages 2030–2038",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On lewis’ simulation method for point processes",
      "author" : [ "Y. Ogata" ],
      "venue" : "IEEE Transactions on Information Theory, 27(1):23–31",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Markov-modulated marked poisson processes for check-in data",
      "author" : [ "J. Pan", "V. Rao", "P. Agarwal", "A. Gelfand" ],
      "venue" : "ICML, pages 2244–2253",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Epidemic processes in complex networks",
      "author" : [ "R. Pastor-Satorras", "C. Castellano", "P. Van Mieghem", "A. Vespignani" ],
      "venue" : "Reviews of modern physics, 87(3):925",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Content-based modeling of reciprocal relationships using hawkes and gaussian processes",
      "author" : [ "X. Tan", "S.A. Naqvi", "A.Y. Qi", "K.A. Heller", "V. Rao" ],
      "venue" : "UAI, pages 726–734",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Know-evolve: Deep temporal reasoning for dynamic knowledge graphs",
      "author" : [ "R. Trivedi", "H. Dai", "Y. Wang", "L. Song" ],
      "venue" : "ICML",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Coevolutionary latent feature processes for continuoustime user-item interactions",
      "author" : [ "Y. Wang", "N. Du", "R. Trivedi", "L. Song" ],
      "venue" : "NIPS, pages 4547–4555",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A stochastic differential equation framework for guiding online user activities in closed loop",
      "author" : [ "Y. Wang", "E. Theodorou", "A. Verma", "L. Song" ],
      "venue" : "arXiv preprint arXiv:1603.09021",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Variational policy for guiding point processes",
      "author" : [ "Y. Wang", "G. Williams", "E. Theodorou", "L. Song" ],
      "venue" : "ICML",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Isotonic hawkes processes",
      "author" : [ "Y. Wang", "B. Xie", "N. Du", "L. Song" ],
      "venue" : "ICML, pages 2226–2234",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Predicting user activity level in point processes with mass transport equation",
      "author" : [ "Y. Wang", "X. Ye", "H. Zha", "L. Song" ],
      "venue" : "NIPS",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Mixture of mutually exciting processes for viral diffusion",
      "author" : [ "S.-H. Yang", "H. Zha" ],
      "venue" : "ICML, pages 1–9",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "From micro to macro: Uncovering and predicting information cascading process with behavioral dynamics",
      "author" : [ "L. Yu", "P. Cui", "F. Wang", "C. Song", "S. Yang" ],
      "venue" : "ICDM",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Seismic: A self-exciting point process model for predicting tweet popularity",
      "author" : [ "Q. Zhao", "M.A. Erdogdu", "H.Y. He", "A. Rajaraman", "J. Leskovec" ],
      "venue" : "KDD",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning social infectivity in sparse low-rank networks using multi-dimensional hawkes processes",
      "author" : [ "K. Zhou", "H. Zha", "L. Song" ],
      "venue" : "AISTAT, volume 31, pages 641–649",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].",
      "startOffset" : 167,
      "endOffset" : 210
    }, {
      "referenceID" : 8,
      "context" : "Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].",
      "startOffset" : 167,
      "endOffset" : 210
    }, {
      "referenceID" : 10,
      "context" : "Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].",
      "startOffset" : 167,
      "endOffset" : 210
    }, {
      "referenceID" : 19,
      "context" : "Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].",
      "startOffset" : 167,
      "endOffset" : 210
    }, {
      "referenceID" : 21,
      "context" : "Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].",
      "startOffset" : 167,
      "endOffset" : 210
    }, {
      "referenceID" : 22,
      "context" : "Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].",
      "startOffset" : 167,
      "endOffset" : 210
    }, {
      "referenceID" : 23,
      "context" : "Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].",
      "startOffset" : 167,
      "endOffset" : 210
    }, {
      "referenceID" : 24,
      "context" : "Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].",
      "startOffset" : 167,
      "endOffset" : 210
    }, {
      "referenceID" : 25,
      "context" : "Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].",
      "startOffset" : 167,
      "endOffset" : 210
    }, {
      "referenceID" : 26,
      "context" : "Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].",
      "startOffset" : 167,
      "endOffset" : 210
    }, {
      "referenceID" : 31,
      "context" : "Particularly, temporal point processes are well-suited to model the event pattern of user behaviors and have been successfully applied in modeling event sequence data [6, 10, 12, 21, 23, 24, 25, 26, 27, 28, 33].",
      "startOffset" : 167,
      "endOffset" : 210
    }, {
      "referenceID" : 11,
      "context" : "For example, for social networking services, an accurate inference of the number of reshares of a post enables the network moderator to detect trending posts and improve its content delivery networks [13, 32]; an accurate estimate of the change of network topology (the number of new followers of a user) facilitates the moderator to identify influential users and suppress the spread of terrorist propaganda and cyber-attacks [12]; an accurate inference of the activity level (number of posts in the network) allows us to gain fundamental insight into the predictability of collective behaviors [22].",
      "startOffset" : 200,
      "endOffset" : 208
    }, {
      "referenceID" : 30,
      "context" : "For example, for social networking services, an accurate inference of the number of reshares of a post enables the network moderator to detect trending posts and improve its content delivery networks [13, 32]; an accurate estimate of the change of network topology (the number of new followers of a user) facilitates the moderator to identify influential users and suppress the spread of terrorist propaganda and cyber-attacks [12]; an accurate inference of the activity level (number of posts in the network) allows us to gain fundamental insight into the predictability of collective behaviors [22].",
      "startOffset" : 200,
      "endOffset" : 208
    }, {
      "referenceID" : 10,
      "context" : "For example, for social networking services, an accurate inference of the number of reshares of a post enables the network moderator to detect trending posts and improve its content delivery networks [13, 32]; an accurate estimate of the change of network topology (the number of new followers of a user) facilitates the moderator to identify influential users and suppress the spread of terrorist propaganda and cyber-attacks [12]; an accurate inference of the activity level (number of posts in the network) allows us to gain fundamental insight into the predictability of collective behaviors [22].",
      "startOffset" : 427,
      "endOffset" : 431
    }, {
      "referenceID" : 20,
      "context" : "For example, for social networking services, an accurate inference of the number of reshares of a post enables the network moderator to detect trending posts and improve its content delivery networks [13, 32]; an accurate estimate of the change of network topology (the number of new followers of a user) facilitates the moderator to identify influential users and suppress the spread of terrorist propaganda and cyber-attacks [12]; an accurate inference of the activity level (number of posts in the network) allows us to gain fundamental insight into the predictability of collective behaviors [22].",
      "startOffset" : 596,
      "endOffset" : 600
    }, {
      "referenceID" : 8,
      "context" : "Moreover, for online merchants such as Amazon, an accurate estimate of the number of future purchases of a product helps optimizing future advertisement placements [10, 25].",
      "startOffset" : 164,
      "endOffset" : 172
    }, {
      "referenceID" : 23,
      "context" : "Moreover, for online merchants such as Amazon, an accurate estimate of the number of future purchases of a product helps optimizing future advertisement placements [10, 25].",
      "startOffset" : 164,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "However, most works [8, 9, 13, 30, 31, 32] are problem specific and only designed for the simple task with f(x) = x; hence these works are",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "However, most works [8, 9, 13, 30, 31, 32] are problem specific and only designed for the simple task with f(x) = x; hence these works are",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 11,
      "context" : "However, most works [8, 9, 13, 30, 31, 32] are problem specific and only designed for the simple task with f(x) = x; hence these works are",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 28,
      "context" : "However, most works [8, 9, 13, 30, 31, 32] are problem specific and only designed for the simple task with f(x) = x; hence these works are",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 29,
      "context" : "However, most works [8, 9, 13, 30, 31, 32] are problem specific and only designed for the simple task with f(x) = x; hence these works are",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 30,
      "context" : "However, most works [8, 9, 13, 30, 31, 32] are problem specific and only designed for the simple task with f(x) = x; hence these works are",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "Second, point process models typically have intertwined stochasticity and can co-evolve over time [12, 25], e.",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 23,
      "context" : "Second, point process models typically have intertwined stochasticity and can co-evolve over time [12, 25], e.",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : ", in the influence propagation problem, the information diffusion over networks can change the structure of networks, which adversely influences the diffusion process [12].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 27,
      "context" : "However, previous works often ignore parts of the stochasticity in the intensity function [29] or make heuristic approximations [13, 32].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "However, previous works often ignore parts of the stochasticity in the intensity function [29] or make heuristic approximations [13, 32].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 30,
      "context" : "However, previous works often ignore parts of the stochasticity in the intensity function [29] or make heuristic approximations [13, 32].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "For example, to infer the number of tweeting and retweeting events of users in the co-evolution model of information diffusion and social link creation [12], our method needs 103 samples and 14.",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "A temporal point process [1] is a random process whose realization consists of a set of discrete events {tk}, localized in time.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "It has been successfully applied to model user behaviors in social networks [16, 17, 19, 23, 24, 25, 28, 30].",
      "startOffset" : 76,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "It has been successfully applied to model user behaviors in social networks [16, 17, 19, 23, 24, 25, 28, 30].",
      "startOffset" : 76,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "It has been successfully applied to model user behaviors in social networks [16, 17, 19, 23, 24, 25, 28, 30].",
      "startOffset" : 76,
      "endOffset" : 108
    }, {
      "referenceID" : 21,
      "context" : "It has been successfully applied to model user behaviors in social networks [16, 17, 19, 23, 24, 25, 28, 30].",
      "startOffset" : 76,
      "endOffset" : 108
    }, {
      "referenceID" : 22,
      "context" : "It has been successfully applied to model user behaviors in social networks [16, 17, 19, 23, 24, 25, 28, 30].",
      "startOffset" : 76,
      "endOffset" : 108
    }, {
      "referenceID" : 23,
      "context" : "It has been successfully applied to model user behaviors in social networks [16, 17, 19, 23, 24, 25, 28, 30].",
      "startOffset" : 76,
      "endOffset" : 108
    }, {
      "referenceID" : 26,
      "context" : "It has been successfully applied to model user behaviors in social networks [16, 17, 19, 23, 24, 25, 28, 30].",
      "startOffset" : 76,
      "endOffset" : 108
    }, {
      "referenceID" : 28,
      "context" : "It has been successfully applied to model user behaviors in social networks [16, 17, 19, 23, 24, 25, 28, 30].",
      "startOffset" : 76,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "Some useful forms include (i) Poisson process: the intensity is a deterministic function, and (ii) Hawkes process [15]: it captures the mutual excitation phenomena between events and its intensity is parameterized as",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 18,
      "context" : "To compute the probability mass of a point process, MC simulates n realizations of history {Hi t} using the thinning algorithm [20].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "Most methods [8, 9, 13, 30, 31, 32] only predict E[N(t)] and are not generalizable to an arbitrary function f .",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "Most methods [8, 9, 13, 30, 31, 32] only predict E[N(t)] and are not generalizable to an arbitrary function f .",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "Most methods [8, 9, 13, 30, 31, 32] only predict E[N(t)] and are not generalizable to an arbitrary function f .",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 28,
      "context" : "Most methods [8, 9, 13, 30, 31, 32] only predict E[N(t)] and are not generalizable to an arbitrary function f .",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 29,
      "context" : "Most methods [8, 9, 13, 30, 31, 32] only predict E[N(t)] and are not generalizable to an arbitrary function f .",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 30,
      "context" : "Most methods [8, 9, 13, 30, 31, 32] only predict E[N(t)] and are not generalizable to an arbitrary function f .",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "Moreover, they typically rely on specific parameterizations of the intensity functions, such as the reinforced Poisson process [13] and Hawkes process [5, 32]; hence they are not applicable to general point processes.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "Moreover, they typically rely on specific parameterizations of the intensity functions, such as the reinforced Poisson process [13] and Hawkes process [5, 32]; hence they are not applicable to general point processes.",
      "startOffset" : 151,
      "endOffset" : 158
    }, {
      "referenceID" : 30,
      "context" : "Moreover, they typically rely on specific parameterizations of the intensity functions, such as the reinforced Poisson process [13] and Hawkes process [5, 32]; hence they are not applicable to general point processes.",
      "startOffset" : 151,
      "endOffset" : 158
    }, {
      "referenceID" : 27,
      "context" : "These works also ignore parts of the stochasticity in the intensity functions [29] or make heuristic approximations to the point process [13, 32].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "These works also ignore parts of the stochasticity in the intensity functions [29] or make heuristic approximations to the point process [13, 32].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 30,
      "context" : "These works also ignore parts of the stochasticity in the intensity functions [29] or make heuristic approximations to the point process [13, 32].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "Theorem 1 extends the Rao-Blackwell (RB) theorem [3] to point processes.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "The main idea in our proof is to show that the equality (v, \u0000̃t) = (v,F [\u0000̃]) holds for any test function v; then \u0000̃t = F [\u0000̃] follows from the fundamental lemma of the calculus of variations [14].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 5,
      "context" : "On each interval, the intensity is continuous and we can use the classic numerical Runge-Kutta (RK) method [7] to solve the ODE.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "Then EHt [\u0000̂n] = follows from the fundamental lemma of the calculus of variations [14].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "The co-evolution model [12] uses a Hawkes process Nus(t) to model information diffusion (tweets/retweets), and a survival process Aus(t) to model the dynamics of network topology (link creation process).",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "Recent works on recommendation systems [10, 25] use a point process Nui(t) to model user u’s sequential interaction with item i.",
      "startOffset" : 39,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : "Recent works on recommendation systems [10, 25] use a point process Nui(t) to model user u’s sequential interaction with item i.",
      "startOffset" : 39,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : "As users interact with items over time, the user latent feature uu(t) and item latent feature iu(t) co-evolve over time, and are mutually dependent [25].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 16,
      "context" : "The accuracy is evaluated by the Kendall-⌧ rank correlation [18] between two lists.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "1 Predicting the activeness and popularity of users in social networks We use a Twitter dataset [2] that contains 280,000 users with 550,000 tweet, retweet, and link creation events during Sep.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "This data is previously used to validate the network co-evolution model [12].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 10,
      "context" : "The parameters for tweeting/retweeting processes and link creation process are learned using maximum likelihood estimation [12].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 23,
      "context" : "In the recommendation system setting, we use two datasets from [25].",
      "startOffset" : 63,
      "endOffset" : 67
    } ],
    "year" : 2017,
    "abstractText" : "Point processes are powerful tools to model user activities and have a plethora of applications in social sciences. Predicting user activities based on point processes is a central problem. However, existing works are mostly problem specific, use heuristics, or simplify the stochastic nature of point processes. In this paper, we propose a framework that provides an efficient estimator of the probability mass function of point processes. In particular, we design a key reformulation of the prediction problem, and further derive a differential-difference equation to compute a conditional probability mass function. Our framework is applicable to general point processes and prediction tasks, and achieves superb predictive and efficiency performance in diverse real-world applications compared to the state of the art.",
    "creator" : null
  }
}