{
  "name" : "c1fea270c48e8079d8ddf7d06d26ab52.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Decomposable Submodular Function Minimization: Discrete and Continuous",
    "authors" : [ "Alina Ene", "Huy L. Nguyễn", "László A. Végh" ],
    "emails" : [ "aene@bu.edu", "hu.nguyen@northeastern.edu", "L.Vegh@lse.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Submodular functions arise in a wide range of applications: graph theory, optimization, economics, game theory, to name a few. A function f : 2V → R on a ground set V is submodular if f(X) + f(Y ) ≥ f(X ∩ Y ) + f(X ∪ Y ) for all sets X,Y ⊆ V . Submodularity can also be interpreted as a diminishing returns property.\nThere has been significant interest in submodular optimization in the machine learning and computer vision communities. The submodular function minimization (SFM) problem arises in problems in image segmentation or MAP inference tasks in Markov Random Fields. Landmark results in combinatorial optimization give polynomial-time exact algorithms for SFM. However, the highdegree polynomial dependence in the running time is prohibitive for large-scale problem instances. The main objective in this context is to develop fast and scalable SFM algorithms.\nInstead of minimizing arbitrary submodular functions, several recent papers aim to exploit special structural properties of submodular functions arising in practical applications. This paper focuses on the popular model of decomposable submodular functions. These are functions that can be written as sums of several “simple” submodular functions defined on small supports.\nSome definitions are needed to introduce our problem setting. Let f : 2V → R be a submodular function, and let n := |V |. We can assume w.l.o.g. that f(∅) = 0. We are interested in solving the submodular function minimization problem:\nmin S⊆V f(S). (SFM)\nFor a vector y ∈ RV and a set S ⊆ V , we use the notation y(S) := ∑ v∈S y(v). The base polytope of a submodular function is defined as\nB(f) := {y ∈ RV : y(S) ≤ f(S) ∀S ⊆ V, y(V ) = f(V )}. One can optimize linear functions over B(f) using the greedy algorithm. The SFM problem can be reduced to finding the minimum norm point of the base polytope B(f) [10].\nmin\n{ 1\n2 ‖y‖22 : y ∈ B(f)\n} . (Min-Norm)\n∗Department of Computer Science, Boston University, aene@bu.edu †College of Computer and Information Science, Northeastern University, hu.nguyen@northeastern.edu ‡Department of Mathematics, London School of Economics, L.Vegh@lse.ac.uk\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nThis reduction is the starting point of convex optimization approaches for SFM. We refer the reader to Sections 44–45 in [28] for concepts and results in submodular optimization, and to [2] on machine learning applications.\nWe assume that f is given in the decomposition f(S) = ∑r i=1 fi(S), where each fi : 2\nV → R is a submodular function. Such functions are called decomposable or Sum-of-Submodular (SoS) in the literature. In the decomposable submodular function minimization (DSFM) problem, we aim to minimize a function given in such a decomposition. We will make the following assumptions.\nFor each i ∈ [r], we assume that two oracles are provided: (i) a value oracle that returns fi(S) for any set S ⊆ V in time EOi; and (ii) a quadratic minimization oracle Oi(w). For any input vector w ∈ Rn, this oracle returns an optimal solution to (Min-Norm) for the function fi + w, or equivalently, an optimal solution to miny∈B(fi) ‖y + w‖22. We let Θi denote the running time of a single call to the oracle Oi, Θmax := maxi∈[r] Θi denote the maximum time of an oracle call, Θavg := 1 r ∑ i∈[r] Θi denote the average time of an oracle call.\n4 We let Fi,max := maxS⊆V |fi(S)|, Fmax := maxS⊆V |f(S)| denote the maximum function values. For each i ∈ [r], the function fi has an effective support Ci such that fi(S) = fi(S ∩ Ci) for every S ⊆ V . DSFM thus requires algorithms on two levels. The level-0 algorithms are the subroutines used to evaluate the oracles Oi for every i ∈ [r]. The level-1 algorithm minimizes the function f using the level-0 algorithms as black boxes."
    }, {
      "heading" : "1.1 Prior work",
      "text" : "SFM has had a long history in combinatorial optimization since the early 1970s, following the influential work of Edmonds [4]. The first polynomial-time algorithm was obtained via the ellipsoid method [14]; recent work presented substantial improvements using this approach [22]. Substantial work focused on designing strongly polynomial combinatorial algorithms [9, 15, 16, 25, 17, 27]. Still, designing practical algorithms for SFM that can be applied to large-scale problem instances remains an open problem.\nLet us now turn to DSFM. Previous work mainly focused on level-1 algorithms. These can be classified as discrete and continuous optimization methods. The discrete approach builds on techniques of classical discrete algorithms for network flows and for submodular flows. Kolmogorov [21] showed that the problem can be reduced to submodular flow maximization, and also presented a more efficient augmenting path algorithm. Subsequent discrete approaches were given in [1, 7, 8]. Continuous approaches start with the convex programming formulation (Min-Norm). Gradient methods were applied for the decomposable setting in [5, 24, 30].\nLess attention has been given to the level-0 algorithms. Some papers mainly focus on theoretical guarantees on the running time of level-1 algorithms, and treat the level-0 subroutines as black-boxes (e.g. [5, 24, 21]). In other papers (e.g. [18, 30]), the model is restricted to functions fi of a simple specific type that are easy to minimize. An alternative assumption is that all Ci’s are small, of size at most k; and thus these oracles can be evaluated by exhaustive search, in 2k value oracle calls (e.g. [1, 7]). Shanu et al. [29] use a block coordinate descent method for level-1, and make no assumptions on the functions fi. The oracles are evaluated via the Fujishige-Wolfe minimum norm point algorithm [11, 31] for level-0.\nLet us note that these experimental studies considered the level-0 and level-1 algorithms as a single “package”. For example, Shanu et al. [29] compare the performance of their SoS Min-Norm algorithm to the continuous approach of Jegelka et al. [18] and the combinatorial approach of Arora et al. [1]. However, these implementations cannot be directly compared, since they use three different level-0 algorithms: Fujishige-Wolfe in SoS Min-Norm, a general QP solver for the algorithm of [18], and exhaustive search for [1]. For potentials of large support, Fujishige-Wolfe outperforms these other level-0 subroutines, hence the level-1 algorithms in [18, 1] could have compared more favorably using the same Fujishige-Wolfe subroutine.\n4For flow-type algorithms for DSFM, a slightly weaker oracle assumption suffices, returning a minimizer of minS⊆Ci fi(S) + w(S) for any given w ∈ R\nCi . This oracle and the quadratic minimization oracle are reducible to each other: the former reduces to a single call to the latter, and one can implement the latter using O(|Ci|) calls to the former (see e.g. [2])."
    }, {
      "heading" : "1.2 Our contributions",
      "text" : "Our paper establishes connections between discrete and continuous methods for DSFM, as well as provides a systematic experimental comparison of these approaches. Our main theoretical contribution improves the worst-case complexity bound of the most recent continuous optimization methods [5, 24] by a factor of r, the number of functions in the decomposition. This is achieved by improving the bounds on the relevant condition numbers. Our proof exploits ideas from the discrete optimization approach. This provides not only better, but also considerably simpler arguments than the algebraic proof in [24].\nThe guiding principle of our experimental work is the clean conceptual distinction between the level-0 and level-1 algorithms, and to compare different level-1 algorithms by using the same level-0 subroutines. We compare the state-of-the-art continuous and discrete algorithms: RCDM and ACDM from [5] with Submodular IBFS from [7]. We consider multiple options for the level-0 subroutines. For certain potential types, we use tailored subroutines exploiting the specific form of the problem. We also consider a variant of the Fujishige-Wolfe algorithm as a subroutine applicable for arbitrary potentials.\nOur experimental results reveal the following tradeoff. Discrete algorithms on level-1 require more calls to the level-0 oracle, but less overhead computation. Hence using algorithms such as IBFS on level-1 can be significantly faster than gradient descent, as long as the potentials have fairly small supports. However, as the size of the potentials grow, or we do need to work with a generic level-0 algorithm, gradient methods are preferable. Gradient methods can perform better for larger potentials also due to weaker requirements on the level-0 subroutines: approximate level-0 subroutines suffice for them, whereas discrete algorithms require exact optimal solutions on level-0.\nPaper outline. The rest of the paper is structured as follows. The level-1 algorithmic frameworks using discrete and convex optimization are described in Sections 2 and 3, respectively. Section 4 gives improved convergence guarantees for the gradient descent algorithms outlined in Section 3. Section 5 discusses the different types of level-0 algorithms and how they can be used together with the level-1 frameworks. Section 6 presents a brief overview of our experimental results.\nThis is an extended abstract. The full paper is available on http://arxiv.org/abs/1703.01830."
    }, {
      "heading" : "2 Discrete optimization algorithms on Level-1",
      "text" : "In this section, we outline a level-1 algorithmic framework for DSFM that is based on a combinatorial framework first studied by Fujishige and Zhang [12] for submodular intersection. The submodular intersection problem is equivalent to DSFM for the sum of two functions, and the approach can be adapted and extended to the general DSFM problem with an arbitrary decomposition. We now give a brief description of the algorithmic framework. The full version exhibits submodular versions of the Edmonds-Karp and preflow-push algorithms.\nAlgorithmic framework. For a decomposable function f , every x ∈ B(f) can be written as x = ∑r i=1 xi, where supp(xi) ⊆ Ci and xi ∈ B(fi) (see e.g. Theorem 44.6 in [28]). A natural algorithmic approach is to maintain an x ∈ B(f) in such a representation, and iteratively update it using the combinatorial framework described below. DSFM can be cast as a maximum network flow problem in a network that is suitably defined based on the current point x. This can be viewed as an analogue of the residual graph in the maxflow/mincut setting, and it is precisely the residual graph if the DSFM instance is a minimum cut instance.\nThe auxiliary graph. For an x ∈ B(f) of the form x = ∑r i=1 xi, we construct the following\ndirected auxiliary graph G = (V,E), with E = ⋃r i=1Ei and capacities c : E → R+. E is a multiset union: we include parallel copies if the same arc occurs in multiple Ei. The arc sets Ei are complete directed graphs (cliques) on Ci, and for an arc (u, v) ∈ Ei, we define c(u, v) := min{fi(S) − xi(S) : S ⊆ Ci, u ∈ S, v /∈ S}. This is the maximum value ε such that x′i ∈ B(fi), where x′i(u) = xi(u) + ε, x ′ i(v) = xi(v)− ε, x′i(z) = xi(z) for z /∈ {u, v}.\nLet N := {v ∈ V : x(v) < 0} and P := {v ∈ V : x(v) > 0}. The algorithm aims to improve the current x by updating along shortest directed paths from N to P with positive capacity; there are several ways to update the solution, and we discuss specific approaches (derived from maximum flow algorithms) in the full version. If there exists no such directed path, then we let S denote the set\nreachable from N on directed paths with positive capacity; thus, S ∩ P = ∅. One can show that S is a minimizer of the function f .\nUpdating along a shortest path Q from N to P amounts to the following. Let ε denote the minimum capacity of an arc on Q. If (u, v) ∈ Q ∩ Ei, then we increase xi(u) by ε and decrease xi(v) by ε. The crucial technical claim is the following. Let d(u) denote the shortest path distance of positive capacity arcs from u to the set P . Then, an update along a shortest directed path from N to P results in a feasible x ∈ B(f), and further, all distance labels d(u) are non-decreasing. We refer the reader to Fujishige and Zhang [12] for a proof of this claim.\nLevel-1 algorithms based on the network flow approach. Using the auxiliary graph described above, and updating on shortest augmenting paths, one can generalize several maximum flow algorithms to a level-1 algorithm of DSFM. In particular, based on the preflow-push algorithm [13], one can obtain a strongly polynomial DSFM algorithm with running time O(n2Θmax ∑r i=1 |Ci|2). A\nscaling variant provides a weakly polynomial running time O(n2Θmax logFmax + n ∑r i=1 |Ci|3Θi). We defer the details to the full version of the paper.\nIn our experiments, we use the submodular IBFS algorithm [7] as the main discrete level-1 algorithm; the same running time estimate as for preflow-push is applicable. If all Ci’s are small, O(1), the running time is O(n2rΘmax); note that r = Ω(n) in this case."
    }, {
      "heading" : "3 Convex optimization algorithms on Level-1",
      "text" : ""
    }, {
      "heading" : "3.1 Convex formulations for DSFM",
      "text" : "Recall the convex quadratic program (Min-Norm) from the Introduction. This program has a unique optimal solution s∗, and the set S = {v ∈ V : s∗(v) < 0} is the unique smallest minimizer to the SFM problem. We will refer to this optimal solution s∗ throughout the section.\nIn the DSFM setting, one can write (Min-Norm) in multiple equivalent forms [18]. For the first formulation, we let P := ∏r i=1B(fi) ⊆ Rrn, and let A ∈ Rn×(rn) denote the following matrix:\nA := [InIn . . . In]︸ ︷︷ ︸ r times .\nNote that, for every y ∈ P , Ay = ∑r i=1 yi, where yi is the i-th block of y, and thus Ay ∈ B(f). The problem (Min-Norm) can be reformulated for DSFM as follows.\nmin\n{ 1\n2 ‖Ay‖22 : y ∈ P\n} . (Prox-DSFM)\nThe second formulation is the following. Let us define the subspace A := {a ∈ Rnr : Aa = 0}, and minimize its distance from P:\nmin { ‖a− y‖22 : a ∈ A, y ∈ P } . (Best-Approx)\nThe set of optimal solutions for both formulations (Prox-DSFM) and (Best-Approx) is the set E := {y ∈ P : Ay = s∗}, where s∗ is the optimum of (Min-Norm). We note that, even though the set of solutions to (Best-Approx) are pairs of points (a, y) ∈ A× P , the optimal solutions are uniquely determined by y ∈ P , since the corresponding a is the projection of y to A."
    }, {
      "heading" : "3.2 Level-1 algorithms based on gradient descent",
      "text" : "The gradient descent algorithms of [24, 5] provide level-1 algorithms for DSFM. We provide a brief overview of these algorithms and we refer the reader to the respective papers for more details.\nThe alternating projections algorithm. Nishihara et al. [24] minimize (Best-Approx) using alternating projections. The algorithm starts with a point a0 ∈ A and it iteratively constructs a sequence { (a(k), x(k)) } k≥0 by projecting onto A and P: x\n(k) = argminx∈P‖a(k) − x‖2, a(k+1) = argmina∈A‖a− x(k)‖2. Random coordinate descent algorithms. Ene and Nguyen [5] minimize (Prox-DSFM) using random coordinate descent. The RCDM algorithm adapts the random coordinate descent algorithm\nof Nesterov [23] to (Prox-DSFM). In each iteration, the algorithm samples a block i ∈ [r] uniformly at random and it updates xi via a standard gradient descent step for smooth functions. ACDM, the accelerated version of the algorithm, presents a further enhancement using techniques from [6]."
    }, {
      "heading" : "3.3 Rates of convergence and condition numbers",
      "text" : "The algorithms mentioned above enjoy a linear convergence rate despite the fact that the objective functions of (Best-Approx) and (Prox-DSFM) are not strongly convex. Instead, the works [24, 5] show that there are certain parameters that one can associate with the objective functions such that the convergence is at the rate (1−α)k, where α ∈ (0, 1) is a quantity that depends on the appropriate parameter. Let us now define these parameters.\nLet A′ be the affine subspace A′ := {a ∈ Rnr : Aa = s∗}. Note that the set E of optimal solutions to (Prox-DSFM) and (Best-Approx) is E = P ∩ A′. For y ∈ Rnr and a closed set K ⊆ Rnr, we let d(y,K) = min {‖y − z‖2 : z ∈ K} denote the distance between y and K. The relevant parameter for the Alternating Projections algorithm is defined as follows.\nDefinition 3.1 ([24]). For every y ∈ (P ∪ A′) \\ E , let\nκ(y) := d(y, E)\nmax {d(y,P), d(y,A′)} , and κ∗ := sup {κ(y) : y ∈ (P ∪ A′) \\ E} .\nThe relevant parameter for the random coordinate descent algorithms is the following.\nDefinition 3.2 ([5]). For every y ∈ P , let y∗ := argminp{‖p− y‖2 : p ∈ E} be the optimal solution to (Prox-DSFM) that is closest to y. We say that the objective function 12‖Ay‖ 2 2 of (Prox-DSFM) is restricted `-strongly convex if, for all y ∈ P , we have\n‖A(y − y∗)‖22 ≥ `‖y − y∗‖22.\nWe define\n`∗ := sup\n{ ` : 1\n2 ‖Ay‖22 is restricted `-strongly convex\n} .\nThe running time dependence of the algorithms on these parameters is given in the following theorems.\nTheorem 3.3 ([24]). Let (a(0), x(0) = argminx∈P‖a(0)−x‖2) be the initial solution and let (a∗, x∗) be an optimal solution to (Best-Approx). The alternating projection algorithm produces in\nk = Θ ( κ2∗ ln ( ‖x(0) − x∗‖2 )) iterations a pair of points a(k) ∈ A and x(k) ∈ P that is -optimal, i.e.,\n‖a(k) − x(k)‖22 ≤ ‖a∗ − x∗‖22 + ε.\nTheorem 3.4 ([5]). Let x(0) ∈ P be the initial solution and let x∗ be an optimal solution to (Prox-DSFM) that minimizes ‖x(0) − x∗‖2. The random coordinate descent algorithm produces in\nk = Θ\n( r\n`∗ ln\n( ‖x(0) − x∗‖2 )) iterations a solution x(k) that is -optimal in expectation, i.e., E [ 1 2‖Ax (k)‖22 ] ≤ 12‖Ax\n∗‖22 + . The accelerated coordinate descent algorithm produces in\nk = Θ ( r √ 1\n`∗ ln\n( ‖x(0) − x∗‖2 )) iterations (specifically, Θ ( ln ( ‖x(0)−x∗‖2 )) epochs with Θ ( r √\n1 `∗\n) iterations in each epoch) a\nsolution x(k) that is -optimal in expectation, i.e., E [ 1 2‖Ax (k)‖22 ] ≤ 12‖Ax ∗‖22 + ."
    }, {
      "heading" : "3.4 Tight analysis for the condition numbers and running times",
      "text" : "We provide a tight analysis for the condition numbers (the parameters κ∗ and `∗ defined above). This leads to improved upper bounds on the running times of the gradient descent algorithms. Theorem 3.5. Let κ∗ and `∗ be the parameters defined in Definition 3.1 and Definition 3.2. We have κ∗ = Θ(n √ r) and `∗ = Θ(1/n2).\nUsing our improved convergence guarantees, we obtain the following improved running time analyses. Corollary 3.6. The total running time for obtaining an -approximate solution5 is as follows.\n• Alternating projections (AP): O ( n2r2Θavg ln ( ‖x(0)−x∗‖2 )) .\n• Random coordinate descent (RCDM): O ( n2rΘavg ln ( ‖x(0)−x∗‖2 )) .\n• Accelerated random coordinate descent (ACDM): O ( nrΘavg ln ( ‖x(0)−x∗‖2 )) .\nWe can upper bound the diameter of the base polytope byO( √ nFmax) [19], and thus ‖x(0)−x∗‖2 =\nO( √ nFmax). For integer-valued functions, a ε-approximate solution can be converted to an exact optimum if ε = O(1/n) [2].\nThe upper bound on κ∗ and the lower bound on `∗ are shown in Theorem 4.2. The lower bound on κ∗ and upper bound on `∗ in Theorem 3.5 follow by constructions in previous work, as explained next. Nishihara et al. showed that κ∗ ≤ nr, and they give a family of minimum cut instances for which κ∗ = Ω(n √ r). Namely, consider a graph with n vertices and m edges, and suppose for simplicity that the edges have integer capacities at most C. The cut function of the graph can be decomposed into functions corresponding to the individual edges, and thus r = m and Θavg = O(1). Already on simple cycle graphs, they show that the running time of AP is Ω(n2m2 ln(nC)), which implies κ∗ = Ω(n √ r).\nUsing the same construction, it is easy to obtain the upper bound `∗ = O(1/n2)."
    }, {
      "heading" : "4 Tight convergence bounds for the convex optimization algorithms",
      "text" : "In this section, we show that the combinatorial approach introduced in Section 2 can be applied to obtain better bounds on the parameters κ∗ and `∗ defined in Section 3. Besides giving a stronger bound, our proof is considerably simpler than the algebraic one using Cheeger’s inequality in [24]. The key is the following lemma. Lemma 4.1. Let y ∈ P and s∗ ∈ B(f). Then there exists a point x ∈ P such that Ax = s∗ and ‖x− y‖2 ≤ √ n 2 ‖Ay − s ∗‖1.\nBefore proving this lemma, we show how it can be used to derive the bounds. Theorem 4.2. We have κ∗ ≤ n √ r/2 + 1 and `∗ ≥ 4/n2.\nProof: We start with the bound on κ∗. In order to bound κ∗, we need to upper bound κ(y) for any y ∈ (P ∪ A′) \\ E . We distinguish between two cases: y ∈ P \\ E and y ∈ A′ \\ E . Case I: y ∈ P \\E . The denominator in the definition of κ(y) is equal to d(y,A′) = ‖Ay − s∗‖2/ √ r. This follows since the closest point a = (a1, . . . , ar) to y in A′ can be obtained as ai = yi + (s∗ − Ay)/r for each i ∈ [r]. Lemma 4.1 gives an x ∈ P such that Ax = s∗ and ‖x − y‖2 ≤√ n 2 ‖Ay − s ∗‖1 ≤ n2 ‖Ay − s ∗‖2. Since Ax = s∗, we have x ∈ E and thus the numerator of κ(y) is\nat most ‖x− y‖2. Thus κ(y) ≤ ‖x− y‖2/(‖Ay − s∗‖2/ √ r) ≤ n √ r/2.\nCase II: y ∈ A′ \\ E . This means that Ay = s∗. The denominator of κ(y) is equal to d(y,P). For each i ∈ [r], let qi ∈ B(fi) be the point that minimizes ‖yi − qi‖2. Let q = (q1, . . . , qr) ∈ P . Then\n5The algorithms considered here solve the optimization problem (Prox-DSFM). An ε-approximate solution to an optimization problem min{f(x) : x ∈ P} is a solution x ∈ P satisfying f(x) ≤ f(x∗) + ε, where x∗ ∈ argminx∈P f(x) is an optimal solution.\nd(y,P) = ‖y − q‖2. Lemma 4.1 with q in place of y gives a point x ∈ E such that ‖q − x‖2 ≤√ n 2 ‖Aq−s ∗‖1. We have ‖Aq−s∗‖1 = ‖Aq−Ay‖1 ≤ ∑r i=1 ‖qi−yi‖1 = ‖q−y‖1 ≤ √ nr‖q−y‖2. Thus ‖q−x‖2 ≤ n √ r\n2 ‖q−y‖2. Since x ∈ E , we have d(y, E) ≤ ‖x−y‖2 ≤ ‖x−q‖2 +‖q−y‖2 ≤( 1 + n √ r\n2\n) ‖q − y‖2 = ( 1 + n √ r\n2\n) d(y,P). Therefore κ(p) ≤ 1 + n √ r\n2 , as desired.\nLet us now prove the bound on `∗. Let y ∈ P and let y∗ := argminp{‖p− y‖2 : y ∈ E}. We need to verify that ‖A(y − y∗)‖22 ≥ 4n2 ‖y − y\n∗‖22. Again, we apply Lemma 4.1 to obtain a point x ∈ P such that Ax = s∗ and ‖x− y‖22 ≤ n4 ‖Ax−Ay‖ 2 1 ≤ n 2 4 ‖Ax−Ay‖ 2 2. Since Ax = s\n∗, the definition of y∗ gives ‖y−y∗‖22 ≤ ‖x−y‖22. Using that Ax = Ay∗ = s∗, we have ‖Ax−Ay‖2 = ‖Ay−Ay∗‖2.\nProof of Lemma 4.1: We give an algorithm that transforms y to a vector x ∈ P as in the statement through a sequence of path augmentations in the auxiliary graph defined in Section 2. We initialize x = y and maintain x ∈ P (and thus Ax ∈ B(f)) throughout. We now define the set of source and sink nodes as N := {v ∈ V : (Ax)(v) < s∗(v)} and P := {v ∈ V : (Ax)(v) > s∗(v)}. Once N = P = ∅, we have Ax = s∗ and terminate. Note that since Ax, s∗ ∈ B(f), we have∑ v(Ax)(v) = ∑ v s ∗(v) = f(V ), and therefore N = ∅ is equivalent to P = ∅. The blocks of x are denoted as x = (x1, x2, . . . , xr), with xi ∈ B(fi). Claim 4.3. If N 6= ∅, then there exists a directed path of positive capacity in the auxiliary graph between the sets N and P .\nProof: We say that a set T is i-tight, if xi(T ) = fi(T ). It is a simple consequence of submodularity that the intersection and union of two i-tight sets are also i-tight sets. For every i ∈ [r] and every u ∈ V , we define Ti(u) as the unique minimal i-tight set containing u. It is easy to see that for an arc (u, v) ∈ Ei, c(u, v) > 0 if and only if v ∈ Ti(u). We note that if u /∈ Ci, then x(u) = fi({u}) = 0 and thus Ti(u) = {u}. Let S be the set of vertices reachable from N on a directed path of positive capacity in the auxiliary graph. For a contradiction, assume S ∩ P = ∅. By the definition of S, we must have Ti(u) ⊆ S for every u ∈ S and every i ∈ [r]. Since the union of i-tight sets is also i-tight, we see that S is i-tight for every i ∈ [r], and consequently, x(S) = f(S). On the other hand, since N ⊆ S, S ∩ P = ∅, and N 6= ∅, we have x(S) < s∗(S). Since s∗ ∈ B(f), we have f(S) = x(S) < s∗(S) ≤ f(S), a contradiction. We conclude that S ∩ P 6= ∅. In every step of the algorithm, we take a shortest directed path Q of positive capacity from N to P , and update x along this path. That is, if (u, v) ∈ Q ∩ Ei, then we increase xi(u) by ε and decrease xi(v) by ε, where ε is the minimum capacity of an arc on Q. Note that this is the same as running the Edmonds-Karp-Dinitz algorithm in the submodular auxiliary graph. Using the analysis of [12], one can show that this change maintains x ∈ P , and that the algorithm terminates in finite (in fact, strongly polynomial) time. We defer the details to the full version of the paper.\nIt remains to bound ‖x − y‖2. At every path update, the change in `∞-norm of x is at most ε, and the change in `1-norm is at most nε, since the length of the path is ≤ n. At the same time,∑ v∈N (s\n∗(v) − (Ax)(v)) decreases by ε. Thus, ‖x − y‖∞ ≤ ‖Ay − s∗‖1/2 and ‖x − y‖1 ≤ n‖Ay − s∗‖1/2. Using the inequality ‖p‖2 ≤ √ ‖p‖1‖p‖∞, we obtain ‖x− y‖2 ≤ √ n 2 ‖Ay − s\n∗‖1, completing the proof."
    }, {
      "heading" : "5 The level-0 algorithms",
      "text" : "In this section, we briefly discuss the level-0 algorithms and the interface between the level-1 and level-0 algorithms.\nTwo-level frameworks via quadratic minimization oracles. Recall from the Introduction the assumption on the subroutines Oi(w) that finds the minimum norm point in B(fi + w) for the input vector w ∈ Rn for each i ∈ [r]. The continuous methods in Section 3 directly use the subroutines Oi(w) for the alternating projection or coordinate descent steps. For the flow-based algorithms in Section 2, the main oracle query is to find the auxiliary graph capacity c(u, v) of an arc (u, v) ∈ Ei for some i ∈ [r]. This can be easily formulated as minimizing the function fi+w for an appropriatew with supp(w) ⊆ Ci. As explained at the beginning of Section 3, an optimal solution to (Min-Norm)\nimmediately gives an optimal solution to the SFM problem for the same submodular function. Hence, the auxiliary graph capacity queries can be implemented via single calls to the subroutines Oi(w). Let us also remark that, while the functions fi are formally defined on the entire ground set V , their effective support is Ci, and thus it suffices to solve the quadratic minimization problems on the ground set Ci.\nWhereas discrete and continuous algorithms require the same type of oracles, there is an important difference between the two algorithms in terms of exactness for the oracle solutions. The discrete algorithms require exact values of the auxiliary graph capacities c(u, v), as they must maintain xi ∈ B(fi) throughout. Thus, the oracle must always return an optimal solution. The continuous algorithms are more robust, and return a solution with the required accuracy even if the oracle only returns an approximate solution. As discussed in Section 6, this difference leads to the continuous methods being applicable in settings where the combinatorial algorithms are prohibitively slow.\nLevel-0 algorithms. We now discuss specific algorithms for quadratic minimization over the base polytopes of the functions fi. Several functions that arise in applications are “simple”, meaning that there is a function-specific quadratic minimization subroutine that is very efficient. If a functionspecific subroutine is not available, one can use a general-purpose submodular minimization algorithm. The works [1, 7] use a brute force search as the subroutine for each each fi, whose running time is 2|Ci|EOi. However, this is applicable only for smallCi’s and is not suitable for our experiments where the maximum clique size is quite large. As a general-purpose algorithm, we used the Fujishige-Wolfe minimum norm point algorithm [11, 31]. This provides an ε-approximate solution inO(|Ci|F 2i,max/ε) iterations, with overall running time bound O((|Ci|4 + |Ci|2EOi)F 2i,max/ε) [3]. The experimental running time of the Fujishige-Wolfe algorithm can be prohibitively large [20]. As we discuss in Section 6, by warm-starting the algorithm and performing only a small number of iterations, we were able to use the algorithm in conjunction with the gradient descent level-1 algorithms."
    }, {
      "heading" : "6 Experimental results",
      "text" : "We evaluate the algorithms on energy minimization problems that arise in image segmentation problems. We follow the standard approach and model the image segmentation task of segmenting an object from the background as finding a minimum cost 0/1 labeling of the pixels. The total labeling cost is the sum of labeling costs corresponding to cliques, where a clique is a set of pixels. We refer to the labeling cost functions as clique potentials.\nThe main focus of our experimental analysis is to compare the running times of the decomposable submodular minimization algorithms. Therefore we have chosen to use the simple hand-tuned potentials that were used in previous work: the edge-based costs [1] and the count-based costs defined by [29, 30]. Specifically, we used the following clique potentials in our experiments, all of which are submodular:\n• Unary potentials for each pixel. The unary potentials are derived from Gaussian Mixture Models of color features [26]. • Pairwise potentials for each edge of the 8-neighbor grid graph. For each graph edge (i, j) between pixels i and j, the cost of a labeling equals 0 if the two pixels have the same label, and exp(−‖vi − vj‖2) for different labels, where vi is the RGB color vector of pixel i. • Square potentials for each 2× 2 square of pixels. The cost of a labeling is the square root of the number of neighboring pixels that have different labels, as in [1]. • Region potentials. We use the algorithm from [30] to identify regions. For each region Ci, the labeling cost is fi(S) = |S||Ci \\ S|, where S and Ci \\ S are the subsets of Ci labeled 0 and 1, respectively, see [29, 30].\nWe used five image segmentation instances to evaluate the algorithms.6 The experiments were carried out on a single computer with a 3.3 GHz Intel Core i5 processor and 8 GB of memory; we reported averaged times over 10 trials.\nWe performed several experiments with various combinations of potentials and parameters. In the minimum cut experiments, we evaluated the algorithms on instances containing only unary and\n6The data is available at http://melodi.ee.washington.edu/~jegelka/cc/index.html and http://research.microsoft.com/en-us/um/cambridge/projects/visionimagevideoediting/ segmentation/grabcut.htm\npairwise potentials; in the small cliques experiments, we used unary, pairwise, and square potentials. Finally, the large cliques experiments used all potentials above. Here, we used two different level-0 algorithms for the region potentials. Firstly, we used an algorithm specific to the particular potential, with running time O(|Ci| log(|Ci|) + |Ci|EOi). Secondly, we used the general Fujishige-Wolfe algorithm for level-0. This turned out to be significantly slower: it was prohibitive to run the algorithm to near-convergence. Hence, we could not implement IBFS in this setting as it requires an exact solution.\nWe were able to implement coordinate descent methods with the following modification of FujishigeWolfe at level-0. At every iteration, we ran Fujishige-Wolfe for 10 iterations only, but we warm-started with the current solution xi ∈ B(fi) for each i ∈ [r]. Interestingly, this turned out to be sufficient for the level-1 algorithm to make progress.\nSummary of results. Figure 1 shows the running times for some of the instances; we defer the full experimental results to the full version of the paper. The IBFS algorithm is significantly faster than the gradient descent algorithms on all of the instances with small cliques. For all of the instances with larger cliques, IBFS (as well as other combinatorial algorithms) are no longer suitable if the only choice for the level-0 algorithms are generic methods such as the Fujishige-Wolfe algorithm. The experimental results suggest that in such cases, the coordinate descent methods together with a suitably modified Fujishige-Wolfe algorithm provides an approach for obtaining an approximate solution."
    } ],
    "references" : [ {
      "title" : "Generic cuts: An efficient algorithm for optimal inference in higher order MRF-MAP",
      "author" : [ "C. Arora", "S. Banerjee", "P. Kalra", "S. Maheshwari" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Learning with submodular functions: A convex optimization perspective",
      "author" : [ "F. Bach" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Provable submodular minimization using Wolfe’s algorithm",
      "author" : [ "D. Chakrabarty", "P. Jain", "P. Kothari" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Submodular functions, matroids, and certain polyhedra",
      "author" : [ "J. Edmonds" ],
      "venue" : "Combinatorial structures and their applications,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1970
    }, {
      "title" : "Random coordinate descent methods for minimizing decomposable submodular functions",
      "author" : [ "A.R. Ene", "H.L. Nguyen" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning (ICML),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Accelerated, parallel, and proximal coordinate descent",
      "author" : [ "O. Fercoq", "P. Richtárik" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Structured learning of sum-of-submodular higher order energy functions",
      "author" : [ "A. Fix", "T. Joachims", "S. Min Park", "R. Zabih" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "A primal-dual algorithm for higher-order multilabel Markov random fields",
      "author" : [ "A. Fix", "C. Wang", "R. Zabih" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "A push-relabel framework for submodular function minimization and applications to parametric optimization",
      "author" : [ "L. Fleischer", "S. Iwata" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2003
    }, {
      "title" : "Lexicographically optimal base of a polymatroid with respect to a weight vector",
      "author" : [ "S. Fujishige" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1980
    }, {
      "title" : "A submodular function minimization algorithm based on the minimum-norm base",
      "author" : [ "S. Fujishige", "S. Isotani" ],
      "venue" : "Pacific Journal of Optimization,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "New algorithms for the intersection problem of submodular systems",
      "author" : [ "S. Fujishige", "X. Zhang" ],
      "venue" : "Japan Journal of Industrial and Applied Mathematics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1992
    }, {
      "title" : "A new approach to the maximum-flow problem",
      "author" : [ "A.V. Goldberg", "R.E. Tarjan" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1988
    }, {
      "title" : "The ellipsoid method and its consequences in combinatorial optimization",
      "author" : [ "M. Grötschel", "L. Lovász", "A. Schrijver" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1981
    }, {
      "title" : "A faster scaling algorithm for minimizing submodular functions",
      "author" : [ "S. Iwata" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2003
    }, {
      "title" : "A combinatorial strongly polynomial algorithm for minimizing submodular functions",
      "author" : [ "S. Iwata", "L. Fleischer", "S. Fujishige" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    }, {
      "title" : "A simple combinatorial algorithm for submodular function minimization",
      "author" : [ "S. Iwata", "J.B. Orlin" ],
      "venue" : "In ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Reflection methods for user-friendly submodular optimization",
      "author" : [ "S. Jegelka", "F. Bach", "S. Sra" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Online submodular minimization for combinatorial structures",
      "author" : [ "S. Jegelka", "J.A. Bilmes" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "On fast approximate submodular minimization",
      "author" : [ "S. Jegelka", "H. Lin", "J.A. Bilmes" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Minimizing a sum of submodular functions",
      "author" : [ "V. Kolmogorov" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "A faster cutting plane method and its implications for combinatorial and convex optimization",
      "author" : [ "Y.T. Lee", "A. Sidford", "S.C.-w. Wong" ],
      "venue" : "In IEEE Foundations of Computer Science (FOCS),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Y. Nesterov" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "On the convergence rate of decomposable submodular function minimization",
      "author" : [ "R. Nishihara", "S. Jegelka", "M.I. Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "A faster strongly polynomial time algorithm for submodular function minimization",
      "author" : [ "J.B. Orlin" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "Grabcut: Interactive foreground extraction using iterated graph cuts",
      "author" : [ "C. Rother", "V. Kolmogorov", "A. Blake" ],
      "venue" : "ACM Transactions on Graphics (TOG),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2004
    }, {
      "title" : "A combinatorial algorithm minimizing submodular functions in strongly polynomial time",
      "author" : [ "A. Schrijver" ],
      "venue" : "Journal of Combinatorial Theory, Series B,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2000
    }, {
      "title" : "Combinatorial optimization - Polyhedra and Efficiency",
      "author" : [ "A. Schrijver" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2003
    }, {
      "title" : "Min norm point algorithm for higher order MRF-MAP inference",
      "author" : [ "I. Shanu", "C. Arora", "P. Singla" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Efficient minimization of decomposable submodular functions",
      "author" : [ "P. Stobbe", "A. Krause" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2010
    }, {
      "title" : "Finding the nearest point in a polytope",
      "author" : [ "P. Wolfe" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1976
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "The SFM problem can be reduced to finding the minimum norm point of the base polytope B(f) [10].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 27,
      "context" : "We refer the reader to Sections 44–45 in [28] for concepts and results in submodular optimization, and to [2] on machine learning applications.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "We refer the reader to Sections 44–45 in [28] for concepts and results in submodular optimization, and to [2] on machine learning applications.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "SFM has had a long history in combinatorial optimization since the early 1970s, following the influential work of Edmonds [4].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "The first polynomial-time algorithm was obtained via the ellipsoid method [14]; recent work presented substantial improvements using this approach [22].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "The first polynomial-time algorithm was obtained via the ellipsoid method [14]; recent work presented substantial improvements using this approach [22].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "Substantial work focused on designing strongly polynomial combinatorial algorithms [9, 15, 16, 25, 17, 27].",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "Substantial work focused on designing strongly polynomial combinatorial algorithms [9, 15, 16, 25, 17, 27].",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "Substantial work focused on designing strongly polynomial combinatorial algorithms [9, 15, 16, 25, 17, 27].",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 24,
      "context" : "Substantial work focused on designing strongly polynomial combinatorial algorithms [9, 15, 16, 25, 17, 27].",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "Substantial work focused on designing strongly polynomial combinatorial algorithms [9, 15, 16, 25, 17, 27].",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 26,
      "context" : "Substantial work focused on designing strongly polynomial combinatorial algorithms [9, 15, 16, 25, 17, 27].",
      "startOffset" : 83,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "Kolmogorov [21] showed that the problem can be reduced to submodular flow maximization, and also presented a more efficient augmenting path algorithm.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "Subsequent discrete approaches were given in [1, 7, 8].",
      "startOffset" : 45,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "Subsequent discrete approaches were given in [1, 7, 8].",
      "startOffset" : 45,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "Subsequent discrete approaches were given in [1, 7, 8].",
      "startOffset" : 45,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "Gradient methods were applied for the decomposable setting in [5, 24, 30].",
      "startOffset" : 62,
      "endOffset" : 73
    }, {
      "referenceID" : 23,
      "context" : "Gradient methods were applied for the decomposable setting in [5, 24, 30].",
      "startOffset" : 62,
      "endOffset" : 73
    }, {
      "referenceID" : 29,
      "context" : "Gradient methods were applied for the decomposable setting in [5, 24, 30].",
      "startOffset" : 62,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "[18, 30]), the model is restricted to functions fi of a simple specific type that are easy to minimize.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 29,
      "context" : "[18, 30]), the model is restricted to functions fi of a simple specific type that are easy to minimize.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 28,
      "context" : "[29] use a block coordinate descent method for level-1, and make no assumptions on the functions fi.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "The oracles are evaluated via the Fujishige-Wolfe minimum norm point algorithm [11, 31] for level-0.",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 30,
      "context" : "The oracles are evaluated via the Fujishige-Wolfe minimum norm point algorithm [11, 31] for level-0.",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 28,
      "context" : "[29] compare the performance of their SoS Min-Norm algorithm to the continuous approach of Jegelka et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] and the combinatorial approach of Arora et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "However, these implementations cannot be directly compared, since they use three different level-0 algorithms: Fujishige-Wolfe in SoS Min-Norm, a general QP solver for the algorithm of [18], and exhaustive search for [1].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "However, these implementations cannot be directly compared, since they use three different level-0 algorithms: Fujishige-Wolfe in SoS Min-Norm, a general QP solver for the algorithm of [18], and exhaustive search for [1].",
      "startOffset" : 217,
      "endOffset" : 220
    }, {
      "referenceID" : 17,
      "context" : "For potentials of large support, Fujishige-Wolfe outperforms these other level-0 subroutines, hence the level-1 algorithms in [18, 1] could have compared more favorably using the same Fujishige-Wolfe subroutine.",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : "For potentials of large support, Fujishige-Wolfe outperforms these other level-0 subroutines, hence the level-1 algorithms in [18, 1] could have compared more favorably using the same Fujishige-Wolfe subroutine.",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "Our main theoretical contribution improves the worst-case complexity bound of the most recent continuous optimization methods [5, 24] by a factor of r, the number of functions in the decomposition.",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "Our main theoretical contribution improves the worst-case complexity bound of the most recent continuous optimization methods [5, 24] by a factor of r, the number of functions in the decomposition.",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "This provides not only better, but also considerably simpler arguments than the algebraic proof in [24].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "We compare the state-of-the-art continuous and discrete algorithms: RCDM and ACDM from [5] with Submodular IBFS from [7].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "We compare the state-of-the-art continuous and discrete algorithms: RCDM and ACDM from [5] with Submodular IBFS from [7].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : "In this section, we outline a level-1 algorithmic framework for DSFM that is based on a combinatorial framework first studied by Fujishige and Zhang [12] for submodular intersection.",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 11,
      "context" : "We refer the reader to Fujishige and Zhang [12] for a proof of this claim.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 12,
      "context" : "In particular, based on the preflow-push algorithm [13], one can obtain a strongly polynomial DSFM algorithm with running time O(n(2)Θmax ∑r i=1 |Ci|(2)).",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "In our experiments, we use the submodular IBFS algorithm [7] as the main discrete level-1 algorithm; the same running time estimate as for preflow-push is applicable.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "In the DSFM setting, one can write (Min-Norm) in multiple equivalent forms [18].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "The gradient descent algorithms of [24, 5] provide level-1 algorithms for DSFM.",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "The gradient descent algorithms of [24, 5] provide level-1 algorithms for DSFM.",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 23,
      "context" : "[24] minimize (Best-Approx) using alternating projections.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "Ene and Nguyen [5] minimize (Prox-DSFM) using random coordinate descent.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "ACDM, the accelerated version of the algorithm, presents a further enhancement using techniques from [6].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 23,
      "context" : "Instead, the works [24, 5] show that there are certain parameters that one can associate with the objective functions such that the convergence is at the rate (1−α), where α ∈ (0, 1) is a quantity that depends on the appropriate parameter.",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "Instead, the works [24, 5] show that there are certain parameters that one can associate with the objective functions such that the convergence is at the rate (1−α), where α ∈ (0, 1) is a quantity that depends on the appropriate parameter.",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 18,
      "context" : "We can upper bound the diameter of the base polytope byO( √ nFmax) [19], and thus ‖x−x‖2 = O( √ nFmax).",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "For integer-valued functions, a ε-approximate solution can be converted to an exact optimum if ε = O(1/n) [2].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 23,
      "context" : "Besides giving a stronger bound, our proof is considerably simpler than the algebraic one using Cheeger’s inequality in [24].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "Using the analysis of [12], one can show that this change maintains x ∈ P , and that the algorithm terminates in finite (in fact, strongly polynomial) time.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "The works [1, 7] use a brute force search as the subroutine for each each fi, whose running time is 2iEOi.",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 6,
      "context" : "The works [1, 7] use a brute force search as the subroutine for each each fi, whose running time is 2iEOi.",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "As a general-purpose algorithm, we used the Fujishige-Wolfe minimum norm point algorithm [11, 31].",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 30,
      "context" : "As a general-purpose algorithm, we used the Fujishige-Wolfe minimum norm point algorithm [11, 31].",
      "startOffset" : 89,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "This provides an ε-approximate solution inO(|Ci|F 2 i,max/ε) iterations, with overall running time bound O((|Ci|(4) + |Ci|(2)EOi)F 2 i,max/ε) [3].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 19,
      "context" : "The experimental running time of the Fujishige-Wolfe algorithm can be prohibitively large [20].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "Therefore we have chosen to use the simple hand-tuned potentials that were used in previous work: the edge-based costs [1] and the count-based costs defined by [29, 30].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 28,
      "context" : "Therefore we have chosen to use the simple hand-tuned potentials that were used in previous work: the edge-based costs [1] and the count-based costs defined by [29, 30].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 29,
      "context" : "Therefore we have chosen to use the simple hand-tuned potentials that were used in previous work: the edge-based costs [1] and the count-based costs defined by [29, 30].",
      "startOffset" : 160,
      "endOffset" : 168
    }, {
      "referenceID" : 25,
      "context" : "The unary potentials are derived from Gaussian Mixture Models of color features [26].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "The cost of a labeling is the square root of the number of neighboring pixels that have different labels, as in [1].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 29,
      "context" : "We use the algorithm from [30] to identify regions.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 28,
      "context" : "For each region Ci, the labeling cost is fi(S) = |S||Ci \\ S|, where S and Ci \\ S are the subsets of Ci labeled 0 and 1, respectively, see [29, 30].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 29,
      "context" : "For each region Ci, the labeling cost is fi(S) = |S||Ci \\ S|, where S and Ci \\ S are the subsets of Ci labeled 0 and 1, respectively, see [29, 30].",
      "startOffset" : 138,
      "endOffset" : 146
    } ],
    "year" : 2017,
    "abstractText" : "This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization. We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms.",
    "creator" : null
  }
}