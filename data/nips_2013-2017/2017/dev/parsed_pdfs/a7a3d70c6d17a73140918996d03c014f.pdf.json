{
  "name" : "a7a3d70c6d17a73140918996d03c014f.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Sample Complexity Measure with Applications to Learning Optimal Auctions",
    "authors" : [ "Vasilis Syrgkanis" ],
    "emails" : [ "vasy@microsoft.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "the expected generalization error is upper bounded by O (√\nlog(τ̂H(2m)) m\n) . Our\nresult is enabled by a strengthening of the Rademacher complexity analysis of the expected generalization error. We show that this sample complexity measure, greatly simplifies the analysis of the sample complexity of optimal auction design, for many auction classes studied in the literature. Their sample complexity can be derived solely by noticing that in these auction classes, ERM on any sample or sub-sample will pick parameters that are equal to one of the points in the sample."
    }, {
      "heading" : "1 Introduction",
      "text" : "The seminal work of [11] gave a recipe for designing the revenue maximizing auction in auction settings where the private information of players is a single number and when the distribution over this number is completely known to the auctioneer. The latter raises the question of how has the auction designer formed this prior distribution over the private information. Recent work, starting from [4], addresses the question of how to design optimal auctions when having access only to samples of values from the bidders. We refer the reader to [5] for an overview of the existing results in the literature. [4, 9, 10, 2] give bounds on the sample complexity of optimal auctions without computational efficiency, while recent work has also focused on getting computationally efficient learning bounds [5, 13, 6].\nThis work solely focuses on sample complexity and not computational efficiency and thus is more related to [4, 9, 10, 2]. The latter work, uses tools from supervised learning, such as pseudodimension [12] (a variant of VC dimension for real-valued functions), compression bounds [8] and Rademacher complexity [12, 14] to bound the sample complexity of simple auction classes. Our work introduces a new measure of sample complexity, which is a strengthening the Rademacher complexity analysis and hence could also be of independent interest outside the scope of the sample complexity of optimal auctions. Moreover, for the case of auctions, this measure greatly simplifies the analysis of their sample complexity in many cases.\nIn particular, we show that in general PAC learning settings, the expected generalization error is upper bounded by the Rademacher complexity not of the whole class of hypotheses, but rather only over the class of hypotheses that could be the outcome of running Expected Risk Minimization (ERM) on a subset of the samples of half the size. If the number of these hypotheses is small, then the latter immediately yields a small generalization error. We refer to the growth rate of the latter set of hypotheses as the split-sample growth rate. This measure of complexity is not restricted to auction design and could be relevant to general statistical learning theory.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nWe then show that for many auction classes such as single-item auctions with player-specific reserves, single item t-level auctions and multiple-item item pricing auctions with additive buyers, the splitsample growth rate can be very easily bounded. The argument boils down to just saying that the Empirical Risk Minimization over this classes will set the parameters of the auctions to be equal to some value of some player in the sample. Then a simple counting argument gives bounds of the same order as in prior work in the literature that used the pseudo-dimension [9, 10]. In multi-item settings we also get improvements on the sample complexity bound.\nSplit-sample growth rate is similar in spirit to the notion of local Rademacher complexity [3], which looks at the Rademacher complexity on a subset of hypotheses with small empirical error. In particular, our proof is based on a refinement of the classic analysis Rademacher complexity analysis of generalization error (see e.g. [14]). However, our bound is more structural, restricting the set to outcomes of the chosen ERM process on a sub-sample of half the size. Moreover, we note that counting the number of possible outputs of ERM also has connections to a counting argument made in [1] in the context of pricing mechanisms. However, in essence the argument there is restricted to transductive settings where the sample “features” are known in advance and fixed and thereby the argument is much more straightforward and more similar to standard notions of “effective hypothesis space” used in VC-dimension arguments.\nOur new measure of sample complexity is applicable in the general statistical learning theory framework and hence could have applications beyond auctions. To convey a high level intuition of settings where split-sample growth could simplify the sample complexity analysis, suppose that the output hypothesis of ERM is uniquely defined by a constant number of sample points (e.g. consider linear separators and assume that the loss is such that the output of ERM is uniquely characterized by choosing O(d) points from the sample). Then this means that the number of possible hypotheses on any subset of size m/2, is at most O( ( m d ) ) = O(md). Then the split sample growth rate analysis\nimmediately yields that the expected generalization error is O( √ d · log(m)/m), or equivalently the sample complexity of learning over this hypothesis class to within an error is O(d · log(1/ )/ 2)."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We look at the sample complexity of optimal auctions. We consider the case of m items, and n bidders. Each bidder has a value function vi drawn independently from a distribution Di and we denote with D the joint distribution.\nWe assume we are given a sample set S = {v1, . . . ,vm}, ofm valuation vectors, where each vt ∼ D. Let H denote the class of all dominant strategy truthful single item auctions (i.e. auctions where no player has incentive to report anything else other than his true value to the auction, independent of what other players do). Moreover, let\nr(h,v) = n∑ i=1 phi (v) (1)\nwhere phi (·) is the payment function of mechanism h, and r(h,v) is the revenue of mechanism h on valuation vector v. Finally, let RD(h) = Ev∼D [r(h,v)] (2) be the expected revenue of mechanism h under the true distribution of values D.\nGiven a sample S of size m, we want to compute a dominant strategy truthful mechanism hS , such that:\nES [RD(hS)] ≥ sup h∈H RD(h)− (m) (3)\nwhere (m)→ 0 as m→∞. We refer to (m) as the expected generalization error. Moreover, we define the sample complexity of an auction class as:\nDefinition 1 (Sample Complexity of Auction Class). The (additive error) sample complexity of an auction class H and a class of distributions D, for an accuracy target is defined as the smallest number of samples m( ), such that for any m ≥ m( ):\nES [RD(hS)] ≥ sup h∈H RD(h)− (4)\nWe might also be interested in a multiplcative error sample complexity, i.e.\nES [RD(hS)] ≥ (1− ) sup h∈H RD(h) (5)\nThe latter is exactly the notion that is used in [4, 5]. If one assumes that the optimal revenue on the distribution is lower bounded by some constant quantity, then an additive error implies a multiplicative error. For instance, if one assumes that player values are bounded away from zero with significant probability, then that implies a lower bound on revenue. Such assumptions for instance, are made in the work of [9]. We will focus on additive error in this work.\nWe will also be interested in proving high probability guarantees, i.e. with probability 1− δ:\nRD(hS) ≥ sup h∈H RD(h)− (m, δ) (6)\nwhere for any δ, (m, δ)→ 0 as m→∞."
    }, {
      "heading" : "3 Generalization Error via the Split-Sample Growth Rate",
      "text" : "We turn to the general PAC learning framework, and we give generalization guarantees in terms of a new notion of complexity of a hypothesis space H , which we denote as split-sample growth rate.\nConsider an arbitrary hypothesis space H and an arbitrary data space Z, and suppose we are given a set S of m samples {z1, . . . , zm}, where each zt is drawn i.i.d. from some distribution D on Z. We are interested in maximizing some reward function r : H × Z → [0, 1], in expectation over distribution D. In particular, denote with RD(h) = Ez∼D [r(h, z)]. We will look at the Expected Reward Maximization algorithm on S, with some fixed tie-breaking rule. Specifically, if we let\nRS(h) = 1\nm m∑ t=1 r(h, zt) (7)\nthen ERM is defined as: hS = arg sup\nh∈H RS(h) (8)\nwhere ties are broken based on some pre-defined manner.\nWe define the notion of a split-sample hypothesis space:\nDefinition 2 (Split-Sample Hypothesis Space). For any sample S, let ĤS , denote the set of all hypothesis hT output by the ERM algorithm (with the pre-defined tie-breaking rule), on any subset T ⊂ S, of size d|S|/2e, i.e.:\nĤS = {hT : T ⊂ S, |T | = d|S|/2e} (9)\nBased on the split-sample hypothesis space, we also define the split-sample growth rate of a hypothesis space H at value m, as the largest possible size of ĤS for any set S of size m. Definition 3 (Split-Sample Growth Rate). The split-sample growth rate of a hypothesis H and an ERM process for H , is defined as:\nτ̂H(m) = sup S:|S|=m\n|ĤS | (10)\nWe first show that the generalization error is upper bounded by the Rademacher complexity evaluated on the split-sample hypothesis space of the union of two samples of size m. The Rademacher complexityR(S,H) of a sample S of size m and a hypothesis space H is defined as:\nR(S,H) = Eσ [ sup h∈H 2 m ∑ zt∈S σt · r(h, zt) ] (11)\nwhere σ = (σ1, . . . , σm) and each σt is an independent binary random variable taking values {−1, 1}, each with equal probability.\nLemma 1. For any hypothesis space H , and any fixed ERM process, we have:\nES [RD(hS)] ≥ sup h∈H\nRD(h)− ES,S′ [ R(S, ĤS∪S′) ] , (12)\nwhere S and S′ are two independent samples of some size m.\nProof. Let h∗ be the optimal hypothesis for distribution D. First we re-write the left hand side, by adding and subtracting the expected empirical reward:\nES [RD(hS)] = ES [RS(hS)]− ES [RS(hS)− RD(hS)] ≥ ES [RS(h∗)]− ES [RS(hS)− RD(hS)] (hS maximizes empirical reward) = RD(h∗)− ES [RS(hS)− RD(hS)] (h∗ is independent of S)\nThus it suffices to upper bound the second quantity in the above equation.\nSince RD(h) = ES′ [RS′(h)] for a fresh sample S′ of size m, we have: ES [RS(hS)− RD(hS)] = ES [RS(hS)− ES′ [RS′(hS)]]\n= ES,S′ [RS(hS)− RS′(hS)]\nNow, consider the set ĤS∪S′ . Since S is a subset of S ∪ S′ of size |S ∪ S′|/2, we have by the definition of the split-sample hypothesis space that hS ∈ ĤS∪S′ . Thus we can upper bound the latter quantity by taking a supremum over h ∈ ĤS∪S′ :\nES [RS(hS)− RD(hS)] ≤ ES,S′ [\nsup h∈ĤS∪S′\nRS(h)− RS′(h)\n]\n= ES,S′ [\nsup h∈ĤS∪S′\n1\nm m∑ t=1 (r(h, zt)− r(h, z′t)) ] Now observe, that we can rename any sample zt ∈ S to z′t and sample z′t ∈ S′ to zt. By doing show we do not change the distribution. Moreover, we do not change the quantity HS∪S′ , since S ∪ S′ is invariant to such swaps. Finally, we only change the sign of the quantity (r(h, zt)− r(h, z′t)). Thus if we denote with σt ∈ {−1, 1}, a Rademacher variable, we get the above quantity is equal to:\nES,S′ [\nsup h∈ĤS∪S′\n1\nm m∑ t=1 (r(h, zt)− r(h, z′t))\n] = ES,S′ [ sup\nh∈ĤS∪S′\n1\nm m∑ t=1 σt (r(h, zt)− r(h, z′t)) ] (13)\nfor any vector σ = (σ1, . . . , σm) ∈ {−1, 1}m. The latter also holds in expectation over σ, where σt is randomly drawn between {−1, 1} with equal probability. Hence:\nES [RS(hS)− RD(hS)] ≤ ES,S′,σ [ sup\nh∈ĤS∪S′\n1\nm m∑ t=1 σt (r(h, zt)− r(h, z′t)) ] By splitting the supremma into a positive and negative part and observing that the two expected quantities are identical, we get:\nES [RS(hS)− RD(hS)] ≤ 2ES,S′,σ [ sup\nh∈ĤS∪S′\n1\nm m∑ t=1 σtr(h, zt) ] = ES,S′ [ R(S, ĤS∪S′)\n] whereR(S,H) denotes the Rademacher complexity of a sample S and hypothesis H .\nObserve, that the latter theorem is a strengthening of the fact that the Rademacher complexity upper bounds the generalization error, simply because:\nES,S′ [ R(S, ĤS∪S′) ] ≤ ES,S′ [R(S,H)] = ES [R(S,H)] (14)\nThus if we can bound the Rademacher complexity of H , then the latter lemma gives a bound on the generalization error. However, the reverse might not be true. Finally, we show our main theorem, which shows that if the split-sample hypothesis space has small size, then we immediately get a generalization bound, without the need to further analyze the Rademacher complexity of H .\nTheorem 2 (Main Theorem). For any hypothesis space H , and any fixed ERM process, we have:\nES [RD(hS)] ≥ sup h∈H\nRD(h)− √ 2 log(τ̂H(2m))\nm (15)\nMoreover, with probability 1− δ:\nRD(hS) ≥ sup h∈H\nRD(h)− 1\nδ\n√ 2 log(τ̂H(2m))\nm (16)\nProof. By applying Massart’s lemma (see e.g. [14]) we have that:\nR(S, ĤS∪S′) ≤\n√ 2 log(|ĤS∪S′ |) m ≤ √ 2 log(τ̂H(2m)) m (17)\nCombining the above with Lemma 1, yields the first part of the theorem.\nFinally, the high probability statement follows from observing that the random variable suph∈H RD(h) − RD(hS) is non-negative and by applying Markov’s inequality: with probability 1− δ\nsup h∈H\nRD(h)−RD(hS) ≤ 1 δ ES [ sup h∈H RD(h)−RD(hS) ] ≤ 1 δ √ 2 log(τ̂H(2m)) m (18)\nThe latter theorem can be trivially extended to the case when r : H ×Z → [α, β], leading to a bound of the form:\nES [RD(hS)] ≥ sup h∈H\nRD(h)− (β − α) √ 2 log(τ̂H(2m))\nm (19)\nWe note that unlike the standard Rademacher complexity, which is defined asR(S,H), our bound, which is based on boundingR(S, ĤS∪S′) for any two datasets S, S′ of equal size, does not imply a high probability bound via McDiarmid’s inequality (see e.g. Chapter 26 of [14] of how this is done for Rademacher complexity analysis), but only via Markov’s inequality. The latter yields a worse dependence on the confidence δ on the high probability bound of 1/δ, rather than log(1/δ). The reason for the latter is that the quantity R(S, ĤS∪S′), depends on the sample S, not only in terms of on which points to evaluate the hypothesis, but also on determining the hypothesis space ĤS∪S′ . Hence, the function:\nf(z1, . . . , zm) = ES′  sup h∈Ĥ{z1,...,zm}∪S′ 1 m m∑ t=1 σt (r(h, zt)− r(h, z′t))  (20) does not satisfy the stability property that |f(z) − f(z′′i , z−i)| ≤ 1m . The reason being that the supremum is taken over a different hypothesis space in the two inputs. This is unlike the case of the function:\nf(z1, . . . , zm) = ES′ [ sup h∈H 1 m m∑ t=1 σt (r(h, zt)− r(h, z′t)) ] (21)\nwhich is used in the standard Rademacher complexity bound analysis, which satisfies the latter stability property. Resolving whether this worse dependence on δ is necessary is an interesting open question."
    }, {
      "heading" : "4 Sample Complexity of Auctions via Split-Sample Growth",
      "text" : "We now present the application of the latter measure of complexity to the analysis of the sample complexity of revenue optimal auctions. Thoughout this section we assume that the revenue of any auction lies in the range [0, 1]. The results can be easily adapted to any other range [α, β], by\nre-scaling the equations, which will lead to blow-ups in the sample complexity of the order of an extra (β − α) multiplicative factor. This limits the results here to bounded distributions of values. However, as was shown in [5], one can always cap the distribution of values up to some upper bound, for the case of regular distributions, by losing only an fraction of the revenue. So one can apply the results below on this capped distribution.\nSingle bidder and single item. Consider the case of a single bidder and single item auction. In this setting, it is known by results in auction theory [11] that an optimal auction belongs to the hypothesis class H = {post a reserve price r for r ∈ [0, 1]}. We consider, the ERM rule, which for any set S, in the case of ties, it favors reserve prices that are equal to some valuation vt ∈ S. Wlog assume that samples v1, . . . , vm are ordered in increasing order. Observe, that for any set S, this ERM rule on any subset T of S, will post a reserve price that is equal to some value vt ∈ T . Any other reserve price in between two values [vt, vt+1] is weakly dominated by posting r = vt+1, as it does not change which samples are allocated and we can only increase revenue. Thus the space ĤS is a subset of {post a reserve price r ∈ {v1, . . . , vm}. The latter is of size m. Thus the split-sample growth of H is τ̂H(m) ≤ m. This yields:\nES [RD(hS)] ≥ sup h∈H\nRD(h)− √ 2 log(2m)\nm (22)\nEquivalently, the sample complexity is mH( ) = O ( log(1/ ) 2 ) .\nMultiple i.i.d. regular bidders and single item. In this case, it is known by results in auction theory [11] that the optimal auction belongs to the space of hypotheses H consisting of second price auctions with some reserve r ∈ [0, 1]. Again if we consider ERM which in case of ties favors a reserve that equals to a value in the sample (assuming that is part of the tied set, or outputs any other value otherwise), then observe that for any subset T of a sample S, ERM on that subset will pick a reserve price that is equal to one of the values in the samples S. Thus τ̂H(m) ≤ n ·m. This yields:\nES [RD(hS)] ≥ sup h∈H\nRD(h)− √\n2 log(2 · n ·m) m\n(23)\nEquivalently, the sample complexity is mH( ) = O ( log(n/ 2) 2 ) .\nNon-i.i.d. regular bidders, single item, second price with player specific reserves. In this case, it is known by results in auction theory [11] that the optimal auction belongs to the space of hypotheses HSP consisting of second price auctions with some reserve ri ∈ [0, 1] for each player i. Again if we consider ERM which in case of ties favors a reserve that equals to a value in the sample (assuming that is part of the tied set, or outputs any other value otherwise), then observe that for any subset T of a sample S, ERM on that subset will pick a reserve price ri that is equal to one of the values vit of player i in the sample S. There are m such possible choices for each player, thus mn possible choices of reserves in total. Thus τ̂H(m) ≤ mn. This yields:\nES [RD(hS)] ≥ sup h∈HSP\nRD(h)− √ 2n log(2m)\nm (24)\nIf H is the space of all dominant strategy truthful mechanisms, then by prophet inequalities (see [7]), we know that suph∈HSP RD(h) ≥ 1 2 suph∈H RD(h). Thus:\nES [RD(hS)] ≥ 1\n2 sup h∈H\nRD(h)− √ 2n log(2m)\nm (25)\nNon-i.i.d. irregular bidders single item. In this case it is known by results in auction theory [11] that the optimal auction belongs to the space of hypotheses H consisting of all virtual welfare maximizing auctions: For each player i, pick a monotone function φ̂i(vi) ∈ [−1, 1] and allocate to the player with the highest non-negative virtual value, charging him the lowest value he could have bid and still win the item. In this case, we will first coarsen the space of all possible auctions.\nIn particular, we will consider the class of t-level auctions of [9]. In this class, we constrain the value functions φ̂i(vi) to only take values in the discrete grid in [0, 1]. We will call this class H . An equivalent representation of these auctions is by saying that for each player i, we define a vector of thresholds 0 = θi0 ≤ θi1 ≤ . . . ≤ θis ≤ θis+1 = 1, with s = 1/ . The index of a player is the largest j for which vi ≥ θj . Then we allocate the item to the player with the highest index (breaking ties lexicographically) and charge the minimum value he has to bid to continue to win.\nObserve that on any sample S of valuation vectors, it is always weakly better to place the thresholds θij on one of the values in the set S. Any other threshold is weakly dominated, as it does not change the allocation. Thus for any subset T of a set S of size m, we have that the thresholds of each player i will take one of the values of player i that appears in set S. We have 1/ thresholds for each player, hence m1/ combinations of thresholds for each player and mn/ combinations of thresholds for all players. Thus τ̂H(m) ≤ mn/ . This yields:\nES [RD(hS)] ≥ sup h∈H\nRD(h)− √ 2n log(2m)\n·m (26)\nMoreover, by [9] we also have that:\nsup h∈H RD(h) ≥ sup h∈H RD(h)− (27)\nPicking, = (\n2n log(2m) m\n)1/3 , we get:\nES [RD(hS)] ≥ sup h∈H\nRD(h)− 2 ( 2n log(2m)\nm\n)1/3 (28)\nEquivalently, the sample complexity is mH( ) = O ( n log(1/ ) 3 ) .\nk items, n bidders, additive valuations, grand bundle pricing. If the reserve price was anonymous, then the reserve price output by ERM on any subset of a sample S of size m, will take the value of one of the m total values for the items of the buyers in S. So τ̂H(m) = m · n. If the reserve price was not anonymous, then for each buyer ERM will pick one of the m total item values, so τ̂H(m) ≤ mn. Thus the sample complexity is mH( ) = O ( n log(1/ ) 2 ) .\nk items, n bidders, additive valuations, item prices. If reserve prices are anonymous, then each reserve price on item j computed by ERM on any subset of a sample S of size m, will take the value of one of the player’s values for item j, i.e. n ·m. So τ̂H(m) = (n ·m)k. If reserve prices are not anonymous, then the reserve price on item j for player i will take the value of one of the player’s values for the item. So τ̂H(m) ≤ mn·k. Thus the sample complexity is mH( ) = O ( nk log(1/ ) 2 ) .\nk items, n bidders, additive valuations, best of grand bundle pricing and item pricing. ERM on the combination will take values on any subset of a sample S of size m, that is at most the product of the values of each of the classes (bundle or item pricing). Thus, for anonymous pricing: τ̂H(m) = (m · n)k+1 and for non-anonymous pricing: τ̂H(m) ≤ mn(k+1). Thus the sample complexity is mH( ) = O ( n(k+1) log(1/ ) 2 ) .\nIn the case of a single bidder, we know that the best of bundle pricing or item pricing is a 1/8 approximation to the overall best truthful mechanism for the true distribution of values, assuming values for each item are drawn independently. Thus in the latter case we have:\nES [RD(hS)] ≥ 1\n6 sup h∈H\nRD(h)− √ 2(k + 1) log(2m)\nm (29)\nwhere H is the class of all truthful mechanisms.\nComparison with [10]. The latter three applications were analyzed by [10], via the notion of the pseudo-dimension, but their results lead to sample complexity bounds of O(nk log(nk) log(1/ ) 2 ). Thus the above simpler analysis removes the extra log factor on the dependence."
    } ],
    "references" : [ {
      "title" : "Mechanism design via machine learning",
      "author" : [ "M.F. Balcan", "A. Blum", "J.D. Hartline", "Y. Mansour" ],
      "venue" : "In 46th Annual IEEE Symposium on Foundations of Computer Science",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "Vitercik. Sample complexity of automated mechanism design",
      "author" : [ "Maria-Florina F Balcan", "Tuomas Sandholm", "Ellen" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Local rademacher complexities",
      "author" : [ "Peter L. Bartlett", "Olivier Bousquet", "Shahar Mendelson" ],
      "venue" : "Ann. Statist., 33(4):1497–1537,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "The sample complexity of revenue maximization",
      "author" : [ "Richard Cole", "Tim Roughgarden" ],
      "venue" : "In 46th,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "The sample complexity of auctions with side information",
      "author" : [ "Nikhil R. Devanur", "Zhiyi Huang", "Christos-Alexandros Psomas" ],
      "venue" : "In Proceedings of the Forty-eighth Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Efficient empirical revenue maximization in singleparameter auction environments",
      "author" : [ "Yannai A. Gonczarowski", "Noam Nisan" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Simple versus optimal mechanisms",
      "author" : [ "Jason D. Hartline", "Tim Roughgarden" ],
      "venue" : "In Proceedings of the 10th ACM Conference on Electronic Commerce,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm",
      "author" : [ "Nick Littlestone" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1988
    }, {
      "title" : "The pseudo-dimension of near-optimal auctions",
      "author" : [ "Jamie Morgenstern", "Tim Roughgarden" ],
      "venue" : "In Proceedings of the 28th International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Learning simple auctions",
      "author" : [ "Jamie Morgenstern", "Tim Roughgarden" ],
      "venue" : "COLT",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Optimal auction design",
      "author" : [ "Roger B Myerson" ],
      "venue" : "Mathematics of operations research,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1981
    }, {
      "title" : "Convergence of Stochastic Processes",
      "author" : [ "D. Pollard" ],
      "venue" : "Springer Series in Statistics",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Ironing in the dark",
      "author" : [ "Tim Roughgarden", "Okke Schrijvers" ],
      "venue" : "In Proceedings of the 2016 ACM Conference on Economics and Computation, EC",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Understanding Machine Learning: From Theory to Algorithms. Understanding Machine Learning: From Theory to Algorithms",
      "author" : [ "S. Shalev-Shwartz", "S. Ben-David" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "The seminal work of [11] gave a recipe for designing the revenue maximizing auction in auction settings where the private information of players is a single number and when the distribution over this number is completely known to the auctioneer.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "Recent work, starting from [4], addresses the question of how to design optimal auctions when having access only to samples of values from the bidders.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 4,
      "context" : "We refer the reader to [5] for an overview of the existing results in the literature.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "[4, 9, 10, 2] give bounds on the sample complexity of optimal auctions without computational efficiency, while recent work has also focused on getting computationally efficient learning bounds [5, 13, 6].",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "[4, 9, 10, 2] give bounds on the sample complexity of optimal auctions without computational efficiency, while recent work has also focused on getting computationally efficient learning bounds [5, 13, 6].",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 9,
      "context" : "[4, 9, 10, 2] give bounds on the sample complexity of optimal auctions without computational efficiency, while recent work has also focused on getting computationally efficient learning bounds [5, 13, 6].",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "[4, 9, 10, 2] give bounds on the sample complexity of optimal auctions without computational efficiency, while recent work has also focused on getting computationally efficient learning bounds [5, 13, 6].",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "[4, 9, 10, 2] give bounds on the sample complexity of optimal auctions without computational efficiency, while recent work has also focused on getting computationally efficient learning bounds [5, 13, 6].",
      "startOffset" : 193,
      "endOffset" : 203
    }, {
      "referenceID" : 12,
      "context" : "[4, 9, 10, 2] give bounds on the sample complexity of optimal auctions without computational efficiency, while recent work has also focused on getting computationally efficient learning bounds [5, 13, 6].",
      "startOffset" : 193,
      "endOffset" : 203
    }, {
      "referenceID" : 5,
      "context" : "[4, 9, 10, 2] give bounds on the sample complexity of optimal auctions without computational efficiency, while recent work has also focused on getting computationally efficient learning bounds [5, 13, 6].",
      "startOffset" : 193,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "This work solely focuses on sample complexity and not computational efficiency and thus is more related to [4, 9, 10, 2].",
      "startOffset" : 107,
      "endOffset" : 120
    }, {
      "referenceID" : 8,
      "context" : "This work solely focuses on sample complexity and not computational efficiency and thus is more related to [4, 9, 10, 2].",
      "startOffset" : 107,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "This work solely focuses on sample complexity and not computational efficiency and thus is more related to [4, 9, 10, 2].",
      "startOffset" : 107,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "This work solely focuses on sample complexity and not computational efficiency and thus is more related to [4, 9, 10, 2].",
      "startOffset" : 107,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : "The latter work, uses tools from supervised learning, such as pseudodimension [12] (a variant of VC dimension for real-valued functions), compression bounds [8] and Rademacher complexity [12, 14] to bound the sample complexity of simple auction classes.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "The latter work, uses tools from supervised learning, such as pseudodimension [12] (a variant of VC dimension for real-valued functions), compression bounds [8] and Rademacher complexity [12, 14] to bound the sample complexity of simple auction classes.",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 11,
      "context" : "The latter work, uses tools from supervised learning, such as pseudodimension [12] (a variant of VC dimension for real-valued functions), compression bounds [8] and Rademacher complexity [12, 14] to bound the sample complexity of simple auction classes.",
      "startOffset" : 187,
      "endOffset" : 195
    }, {
      "referenceID" : 13,
      "context" : "The latter work, uses tools from supervised learning, such as pseudodimension [12] (a variant of VC dimension for real-valued functions), compression bounds [8] and Rademacher complexity [12, 14] to bound the sample complexity of simple auction classes.",
      "startOffset" : 187,
      "endOffset" : 195
    }, {
      "referenceID" : 8,
      "context" : "Then a simple counting argument gives bounds of the same order as in prior work in the literature that used the pseudo-dimension [9, 10].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 9,
      "context" : "Then a simple counting argument gives bounds of the same order as in prior work in the literature that used the pseudo-dimension [9, 10].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 2,
      "context" : "Split-sample growth rate is similar in spirit to the notion of local Rademacher complexity [3], which looks at the Rademacher complexity on a subset of hypotheses with small empirical error.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : "Moreover, we note that counting the number of possible outputs of ERM also has connections to a counting argument made in [1] in the context of pricing mechanisms.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "The latter is exactly the notion that is used in [4, 5].",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "The latter is exactly the notion that is used in [4, 5].",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "Such assumptions for instance, are made in the work of [9].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "Chapter 26 of [14] of how this is done for Rademacher complexity analysis), but only via Markov’s inequality.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 4,
      "context" : "However, as was shown in [5], one can always cap the distribution of values up to some upper bound, for the case of regular distributions, by losing only an fraction of the revenue.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "In this setting, it is known by results in auction theory [11] that an optimal auction belongs to the hypothesis class H = {post a reserve price r for r ∈ [0, 1]}.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "In this case, it is known by results in auction theory [11] that the optimal auction belongs to the space of hypotheses H consisting of second price auctions with some reserve r ∈ [0, 1].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "In this case, it is known by results in auction theory [11] that the optimal auction belongs to the space of hypotheses HSP consisting of second price auctions with some reserve ri ∈ [0, 1] for each player i.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "If H is the space of all dominant strategy truthful mechanisms, then by prophet inequalities (see [7]), we know that suph∈HSP RD(h) ≥ 1 2 suph∈H RD(h).",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "In this case it is known by results in auction theory [11] that the optimal auction belongs to the space of hypotheses H consisting of all virtual welfare maximizing auctions: For each player i, pick a monotone function φ̂i(vi) ∈ [−1, 1] and allocate to the player with the highest non-negative virtual value, charging him the lowest value he could have bid and still win the item.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "In particular, we will consider the class of t-level auctions of [9].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "Moreover, by [9] we also have that: sup h∈H RD(h) ≥ sup h∈H RD(h)− (27)",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 9,
      "context" : "The latter three applications were analyzed by [10], via the notion of the pseudo-dimension, but their results lead to sample complexity bounds of O( log(nk) log(1/ ) 2 ).",
      "startOffset" : 47,
      "endOffset" : 51
    } ],
    "year" : 2017,
    "abstractText" : "We introduce a new sample complexity measure, which we refer to as split-sample growth rate. For any hypothesis H and for any sample S of size m, the splitsample growth rate τ̂H(m) counts how many different hypotheses can empirical risk minimization output on any sub-sample of S of size m/2. We show that the expected generalization error is upper bounded by O (√ log(τ̂H(2m)) m ) . Our result is enabled by a strengthening of the Rademacher complexity analysis of the expected generalization error. We show that this sample complexity measure, greatly simplifies the analysis of the sample complexity of optimal auction design, for many auction classes studied in the literature. Their sample complexity can be derived solely by noticing that in these auction classes, ERM on any sample or sub-sample will pick parameters that are equal to one of the points in the sample.",
    "creator" : null
  }
}