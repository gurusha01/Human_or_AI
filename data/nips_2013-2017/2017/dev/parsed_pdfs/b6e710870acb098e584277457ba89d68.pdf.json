{
  "name" : "b6e710870acb098e584277457ba89d68.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction",
    "authors" : [ "Zhan Shi", "Xinhua Zhang", "Yaoliang Yu" ],
    "emails" : [ "zshi22@uic.edu", "zhangx@uic.edu", "yaoliang.yu@uwaterloo.ca" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Many algorithmic advances have been achieved in machine learning by finely leveraging the separability in the model. For example, stochastic gradient descent (SGD) algorithms typically exploit the fact that the objective is an expectation of a random function, with each component corresponding to a training example. A “dual” approach partitions the problem into blocks of coordinates and processes them in a stochastic fashion [1]. Recently, by exploiting the finite-sum structure of the model, variance-reduction based stochastic methods have surpassed the well-known sublinear lower bound of SGD. Examples include SVRG [2], SAGA [3], SAG [4], Finito [5], MISO [6], and SDCA [7, 8], just to name a few. Specialized algorithms have also been proposed for accommodating proximal terms [9], and for further acceleration through the condition number [10–13].\nHowever, not all empirical risks are separable in its plain form, and in many cases dualization is necessary for achieving separability. This leads to a composite saddle-point problem with convexconcave (saddle) functions K and M :\n(x∗, y∗) = arg minx maxyK(x, y) +M(x, y), where K(x, y) = 1n ∑n k=1 ψk(x, y). (1)\nMost commonly used supervised losses for linear models can be written as g?(Xw), where g? is the Fenchel dual of a convex function g, X is the design matrix, and w is the model vector. So the regularized risk minimization can be naturally written as minw maxα α′Xw + Ω(w)− g(α), where Ω is a regularizer. This fits into our framework (1) with a bilinear function K and a decoupled function M . Optimization for this specific form of saddle-point problems has been extensively studied. For example, [14] and [15] performed batch updates on w and stochastic updates on α, while [16] and [17] performed doubly stochastic updates on both w and α, achieving O( 1 ) and O(log 1 ) rates respectively. The latter two also studied the more general form (1). Our interest in this paper is double stochasticity, aiming to maximally harness the power of separability and stochasticity.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nAdversarial machines, where the learner competes against an adversary, have re-gained much recent interest in machine learning [18–20]. On one hand they fit naturally into the saddle-point optimization framework (1) but on the other hand they are known to be notoriously challenging to solve. The central message of this work is that certain adversarial machines can be solved significantly faster than they used to be. Key to our development is a new extension of the stochastic variance-reduced algorithm in [17] such that it is compatible with any Bregman divergence, hence opening the possibility to largely reduce the quadratic condition number in [17] by better adapting to the underlying geometry using non-Euclidean norms and Bregman divergences.\nImproving condition numbers by Bregman divergence has long been studied in (stochastic, proximal) gradient descent [21, 22]. The best known algorithm is arguably stochastic mirror descent [23], which was extended to saddle-points by [16] and to ADMM by [24]. However, they can only achieve the sublinear rateO(1/ ) (for an -accurate solution). On the other hand, many recent stochastic variancereduced methods [2–6, 9, 17] that achieve the much faster linear rate O(log 1/ ) rely inherently on the Euclidean structure, and their extension to Bregman divergence, although conceptually clear, remains challenging in terms of the analysis. For example, the analysis of [17] relied on the resolvent of monotone operators [25] and is hence restricted to the Euclidean norm. In §2 we extend the notion of Bregman divergence to saddle functions and we prove a new Pythagorean theorem that may be of independent interest for analyzing first order algorithms. In §4 we introduce a fundamentally different proof technique (details relegated to Appendix C) that overcomes several challenges arising from a general Bregman divergence (e.g. asymmetry and unbounded gradient on bounded domain), and we recover similar quantitative linear rate of convergence as [17] but with the flexibility of using suitable Bregman divergences to reduce the condition number.\nThe new stochastic variance-reduced algorithm Breg-SVRG is then applied to the adversarial prediction framework (with multivariate losses such as F-score) [19, 20]. Here we make three novel contributions: (a) We provide a significant reformulation of the adversarial prediction problem that reduces the dimension of the optimization variable from 2n to n2 (where n is the number of samples), hence making it amenable to stochastic variance-reduced optimization (§3). (b) We develop a new efficient algorithm for computing the proximal update with a separable saddle KL-divergence (§5). (c) We verify that Breg-SVRG accelerates its Euclidean alternative by a factor of n in both theory and practice (§6), hence confirming again the uttermost importance of adapting to the underlying problem geometry. To our best knowledge, this is the first time stochastic variance-reduced methods have been shown with great promise in optimizing adversarial machines.\nFinally, we mention that we expect our algorithm Breg-SVRG to be useful for solving many other saddle-point problems, and we provide a second example (LPboosting) in experiments (§6)."
    }, {
      "heading" : "2 Bregman Divergence and Saddle Functions",
      "text" : "In this section we set up some notations, recall some background materials, and extend Bregman divergences to saddle functions, a key notion in our later analysis.\nBregman divergence. For any convex and differentiable function ψ over some closed convex set C ⊆ Rd, its induced Bregman divergence is defined as:\n∀x ∈ int(C), x′ ∈ C, ∆ψ(x′, x) := ψ(x′)− ψ(x)− 〈∇ψ(x), x′ − x〉 , (2)\nwhere ∇ψ is the gradient and 〈·, ·〉 is the standard inner product in Rd. Clearly, ∆ψ(x′, x) ≥ 0 since ψ is convex. We mention two familiar examples of Bregman divergence.\n• Squared Euclidean distance: ∆ψ(x′, x) = 12 ‖x ′ − x‖22 , ψ(x) = 1 2 ‖x‖ 2 2, where ‖ · ‖2 is `2 norm. • (Unnormalized) KL-divergence: ∆ψ(x′, x) = ∑ i x ′ i log x′i xi − x′i + xi, ψ(x) = ∑ i xi log xi.\nStrong convexity. Following [26] we call a function f ψ-convex if f−ψ is convex, i.e. for all x, x′\nf(x′) ≥ f(x) + 〈∂f(x), x′ − x〉+ ∆ψ(x′, x). (3)\nSmoothness. A function f is L-smooth wrt a norm ‖·‖ if its gradient∇f is L-Lipschitz continuous, i.e., for all x and x′, ‖∇f(x′)−∇f(x)‖∗ ≤ L ‖x′ − x‖ , where ‖ · ‖∗ is the dual norm of ‖ · ‖. The change of a smooth function, in terms of its induced Bregman divergence, can be upper bounded by the change of its input and lower bounded by the change of its slope, cf. Lemma 2 in Appendix A.\nSaddle functions. Recall that a function φ(x, y) over Cz = Cx × Cy is called a saddle function if it is convex in x for any y ∈ Cy, and concave in y for any x ∈ Cx. Given a saddle function φ, we call (x∗, y∗) its saddle point if\n∀x ∈ Cx, ∀y ∈ Cy, φ(x∗, y) ≤ φ(x∗, y∗) ≤ φ(x, y∗), (4)\nor equivalently (x∗, y∗) ∈ arg minx∈Cx maxy∈Cy φ(x, y). Assuming φ is differentiable, we denote\nGφ(x, y) := [∂xφ(x, y);−∂yφ(x, y)]. (5)\nNote the negation sign due to the concavity in y. We can quantify the notion of “saddle”: A function f(x, y) is called φ-saddle iff f − φ is a saddle function, or equivalently, ∆f (z′, z) ≥ ∆φ(z′, z) (see below). Note that any saddle function φ is 0-saddle and φ-saddle.\nBregman divergence for saddle functions. We now define the Bregman divergence induced by a saddle function φ: for z = (x, y) and z′ = (x′, y′) in Cz,\n∆φ(z ′, z) := ∆φy (x ′, x) + ∆−φx(y ′, y) = φ(x′, y)− φ(x, y′)− 〈Gφ(z), z′ − z〉 , (6)\nwhere φy(x) = φ(x, y) is a convex function of x for any fixed y, and similarly φx(y) = φ(x, y) is a concave (hence the negation) function of y for any fixed x. The similarity between (6) and the usual Bregman divergence ∆ψ in (2) is apparent. However, φ is never evaluated at z′ but z (for G) and the cross pairs (x′, y) and (x, y′). Key to our subsequent analysis is the following lemma that extends a result of [27] to saddle functions (proof in Appendix A). Lemma 1. Let f and g be φ-saddle and ϕ-saddle respectively, with one of them being differentiable. Then, for any z = (x, y) and any saddle point (if exists) z∗ := (x∗, y∗) ∈ arg minx maxy {f(z) + g(z)} , we have f(x, y∗)+g(x, y∗) ≥ f(x∗, y)+g(x∗, y)+∆φ+ϕ(z, z∗).\nGeometry of norms. In the sequel, we will design two convex functions ψx(x) and ψy(y) such that their induced Bregman divergences are “distance enforcing” (a.k.a. 1-strongly convex), that is, w.r.t. two norms ‖·‖x and ‖·‖y that we also design, the following inequality holds:\n∆x(x, x ′) := ∆ψx(x, x ′) ≥ 12 ‖x− x ′‖2x , ∆y(y, y ′) := ∆ψy(y, y ′) ≥ 12 ‖y − y ′‖2y . (7)\nFurther, for z = (x, y), we define\n∆z(z, z ′) := ∆ψx−ψy(z, z ′) ≥ 12 ‖z − z ′‖2z , where ‖z‖ 2 z := ‖x‖ 2 x + ‖y‖ 2 y (8)\nWhen it is clear from the context, we simply omit the subscripts and write ∆, ‖·‖, and ‖·‖∗."
    }, {
      "heading" : "3 Adversarial Prediction under Multivariate Loss",
      "text" : "A number of saddle-point based machine learning problems have been listed in [17]. Here we give another example (adversarial prediction under multivariate loss) that is naturally formulated as a saddle-point problem but also requires a careful adaptation to the underlying geometry—a challenge that was not addressed in [17] since their algorithm inherently relies on the Euclidean norm. We remark that adaptation to the underlying geometry has been studied in the (stochastic) mirror descent framework [23], with significant improvements on condition numbers or gradient norm bounds. Surprisingly, no analogous efforts have been attempted in the stochastic variance reduction framework—a gap we intend to fill in this work.\nThe adversarial prediction framework [19, 20, 28], arising naturally as a saddle-point problem, is a convex alternative to the generative adversarial net [18]. Given a training sample X = [x1, . . . ,xn] and ỹ = [ỹ1, . . . , ỹn] ∈ {0, 1}n, adversarial prediction optimizes the following saddle function that is an expectation of some multivariate loss `(y, z) (e.g. F-score) over the labels y, z ∈ {0, 1}n of all data points:\nmin p∈∆2n [ max q∈∆2n E y∼p,z∼q `(y, z), s.t. E z∼q ( 1nXz) = 1 nXỹ ] (9)\nHere the proponent tries to find a distribution p(·) over the labeling on the entire training set in order to minimize the loss (∆2 n\nis the 2n dimensional probability simplex). An opponent in contrast tries to maximize the expected loss by finding another distribution q(·), but his strategy is subject to the constraint that the feature expectation matches that of the empirical distribution. Introducing a\nLagrangian variable θ to remove the feature expectation constraint and specializing the problem to F-score where `(y, z) = 2y\n′z 1′y+1′z and `(0,0) := 1, the partial dual problem can be written as\nmax θ − λ2 ‖θ‖ 2 2 + 1 nθ ′Xỹ + min p∈∆2n max q∈∆2n E y∼p,z∼q\n[ 2y′z\n1′y+1′z − 1 nθ ′Xy\n] , (10)\nwhere we use y′z to denote the standard inner product and we followed [19] to add an `22 regularizer on θ penalizing the dual variables on the constraints over the training data. It appears that solving (10) can be quite challenging, because the variables p and q in the inner minimax problem have 2n entries! A constraint sampling algorithm was adopted in [19] to address this challenge, although no formal guarantee was established. Note that we can maximize the outer unconstrained variable θ (with dimension the same as the number of features) relatively easily using for instance gradient ascent, provided that we can solve the inner minimax problem quickly—a significant challenge to which we turn our attention below.\nSurprisingly, we show here that the inner minimax problem in (10) can be significantly simplified. The key observation is that the expectation in the objective depends only on a few sufficient statistics of p and q. Indeed, by interpreting p and q as probability distributions over {0, 1}n we have:\nE 2y′z\n1′y + 1′z = p({0})q({0}) + n∑ i=1 n∑ j=1 E ( 2y′z 1′y+1′z [[1 ′y = i]][[1′z = j]] )\n(11)\n= p({0})q({0}) + n∑ i=1 n∑ j=1 2ij i+ j · 1 i E (y[[1′y = i]])︸ ︷︷ ︸\nαi\n′ · 1 j E (z[[1′z = j]])︸ ︷︷ ︸\nβj\n, (12)\nwhere [[·]] = 1 if · is true, and 0 otherwise. Crucially, the variables αi and βj are sufficient for re-expressing (10), since\n1′αi = 1\ni E (1′y[[1′y = i]]) = E[[1′y = i]] = p({1′y = i}), (13)∑\ni iαi = ∑ i E (y[[1′y = i]]) = Ey, (14)\nand similar equalities also hold for βj . In details, the inner minimax problem of (10) simplifies to:\nmin α∈S max β∈S\n1\nn2 n∑ i=1 n∑ j=1 [ 2ijn2 i+j α ′ iβj+n 2α′i11 ′βj︸ ︷︷ ︸\nfij(αi,βj)\n−n1′αi−n1′βj−θ′Xiαi ] +Ω(α)−Ω(β), (15)\nwhere S = {α ≥ 0 : 1′α ≤ 1,∀i, ‖iαi‖∞ ≤ ‖αi‖1}, Ω(α) = µ ∑ i,j αij log(αij). (16)\nImportantly, α = [α1; . . . ,αn] (resp. β) has n2 entries, which is significantly smaller than the 2n entries of p (resp. q) in (10). For later purpose we have also incorporated an entropy regularizer for α and β respectively in (15).\nTo justify the constraint set S, note from (12) and (13) that for any distribution p of y: since α ≥ 0 and y ∈ {0, 1}n, ‖iαi‖∞ ≤ E‖y[[1′y = i]]‖∞ ≤ E[[1′y = i]] = ‖αi‖1. (17) Conversely, for any α ∈ S, we can construct a distribution p such that iαij = E (yj [[1′y = i]]) = p({1′y = i, yj = 1}) in the following algorithmic way: Fix i and for each j define Yj = {y ∈ {0, 1}n : 1′y = i, yj = 1}. Let U = {1, . . . , n}. Find an index j in U that minimizes αij and set p({y}) = iαij/|Yj | for each y ∈ Yj . Perform the following updates:\nU ← U \\ {j}, ∀k 6= j, Yk ← Yk \\ Yj , αik ← αik − αij |Yk ∩ Yj |/|Yj | (18) Continue this procedure until U is empty. Due to the way we choose j, α remains nonnegative and by construction αij = p({1′y = i, yj = 1}) once we remove j from U . The objective function in (15) fits naturally into the framework of (1), with Ω(α) − Ω(β) and constraints corresponding to M , and the rest terms to K. The entropy function Ω is convex wrt the KL-divergence, which is in turn distance enforcing wrt the `1 norm over the probability simplex [23]. In the next section we propose the SVRG algorithm with Bregman divergence (Breg-SVRG) that (a) provably optimizes strongly convex saddle function with a linear convergence rate, and (b) adapts to the underlying geometry by choosing an appropriate Bregman divergence. Then, in §5 we apply Breg-SVRG to (15) and achieve a factor of n speedup over a straightforward instantiation of [17].\n4 Breg-SVRG for Saddle-Point Algorithm 1: Breg-SVRG for Saddle-Point 1 Initialize z0 randomly. Set z̃ = z0. 2 for s = 1, 2, . . . do . epoch index 3 µ̃← µ̃s := ∇K(z̃), z0 ← zs0 := zm 4 for t = 1, . . . ,m do . iter index 5 Randomly pick ξ ∈ {1, . . . , n}. 6 Compute vt using (20). 7 Update zt using (21).\n8 z̃ ← z̃s := m∑ t=1 (1 + η)tzt / m∑ t=1 (1 + η)t. In this section we propose an efficient algorithm for solving the general saddle-point problem in (1) and prove its linear rate of convergence. Our main assumption is: Assumption 1. There exist two norms ‖·‖x and ‖·‖y such that each ψk is a saddle function and L-smooth; M is (ψx − ψy)-saddle; and ψx and ψy are distance enforcing (cf. (7)).\nNote that w.l.o.g. we have scaled the norms so that the usual strong convexity parameter of M is 1.\nRecall we defined ‖z‖z and ∆z in (8). For saddle-point optimization, it is common to define a signed gradient G(z) := [∂xK(z);−∂yK(z)] (since K is concave in y). Recall J = K +M , and (x∗, y∗) is a saddle-point of J . Using Assumption 1, we measure the gap of an iterate zt = (xt, yt) as follows:\nt = (zt) = J(xt, y ∗)− J(x∗, yt) ≥ ∆(zt, z∗) ≥ 12 ‖zt − z ∗‖2 ≥ 0. (19)\nInspired by [2, 9, 17], we propose in Algorithm 1 a new stochastic variance-reduced algorithm for solving the saddle-point problem (1) using Bregman divergences. The algorithm proceeds in epochs. In each epoch, we first compute the following stochastic estimate of the signed gradient G(zt) by drawing a random component from K:\nvt = ( vx(zt) −vy(zt) ) where { vx(zt) := ∂xψξ(zt)− ∂xψξ(z̃) + ∂xK(z̃) vy(zt) := ∂yψξ(zt)− ∂yψξ(z̃) + ∂yK(z̃) . (20)\nHere z̃ is the pivot chosen after completing the previous epoch. We make two important observations: (1) By construction the stochastic gradient vt is unbiased: Eξ[vt] = G(zt); (2) The expensive gradient evaluation ∂K(z̃) need only be computed once in each epoch since z̃ is held unchanged. If z̃ → z∗, then the variance of vt would be largely reduced hence faster convergence may be possible.\nNext, Algorithm 1 performs the following joint proximal update:\n(xt+1, yt+1)=arg min x max y\nη 〈vx(zt), x〉+ η 〈vy(zt), y〉+ ηM(x, y) + ∆(x, xt)−∆(y, yt), (21)\nwhere we have the flexibility in choosing a suitable Bregman divergence to better adapt to the underlying geometry. When ∆(x, xt) = 12‖x− xt‖ 2 2, we recover the special case in [17]. However, to handle the asymmetry in a general Bregman divergence (which does not appear for the Euclidean distance), we have to choose the pivot z̃ in a significantly different way than [2, 9, 17].\nWe are now ready to present our main convergence guarantee for Breg-SVRG in Algorithm 1. Theorem 1. Let Assumption 1 hold, and choose a sufficiently small η > 0 such that m :=⌈\nlog (\n1−ηL 18ηL2−η−1\n) /log(1 + η) ⌉ ≥1. Then Breg-SVRG enjoys linear convergence in expectation:\nE (z̃s) ≤ (1 + η)−ms[∆(z∗, z0) + c(Z + 1) (z0)], where Z = ∑m−1 t=0 (1+η) t, c = 18η 2L2 1−ηL . (22)\nFor example, we may set η = 145L2 , which leads to c = O(1/L 2), m = Θ ( L2 ) , (1 + η)m ≥ 6445 , and Z = O(L2). Therefore, between epochs, the gap (z̃s) decays (in expectation) by a factor of 4564 , and each epoch needs to conduct the proximal update (21) for m = Θ(L2) number of times. (We remind that w.l.o.g. we have scaled the norms so that the usual strong convexity parameter is 1.) In total, to reduce the gap below some threshold , Breg-SVRG needs to call the proximal update (21) O(L2 log 1 ) number of times, plus a similar number of component gradient evaluations.\nDiscussions. As mentioned, Algorithm 1 and Theorem 1 extend those in [17] which in turn extend [2, 9] to saddle-point problems. However, [2, 9, 17] all heavily exploit the Euclidean structure (in particular symmetry) hence their proofs cannot be applied to an asymmetric Bregman divergence. Our innovations here include: (a) A new Pythagorean theorem for the newly introduced saddle Bregman divergence (Lemma 1). (b) A moderate extension of the variance reduction lemma in [9] to accommodate any norm (Appendix B). (c) A different pivot z̃ is adopted in each epoch to handle\nasymmetry. (d) A new analysis technique through introducing a crucial auxiliary variable that enables us to bound the function gap directly. See our proof in Appendix C for more details. Compared with classical mirror descent algorithms [16, 23] that can also solve saddle-point problems with Bregman divergences, our analysis is fundamentally different and we achieve the significantly stronger rate O(log(1/ ) than the sublinear O(1/ ) rate of [16], at the expense of a squared instead of linear dependence on L. Similar tradeoff also appeared in [17]. We will return to this issue in Section 5.\nVariants and acceleration. Our analysis also supports to use different ξ in vx and vy . The standard acceleration methods such as universal catalyst [10] and non-uniform sampling can be applied directly (see Appendix E where L, the largest smoothness constant over all pieces, is replaced by their mean)."
    }, {
      "heading" : "5 Application of Breg-SVRG to Adversarial Prediction",
      "text" : "The quadratic dependence on L, the smoothness parameter, in Theorem 1 reinforces the need to choose suitable Bregman divergences. In this section we illustrate how this can be achieved for the adversarial prediction problem in Section 3. As pointed out in [17], the factorization ofK is important, and we consider three schemes: (a) ψk = fij ; (b) ψk = 1n ∑n j=1 fk,j ; and (c) ψk = 1 n ∑n i=1 fi,k. W.l.o.g. let us fix the µ in (16) to 1.\nComparison of smoothness constant. Both α and β are n2-dimensional, and the bilinear function fij can be written as α′Aijβ, where Aij ∈ Rn\n2×n2 is an n-by-n block matrix, with the (i, j)-th block being n2( 2iji+j I + 11\n′) and all other blocks being 0. The linear terms in (15) can be absorbed into the regularizer Ω without affecting the smoothness parameter.\nFor scheme (a), the smoothness constant L2 under `2 norm depends on the spectral norm of Aij : L2 = maxi,j n 2(n+ 2iji+j )) = Θ(n 3). In contrast the smoothness constant L1 under `1 norm depends on the absolute value of the entries inAij : L1 = maxi,j n2(1+ 2iji+j ) = Θ(n 3); no saving is achieved.\nFor scheme (b), the bilinear function ψk corresponds to 1nα ′∑n j=1Akjβ. Then L1 = O(n 2) while\nL22 = 1\nn2 max k max v:‖v‖2=1\n∑n j=1 ∥∥Akjv∥∥22 ≥ n2 max‖v‖2=1 ∑n j=1 ‖11′v‖2 = n5. (23)\nTherefore, L21 saves a factor of n compared with L 2 2.\nComparison of smoothness constant for the overall problem. By strong duality, we may push the maximization over θ to the innermost level of (10), arriving at an overall problem in α and β only:\nmin {αi}∈S max {βj}∈S\n1\nn2 n∑ i=1 n∑ j=1 [ fij(αi,βj)− i λn c′Xαi + ij 2λ α′iX ′Xαj + 1 2λn2 ‖c‖22 ] . (24)\nwhere c = Xỹ. The quadratic term w.r.t. α can be written as α′Bijα, where Bij ∈ Rn 2×n2 is an n-by-n block matrix, with its (i, j)-th block being ij2λX ′X and all other blocks being 0. And we assume each ‖xi‖2 ≤ 1. The smoothness constant can be bounded separately from Aij and Bij ; see (128) in Appendix F.\nFor scheme (a), the smoothness constant square L22 under `2 norm is upper bounded by the sum of spectral norm square ofAij andBij . SoL22 ≥ maxi,j ( ij 2λn )2 = Ω(n6), i.e.L2 = Θ(n3). In contrast the smoothness constant square L21 under `1 norm is at most the sum of square of maximum absolute\nvalue of the entries in Aij and Bij . Hence L21 ≤ maxi,j ( n2(1+ 2iji+j ) )2 + maxi,j ( ij 2λ )2 = Θ(n6), i.e. L1 =Θ(n3). So no saving is achieved here.\nFor scheme (b), ψk corresponds to 1n (α ′∑n j=1Akjβ + α ′∑n j=1Bkjα). Then\nL21 ≤ 1\nn2 max k\n[ max\nv:‖v‖1=1\n∥∥∑n j=1 Akjv ∥∥2 ∞ + maxv:‖v‖1=1 ∥∥∑n j=1 Bkjv ∥∥2 ∞ ] (by (128)) (25)\n≤ 1 n2 max k max j [( n2(1+ 2kjk+j ) )2 + ( kj 2 )2] = n4, (26)\nand by setting β to 0 in (126), we get L22 ≥ n5 similar to (23). Therefore, L21 saves a factor of n compared with L22. Similar results apply to scheme (c) too. We also tried non-uniform sampling, but\nit does not change the order in n. It can also be shown that if our scheme randomly samples n entries from {Aij , Bij}, the above L1 and L2 cannot be improved by further engineering the factorization. Computational complexity. We finally seek efficient algorithms for the proximal update (21) used by Breg-SVRG. When M(α,β) = Ω(α)− Ω(β) as in (16), we can solve α and β separately as:\nmin α ∑ ik αik log(αik/bik)− cik, s.t. 1′α ≤ 1, ∀i ∀k, 0 ≤ iαik ≤ 1′αi. (27)\nwhere bik and cik are constants. In Appendix D we designe an efficient “closed form” algorithm which finds an accurate solution in O(n2 log2 1 ) time, which is also on par with that for computing the stochastic gradient in schemes (b) and (c). Although scheme (a) reduces the cost of gradient computation to O(n), its corresponding smoothness parameter L21 is increased by n\n2 times, hence not worthwhile. We did manage to design an Õ(n) algorithm for the proximal update in scheme (a), but empirically the overall convergence is rather slow.\nIf we use the Euclidean squared distance as the Bregman divergence, then a term ‖α−αt‖22 needs to be added to the objective (27). No efficient “closed form” solution is available, and so in experiments we simply absorbed M into K, and then the proximal update becomes the Euclidean projection onto S, which does admit a competitive O(n2 log2(1/ )) time solution."
    }, {
      "heading" : "6 Experimental Results",
      "text" : "Our major goal here is to show that empirically Entropy-SVRG (Breg-SVRG with KL divergence) is significantly more efficient than Euclidean-SVRG (Breg-SVRG with squared Euclidean distance) on some learning problems, especially those with an entropic regularizer and a simplex constraint."
    }, {
      "heading" : "6.1 Entropy regularized LPBoost",
      "text" : "We applied Breg-SVRG to an extension of LP Boosting using entropy regularization [29]. In a binary classification setting, the base hypotheses over the training set can be compactly represented as U = (y1x1, . . . , ynxn)\n′. Then the model considers a minimax game between a distribution d ∈ ∆n over training examples and a distribution w ∈ ∆m over the hypotheses:\nmin d∈∆n,di≤ν max w∈∆m\nd′Uw + λΩ(d)− γΩ(w). (28)\nHere w tries to combine the hypotheses to maximize the edge (prediction confidence) yix′iw, while the adversary d tries to place more weights (bounded by ν) on “hard” examples to reduce the edge.\nSettings. We experimented on the adult dataset from the UCI repository, which we partitioned into n = 32, 561 training examples and 16,281 test examples, with m = 123 features. We set λ = γ = 0.01 and ν = 0.1 due to its best prediction accuracy. We tried a range of values of the step size η, and the best we found was 10−3 for Entropy-SVRG and 10−6 for Euclidean-SVRG (larger step size for Euclidean-SVRG fluctuated even worse). For both methods, m = 32561/50 gave good results.\nThe stochastic gradient in d was computed by U:jwj , where U:j is the j-th column and j is randomly sampled. The stochastic gradient in w is diU ′i:. We tried with Uijwj and Uijdi (scheme (a) in §5), but they performed worse. We also tried with the universal catalyst in the same form as [17], which can be directly extended to Entropy-SVRG. Similarly we used the non-uniform sampling based on the `2 norm of the rows and columns of U . It turned out that the Euclidean-SVRG can benefit slightly from it, while Entropy-SVRG does not. So we only show the “accelerated” results for the former.\nTo make the computational cost comparable across machines, we introduced a counter called effective number of passes: #pass. Assume the proximal operator has been called #po number of times, then\n#pass := number of epochs so far + n+mnm · #po. (29)\nWe also compared with a “convex” approach. Given d, the optimal w in (28) obviously admits a closed-form solution. General saddle-point problems certainly do not enjoy such a convenience. However, we hope to take advantage of this opportunity to study the following question: suppose we solve (28) as a convex optimization in d and the stochastic gradient were computed from the optimal\n0 200 400 600 800 Number of effective passes\n10-5\n10-4\n10-3\n10-2\n10-1\n100\nP rim\nal g\nap\nEntropy, Saddle Entropy, Convex Euclidean, Saddle Euclidean, Convex\n(a) Primal gap v.s. #pass\n0 200 400 600 800 Number of effective passes\n75\n77\n79\n81\n83\n85\nT es\nt a cc\nur ac\ny (% ) Entropy, Saddle Entropy, Convex Euclidean, Saddle Euclidean, Convex\n(b) Test accuracy v.s. #pass\nFigure 1: Entropy Regularized LPBoost on adult\nw, would it be faster than the saddle SVRG? Since solving w requires visiting the entire U , strictly speaking the term n+mnm ·#po in the definition of #pass in (29) should be replaced by #po. However, we stuck with (29) because our interest is whether a more accurate stochastic gradient in d (based on the optimal w) can outperform doubly stochastic (saddle) optimization. We emphasize that this comparison is only for conceptual understanding, because generally optimizing the inner variable requires costly iterative methods.\nResults. Figure 1(a) demonstrated how fast the primal gap (with w optimized out for each d) is reduced as a function of the number of effective passes. Methods based on entropic prox are clearly much more efficient than Euclidean prox. This corroborates our theory that for problems like (28), Entropy-SVRG is more suitable for the underlying geometry (entropic regularizer with simplex constraints).\nWe also observed that using entropic prox, our doubly stochastic method is as efficient as the “convex” method, meaning that although at each iteration the w in saddle SVRG is not the optimal for the current d, it still allows the overall algorithm to perform as fast as if it were. This suggests that for general saddle-point problems where no closed-form inner solution is available, our method will still be efficient and competitive. Note this “convex” method is similar to the optimizer used by [29].\nFinally, we investigated the increase of test accuracy as more passes over the data are performed. Figure 1(b) shows, once more, that the entropic prox does allow the accuracy to be improved much faster than Euclidean prox. Again, the convex and saddle methods perform similarly.\nAs a final note, the Euclidean/entropic proximal operator for both d and w can be solved in either closed form, or by a 1-D line search based on partial Lagrangian. So their computational cost differ in the same order of magnitude as multiplication v.s. exponentiation, which is much smaller than the difference of #pass shown in Figure 1."
    }, {
      "heading" : "6.2 Adversarial prediction with F-score",
      "text" : "Datasets. Here we considered two datasets. The first is a synthetic dataset where the positive examples are drawn from a 200 dimensional normal distribution with mean 0.1 · 1 and covariance 0.5 · I , and negative examples are drawn from N (−0.1 · 1, 0.5 · I). The training set has n = 100 samples, half are positive and half are negative. The test set has 200 samples with the same class ratio. Notice that n = 100 means we are optimizing over two 100-by-100 matrices constrained to a challenging set S. So the optimization problem is indeed not trivial.\nThe second dataset, ionosphere, has 211 training examples (122 pos and 89 neg). 89 examples were used for testing (52 pos and 37 neg). Each example has 34 features.\nMethods. To apply saddle SVRG, we used strong duality to push the optimization over θ to the inner-most level of (10), and then eliminated θ because it is a simple quadratic. So we ended up with the convexconcave optimization as shown in (24), where theK part of (15) is augmented with a quadratic term in α. The formulae for computing the stochastic gradient using scheme (b) are detailed in Appendix G. We fixed µ = 1, λ = 0.01 for the ionosphere dataset, and µ = 1, λ = 0.1 for the synthetic dataset.\nWe also tried the universal catalyst along with non-uniform sampling where each i was sam-\npled with a probability proportional to ∑n k=1 ‖Aik‖ 2 F , and similarly for j. Here ‖·‖F is the Frobenious norm.\nParameter Tuning. Since each entry in the n× n matrix α is relatively small when n is large, we needed a relatively small step size. When n = 100, we used 10−2 for Entropy-SVRG and 10−6 for Euclidean-SVRG (a larger step size makes it over-fluctuate). When applying catalyst, the catalyst regularizor can suppress the noise from larger step size. After a careful trade off between catalyst regularizor parameter and larger step size, we managed to achieve faster convergence empirically.\nResults. The results on the two datasets are shown in Figures 2 and 3 respectively. We truncated the #pass and CPU time in subplots (c) and (d) because the F-score has stabilized and we would rather zoom in to see the initial growing phase. In terms of primal gap versus #pass (subplot a), the entropy based method is significantly more effective than Euclidean methods on both datasets (Figure 2(a) and 3(a)). Even with catalyst, Euclidean-Saddle is still much slower than the entropy based methods on the synthetic dataset in Figure 2(a). The CPU time comparisons (subplot b) follow the similar trend, except that the “convex methods” should be ignored because they are introduced only to compare #pass.\nThe F-score is noisy because, as is well known, it is not monotonic with the primal gap and glitches can appear. In subplots 2(d) and 3(d), the entropy based methods achieve higher F-score significantly faster than the plain Euclidean based methods on both datasets. In terms of passes (subplots 2(c) and 3(c)), Euclidean-Saddle and Entropy-Saddle achieved a similar F-score at first because their primal gaps are comparable at the beginning. After 20 passes, the F-score of Euclidean-Saddle is overtaken by Entropy-Saddle as the primal gap of Entropy-Saddle become much smaller than Euclidean-Saddle."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "We have proposed Breg-SVRG to solve saddle-point optimization and proved its linear rate of convergence. Application to adversarial prediction confirmed its effectiveness. For future work, we are interested in relaxing the (potentially hard) proximal update in (21). We will also derive similar reformulations for DCG and precision@k, with a quadratic number of variables and with a finite sum structure that is again amenable to Breg-SVRG, leading to a similar reduction of the condition number compared to Euclidean-SVRG. These reformulations, however, come with different constraint sets, and new proximal algorithms with similar complexity as for the F-score can be developed."
    } ],
    "references" : [ {
      "title" : "Block-coordinate frank-wolfe optimization for structural SVMs",
      "author" : [ "S. Lacoste-Julien", "M. Jaggi", "M. Schmidt", "P. Pletscher" ],
      "venue" : "In ICML",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "In NIPS",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "A. Defazio", "F. Bach", "S. Lacoste-Julien" ],
      "venue" : "In NIPS",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "M. Schmidt", "N.L. Roux", "F. Bach" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Finito: A faster, permutable incremental gradient method for big data problems",
      "author" : [ "A.J. Defazio", "T.S. Caetano", "J. Domke" ],
      "venue" : "In ICML",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Incremental majorization-minimization optimization with application to large-scale machine learning",
      "author" : [ "J. Mairal" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "SDCA without duality, regularization, and individual convexity",
      "author" : [ "S. Shalev-Shwartz" ],
      "venue" : "In ICML",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "L. Xiao", "T. Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "A universal catalyst for first-order optimization",
      "author" : [ "H. Lin", "J. Mairal", "Z. Harchaoui" ],
      "venue" : "In NIPS",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Stochastic proximal gradient descent with acceleration techniques",
      "author" : [ "A. Nitanda" ],
      "venue" : "In NIPS",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "In ICML",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Stop wasting my gradients: Practical svrg",
      "author" : [ "R. Babanezhad", "M.O. Ahmed", "A. Virani", "M. Schmidt", "J. Konečný", "S. Sallinen" ],
      "venue" : "In NIPS",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Stochastic primal-dual coordinate method for regularized empirical risk minimization",
      "author" : [ "Y. Zhang", "L. Xiao" ],
      "venue" : "In ICML",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Adaptive stochastic primal-dual coordinate descent for separable saddle point problems",
      "author" : [ "Z. Zhu", "A.J. Storkey" ],
      "venue" : "In Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Stochastic variance reduction methods for saddle-point problems",
      "author" : [ "P. Balamurugan", "F. Bach" ],
      "venue" : "In NIPS",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : "In NIPS",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Adversarial prediction games for multivariate losses",
      "author" : [ "H. Wang", "W. Xing", "K. Asif", "B.D. Ziebart" ],
      "venue" : "In NIPS",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "A minimax approach to supervised learning",
      "author" : [ "F. Farnia", "D. Tse" ],
      "venue" : "In NIPS",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2005
    }, {
      "title" : "Excessive gap technique in nonsmooth convex minimization",
      "author" : [ "Y. Nesterov" ],
      "venue" : "SIAM J. on Optimization,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "Mirror descent and nonlinear projected subgradient methods for convex optimization",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "Operations Research Letters,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2003
    }, {
      "title" : "Bregman alternating direction method of multipliers",
      "author" : [ "H. Wang", "A. Banerjee" ],
      "venue" : "In NIPS",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Monotone operators associated with saddle functions and minimax problems",
      "author" : [ "R.T. Rockafellar" ],
      "venue" : "Nonlinear Functional Analysis,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1970
    }, {
      "title" : "Composite objective mirror descent",
      "author" : [ "J.C. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari" ],
      "venue" : "In Proc. Annual Conf. Computational Learning Theory",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "On accelerated proximal gradient methods for convex-concave optimization",
      "author" : [ "P. Tseng" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2009
    }, {
      "title" : "Adversarial cost-sensitive classification",
      "author" : [ "K. Asif", "W. Xing", "S. Behpour", "B.D. Ziebart" ],
      "venue" : "In UAI",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Entropy regularized LPBoost",
      "author" : [ "M.K. Warmuth", "K.A. Glocer", "S.V.N. Vishwanathan" ],
      "venue" : "Proc. Intl. Conf. Algorithmic Learning Theory,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2008
    }, {
      "title" : "Proximal splitting methods in signal processing. Fixed-Point Algorithms for Inverse Problems",
      "author" : [ "P.L. Combettes", "J.-C. Pesquet" ],
      "venue" : "Science and Engineering,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "A “dual” approach partitions the problem into blocks of coordinates and processes them in a stochastic fashion [1].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "Examples include SVRG [2], SAGA [3], SAG [4], Finito [5], MISO [6], and SDCA [7, 8], just to name a few.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "Examples include SVRG [2], SAGA [3], SAG [4], Finito [5], MISO [6], and SDCA [7, 8], just to name a few.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "Examples include SVRG [2], SAGA [3], SAG [4], Finito [5], MISO [6], and SDCA [7, 8], just to name a few.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "Examples include SVRG [2], SAGA [3], SAG [4], Finito [5], MISO [6], and SDCA [7, 8], just to name a few.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "Examples include SVRG [2], SAGA [3], SAG [4], Finito [5], MISO [6], and SDCA [7, 8], just to name a few.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "Examples include SVRG [2], SAGA [3], SAG [4], Finito [5], MISO [6], and SDCA [7, 8], just to name a few.",
      "startOffset" : 77,
      "endOffset" : 83
    }, {
      "referenceID" : 7,
      "context" : "Examples include SVRG [2], SAGA [3], SAG [4], Finito [5], MISO [6], and SDCA [7, 8], just to name a few.",
      "startOffset" : 77,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "Specialized algorithms have also been proposed for accommodating proximal terms [9], and for further acceleration through the condition number [10–13].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : "For example, [14] and [15] performed batch updates on w and stochastic updates on α, while [16] and [17] performed doubly stochastic updates on both w and α, achieving O( 1 ) and O(log 1 ) rates respectively.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 14,
      "context" : "For example, [14] and [15] performed batch updates on w and stochastic updates on α, while [16] and [17] performed doubly stochastic updates on both w and α, achieving O( 1 ) and O(log 1 ) rates respectively.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "For example, [14] and [15] performed batch updates on w and stochastic updates on α, while [16] and [17] performed doubly stochastic updates on both w and α, achieving O( 1 ) and O(log 1 ) rates respectively.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "For example, [14] and [15] performed batch updates on w and stochastic updates on α, while [16] and [17] performed doubly stochastic updates on both w and α, achieving O( 1 ) and O(log 1 ) rates respectively.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "Key to our development is a new extension of the stochastic variance-reduced algorithm in [17] such that it is compatible with any Bregman divergence, hence opening the possibility to largely reduce the quadratic condition number in [17] by better adapting to the underlying geometry using non-Euclidean norms and Bregman divergences.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 16,
      "context" : "Key to our development is a new extension of the stochastic variance-reduced algorithm in [17] such that it is compatible with any Bregman divergence, hence opening the possibility to largely reduce the quadratic condition number in [17] by better adapting to the underlying geometry using non-Euclidean norms and Bregman divergences.",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 20,
      "context" : "Improving condition numbers by Bregman divergence has long been studied in (stochastic, proximal) gradient descent [21, 22].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 21,
      "context" : "Improving condition numbers by Bregman divergence has long been studied in (stochastic, proximal) gradient descent [21, 22].",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 22,
      "context" : "The best known algorithm is arguably stochastic mirror descent [23], which was extended to saddle-points by [16] and to ADMM by [24].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "The best known algorithm is arguably stochastic mirror descent [23], which was extended to saddle-points by [16] and to ADMM by [24].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 23,
      "context" : "The best known algorithm is arguably stochastic mirror descent [23], which was extended to saddle-points by [16] and to ADMM by [24].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 16,
      "context" : "For example, the analysis of [17] relied on the resolvent of monotone operators [25] and is hence restricted to the Euclidean norm.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 24,
      "context" : "For example, the analysis of [17] relied on the resolvent of monotone operators [25] and is hence restricted to the Euclidean norm.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "asymmetry and unbounded gradient on bounded domain), and we recover similar quantitative linear rate of convergence as [17] but with the flexibility of using suitable Bregman divergences to reduce the condition number.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "The new stochastic variance-reduced algorithm Breg-SVRG is then applied to the adversarial prediction framework (with multivariate losses such as F-score) [19, 20].",
      "startOffset" : 155,
      "endOffset" : 163
    }, {
      "referenceID" : 19,
      "context" : "The new stochastic variance-reduced algorithm Breg-SVRG is then applied to the adversarial prediction framework (with multivariate losses such as F-score) [19, 20].",
      "startOffset" : 155,
      "endOffset" : 163
    }, {
      "referenceID" : 25,
      "context" : "Following [26] we call a function f ψ-convex if f−ψ is convex, i.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 26,
      "context" : "Key to our subsequent analysis is the following lemma that extends a result of [27] to saddle functions (proof in Appendix A).",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 16,
      "context" : "A number of saddle-point based machine learning problems have been listed in [17].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "Here we give another example (adversarial prediction under multivariate loss) that is naturally formulated as a saddle-point problem but also requires a careful adaptation to the underlying geometry—a challenge that was not addressed in [17] since their algorithm inherently relies on the Euclidean norm.",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 22,
      "context" : "We remark that adaptation to the underlying geometry has been studied in the (stochastic) mirror descent framework [23], with significant improvements on condition numbers or gradient norm bounds.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 18,
      "context" : "The adversarial prediction framework [19, 20, 28], arising naturally as a saddle-point problem, is a convex alternative to the generative adversarial net [18].",
      "startOffset" : 37,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "The adversarial prediction framework [19, 20, 28], arising naturally as a saddle-point problem, is a convex alternative to the generative adversarial net [18].",
      "startOffset" : 37,
      "endOffset" : 49
    }, {
      "referenceID" : 27,
      "context" : "The adversarial prediction framework [19, 20, 28], arising naturally as a saddle-point problem, is a convex alternative to the generative adversarial net [18].",
      "startOffset" : 37,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "The adversarial prediction framework [19, 20, 28], arising naturally as a saddle-point problem, is a convex alternative to the generative adversarial net [18].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 18,
      "context" : "where we use y′z to denote the standard inner product and we followed [19] to add an `(2)2 regularizer on θ penalizing the dual variables on the constraints over the training data.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "It appears that solving (10) can be quite challenging, because the variables p and q in the inner minimax problem have 2 entries! A constraint sampling algorithm was adopted in [19] to address this challenge, although no formal guarantee was established.",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 22,
      "context" : "The entropy function Ω is convex wrt the KL-divergence, which is in turn distance enforcing wrt the `1 norm over the probability simplex [23].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "Then, in §5 we apply Breg-SVRG to (15) and achieve a factor of n speedup over a straightforward instantiation of [17].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "Inspired by [2, 9, 17], we propose in Algorithm 1 a new stochastic variance-reduced algorithm for solving the saddle-point problem (1) using Bregman divergences.",
      "startOffset" : 12,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "Inspired by [2, 9, 17], we propose in Algorithm 1 a new stochastic variance-reduced algorithm for solving the saddle-point problem (1) using Bregman divergences.",
      "startOffset" : 12,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "Inspired by [2, 9, 17], we propose in Algorithm 1 a new stochastic variance-reduced algorithm for solving the saddle-point problem (1) using Bregman divergences.",
      "startOffset" : 12,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "When ∆(x, xt) = 12‖x− xt‖ 2 2, we recover the special case in [17].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 1,
      "context" : "However, to handle the asymmetry in a general Bregman divergence (which does not appear for the Euclidean distance), we have to choose the pivot z̃ in a significantly different way than [2, 9, 17].",
      "startOffset" : 186,
      "endOffset" : 196
    }, {
      "referenceID" : 8,
      "context" : "However, to handle the asymmetry in a general Bregman divergence (which does not appear for the Euclidean distance), we have to choose the pivot z̃ in a significantly different way than [2, 9, 17].",
      "startOffset" : 186,
      "endOffset" : 196
    }, {
      "referenceID" : 16,
      "context" : "However, to handle the asymmetry in a general Bregman divergence (which does not appear for the Euclidean distance), we have to choose the pivot z̃ in a significantly different way than [2, 9, 17].",
      "startOffset" : 186,
      "endOffset" : 196
    }, {
      "referenceID" : 16,
      "context" : "As mentioned, Algorithm 1 and Theorem 1 extend those in [17] which in turn extend [2, 9] to saddle-point problems.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "As mentioned, Algorithm 1 and Theorem 1 extend those in [17] which in turn extend [2, 9] to saddle-point problems.",
      "startOffset" : 82,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "As mentioned, Algorithm 1 and Theorem 1 extend those in [17] which in turn extend [2, 9] to saddle-point problems.",
      "startOffset" : 82,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "However, [2, 9, 17] all heavily exploit the Euclidean structure (in particular symmetry) hence their proofs cannot be applied to an asymmetric Bregman divergence.",
      "startOffset" : 9,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "However, [2, 9, 17] all heavily exploit the Euclidean structure (in particular symmetry) hence their proofs cannot be applied to an asymmetric Bregman divergence.",
      "startOffset" : 9,
      "endOffset" : 19
    }, {
      "referenceID" : 16,
      "context" : "However, [2, 9, 17] all heavily exploit the Euclidean structure (in particular symmetry) hence their proofs cannot be applied to an asymmetric Bregman divergence.",
      "startOffset" : 9,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "(b) A moderate extension of the variance reduction lemma in [9] to accommodate any norm (Appendix B).",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "Compared with classical mirror descent algorithms [16, 23] that can also solve saddle-point problems with Bregman divergences, our analysis is fundamentally different and we achieve the significantly stronger rate O(log(1/ ) than the sublinear O(1/ ) rate of [16], at the expense of a squared instead of linear dependence on L.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 22,
      "context" : "Compared with classical mirror descent algorithms [16, 23] that can also solve saddle-point problems with Bregman divergences, our analysis is fundamentally different and we achieve the significantly stronger rate O(log(1/ ) than the sublinear O(1/ ) rate of [16], at the expense of a squared instead of linear dependence on L.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : "Compared with classical mirror descent algorithms [16, 23] that can also solve saddle-point problems with Bregman divergences, our analysis is fundamentally different and we achieve the significantly stronger rate O(log(1/ ) than the sublinear O(1/ ) rate of [16], at the expense of a squared instead of linear dependence on L.",
      "startOffset" : 259,
      "endOffset" : 263
    }, {
      "referenceID" : 16,
      "context" : "Similar tradeoff also appeared in [17].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "The standard acceleration methods such as universal catalyst [10] and non-uniform sampling can be applied directly (see Appendix E where L, the largest smoothness constant over all pieces, is replaced by their mean).",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "As pointed out in [17], the factorization ofK is important, and we consider three schemes: (a) ψk = fij ; (b) ψk = 1 n ∑n j=1 fk,j ; and (c) ψk = 1 n ∑n i=1 fi,k.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 28,
      "context" : "We applied Breg-SVRG to an extension of LP Boosting using entropy regularization [29].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "We also tried with the universal catalyst in the same form as [17], which can be directly extended to Entropy-SVRG.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 28,
      "context" : "Note this “convex” method is similar to the optimizer used by [29].",
      "startOffset" : 62,
      "endOffset" : 66
    } ],
    "year" : 2017,
    "abstractText" : "Adversarial machines, where a learner competes against an adversary, have regained much recent interest in machine learning. They are naturally in the form of saddle-point optimization, often with separable structure but sometimes also with unmanageably large dimension. In this work we show that adversarial prediction under multivariate losses can be solved much faster than they used to be. We first reduce the problem size exponentially by using appropriate sufficient statistics, and then we adapt the new stochastic variance-reduced algorithm of Balamurugan & Bach (2016) to allow any Bregman divergence. We prove that the same linear rate of convergence is retained and we show that for adversarial prediction using KL-divergence we can further achieve a speedup of #example times compared with the Euclidean alternative. We verify the theoretical findings through extensive experiments on two example applications: adversarial prediction and LPboosting.",
    "creator" : null
  }
}