{
  "name" : "e655c7716a4b3ea67f48c6322fc42ed6.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multitask Spectral Learning of Weighted Automata",
    "authors" : [ "Guillaume Rabusseau", "Borja Balle", "Joelle Pineau" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "One common task in machine learning consists in estimating an unknown function f : X → Y from a training sample of input-output data {(xi, yi)}Ni=1 where each yi ' f(xi) is a (possibly noisy) estimate of f(xi). In multitask learning, the learner is given several such learning tasks f1, · · · , fm. It has been shown, both experimentally and theoretically, that learning related tasks simultaneously can lead to better performances relative to learning each task independently (see e.g. [1, 7], and references therein). Multitask learning has proven particularly useful when few data points are available for each task, or when it is difficult or costly to collect data for a target task while much data is available for related tasks (see e.g. [28] for an example in healthcare). In this paper, we propose a multitask learning algorithm for the case where the input space X consists of sequence data. Many tasks in natural language processing, computational biology, or reinforcement learning, rely on estimating functions mapping sequences of observations to real numbers: e.g. inferring probability distributions over sentences in language modeling or learning the dynamics of a model of the environment in reinforcement learning. In this case, the function f to infer from training data is defined over the set Σ∗ of strings built on a finite alphabet Σ. Weighted finite automata (WFA) are finite state machines that allow one to succinctly represent such functions. In particular, WFAs can compute any probability distribution defined by a hidden Markov model (HMM) [13] and can model the transition and observation behavior of partially observable Markov decision processes [26]. A recent line of work has led to the development of spectral methods for learning HMMs [17], WFAs [2, 4] and related models, offering an alternative to EM based algorithms with the benefits of being computationally efficient and providing consistent estimators. Spectral learning algorithms have led to competitive results in the fields of natural language processing [12, 3] and robotics [8].\nWe consider the problem of multitask learning for WFAs. As a motivational example, consider a natural language modeling task where one needs to make predictions in different contexts (e.g. online chat vs. newspaper articles) and has access to datasets in each of them; it is natural to expect that basic grammar is shared across the datasets and that one could benefit from simultaneously ∗guillaume.rabusseau@mail.mcgill.ca †pigem@amazon.co.uk ‡jpineau@cs.mcgill.ca\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nlearning these tasks. The notion of relatedness between tasks can be expressed in different ways; one common assumption in multitask learning is that the multiple tasks share a common underlying representation [6, 11]. In this paper, we present a natural notion of shared representation between functions defined over strings and we propose a learning algorithm that encourages the discovery of this shared representation. Intuitively, our notion of relatedness captures to which extent several functions can be computed by WFAs sharing a joint forward feature map. In order to formalize this notion of relatedness, we introduce the novel model of vector-valued WFA (vv-WFA) which generalizes WFAs to vector-valued functions and offer a natural framework to formalize the multitask learning problem. Givenm tasks f1, · · · , fm : Σ∗ → R, we consider the function ~f = [f1, · · · , fm] : Σ∗ → Rm whose output for a given input string x is the m-dimensional vector having entries fi(x) for i = 1, · · · ,m. We show that the notion of minimal vv-WFA computing ~f exactly captures our notion of relatedness between tasks and we prove that the dimension of such a minimal representation is equal to the rank of a flattening of the Hankel tensor of ~f (Theorem 3). Leveraging this result, we design a spectral learning algorithm for vv-WFAs which constitutes a sound multitask learning algorithm for WFAs: by learning ~f in the form of a vv-WFA, rather than independently learning a WFA for each task fi, we implicitly enforce the discovery of a joint feature space shared among all tasks. After giving a theoretical insight on the benefits of this multitask approach (by leveraging a recent result on asymmetric bounds for singular subspace estimation [9]), we conclude by showcasing these benefits with experiments on both synthetic and real world data.\nRelated work. Multitask learning for sequence data has previously received limited attention. In [16], mixtures of Markov chains are used to model dynamic user profiles. Tackling the multitask problem with nonparametric Bayesian methods is investigated in [15] to model related time series with Beta processes and in [23] to discover relationships between related datasets using nested Dirichlet process and infinite HMMs. Extending recurrent neural networks to the multitask setting has also recently received some interest (see e.g. [21, 22]). To the best of our knowledge, this paper constitutes the first attempt to tackle the multitask problem for the class of functions computed by general WFAs."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We first present notions on weighted automata, spectral learning of weighted automata and tensors. We start by introducing some notation. We denote by Σ∗ the set of strings on a finite alphabet Σ. The empty string is denoted by λ and the length of a string x by |x|. For any integer k we let [k] = {1, 2, · · · , k}. We use lower case bold letters for vectors (e.g. v ∈ Rd1), upper case bold letters for matrices (e.g. M ∈ Rd1×d2) and bold calligraphic letters for higher order tensors (e.g. T ∈ Rd1×d2×d3). The ith row (resp. column) of a matrix M will be denoted by Mi,: (resp. M:,i). This notation is extended to slices of a tensor in the straightforward way. Given a matrix M ∈ Rd1×d2 , we denote by M† its Moore-Penrose pseudo-inverse and by vec(M) ∈ Rd1d2 its vectorization.\nWeighted finite automaton. A weighted finite automaton (WFA) with n states is a tuple A = (α, {Aσ}σ∈Σ,ω) where α,ω ∈ Rn are the initial and final weights vectors respectively, and Aσ ∈ Rn×n is the transition matrix for each symbol σ ∈ Σ. A WFA computes a function fA : Σ∗ → R defined for each word x = x1x2 · · ·xk ∈ Σ∗ by fA(x) = α>Ax1Ax2 · · ·Axkω. By letting Ax = Ax1Ax2 · · ·Axk for any word x = x1x2 · · ·xk ∈ Σ∗ we will often use the shorter notation fA(x) = α>Axω. A WFA A with n states is minimal if its number of states is minimal, i.e. any WFA B such that fA = fB has at least n states. A function f : Σ∗ → R is recognizable if it can be computed by a WFA. In this case the rank of f is the number of states of a minimal WFA computing f , if f is not recognizable we let rank(f) =∞.\nHankel matrix. The Hankel matrix Hf ∈ RΣ ∗×Σ∗ associated with a function f : Σ∗ → R is the infinite matrix with entries (Hf )u,v = f(uv) for u, v ∈ Σ∗. The spectral learning algorithm for WFAs relies on the following fundamental relation between the rank of f and the rank of Hf . Theorem 1. [10, 14] For any function f : Σ∗ → R, rank(f) = rank(Hf ).\nSpectral learning. Showing that the rank of the Hankel matrix is upper bounded by the rank of f is easy: given a WFA A = (α, {Aσ}σ∈Σ,ω) with n states, we have the rank n factorization Hf = PS where the matrices P ∈ RΣ∗×n and S ∈ Rn×Σ∗ are defined by Pu,: = α>Au and S:,v = Avω for\nall u, v ∈ Σ∗. The converse is more tedious to show but its proof is constructive, in the sense that it allows one to build a WFA computing f from any rank n factorization of Hf . This construction is the cornerstone of the spectral learning algorithm and is given in the following corollary.\nCorollary 2. [4, Lemma 4.1] Let f : Σ∗ → R be a recognizable function with rank n, let H ∈ RΣ∗×Σ∗ be its Hankel matrix, and for each σ ∈ Σ let Hσ ∈ RΣ∗×Σ∗ be defined by Hσu,v = f(uσv) for all u, v ∈ Σ∗.\nThen, for any P ∈ RΣ∗×n, S ∈ Rn×Σ∗ such that H = PS, the WFA A = (α, {Aσ}σ∈Σ,ω) where α> = Pλ,:, ω = S:,λ, and Aσ = P†HσS† is a minimal WFA for f .\nIn practice, finite sub-blocks of the Hankel matrices are used. Given finite sets of prefixes and suffixes P,S ⊂ Σ∗, let HP,S , {HσP,S}σ∈Σ be the finite sub-blocks of H whose rows (resp. columns) are indexed by prefixes in P (resp. suffixes in S). One can show that if P and S are such that λ ∈ P ∩ S and rank(H) = rank(HP,S), then the previous corollary still holds, i.e. a minimal WFA computing f can be recovered from any rank n factorization of HP,S . The spectral method thus consists in estimating the matrices HP,S ,HσP,S from training data (using e.g. empirical frequencies if f is stochastic), finding a low-rank factorization of HP,S (using e.g. SVD) and constructing a WFA approximating f using Corollary 2.\nTensors. We make a sporadic use of tensors in this paper, we thus introduce the few necessary definitions and notations; more details can be found in [18]. A 3rd order tensor T ∈ Rd1×d2×d3 can be seen as a multidimensional array (T i1,i2,i3 : i1 ∈ [d1], i2 ∈ [d2], , i3 ∈ [d3]). The mode-n fibers of T are the vectors obtained by fixing all indices except the nth one, e.g. T :,i2,i3 ∈ Rd1 . The nth mode flattening of T is the matrix having the mode-n fibers of T for columns and is denoted by e.g. T (1) ∈ Rd1×d2d3 . The mode-1 matrix product of a tensor T ∈ Rd1×d2×d3 and a matrix X ∈ Rm×d1 is a tensor of size m× d2 × d3 denoted by T ×1 X and defined by the relation Y = T ×1 X⇔ Y(1) = XT (1); the mode-n product for n = 2, 3 is defined similarly."
    }, {
      "heading" : "3 Vector-Valued WFAs for Multitask Learning",
      "text" : "In this section, we present a notion of relatedness between WFAs that we formalize by introducing the novel model of vector-valued weighted automaton. We then propose a multitask learning algorithm for WFAs by designing a spectral learning algorithm for vector-valued WFAs.\nA notion of relatedness between WFAs. The basic idea behind our approach emerges from interpreting the computation of a WFA as a linear model in some feature space. Indeed, the computation of a WFA A = (α, {Aσ}σ∈Σ,ω) with n states on a word x ∈ Σ∗ can be seen as first mapping x to an n-dimensional feature vector through a compositional feature map φ : Σ∗ → Rn, and then applying a linear form in the feature space to obtain the final value fA(x) = 〈φ(x),ω〉. The feature map is defined by φ(x)> = α>Ax for all x ∈ Σ∗ and it is compositional in the sense that for any x ∈ Σ∗ and any σ ∈ Σ we have φ(xσ)> = φ(x)>Aσ . We will say that such a feature map is minimal if the linear space V ⊂ Rn spanned by the vectors {φ(x)}x∈Σ∗ is of dimension n. Theorem 1 implies that the dimension of V is actually equal to the rank of fA, showing that the notion of minimal feature map naturally coincides with the notion of minimal WFA.\nA notion of relatedness between WFAs naturally arises by considering to which extent two (or more) WFAs can share a joint feature map φ. More precisely, consider two recognizable functions f1, f2 : Σ\n∗ → R of rank n1 and n2 respectively, with corresponding feature maps φ1 : Σ∗ → Rn1 and φ2 : Σ∗ → Rn2 . Then, a joint feature map for f1 and f2 always exists and is obtained by considering the direct sum φ1 ⊕ φ2 : Σ∗ → Rn1+n2 that simply concatenates the feature vectors φ1(x) and φ2(x) for any x ∈ Σ∗. However, this feature map may not be minimal, i.e. there may exist another joint feature map of dimension n < n1 + n2. Intuitively, the smaller this minimal dimension n is the more related the two tasks are, with the two extremes being on the one hand n = n1 + n2 where the two tasks are independent, and on the other hand e.g. n = n1 where one of the (minimal) feature maps φ1, φ2 is sufficient to predict both tasks.\nVector-valued WFA. We now introduce a computational model for vector-valued functions on strings that will help formalize this notion of relatedness between WFAs.\nDefinition 1. A d-dimensional vector-valued weighted finite automaton (vv-WFA) with n states is a tuple A = (α, {Aσ}σ∈Σ,Ω) where α ∈ Rn is the initial weights vector, Ω ∈ Rn×d is the matrix of final weights, and Aσ ∈ Rn×n is the transition matrix for each symbol σ ∈ Σ. A vv-WFA computes a function ~fA : Σ∗ → Rd defined by\n~fA(x) = α >Ax1Ax2 · · ·AxkΩ\nfor each word x = x1x2 · · ·xk ∈ Σ∗.\nWe extend the notions of recognizability, minimality and rank of a WFA in the straightforward way: a function ~f : Σ∗ → Rd is recognizable if it can be computed by a vv-WFA, a vv-WFA is minimal if its number of states is minimal, and the rank of ~f is the number of states of a minimal vv-WFA computing ~f . A d-dimensional vv-WFA can be seen as a collection of d WFAs that all share their initial vectors and transition matrices but have different final vectors. Alternatively, one could take a dual approach and define vv-WFAs as a collection of WFAs sharing transitions and final vectors4.\nvv-WFAs and relatedness between WFAs. We now show how the vv-WFA model naturally captures the notion of relatedness presented above. Recall that this notion intends to capture to which extent two recognizable functions f1, f2 : Σ∗ → R, of ranks n1 and n2 respectively, can share a joint forward feature map φ : Σ∗ → Rn satisfying f1(x) = 〈φ(x),ω1〉 and f2(x) = 〈φ(x),ω2〉 for all x ∈ Σ∗, for some ω1,ω2 ∈ Rn. Consider the vector-valued function ~f = [f1, f2] : Σ∗ → R2 defined by ~f(x) = [f1(x), f2(x)] for all x ∈ Σ∗. It can easily be seen that the minimal dimension of a shared forward feature map between f1 and f2 is exactly the rank of ~f , i.e. the number of states of a minimal vv-WFA computing ~f . This notion of relatedness can be generalized to more than two functions by considering ~f = [f1, · · · , fm] for m different recognizable functions f1, · · · , fm of respective ranks n1, · · · , nm. In this setting, it is easy to check that the rank of ~f lies between max(n1, · · · , nm) and n1 + · · ·+nm; smaller values of this rank leads to a smaller dimension of the minimal forward feature map and thus, intuitively, to more closely related tasks. We now formalize this measure of relatedness between recognizable functions. Definition 2. Given m recognizable functions f1, · · · , fm, we define their relatedness measure by τ(f1, · · · , fm) = 1− (rank(~f)−maxi rank(fi))/ ∑ i rank(fi) where ~f = [f1, · · · , fm].\nOne can check that this measure of relatedness takes its values in (0, 1]. We say that tasks are maximally related when their relatedness measure is 1 and independent when it is minimal. Observe that the rank R of a vv-WFA does not give enough information to determine whether one set of tasks is more related than another: the degree of relatedness depends on the relation between R and the ranks of each individual task. The relatedness parameter τ circumvents this issue by measuring where R stands between the maximum rank over the different tasks and the sum of their ranks. Example 1. Let Σ = {a, b, c} and let |x|σ denotes the number of occurrences of σ in x for any σ ∈ Σ. Consider the functions defined by f1(x) = 0.5|x|a + 0.5|x|b, f2(x) = 0.3|x|b − 0.6|x|c and f3(x) = |x|c for all x ∈ Σ∗. It is easy to check that rank(f1) = rank(f2) = 4 and rank(f3) = 2. Moreover, f2 and f3 are maximally related (indeed rank([f2, f3]) = 4 = rank(f2) thus τ(f2, f3) = 1), f1 and f3 are independent (indeed τ(f1, f3) = 2/3 is minimal since rank([f1, f3]) = 6 = rank(f1) + rank(f3)), and f1 and f2 are related but not maximally related (since 4 = rank(f1) = rank(f2) < rank([f1, f2]) = 6 < rank(f1) + rank(f2) = 8).\nSpectral learning of vv-WFAs. We now design a spectral learning algorithm for vv-WFAs. Given a function ~f : Σ∗ → Rd, we define its Hankel tensor H ∈ RΣ∗×d×Σ∗ by Hu,:,v = ~f(uv) for all u, v ∈ Σ∗. We first show in Theorem 3 (whose proof can be found in the supplementary material) that the fundamental relation between the rank of a function and the rank of its Hankel matrix can naturally be extended to the vector-valued case. Compared with Theorem 1, the Hankel matrix is now replaced by the mode-1 flattening H(1) of the Hankel tensor (which can be obtained by concatenating the matrices H:,i,: along the horizontal axis).\nTheorem 3 (Vector-valued Fliess Theorem). Let ~f : Σ∗ → Rd and let H be its Hankel tensor. Then rank(~f) = rank(H(1)).\n4Both definitions performed similarly in multitask experiments on the dataset used in Section 5.2, we thus chose multiple final vectors as a convention.\nSimilarly to the scalar-valued case, this theorem can be leveraged to design a spectral learning algorithm for vv-WFAs. The following corollary (whose proof can be found in the supplementary material) shows how a vv-WFA computing a recognizable function ~f : Σ∗ → Rd of rank n can be recovered from any rank n factorization of its Hankel tensor.\nCorollary 4. Let ~f : Σ∗ → Rd be a recognizable function with rank n, let H ∈ RΣ∗×d×Σ∗ be its Hankel tensor, and for each σ ∈ Σ let Hσ ∈ RΣ∗×d×Σ∗ be defined by Hσu,:,v = ~f(uσv) for all u, v ∈ Σ∗. Then, for any P ∈ RΣ∗×n and S ∈ Rn×d×Σ∗ such that H = S ×1 P, the vv-WFA A = (α, {Aσ}σ∈Σ,Ω) defined by α> = Pλ,:, Ω = S :,:,λ, and Aσ = P†Hσ(1)(S(1))† is a minimal vv-WFA computing ~f .\nSimilarly to the scalar-valued case, one can check that the previous corollary also holds for any finite sub-tensors HP,S , {HσP,S}σ∈Σ of H indexed by prefixes and suffixes in P,S ⊂ Σ∗, whenever P and S are such that λ ∈ P ∩ S and rank(H(1)) = rank((HP,S)(1)); we will call such a basis (P,S) complete. The spectral learning algorithm for vv-WFAs then consists in estimating these Hankel tensors from training data and using Corollary 4 to recover a vv-WFA approximating the target function. Of course a noisy estimate of the Hankel tensor Ĥ will not be of low rank and the factorization Ĥ = S ×1 P should only be performed approximately in order to counter the presence of noise. In practice a low rank approximation of Ĥ(1) is obtained using truncated SVD.\nMultitask learning of WFAs. Let us now go back to the multitask learning problem and let f1, · · · fm : Σ∗ → R be multiple functions we wish to infer in the form of WFAs. The spectral learning algorithm for vv-WFAs naturally suggests a way to tackle this multitask problem: by learning ~f = [f1, · · · , fm] in the form of a vv-WFA, rather than independently learning a WFA for each task fi, we implicitly enforce the discovery of a joint forward feature map shared among all tasks.\nWe will now see how a further step can be added to this learning scheme to enforce more robustness to noise. The motivation for this additional step comes from the observation that even though a d-dimensional vv-WFA A = (α, {Aσ}σ∈Σ,Ω) may be minimal, the corresponding scalar-valued WFAsAi = 〈α, {Aσ}σ∈Σ,Ω:,i〉 for i ∈ [d] may not be. Suppose for example thatA1 is not minimal. This implies that some part of its state space does not contribute to the function f1 but comes from asking for a rich enough state representation that can predict other tasks as well. Moreover, when one learns a vv-WFA from noisy estimates of the Hankel tensors, the rank R approximation Ĥ(1) ' PS(1) somehow annihilates the noise contained in the space orthogonal to the top R singular vectors of Ĥ(1), but when the WFA A1 has rank R1 < R we intuitively see that there is still a subspace of dimension R−R1 containing only irrelevant features. In order to circumvent this issue, we would like to project down the (scalar-valued) WFAs Ai down to their true dimensions, intuitively enforcing each predictor to use as few features as possible for each task, and thus annihilating the noise lying in the corresponding irrelevant subspaces. To achieve this we will make use of the following proposition that explicits the projections needed to obtain minimal scalar-valued WFAs from a given vv-WFA (the proof is given in the supplementary material).\nProposition 1. Let ~f : Σ∗ → Rd be a function computed by a minimal vv-WFA A = (α, {Aσ}σ∈Σ,Ω) with n states and let P,S ⊆ Σ∗ be a complete basis for ~f . For any i ∈ [d], let fi : Σ∗ → R be defined by fi(x) = ~f(x)i for all x ∈ Σ∗ and let ni denote the rank of fi.\nLet P ∈ RP×n be defined by Px,: = α>Ax for all x ∈ P and, for i ∈ [d], let Hi ∈ RP×S be the Hankel matrix of fi and let Hi = UiDiV>i be its thin SVD (i.e. Di ∈ Rni×ni ). Then, for any i ∈ [d], the WFA Ai = 〈αi, {Aσi }σ∈Σ},ωi〉 defined by\nα>i = α >P†Ui, ωi = U>i PΩ:,i and A σ i = U > i PA σP†Ui for each σ ∈ Σ,\nis a minimal WFA computing fi.\nGiven noisy estimates Ĥ, {Ĥσ}σ∈Σ of the Hankel tensors of a function ~f and estimates R of the rank of ~f and Ri of the ranks of the fi’s, the first step of the learning algorithm consists in applying Corollary 4 to the factorization Ĥ(1) ' U(DV>) obtained by truncated SVD to get a\nvv-WFA A approximating ~f . Then, Proposition 1 can be used to project down each WFA Ai by estimating Ui with the top Ri left singular vectors of Ĥ:,i,:. The overall procedure for our Multi-Task Spectral Learning (MT-SL) is summarized in Algorithm 1 where lines 1-3 correspond to the vv-WFA estimation while lines 4-7 correspond to projecting down the corresponding scalar-valued WFAs. To further motivate the projection step, let us consider the case when m tasks are completely unrelated, and each of them requires n states. Single-task learning would lead to a model with O ( |Σ|mn2\n) parameters, while the multi-task learning approach would return a larger model of sizeO ( |Σ|(mn)2 ) ; the projection step eliminates such redundancy.\nAlgorithm 1 MT-SL: Spectral Learning of vector-valued WFA for multitask learning\nInput: Empirical Hankel tensors Ĥ, {Ĥσ}σ∈Σ of size P × m × S for the target function ~f = [f1, · · · , fm] (where P,S are subsets of Σ∗ both containing λ), a common rank R, and task specific ranks Ri for i ∈ [m]. Output: WFAs Ai approximating fi for each i ∈ [d]. 1: Compute the rank R truncated SVD Ĥ(1) ' UDV>. 2: Let A = (α, {Aσ}σ∈Σ,Ω) be the vv-WFA defined by\nα> = Uλ,:, ,Ω = U >(Ĥ:,:,λ) and Aσ = U>Ĥσ(1)(Ĥ(1))†U for each σ ∈ Σ.\n3: for i = 1 to m do 4: Compute the rank Ri truncated SVD Ĥ:,i,: ' UiDiV>i . 5: Let Ai = 〈U>i Uα, {U>i UAσU>Ui}σ∈Σ,U>i UΩ:,i〉 6: end for 7: return A1, · · · , Am."
    }, {
      "heading" : "4 Theoretical Analysis",
      "text" : "Computational complexity. The computational cost of the classical spectral learning algorithm (SL) is in O ( N +R|P||S|+R2|P||Σ| ) where the first term corresponds to estimating the Hankel matrices from a sample of size N , the second one to the rank R truncated SVD, and the third one to computing the transition matrices Aσ. In comparison, the computational cost of MT-SL is in O ( mN + (mR+ ∑ iRi)|P||S|+ (mR2 + ∑ iR 2 i )|P||Σ| ) , showing that the increase in complexity is essentially linear in the number of tasks m.\nRobustness in subspace estimation. In order to give some theoretical insights on the potential benefits of MT-SL, let us consider the simple case where the tasks are maximally related with common rank R = R1 = · · · = Rm. Let Ĥ1, · · · , Ĥm ∈ RP×S be the empirical Hankel matrices for the m tasks and let Ei = Ĥi −Hi be the error terms, where Hi is the true Hankel matrix for the ith task. Then the flattening Ĥ = Ĥ(1) ∈ R|P|×m|S| (resp. H = H(1)) can be obtained by stacking the matrices Ĥi (resp. Hi) along the horizontal axis. Consider the problem of learning the first task. One key step of both SL and MT-SL resides in estimating the left singular subspace of H1 and H respectively from their noisy estimates. When the tasks are maximally related, this space U is the same for H and H1, · · · ,Hm and we intuitively see that the benefits of MT-SL will stem from the fact that the SVD of Ĥ should lead to a more accurate estimation of U than the one only relying on Ĥ1. It is also intuitive to see that since the Hankel matrices Ĥi have been stacked horizontally, the estimation of the right singular subspace might not benefit from performing SVD on Ĥ. However, classical results on singular subspace estimation (see e.g. [29, 20]) provide uniform bounds for both left and right singular subspaces (i.e. bounds on the maximum of the estimation errors for the left and right spaces). To circumvent this issue, we use a recent result on rate optimal asymmetric perturbation bounds for left and right singular spaces [9] to obtain the following theorem relating the ratio between the dimensions of a matrix to the quality of the subspace estimation provided by SVD (the proof can be found in the supplementary material).\nTheorem 5. Let M ∈ Rd1×d2 be of rank R and let M̂ = M + E where E is a random noise term such that vec(E)‖E‖F follows a uniform distribution on the unit sphere in R d1d2 . Let ΠU ,ΠÛ ∈ Rd1×d1\nbe the matrices of the orthogonal projections onto the space spanned by the top R left singular vectors of M and M̂ respectively.\nLet δ > 0, let α = sR(M) be the smallest non-zero singular value of M and suppose that ‖E‖F ≤ α/2. Then, with probability at least 1− δ,\n‖ΠU −ΠÛ‖F ≤ 4 √ (d1 −R)R+ 2 log(1/δ) d1d2 ‖E‖F α + ‖E‖2F α2  . A few remarks on this theorem are in order. First, the Frobenius norm between the projection matrices measures the distance between the two subspaces (it is in fact proportional to the classical sin-theta distance between subspaces). Second, the assumption ‖E‖F ≤ α/2 corresponds to the magnitude of the noise being small compared to the magnitude of M (and in particular it implies ‖E‖Fα < 1); this is a reasonable and common assumption in subspace identification problems, see e.g. [30]. Lastly, as d2 grows the first term in the upper bound becomes irrelevant and the error is dominated by the quadratic term, which decreases with ‖E‖F faster than classical results. Intuitively this tells us that there is a first regime where growing d2 (i.e. adding more tasks) is beneficial, until the point where the quadratic term dominates (and where the bound becomes somehow independent of d2).\nGoing back to the power of MT-SL to leverage information from related tasks, let E ∈ R|P|×m|S| be the matrix obtained by stacking the noise matrices Ei along the horizontal axis. If we assume that the entries of the error terms Ei are i.i.d. from e.g. a normal distribution, we can apply the previous proposition to the left singular subspaces of Ĥ(1) and H(1). One can check that in this case we have ‖E‖2F = ∑m i=1 ‖Ei‖2F and α2 = sR(H)2 ≥ ∑m i=1 sR(Hi)\n2 (since R = R1 = · · · = Rm when tasks are maximally related). Thus, if the norms of the noise terms Ei are roughly the same, and so are the smallest non-zero singular values of the matrices Hi, we get ‖E‖F α ≤ O (‖E1‖F /sR(H1)). Hence, given enough tasks, the estimation error of the left singular subspace of H1 in the multitask setting (i.e. by performing SVD on Ĥ(1)) is intuitively in O ( ‖E1‖2F /sR(H1)2 ) while it is only in O (‖E1‖F /sR(H1)) when relying solely on Ĥ1, which shows the potential benefits of MT-SL. Indeed, as the amount of training data increases the error in the estimated matrices decreases, thus T = ‖E1‖F /sR(H1) goes to 0 and an error of order O ( T 2 ) decays faster than one of order O (T )."
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate the performance of the proposed multitask learning method (MT-SL) on both synthetic and real world data. We use two performance metrics: perplexity per character on a test set T , which is defined by perp(h) = 2− 1 M ∑ x∈T log(h(x)) where M is the number of symbols in the test set and h is the hypothesis, and word error rate (WER) which measures the proportion of mis-predicted symbols averaged over all prefixes in the test set (when the most likely symbol is predicted). Both experiments are in a stochastic setting, i.e. the functions to be learned are probability distributions, and explore the regime where the learner has access to a small training sample drawn from the target task, while larger training samples are available for related tasks. We compare MT-SL with the classical spectral learning method (SL) for WFAs (note that SL has been extensively compared to EM and n-gram in the literature, see e.g. [4] and [5] and references therein). For both methods the prefix set P (resp. suffix set S) is chosen by taking the 1, 000 most frequent prefixes (resp. suffixes) in the training data of the target task, and the values of the ranks are chosen using a validation set."
    }, {
      "heading" : "5.1 Synthetic Data",
      "text" : "We first assess the validity of MT-SL on synthetic data. We randomly generated stochastic WFAs using the process used for the PAutomaC competition [27] with symbol sparsity 0.4 and transition sparsity 0.15, for an alphabet Σ of size 10. We generated related WFAs5 sharing a joint feature\n5More precisely, we first generate a probabilistic automaton (PA) AS = (αS , {AσS}σ∈Σ,ωS) with dS states. Then, for each task i = 1, · · · ,m we generate a second PA AT = (αT , {AσT }σ∈Σ,ωT ) with dT states and a random vector ω ∈ [0, 1]dS+dT . Both PAs are generated using the process described in [27]. The task fi is then obtained as the distribution computed by the stochastic WFA 〈αS ⊕αT , {AσS ⊕AσT }σ∈Σ, ω̃〉 with ω̃ = ω/Z where the constant Z is chosen such that ∑ x∈Σ∗ fi(x) = 1.\nspace of dimension dS = 10 and each having a task specific feature space of dimension dT , i.e. for m tasks f1, · · · , fm each WFA computing fi has rank dS + dT and the vv-WFA computing ~f = [f1, · · · , fm] has rank dS + mdT . We generated 3 sets of WFAs for different task specific dimensions dT = 0, 5, 10. The learner had access to training samples of size 5, 000 drawn from each related tasks f2, · · · , fm and a training sample of sizes ranging from 50 to 5, 000 drawn from the target task f1. Results on a test set of size 1, 000 averaged over 10 runs are reported in Figure 1.\nFor both evaluation measures, when the task specific dimension is small compared to the dimension of the joint feature space, i.e. dT = 0, 5, MT-SL clearly outperforms SL that only relies on the target task data. Moreover, increasing the number of related tasks tends to improve the performances of MT-SL. However, when dS = dT = 10, MT-SL performs similarly in terms of perplexity and WER, showing that the multitask approach offers no benefits when the tasks are too loosely related. Additional experimental results for the case of totally unrelated tasks (dS = 0, dT = 10) as well as comparisons with MT-SL without the projection step (i.e. without lines 4-7 of Algorithm 1) are presented in the supplementary material."
    }, {
      "heading" : "5.2 Real Data",
      "text" : "We evaluate MT-SL on 33 languages from the Universal Dependencies (UNIDEP) 1.4 treebank [24], using the 17-tag universal Part of Speech (PoS) tagset. This dataset contains sentences from various languages where each word is annotated with Google universal PoS tags [25], and thus can be seen as a collection of samples drawn from 33 distributions over strings on an alphabet of size 17. For each language, the available data is split between a training, a validation and a test set (80%, 10%, 10%). For each language and for various sizes of training samples, we compare independently learning the target task with SL against using MT-SL to exploit training data from related tasks. We tested two ways of selecting the related tasks: (1) all other languages are used and (2) for each language we selected the 4 closest languages w.r.t. the distance between the subspaces spanned by the top 50 left singular vectors of their Hankel matrices6.\nWe compare MT-SL against SL (using only the training data for the target task) and against a naive baseline where all data from different tasks are bagged together and used as a training set for SL (SL-bagging). We also include the results obtained using MT-SL without the projection step (MTSL-noproj). We report the average relative improvement of MT-SL, SL-bagging and MT-SL-noproj w.r.t. SL over all languages in Table 1, e.g. for perplexity we report 100 · (psl − pmt)/psl where psl (resp. pmt) is the perplexity obtained by SL (resp. MT-SL) on the test set. We see that the multitask approach leads to improved results for both metrics, that the benefits tend to be greater for small training sizes, and that restricting the number of auxiliary tasks is overall beneficial. To give a\n6The common basis (P,S) for these Hankel matrices is chosen by taking the union of the 100 most frequent prefixes and suffixes in each training sample.\nconcrete example, on the Basque task with a training set of size 500, the WER was reduced from ∼ 76% for SL to ∼ 70% using all other languages as related tasks, and to ∼ 65% using the 4 closest tasks (Finnish, Polish, Czech and Indonesian). Overall, both SL-bagging and MT-SL-noproj obtain worst performance than MT-SL (though MT-SL-noproj still outperforms SL in terms are perplexity while SL-bagging performs almost always worse than SL). Detailed results on all languages, along with the list of closest languages used for method (2), are reported in the supplementary material."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduced the novel model of vector-valued WFA that allowed us to define a notion of relatedness between recognizable functions and to design a multitask spectral learning algorithm for WFAs (MTSL). The benefits of MT-SL have been theoretically motivated and showcased on both synthetic and real data experiments. In future works, we plan to apply MT-SL in the context of reinforcement learning and to identify other areas of machine learning where vv-WFAs could prove to be useful. It would also be interesting to investigate a weighted approach such as the one presented in [19] for classical spectral learning; this could prove useful to handle the case where the amount of available training data differs greatly between tasks."
    }, {
      "heading" : "Acknowledgments",
      "text" : "G. Rabusseau acknowledges support of an IVADO postdoctoral fellowship. B. Balle completed this work while at Lancaster University. We thank NSERC and CIFAR for their financial support."
    } ],
    "references" : [ {
      "title" : "Multi-task feature learning",
      "author" : [ "Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil" ],
      "venue" : "In NIPS, pages 41–48,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Grammatical inference as a principal component analysis problem",
      "author" : [ "Raphaël Bailly", "François Denis", "Liva Ralaivola" ],
      "venue" : "In ICML,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Learning Finite-State Machines: Algorithmic and Statistical Aspects",
      "author" : [ "Borja Balle" ],
      "venue" : "PhD thesis, Universitat Politècnica de Catalunya,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Spectral learning of weighted automata",
      "author" : [ "Borja Balle", "Xavier Carreras", "Franco M Luque", "Ariadna Quattoni" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Methods of moments for learning stochastic languages: Unified presentation and empirical comparison",
      "author" : [ "Borja Balle", "William L. Hamilton", "Joelle Pineau" ],
      "venue" : "In ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "A model of inductive bias learning",
      "author" : [ "Jonathan Baxter" ],
      "venue" : "Journal of Artifical Intelligence Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2000
    }, {
      "title" : "Exploiting task relatedness for multiple task learning",
      "author" : [ "Shai Ben-David", "Reba Schuller" ],
      "venue" : "In Learning Theory and Kernel Machines,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Closing the learning-planning loop with predictive state representations",
      "author" : [ "Byron Boots", "Sajid M. Siddiqi", "Geoffrey J. Gordon" ],
      "venue" : "International Journal of Robotics Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Rate-optimal perturbation bounds for singular subspaces with applications to high-dimensional statistics",
      "author" : [ "T Tony Cai", "Anru Zhang" ],
      "venue" : "arXiv preprint arXiv:1605.00353,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Realizations by stochastic finite automata",
      "author" : [ "Jack W. Carlyle", "Azaria Paz" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1971
    }, {
      "title" : "Multitask learning. In Learning to learn, pages 95–133",
      "author" : [ "Rich Caruana" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "Experiments with spectral learning of latent-variable pcfgs",
      "author" : [ "Shay B. Cohen", "Karl Stratos", "Michael Collins", "Dean P. Foster", "Lyle H. Ungar" ],
      "venue" : "In NAACL-HLT,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "On rational stochastic languages",
      "author" : [ "François Denis", "Yann Esposito" ],
      "venue" : "Fundamenta Informaticae,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "Matrices de Hankel",
      "author" : [ "Michel Fliess" ],
      "venue" : "Journal de Mathématiques Pures et Appliquées,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1974
    }, {
      "title" : "Sharing features among dynamical systems with beta processes",
      "author" : [ "Emily Fox", "Michael I Jordan", "Erik B Sudderth", "Alan S Willsky" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Simplicial mixtures of markov chains: Distributed modelling of dynamic user profiles",
      "author" : [ "Mark A Girolami", "Ata Kabán" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "A spectral algorithm for learning hidden markov models",
      "author" : [ "Daniel J. Hsu", "Sham M. Kakade", "Tong Zhang" ],
      "venue" : "In COLT,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "Tamara G Kolda", "Brett W Bader" ],
      "venue" : "SIAM review,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Low-rank spectral learning with weighted loss functions",
      "author" : [ "Alex Kulesza", "Nan Jiang", "Satinder Singh" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Relative perturbation theory: II. eigenspace and singular subspace variations",
      "author" : [ "Ren-Cang Li" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1998
    }, {
      "title" : "Recurrent neural network for text classification with multi-task learning",
      "author" : [ "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Multi-task sequence to sequence learning",
      "author" : [ "Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser" ],
      "venue" : "arXiv preprint arXiv:1511.06114,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Multi-task learning for sequential data via ihmms and the nested dirichlet process",
      "author" : [ "Kai Ni", "Lawrence Carin", "David Dunson" ],
      "venue" : "In ICML,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2007
    }, {
      "title" : "Universal dependencies 1.4, 2016",
      "author" : [ "Joakim Nivre", "Zeljko Agić", "Lars Ahrenberg" ],
      "venue" : "LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "A universal part-of-speech tagset",
      "author" : [ "Slav Petrov", "Dipanjan Das", "Ryan McDonald" ],
      "venue" : "arXiv preprint arXiv:1104.2086,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Links between multiplicity automata, observable operator models and predictive state representations: a unified learning framework",
      "author" : [ "Michael Thon", "Herbert Jaeger" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    }, {
      "title" : "Results of the pautomac probabilistic automaton learning competition",
      "author" : [ "Sicco Verwer", "Rémi Eyraud", "Colin De La Higuera" ],
      "venue" : "In ICGI,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Multitask generalized eigenvalue program",
      "author" : [ "Boyu Wang", "Joelle Pineau", "Borja Balle" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "Perturbation bounds in connection with singular value decomposition",
      "author" : [ "Per-Åke Wedin" ],
      "venue" : "BIT Numerical Mathematics,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1972
    }, {
      "title" : "On the convergence of eigenspaces in kernel principal component analysis",
      "author" : [ "Laurent Zwald", "Gilles Blanchard" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "In particular, WFAs can compute any probability distribution defined by a hidden Markov model (HMM) [13] and can model the transition and observation behavior of partially observable Markov decision processes [26].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : "In particular, WFAs can compute any probability distribution defined by a hidden Markov model (HMM) [13] and can model the transition and observation behavior of partially observable Markov decision processes [26].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 16,
      "context" : "A recent line of work has led to the development of spectral methods for learning HMMs [17], WFAs [2, 4] and related models, offering an alternative to EM based algorithms with the benefits of being computationally efficient and providing consistent estimators.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "A recent line of work has led to the development of spectral methods for learning HMMs [17], WFAs [2, 4] and related models, offering an alternative to EM based algorithms with the benefits of being computationally efficient and providing consistent estimators.",
      "startOffset" : 98,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "A recent line of work has led to the development of spectral methods for learning HMMs [17], WFAs [2, 4] and related models, offering an alternative to EM based algorithms with the benefits of being computationally efficient and providing consistent estimators.",
      "startOffset" : 98,
      "endOffset" : 104
    }, {
      "referenceID" : 11,
      "context" : "Spectral learning algorithms have led to competitive results in the fields of natural language processing [12, 3] and robotics [8].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "Spectral learning algorithms have led to competitive results in the fields of natural language processing [12, 3] and robotics [8].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : "Spectral learning algorithms have led to competitive results in the fields of natural language processing [12, 3] and robotics [8].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "The notion of relatedness between tasks can be expressed in different ways; one common assumption in multitask learning is that the multiple tasks share a common underlying representation [6, 11].",
      "startOffset" : 188,
      "endOffset" : 195
    }, {
      "referenceID" : 10,
      "context" : "The notion of relatedness between tasks can be expressed in different ways; one common assumption in multitask learning is that the multiple tasks share a common underlying representation [6, 11].",
      "startOffset" : 188,
      "endOffset" : 195
    }, {
      "referenceID" : 8,
      "context" : "After giving a theoretical insight on the benefits of this multitask approach (by leveraging a recent result on asymmetric bounds for singular subspace estimation [9]), we conclude by showcasing these benefits with experiments on both synthetic and real world data.",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 15,
      "context" : "In [16], mixtures of Markov chains are used to model dynamic user profiles.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "Tackling the multitask problem with nonparametric Bayesian methods is investigated in [15] to model related time series with Beta processes and in [23] to discover relationships between related datasets using nested Dirichlet process and infinite HMMs.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "Tackling the multitask problem with nonparametric Bayesian methods is investigated in [15] to model related time series with Beta processes and in [23] to discover relationships between related datasets using nested Dirichlet process and infinite HMMs.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "[10, 14] For any function f : Σ∗ → R, rank(f) = rank(Hf ).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 13,
      "context" : "[10, 14] For any function f : Σ∗ → R, rank(f) = rank(Hf ).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "We make a sporadic use of tensors in this paper, we thus introduce the few necessary definitions and notations; more details can be found in [18].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 28,
      "context" : "[29, 20]) provide uniform bounds for both left and right singular subspaces (i.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 19,
      "context" : "[29, 20]) provide uniform bounds for both left and right singular subspaces (i.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 8,
      "context" : "To circumvent this issue, we use a recent result on rate optimal asymmetric perturbation bounds for left and right singular spaces [9] to obtain the following theorem relating the ratio between the dimensions of a matrix to the quality of the subspace estimation provided by SVD (the proof can be found in the supplementary material).",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 26,
      "context" : "We randomly generated stochastic WFAs using the process used for the PAutomaC competition [27] with symbol sparsity 0.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 26,
      "context" : "Both PAs are generated using the process described in [27].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 23,
      "context" : "4 treebank [24], using the 17-tag universal Part of Speech (PoS) tagset.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 24,
      "context" : "This dataset contains sentences from various languages where each word is annotated with Google universal PoS tags [25], and thus can be seen as a collection of samples drawn from 33 distributions over strings on an alphabet of size 17.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 18,
      "context" : "It would also be interesting to investigate a weighted approach such as the one presented in [19] for classical spectral learning; this could prove useful to handle the case where the amount of available training data differs greatly between tasks.",
      "startOffset" : 93,
      "endOffset" : 97
    } ],
    "year" : 2017,
    "abstractText" : "We consider the problem of estimating multiple related functions computed by weighted automata (WFA). We first present a natural notion of relatedness between WFAs by considering to which extent several WFAs can share a common underlying representation. We then introduce the novel model of vector-valued WFA which conveniently helps us formalize this notion of relatedness. Finally, we propose a spectral learning algorithm for vector-valued WFAs to tackle the multitask learning problem. By jointly learning multiple tasks in the form of a vector-valued WFA, our algorithm enforces the discovery of a representation space shared between tasks. The benefits of the proposed multitask approach are theoretically motivated and showcased through experiments on both synthetic and real world datasets.",
    "creator" : null
  }
}