{
  "name" : "9f44e956e3a2b7b5598c625fcc802c36.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Adaptive Active Hypothesis Testing under Limited Information",
    "authors" : [ "Fabio Cecchi", "Nidhi Hegde" ],
    "emails" : [ "f.cecchi@tue.nl", "nidhi.hegde@nokia-bell-labs.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of active sequential hypothesis testing with incomplete information. The original problem, first studied by Chernoff [1], is one where a Bayesian decision maker must infer the correct hypothesis from a set of J hypotheses. At each step the decision maker may choose from W actions where the outcome of an action is a random variable that depends on the action and the true (hidden) hypothesis. In prior work, the probability distribution functions on the outcomes are assumed to be known. In the present work we assume that these distributions are not known, and only some rough information about the outcomes of the actions is known, to be made more precise further on.\nActive hypothesis testing is an increasingly important problem these days, with applications that include the following. (a) Medical diagnostics ([2]) systems that include clinical trials for testing a new treatment, or diagnostics of a new disease. (b) Crowdsourcing: online platforms for task-worker matching such as Amazon’s Mechanical Turk or TaskRabbit, where, as new tasks arrive, they must be matched to workers capable of working on them. (c) Customer hotline centres or Q&A forums: online platforms such as StackExchange where questions are submitted, and users with varying capabilities are available for providing an answer. This includes customer service centres where customer tickets are submitted and the nature of the problem must be learned before its treatment (an example where supervised learning techniques are used is [3]). (d) Content search problems where an incoming image must be matched to known contents, as studied in [4].\nWe now informally describe our model. In the general instance of our problem, the true hypothesis, θ∗ is one in a set of J hypotheses, J = {θ1, . . . , θJ}, and a set of W actions is available, where the outcomes of the actions depend on the true hypothesis. When the true hypothesis is θj and\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\naction w is chosen, a noisy outcome Xw,j ∈ J is observed, whose distribution, pw,j(·) ∈ P(J ), is given. The objective then is to select an action at each step so as to infer the true hypothesis in a minimum number of steps, with a given accuracy. In our model, we assume that the decision maker has limited information about the outcome distributions. We define the principal set of an action w as Jw ⊆ J . When action w is sampled, a noisy binary outcome y ∈ {−1, 1} is observed, which gives an indication on whether the action classifies the hypothesis in the set Jw. The quality of action w, αw is related to the noise in the outcome. Rather than the distributions pw,j(·), we assume that the decision maker only has knowledge of the principal set Jw and quality αw of each action."
    }, {
      "heading" : "1.1 Related work",
      "text" : "Since the seminal work by Chernoff [1], active hypothesis testing and variants of the problemhave been studied through various perspectives (see [5] for a brief survey). Chernoff derived a simple heuristic algorithm whose performance is shown to achieve asymptotic optimality in the regime where the probability of error vanishes. Specifically, it is shown that as the probability of error δ decreases the expected number of samples needed by Chernoff’s algorithm grows as − log(δ). Most of the past literature in active sequential hypothesis testing has dealt with extensions of Chernoff’s model, and has shown that Chernoff’s algorithm performs well in more general settings [6, 7]. A notable exception is [8], where the impact of the number of hypotheses is analyzed and an algorithm that performs better than Chernoff’s benchmark is provided for the case of large values of J .\nOur work differs from prior work in a few ways. First, the hypothesis need not be locally identifiable. While in [1] each action is able to distinguish each pair of hypotheses, we assume that each hypothesis is globally identifiable, i.e., each pair of hypotheses can be discerned by at least one action. This is a common assumption in the area of distributed hypothesis testing ([9, 10]) and a weaker assumption than that of Chernoff. Note that dropping this assumption is not novel in itself, and has been done in other work such as [8]. Second, a novel extension in our work, differing from [8] is that we do not assume full knowledge on the actions’ statistical parameters. The responses of actions are noisy, and in past literature the probability distributions governing them was assumed to be known. In our model, we drop this assumption, and we only require to know a lower bound αw > 1/2 on the probability that action w will provide a correct response, no matter the hypothesis we want to test. As far as we know, no previous work in active sequential learning has tackled the problem of incomplete statistical information and we believe that such an extension may provide a non-negligible impact in real-life applications.\nActive hypothesis testing is similar to the problem of Bayesian active learning. This latter perspective in considered in [11] where noisy Bayesian active learning setting is used on the hypothesis testing problem with asymmetric noise and a heuristic based on the extrinsic Jensen-Shannon (EJS) divergence [12] is proposed. As in [8], full knowledge of the probability distributions governing the noise is available. In contrast, in our work we consider a more restricted model where, only a binary outcome with noise is given by the actions on the large hypothesis space. Inference with binary responses is considered in work on generalized binary search (GBS) [13], which is special case where the label set (outcome of actions) is binary with the case of symmetric, non-peristent noise. Our work differs from this type of work in that we consider asymmetric label-dependent noise, that is, αw varies with action w.\nWe thus position our work between [11, 8] and [13]. While the former assumes full knowledge on the noise distributions, we assume that only a binary response is provided and only a lower bound on the value that governs the outcome is known, and while the latter considers symmetric noise, we extend to asymmetric label-dependent noise.\nOur contribution. Our main objective is to investigate the minimum sample query size of this system for a certain level of accuracy in the inference of the true hypothesis, and to design efficient policies for this inference. Our contributions in the present paper are as follows. First, we consider the system under limited knowledge of outcome distribution. This restricted scenario adds a significant constraint for the action selection policy, and the belief vector update policy. To the best of our knowledge, this restricted scenario has not been considered in past literature. Second, under the limited knowledge constraint, we propose the Incomplete-Bayesian Adaptive Gradient (IBAG) policy which includes a belief vector update rule that we call Incomplete-Bayesian, and an action selection rule, named Adaptive Gradient, that follows the drift of the (unknown) coordinate of interest in the\nbelief vector. Third, we derive a lower bound on the sample size for the system under incomplete information, and show that the performance of IBAG matches this bound. We also carry out numerical experiments to compare IBAG to prior work."
    }, {
      "heading" : "2 Model",
      "text" : "The classic model of the active sequential learning problem consists in sequentially selecting one of several available sensing actions, in order to collect enough information to identify the true hypothesis, as considered in [1]. We thus consider a system where a decision maker has at his disposal a finite set of actions W = {1, . . . ,W}, and there are a set of J = |J | < ∞ possible hypothesis, J = {θ1, . . . , θJ}. (For the rest of the paper, we refer to a hypothesis only by its index, i.e., j for hypothesis θj , for ease of notation.) When the true hypothesis is j and actionw is sensed, the outcome Xw,j ∈ J is sampled from the distribution pw,j(·) ∈ P(J ), i.e., P{Xw,j = j′} = pw,j(j′). In our model, we assume to have limited information about the actions and this affects the classic model in two ways. First, for every sampled action w, a binary outcome y ∈ {−1, 1} is observed, indicating whether the inference of hypothesis by this action is in Jw or not, i.e., the response observed is Yw,j ∈ {−1, 1} where\nYw,j = { 1, if Xw,j ∈ Jw, −1, if Xw,j /∈ Jw.\nThe subset Jw ⊆ J is assumed to be known, and it is described by the matrix g ∈ {−1, 1}W×J where\ngw,j = { 1, if j ∈ Jw, −1, if j /∈ Jw.\n(1)\nObserve that the probability an action w correctly identifies the subset to which the true hypothesis j belongs is given by qw,j := P{Yw,j = gw,j} = ∑ j′:gw,j=gw,j′ pw,j(j ′). However, as a second restriction, instead of knowing qw,j , the capacity, or quality, of an action w is captured by αw where we assume that qw,j ≥ αw, ∀ j ∈ J , w ∈ W. (2) We thus characterize each action by its principal set, Jw, and its quality, αw. Assumption 1. For every action w ∈ W , the principal sets Jw ⊆ J and the quality αw ∈ (1/2, 1) are known. Denote by ∆w = 2αw − 1 where ∆w ∈ [∆m,∆M ] and ∆m,∆M ∈ (0, 1).\nSince each action can only indicate whether the hypothesis belongs to a subset or not, there must exist an action w ∈ W for which j1 and j2 belong to different subsets, for all pairs j1, j2 ∈ J . Define the subsetWj1,j2 ⊆ W asWj1,j2 = {w ∈ W : gw,j1gw,j2 = −1}. Assumption 2. For every j1, j2 ∈ J , the subsetWj1,j2 is nonempty, i.e., each hypothesis is globally identifiable.\nFor every action w ∈ W and hypothesis j ∈ J we define the subsets Jw,+j and Jw,−j which are, respectively, given by the hypotheses that action w cannot and can distinguish from j, i.e.,\nJw,+j = {j′ ∈ J : gw,j′gw,j = 1}, Jw,−j = {j′ ∈ J : gw,j′gw,j = −1}.\nNote that w ∈ Wj1,j2 if and only if j2 ∈ Jw,−j1 (or equivalently j1 ∈ Jw,−j2 ). We aim to design a simple algorithm to infer the correct hypothesis using as few actions as possible. The true hypothesis will be denoted by j∗ ∈ J . The learning process is captured by the evolution of the belief vector ν(t) ∈ P(J ), where νj(t) denotes the decision maker’s confidence at time t that the true hypothesis is j. At the initial step t = 1, the belief vector ν(1) ∈ P(J ) is initialized so that νj(1) > 0, j ∈ J . Since we assume to initially lack any information on the true hypothesis, without loss of generality, we set νj(1) = 1/J for every j ∈ J . At every step t ≥ 1, according to the belief vector ν(t), the decision maker determines the next action to sense FW (ν(t)) = w(t) ∈ W according to some selection rule FW (·). The outcome y(t) ∈ {−1, 1} from the chosen action w(t) is used to update the belief vector according to an update rule FU ( ν(t), w(t), y(t) ) = ν(t + 1) ∈ P(J ). The algorithm ends at time T ∗, and the\ninferred hypothesis is given by ĵ = arg maxj∈J νj(T ∗) . Sensing actions is stopped when one of the posteriors is larger than 1− δ, for some δ > 0:\nT ∗ = inf t≥0 {max j∈J νj(t) > 1− δ}. (3)"
    }, {
      "heading" : "3 The Incomplete-Bayesian update rule",
      "text" : "We now describe how the decision maker updates the belief vector after he observes the outcome of an action. Given a belief vector ν ∈ P(J ) and the observation y ∈ {−1, 1} obtained from action w ∈ W , define\nf̃(y, j, w) =\n{ qw,j , y = gw,j ,\n1− qw,j , y = −gw,j , f(y, j, w) =\n{ αw, y = gw,j ,\n1− αw, y = −gw,j .\nNote that f̃(y, j, w) denotes the probability of having outcome y given that the action w is chosen and the true hypothesis is j. The standard Bayesian update rule is given by the map FBU (ν, w, y), where FBU,j(ν, w, y) = f̃(y,j,w)νj∑\ni∈J f̃(y,i,w)νi . In our model, however, the values qw,j for w ∈ W are unknown to\nthe decision maker. Hence, we introduce the Incomplete Bayesian (IB) update rule, which mimics the Bayesian rule, but with limited knowledge on outcome probailities. The IB update rule is given by the map FU (ν, w, y), where\nFU,j(ν, w, y) = f(y, j, w)νj∑ i∈J f(y, i, w)νi . (4)\nObserve that Bayesian and IB update rules are identical when qw,j = αw.\nIn practice, the νj(t) evolves according to both the quality of the chosen action, αw, and the relation between this action’s principal set Jw and the current state of the belief vector ν(t). This dependence is formalized in the following lemma whose proof is included in the supplementary material, Section B. Lemma 1. Given ν(t) ∈ P(J ) and w(t) ∈ W , then it holds that\nνj∗(t+ 1) νj(t+ 1) = νj∗(t) νj(t) ×  1, w.p. indic1{w(t) /∈ Wj∗,j}, 1+∆w(t) 1−∆w(t)\n, w.p. 1{w(t) ∈ Wj∗,j}qw(t),j∗ , 1−∆w(t) 1+∆w(t) , w.p. 1{w(t) ∈ Wj∗,j}(1− qw(t),j∗)."
    }, {
      "heading" : "3.1 A lower bound on the sample size",
      "text" : "Note that the IB update rule alone sets some constraints on the performance. In particular, if we require the error probability to be low, then the expected number of samples is necessarily larger than a certain quantity depending on the model parameters. We show that this quantity asymptotically grows as − log δ in the asymptotic regime where δ → 0. Theorem 1. Assume the IB update rule is applied to the belief vector and that\nlim δ→0\nP{νj∗(T ∗) ≤ δ} ≤ γ̃ < 1.\nThen, there exist functions Kl0(δ), K l 1(δ) such that\nE[T ∗] ≥ Kl1(δ) log 1\nδ +Kl0(δ), lim δ→0 Kli(δ) ≥ Kli > 0, for i = 0, 1.\nThe proof of this result is presented in the supplement, Section A.2. We sketch the proof here. We first define\nSt(j1, j2) = log νj1(t)\nνj2(t) , S(j1, j2) = ST∗(j1, j2), and show that, on the one hand, if P{ĵ 6= j∗} is small, then ∑ j 6=j∗ S(j\n∗, j) is large with high probability, and on the other hand, if t is small, then ∑ j 6=j∗ St(j ∗, j) is small with high probability.\nWe use these properties to derive a lower bound on the tail probability of T ∗, and thus on its expected value.\nFurther, we can control the belief vector evolution by deriving bounds on the ratio between coordinates of the belief vector under the IB policy. Specifically, in the supplementary material Section A.3, we bound the probability that νj(t) > νj∗(t) at a certain time, and investigate how this probability evolves with t."
    }, {
      "heading" : "4 Adaptive Gradient: the action selection policy",
      "text" : ""
    }, {
      "heading" : "4.1 A gradient-based selection policy",
      "text" : "We now present an action selection policy that, together with the IB update rule, defines our active learning algorithm, which we call the Incomplete-Bayesian Adaptive Gradient (IBAG) policy. We will then analyze the complete algorithm showing that its performance asymptotically matches the lower bound provided in Theorem 1 as δ → 0. We focus on the j∗-th coordinate of the belief vector, and define the drift at time t as\nDw(ν(t)) = E[νj∗(t+ 1)|ν(t), w(t) = w]− νj∗(t). Simple algebra and (4) yield the following Lemma. Lemma 2. It holds that\nDw(ν(t)) = 4∆wνj∗(t)νw,−j∗(t) (qw,j∗ − αw + ∆wνw,−j∗(t)\n1−∆2w ( 1− 2νw,−j∗(t) )2 ), (5) where\nνw,+j = ∑\nj∈Jw,+j\nνj , νw,−j = ∑\nj∈Jw,−j\nνj .\nAssume for a moment that we know the true hypothesis j∗ and qw,j∗ for every w ∈ W . Then, in order to let νj∗(t) grow as much as possible, we would greedily select the action w which maximizes Dw(ν(t)). Our worker selection policy will attempt to mimic as closely as possible this greedy policy, while operating without complete information. Lemma 3. It holds that Dw(ν(t)) ≥ DLw(ν(t)), where\nDLw(ν(t)) = 4νj∗(t) ∆2wν 2 −w(t) 1−∆2w ( 1− 2ν−w(t) )2 , (6) and\nν−w(t) = min { ∑ j∈Jw νj(t), ∑ j /∈Jw νj(t) }\nThe proof follows from the fact that Dw(ν(t)) is increasing both in qw,j∗ and νw,−j∗(t) for every w ∈ W , and the observation that that qw,j∗ ≥ αw and νw,−j∗(t) ≥ ν−w(t).\nNote that DLw(ν(t)) provides us a tight lower bound on the expected growth of the coordinate of the true hypothesis if action w is chosen at step t. Indeed, DLw(ν(t)) can be decomposed to a part that uses the j∗-th coordinate of the belief vector and a part than can be computed without knowing j∗. The Adaptive Gradient (AG) selection rule, then chooses at step t, the action wD(t) ∈ W such that\nwD(t) = FW (ν(t)) = arg max w∈W\nG(ν−w,∆w), G(v, d) = d2v2 1− d2 ( 1− 2v )2 , (7) i.e., we select the action maximizing the current lower bound on the expected growth of the j∗coordinate of the belief vector. Ties are broken uniformly.\nRemark: Assume the actions have different costs of sensing. The AG selection rule can then be generalized as follows:\nwD(t) = F cW (ν(t)) = arg max w∈W\nG(ν−w,∆w)\ncw . (8)"
    }, {
      "heading" : "4.2 An upper bound",
      "text" : "We now present our main result. We show that the expected number of samples required by our algorithm IBAG asymptotically matches the lower bound obtained in Theorem 1. Theorem 2. Under the IBAG algorithm, there exist constants Ku0 , Ku1 > 0 independent of δ such that\nE[T ∗] ≤ Ku1 log 1\nδ +Ku0 .\nThe proof is provided in supplementary material, Section A.5. This result is based on the intuition that IBAG never selects an action that is too uninformative relative to the other actions. Specifically, the information provided by an action w at time t depends on its quality αw and outcome over the subset Jw,−j∗ . In other words, the value νw,−j∗ must decrease to 0, hence the higher this value is for a given action w, the more we can still learn from sensing this action. As a proxy for νw,−j∗ we use ν−w which also must be as large as possible. The following lemma, whose proof is given in supplementary material, Section B, provides bounds on the relative quality of ν−wD(t) compared to ν−w. Lemma 4. For every w ∈ W , it holds that ν−wD(t) ≥ ∆ m ∆M ν−w."
    }, {
      "heading" : "5 Numerical results",
      "text" : "We now present numerical results based on simulations. In order to gain practical insight, we will focus on a task labelling application. A task labelling problem might arise in a crowdsourcing scenrio such as Amazon’s Mechanical Turk or Content search problems where an incoming image must be matched to known contents. The mapping to the hypothesis testing problem is as follows. The set of hypotheses J corresponds to the set of task labels, with j∗ the true hypothesis being the latent task label that must be inferred. The set of W actions corresponds to W workers who perform the labelling when sampled, where pw,j(j′) is the probability that worker w assigns the task the label j′ when the true label is j. For each worker w, we will call Jw the expertise of the worker (principal set of the actions), and αw will be the quality of the worker. We will first investigate the impact of the lack of exact knowledge, i.e., the difference between αw and qw,j , that we call slack. We then compare our algorithm to that in [1] and that of [13] for a few scenarios of interest."
    }, {
      "heading" : "5.1 The effect of the slack",
      "text" : "Here we present a simulated scenario with J = 100, W = 15, and fixed subsets {Jw}w∈W satisfying Assumption 2. We set δ ≈ 0.001, and assume the incoming job-type to be j∗ = 1. In Figure 1 we present the results of 1000 runs of the simulation for every instance of respectively the first and second scenario described below. Recalling that the simulation stops as soon as maxj νj(t) > 1− δ, we specify that out of the entire set of simulations of these scenarios the algorithm never failed to infer the correct incoming job type j∗ = 1. For both scenarios, in Figure 1(left) we display the averaged sample paths of the coordinate νj∗(t) and in Figure 1(right) the average sample size required for the decision maker to make an inference.\nThe performance upper bound is pessimistic. In the first set of simulations, scenario A, we fix the quality vector α with αw ∈ (0.55, 0.6) for every worker w ∈ W . We then let the parameter s vary in {0, .05, .1, .15, .2, .25, .3} and assume qw,j∗ = αw + s for every w ∈ W . In Theorem 2 we proved an upper bound for E[T ∗] when the IBAG algorithm is employed. It can be observed that the upper bound does not depend on qw,j∗ , but only on αw. In fact, the upper bound is obtained by looking at the worst case scenario, where qw,j∗ = αw for every w ∈ W and j ∈ J . As the slack s grows, the performance of the algorithm drastically improves even if it is not reflected in the upper bound term.\nRobustness to perturbations in estimate of worker skills. In the second set of simulations, scenario B we fix the quality vector qw,j∗ ∈ (0.85, 0.9) for every worker w ∈ W . We then let the parameter s vary in {0, .05, .1, .15, .2, .25, .3} and set αw = qw,j∗ − s for every w ∈ W . It is observed that the IBAG algorithm performs well even when the decision maker’s knowledge of the skills is not precise, and he decides to play safe by reducing the lower bound α(w).\nWe therefore deduce that the learning process strongly depends on the true skills of the worker qw,j (Figure 1(a)), however their exact knowledge is not fundamental for IBAG to behave well (Figure 1(b)) - it is robust to small perturbations."
    }, {
      "heading" : "5.2 Comparison to existing algorithms",
      "text" : "Chernoff algorithm. As we mentioned, most of the existing sequential hypothesis testing algorithms are based on Chernoff’s algorithm presented in [1]. Such an algorithm, at step t identifies the job-types j1, j2 ∈ J associated with the two highest values of ν(t) and selects the class of workers wC that best distinguishes j1 and j2, i.e., wC = arg maxw∈Wj1,j2 αw. In the asymptotic regime with δ → 0, the expected sample size required by the Chernoff’s algorithm is of order − log δ, exactly as with IBAG. This has been proven ([1, 8]) in the case with full knowledge of the matrix pw,j(·). What we emphasize here is that by focusing only on the two highest components of ν(t), the decision maker loses information that might help him make a better selection of worker w(t). In particular, Chernoff’s algorithm bases its decision largely on the workers’ skills and thus does not behave as well as it should when these are not informative enough.\nSoft-Decision GBS algorithm. The algorithm proposed in [13] generalizes the intuition behind optimal GBS algorithms in noiseless environments. This algorithm, given a belief vector ν(t) at step t picks the worker w̄ such that w̄ = arg minw ∣∣∑ j∈J νjgw,j ∣∣ = arg minw ∣∣∑j∈Jw νj −∑ j /∈Jw νj\n∣∣ = arg maxw{ν−w}. Intuitively, the Soft-Decision GBS algorithm selects the worker that is the most \"unsure\", in the sense that the worker splits the belief vector as evenly as possible. Since the model in [13] does not allow for different qualities of the workers (noise is symmetric there), this feature does not play a role on the worker selection policy. Note that when the quality of all workers are identical, the Soft-Decision GBS and the IBAG algorithms are identical. In [13], an asymptotic performance analysis is presented, and under certain constraints on the problem geometry, it is shown that the sample size required is of order − log δ + log J , and once again the performance in terms of the error probability matches with IBAG.\nWe now compare our algorithm IBAG with the Chernoff algorithm under three scenarios and with Soft-Decision GBS only for the third scenario where the quality αw or workers (noise in GBS) differ among the workers.\nIn the first scenario, we set J = 32, j∗ = 1, and δ = 0.003. We assume two kinds of worker classes. We have 5 ‘generalist’ workers, each of whom has |Jw| = J/2 = 16 and moreover for every pair of job types (j1, j2) there exists a generalist belonging toWj1,j2 . In addition, we have 32 ‘specialist’ workers who can distinguish exactly 1 job-type, i.e., |Jw| = 1. We assume that there is one specialist per job-type, and note that among them there is also w∗ such that Jw∗ = {j∗}. We consider two cases: in case A, the skills of the workers are identical, αw = 0.8 for every w ∈ W , and in case B we drop the generalists’ skill level to αw = 0.75. We assume qw,j = αw for every w ∈ W and j ∈ J . In the second scenario, we set J = 30 with only specialists present. We set δ = 0.003 and j∗ = 1. In this scenario we consider two cases as well, in case A αw = 0.7 for every worker, while in case B we drop the skill level of the specialist on job-type j∗ to 0.65, representing a situation where the system is ill-prepared for an incoming job. We assume qw,j = αw for every w ∈ W and j ∈ J . We display the results for both scenarios in Figure 2. In Figure 2(top) we display boxplots of the number of queries required and in Figure 2(bottom) we show the expectation of the number of queries per kind of worker. In both scenarios, the performance of Chernoff’s algorithm is drastically\nweakened by only a tiny variation in αw, yielding a very different behavior. In the first scenario, although it is very informative to query the generalists in an early explorative stage, under Chernoff’s algorithm the selection of the workers relies too much on the skill levels and therefore always queries the specialists. The IBAG algorithm, on the other hand, sensibly decides at each step on the trade-off between getting rough information on a larger set of job pairs, or getting more precise information on a smaller set, and seems to better grasp this quality vs quantity dilemma.\nSimilarly, in case B of the second scenario, the low-quality workers (the specialist in j∗) are never selected by Chernoff’s algorithm, even if their responses have a large impact on the growth of νj∗(t). For both cases A and B we see that IBAG outperforms Chernoff.\nIn the third scenario we set J = 32, W = 42, and δ = 0.03. We have five low-quality generalist workers with αw = 0.55, five high-quality generalist workers with αw = 0.75. The remaining 32 workers are specialists with αw = 0.8. The plots comparing all three algorithms is shown in Figure 2(iii). We observe again that the Chernoff algorithm never queries generalists and performs the worst. IBAG outperforms Soft-GBS because it queries high-quality workers preferentially while Soft-GBS doesn’t consider quality."
    }, {
      "heading" : "6 Discussion and conclusion",
      "text" : "We have presented and analyzed the IBAG algorithm, an intuitive active sequential learning algorithm which requires only a rough knowledge of the quality and principal set of each available action. The algorithm is shown to be competitive and in many cases outperforms Chernoff’s algorithm, the benchmark in the area.\nAs far as we know, this is the first attempt to analyze a scenario where the decision maker has limited knowledge of the system parameters. In Section 5 we studied through simulations, the effect of this lack of exact knowledge on the performances of the system, in order to quantify the tradeoff between caution, i.e., how close αw is to qw,j , and the cost. The numerical analysis suggests that a moderate caution does not worsen drastically the performance. In the supplement Section C we analyze formally this tradeoff and show results on how cautious the decision maker can be while still ensuring good performance.\nA further element of incomplete knowledge would be to allow slight perturbations on the principal sets of the actions. In the present paper we have assumed to know with certainty, for every w ∈ W and j ∈ J , whether w has j in its principal set (j ∈ Jw), or not. In future work we will investigate the impact of uncertainty in the expertise, for instance having j ∈ Jw with some probability pj,w.\nAs a last remark, it would be interesting to analyze the model when the different actions have heterogeneous costs. Note that the IBAG algorithm naturally extends to such case, as mentioned in equation (8). The IBAG algorithm in the framework of the task-worker system could give definitive answers on whether it is better to sample a response from a cheap worker with a general expertise and low skill or from more expensive workers with narrow expertise and higher skill."
    } ],
    "references" : [ {
      "title" : "Sequential design of experiments",
      "author" : [ "H. Chernoff" ],
      "venue" : "The Annals of Mathematical Statistics, vol. 30, no. 3, pp. 755–770, 1959.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "Bayesian Adaptive Methods for Clinical Trials",
      "author" : [ "S. Berry", "B. Carlin", "J. Lee", "P. Muller" ],
      "venue" : "CRC press,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Data mining for customer service support",
      "author" : [ "S.C. Hui", "G. Jha" ],
      "venue" : "Information & Management, vol. 38, no. 1, pp. 1–13, 2000.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Active sequential hypothesis testing with application to a visual search problem",
      "author" : [ "N. Vaidhiyan", "S.P. Arun", "R. Sundaresan" ],
      "venue" : "2012 IEEE International Symposium on Information Theory Proceedings (ISIT), pp. 2201–2205, IEEE, 2012.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A brief history of sequential analysis",
      "author" : [ "B. Ghosh" ],
      "venue" : "Handbook of Sequential Analysis, vol. 1, 1991.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "The sequential design of experiments for infinitely many states of nature",
      "author" : [ "A. Albert" ],
      "venue" : "The Annals of Mathematical Statistics, vol. 32, pp. 774–799, 1961.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1961
    }, {
      "title" : "Asymptotically optimum sequential inference and design",
      "author" : [ "J. Kiefer", "J. Sacks" ],
      "venue" : "The Annals of Mathematical Statistics, vol. 34, pp. 705–750, 1963.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1963
    }, {
      "title" : "Active sequential hypothesis testing",
      "author" : [ "M. Naghshvar", "T. Javidi" ],
      "venue" : "The Annals of Statistics, vol. 41, no. 6, pp. 2703–2738, 2013.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Social learning and distributed hypothesis testing",
      "author" : [ "A. Lalitha", "A. Sarwate", "T. Javidi" ],
      "venue" : "Information Theory (ISIT), 2014 IEEE International Symposium on, pp. 551–555, IEEE, 2014.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Consensus and cooperation in networked multi-agent systems",
      "author" : [ "R. Olfati-Saber", "J. Fax", "R. Murray" ],
      "venue" : "Proceedings of the IEEE, vol. 95, no. 1, pp. 215–233, 2007.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Noisy bayesian active learning",
      "author" : [ "M. Naghshvar", "T. Javidi", "K. Chaudhuri" ],
      "venue" : "Communication, Control, and Computing (Allerton), 2012 50th Annual Allerton Conference on, pp. 1626–1633, IEEE, 2012.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Extrinsic jensen-shannon divergence with application in active hypothesis testing",
      "author" : [ "M. Naghshvar", "T. Javidi" ],
      "venue" : "IEEE International Symposium on Information Theory (ISIT), 2012.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Noisy generalized binary search",
      "author" : [ "R. Nowak" ],
      "venue" : "Advances in neural information processing systems, pp. 1366–1374, 2009. 9",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The original problem, first studied by Chernoff [1], is one where a Bayesian decision maker must infer the correct hypothesis from a set of J hypotheses.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "(a) Medical diagnostics ([2]) systems that include clinical trials for testing a new treatment, or diagnostics of a new disease.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "This includes customer service centres where customer tickets are submitted and the nature of the problem must be learned before its treatment (an example where supervised learning techniques are used is [3]).",
      "startOffset" : 204,
      "endOffset" : 207
    }, {
      "referenceID" : 3,
      "context" : "(d) Content search problems where an incoming image must be matched to known contents, as studied in [4].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Since the seminal work by Chernoff [1], active hypothesis testing and variants of the problemhave been studied through various perspectives (see [5] for a brief survey).",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "Since the seminal work by Chernoff [1], active hypothesis testing and variants of the problemhave been studied through various perspectives (see [5] for a brief survey).",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 5,
      "context" : "Most of the past literature in active sequential hypothesis testing has dealt with extensions of Chernoff’s model, and has shown that Chernoff’s algorithm performs well in more general settings [6, 7].",
      "startOffset" : 194,
      "endOffset" : 200
    }, {
      "referenceID" : 6,
      "context" : "Most of the past literature in active sequential hypothesis testing has dealt with extensions of Chernoff’s model, and has shown that Chernoff’s algorithm performs well in more general settings [6, 7].",
      "startOffset" : 194,
      "endOffset" : 200
    }, {
      "referenceID" : 7,
      "context" : "A notable exception is [8], where the impact of the number of hypotheses is analyzed and an algorithm that performs better than Chernoff’s benchmark is provided for the case of large values of J .",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "While in [1] each action is able to distinguish each pair of hypotheses, we assume that each hypothesis is globally identifiable, i.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "This is a common assumption in the area of distributed hypothesis testing ([9, 10]) and a weaker assumption than that of Chernoff.",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 9,
      "context" : "This is a common assumption in the area of distributed hypothesis testing ([9, 10]) and a weaker assumption than that of Chernoff.",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "Note that dropping this assumption is not novel in itself, and has been done in other work such as [8].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "Second, a novel extension in our work, differing from [8] is that we do not assume full knowledge on the actions’ statistical parameters.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "This latter perspective in considered in [11] where noisy Bayesian active learning setting is used on the hypothesis testing problem with asymmetric noise and a heuristic based on the extrinsic Jensen-Shannon (EJS) divergence [12] is proposed.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "This latter perspective in considered in [11] where noisy Bayesian active learning setting is used on the hypothesis testing problem with asymmetric noise and a heuristic based on the extrinsic Jensen-Shannon (EJS) divergence [12] is proposed.",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 7,
      "context" : "As in [8], full knowledge of the probability distributions governing the noise is available.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 12,
      "context" : "Inference with binary responses is considered in work on generalized binary search (GBS) [13], which is special case where the label set (outcome of actions) is binary with the case of symmetric, non-peristent noise.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "We thus position our work between [11, 8] and [13].",
      "startOffset" : 34,
      "endOffset" : 41
    }, {
      "referenceID" : 7,
      "context" : "We thus position our work between [11, 8] and [13].",
      "startOffset" : 34,
      "endOffset" : 41
    }, {
      "referenceID" : 12,
      "context" : "We thus position our work between [11, 8] and [13].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "The classic model of the active sequential learning problem consists in sequentially selecting one of several available sensing actions, in order to collect enough information to identify the true hypothesis, as considered in [1].",
      "startOffset" : 226,
      "endOffset" : 229
    }, {
      "referenceID" : 0,
      "context" : "We then compare our algorithm to that in [1] and that of [13] for a few scenarios of interest.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "We then compare our algorithm to that in [1] and that of [13] for a few scenarios of interest.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "As we mentioned, most of the existing sequential hypothesis testing algorithms are based on Chernoff’s algorithm presented in [1].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "This has been proven ([1, 8]) in the case with full knowledge of the matrix pw,j(·).",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "This has been proven ([1, 8]) in the case with full knowledge of the matrix pw,j(·).",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : "The algorithm proposed in [13] generalizes the intuition behind optimal GBS algorithms in noiseless environments.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "Since the model in [13] does not allow for different qualities of the workers (noise is symmetric there), this feature does not play a role on the worker selection policy.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 12,
      "context" : "In [13], an asymptotic performance analysis is presented, and under certain constraints on the problem geometry, it is shown that the sample size required is of order − log δ + log J , and once again the performance in terms of the error probability matches with IBAG.",
      "startOffset" : 3,
      "endOffset" : 7
    } ],
    "year" : 2017,
    "abstractText" : "We consider the problem of active sequential hypothesis testing where a Bayesian decision maker must infer the true hypothesis from a set of hypotheses. The decision maker may choose for a set of actions, where the outcome of an action is corrupted by independent noise. In this paper we consider a special case where the decision maker has limited knowledge about the distribution of observations for each action, in that only a binary value is observed. Our objective is to infer the true hypothesis with low error, while minimizing the number of action sampled. Our main results include the derivation of a lower bound on sample size for our system under limited knowledge and the design of an active learning policy that matches this lower bound and outperforms similar known algorithms.",
    "creator" : null
  }
}