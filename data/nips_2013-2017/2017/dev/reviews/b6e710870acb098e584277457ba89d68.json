{"title": "Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction", "abstract": "Adversarial machines, where a learner competes against an adversary, have regained much recent interest in machine learning. They are naturally in the form of saddle-point optimization, often with separable structure but sometimes also with unmanageably large dimension. In this work we show that adversarial prediction under multivariate losses can be solved much faster than they used to be. We first reduce the problem size exponentially by using appropriate sufficient statistics, and then we adapt the new stochastic variance-reduced algorithm of Balamurugan &amp; Bach (2016) to allow any Bregman divergence. We prove that the same linear rate of convergence is retained and we show that for adversarial prediction using KL-divergence we can further achieve a speedup of #example times compared with the Euclidean alternative. We verify the theoretical findings through extensive experiments on two example applications: adversarial prediction and LPboosting.", "id": "b6e710870acb098e584277457ba89d68", "authors": ["Zhan Shi", "Xinhua Zhang", "Yaoliang Yu"], "conference": "NIPS2017", "accepted": true, "reviews": [{"comments": "This paper shows that \"certain\" adversarial prediction problems under multivariate losses can be solved \"much faster than they used to be\". The paper stands on two main ideas: (1) that the general saddle function optimization problem stated in eq. (9) can be simplified from an exponential to a quadratic complexity (on the sample size), and (2) that the simplified optimization problem, with some regularization, can be solved using some extension of the SVRG (stochastic variance reduction gradient) method to Bregman divergences.\n\nThe paper is quite focused on the idea of obtaining a faster solution of the adversarial problem. However, the key simplification is applied to a specific loss, the F-score, so one may wonder if the benefits of the proposed method could be extended to other losses. The extension of the SVRG is a more general result, it seems that the paper could have been focused on proposing Breg-SVRG, showing the adversarial optimization with the F-score as a particular application.\n\nIn any case, I think the paper is technically correct and interesting enough to be accepted.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "I found this an interesting paper with strong technical results (although it is far from my areas of expertise, do I don't have high confidence in my review).\n\nA few comments:\n\n* Sections 3 and 4 feel a little bit disjointed - they are two very separate threads, and it feels like the paper lacks a little focus. Would it make sense to put section 4 first?\n\n* The inclusion of the regularizer ||\\theta||^2 in (10) is a little abrupt - not very well explained - more explanation/justification would be helpful.\n\n* The application to LP boosting using entropy regularization would benefit from more discussion: how does the resulting algorithm compare to the methods introduced in the original paper?", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper extends a saddle point derivative of the SVRG algorithm to the case of entropy regularization (rather than Euclidean). The algorithm is applied to the problem of adversarial prediction, where the authors simplify the objective function so that it can be solved efficiently. The algorithm is then evaluated experimentally.\n\nThe only comment that I have about the paper is its presentation and writing. It seems the authors have tried to cram a lot of material into such a short paper. This makes the paper hard-to-follow at times and its reading tiresome.\n\nNonetheless, the contributions are novel albeit straight-forward, and as such I suggest its acceptance.\n\nReferences\n========\n\nP. Balamurugan and F. Bach. Stochastic variance reduction methods for saddle-point problems.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
