{"title": "PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs", "abstract": "The predictive learning of spatiotemporal sequences aims to generate future images by learning from the historical frames, where spatial appearances and temporal variations are two crucial structures. This paper models these structures by presenting a predictive recurrent neural network (PredRNN). This architecture is enlightened by the idea that spatiotemporal predictive learning should memorize both spatial appearances and temporal variations in a unified memory pool. Concretely, memory states are no longer constrained inside each LSTM unit. Instead, they are allowed to zigzag in two directions: across stacked RNN layers vertically and through all RNN states horizontally. The core of this network is a new Spatiotemporal LSTM (ST-LSTM) unit that extracts and memorizes spatial and temporal  representations simultaneously. PredRNN achieves the state-of-the-art prediction performance on three video prediction datasets and is a more general framework, that can be easily extended to other predictive learning tasks by integrating with other architectures.", "id": "e5f6ad6ce374177eef023bf5d0c018b6", "authors": ["Yunbo Wang", "Mingsheng Long", "Jianmin Wang", "Zhifeng Gao", "Philip S. Yu"], "conference": "NIPS2017", "accepted": true, "reviews": [{"comments": "The work introduces a new architecture for conditional video generation. It is is based heavily on convolutional LSTMs, with the insight that the prior architecture where each layer of hierarchy represents increasingly abstract properties of the scene may be suboptimal for video generation (unlike classification, a generative model needs, even at the output layer, to retain information about the precise position of objects in the scene). The model introduced here, PredRNN extends convolutional LSTMs to contain two memory cells, one which flows through time at the same layer (like the original ConvLSTM), and one which flows up through the layers. The authors test the model on two datasets: moving MNIST digits and KTH action recognition dataset and demonstrate superior MSE in video prediction to prior work. The paper is fairly well-written and cites prior work.\n\nThe architecture is novel and interesting. I think the comparison with prior work could be strengthened which would make it easier to understand the empirical strength of these results. For example, the Kalchbrenner, 2016 claims to reach near the lower-bound the MNIST digit task, but is not included as a baseline. Prior work on MNIST (e.g. (Kalchbrenner, 2016) and the original introduction of the dataset, (Srivastava, 2016) report the cross-entropy loss. Here, the authors focus on the maximum likelihood output, but it would be helpful for comparison with prior work to also report the likelihood.\n\nAdditionally, the computational complexity is mentioned as an advantage of this model, but no detailed analysis or comparison is performed so its hard to know how this compares computational complexity with prior work.\n\nMinor notational suggestion:\n\nIt might be easier for the reader to follow if you use M instead of C for the cell state in equation 3 so that the connection with equation 4 is clearer.\n\n[I've read the author's response. I think this paper is stronger for comparison with prior work (which I assume they'll include in the final version) so I have raised my evaluation. I'm still unclear if they are training with MSE and other approaches are using different losses, doesn't that provide an advantage to this model when evaluating using MSE?]", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper deals with predictive learning (mainly video prediction), using a RNN type of structure. A new (to my knowledge) variation of LSTM is introduced, called ST-LSTM, with recurrent connections not only in the forward time direction.\nThe predictive network is composed of ST-LSTM blocks. Each of these block resemble a convolutional LSTM unit, with some differences to include an additional input. This extra input comes from the last layer of the previous time step, and enters at the first layer of the current time step, it is then propagated through the layers at the current time step.\nThe resulting ST-LSTM is similar to the combination of two independent LSTM units, interacting through concatenation of the memories (figure 2(left)).\n\nI couldn't find an explicit formulation of the loss (the closest I could find is equation 1). I am assuming that it is a MSE loss, but this should be confirmed, since other forms are possible.\n\nResults are presented on the Moving MNIST (synthetic) and KTH (natural) datasets, showing both PSNR/SSIM and generations. The authors compare their PredRNN with other baselines, and show the best results on these datasets. On Moving MNIST, there is a comparison of different LSTM schemes, and ST-LSTM show the best results, which is a good result.\n\nHowever, the experimental section could be stronger by having more datasets (the two datasets presented have little ambiguity in their future, it could be interesting to see how the model preforms in less contained dataset, such as Sports1m, UCF101 or the Google \"Push\" dataset, for instance). Although this paper present a complex (and, as it seems, good) structure for the predictor, the loss function is almost never mentioned. As (among others) [17] and [19] mention, a simple loss such as the MSE cannot work well when the future distribution is multimodal. The paper would also be stronger if compared with other methods (non LSTM-based), such as the ones presented in section 1.2. In particular, VPNs and GANs seem like strong competitors.\n\nNotes:\n- Figure 1: Although the figure is clear, I do not quite understand what the orange arrows mean (compared to the black arrows).\n- Figure 2(right): As I understand it, each W box should also have an Xt input (not just W1)", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper introduces a novel convolutional LSTM based architecture for next frame video prediction. The difference with previous attempts is that spatial and temporal variations are gathered in a single memory pool.\n \n\nComments\n\nThe paper is mostly well written, proposes a new approach that appears to be fruitful on two relatively simple datasets.\n\nProviding generated videos for such paper submission would be appreciated.\n\nThe results of the paper seem good but the evaluation of the proposed approach to real natural images would be more convincing.\n\nThe KTH dataset is described as a dataset of \"natural images sequences\" but remain very simple to analyse: very low resolution, uniform foreground and background... As the proposed approach is claimed to be memory efficient, it shouldn't be a problem.\n\nCould you provide an idea of the training time?\n\nl.84-86 the authors describe the applications of video prediction as a fact in numerous domains, without citing any references. The reader is therefore curious if these applications are already happening in this case, the authors should provide references, or might happen later, in this case the authors should rephrase (finds -> could find)\n\nl. 105-106 I don't understand the introduction of the deconvolution operator, since it seems unused in the equations and the rest of the paper.\n\n\n\nMinor:\n\nl.47 no comma\nl 57-58 they ... always -> they always\nl 57 the next one -> the next\nl. 135 no comma\nl 141 in in -> in\nl 135-137 I don't understand this sentence\nl 154 : We -> we\nl 242: ignores -> ignore\n[9] {LSTM} ICML 15\n[15] ICLR workshop 16\n[19]: ICLR 16\n[23] {SVM}\n[24] ICLR 15\nplease check other references", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
