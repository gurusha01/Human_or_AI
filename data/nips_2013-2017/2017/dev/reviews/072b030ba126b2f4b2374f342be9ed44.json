{"title": "Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization", "abstract": "Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints.   In this work, we propose and analyze ProxASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.", "id": "072b030ba126b2f4b2374f342be9ed44", "authors": ["Fabian Pedregosa", "R\u00e9mi Leblond", "Simon Lacoste-Julien"], "conference": "NIPS2017", "accepted": true, "reviews": [{"comments": "This paper considers solving the finite sum optimization via the SAGA framework. This work extends from Leblond et al. 2017 by considering the composite optimization, where the additional part is the non-smooth separable regularization. The key improvement lies on how to deal with the non-smooth regularization while obey the principle of sparse update. The proposed trick to split the non-smooth regularization looks interesting to me. The analysis basically follow the framework in Leblond et al. 2017, the proof for the asynchronous variant follows the idea in Mania et al. 2015.\n\nMajor comments:\n- Authors need to explain the \"inconsistent read\" more clear. In particular, what is hat{x}_k and how to decide the index of k. Is it the same as Lian et al. 2016 and Liu and Wright 2015? What is the key difference?\n- It is unclear to me how to obtain the neat representation of the difference between hat{x}_t and x_t.\n- To obtain the linear convergence rate and speedup, this paper makes several implicit assumptions. Authors should explicitly indicate the assumption used in this paper. \n\nMinor comments / typos:\n- The definition of \\Delta in line 217 was incorrect.\n\nMissing reference:\nThe following is related to asynchronous greedy SDCA.\n- Asynchronous parallel greedy coordinate descent, NIPS, 2016.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes an asynchronous variant of SAGA, a stochastic gradient method for finite sum convex optimization (Defazio et al, 2014). To my understanding, the paper addresses two challenges: asynchronicity and proximity. The authors achieve three improvements over the original SAGA. First, they propose a sparse update based on block coordinate wise using extended support for the gradients when the regularization is decomposable. Second, they design an asynchronous variant of SAGA where the delay quantity can be up to sqrt(n)/10. Finally, they can deal with nonsmooth regularizers via proximal operators. In terms of convergence results, the still achieve a linear convergence rate under the strong convexity of the overall sum function f, and individual Lipschitz gradient of fi. Although the step-size is slightly smaller than the one of SAGA, the convergence factor remains comparable. This is probably due to different assumption. \nIn fact, the paper combines several advanced ideas from existing works such as SAGA with variance reduction, sparse updates, Hogwild, asynchorinization such as Arock, etc, to improve over all these techniques. The proof is quite clean and well organized.\nIn my opinion, such improvements are significant and are important in practice due to obvious reasons. The numerical results also strongly support their theoretical contribution. These experiments are carried out on three large-scale data sets, and empirically show a linear speed up of the new algorithm.\nOverall, this paper has significant contribution both in terms of theory and experiments. It merits to be accepted for NIPS.\nMinor comments.\nSome concepts and notation should be defined. For example, support (supp), \\Omega(\\cdot), inconsistence read.\nThe phrase \u201cstep size\u201d should be consistent on all text. For example, on line 46 and line 142 they are not consistent.\nLine 152, delete \u201ca\u201d or \u201cthe\u201d.\nLine 361, the y should be in bold.\nLine 127: remove one \u201ca\u201d.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The main contribution of this paper is to offer a convergence proof for minimizing sum fi(x) + g(x) where fi(x) is smooth, and g is nonsmooth, in an asynchronous setting. The problem is well-motivated; there is indeed no known proof for this, in my knowledge.\n\nA key aspect of this work is the block separability of the variable, which allows for some decomposition gain in the prox step (since the gradient of each fi may only effect certain indices of x.) This is an important practical assumption, as otherwise the prox causes locks, which may be unavoidable. (It would be interesting to see element-wise proxes, such as shrinkage, being decomposed as well.)\n\nTheir are two main theoretical results. Theorem 1 gives a convergance rate for proxSAGA, which is incrementally better than a previous result. Theorem 2 gives the rate for an asynchronous setting, which is more groundbreaking.\n\nOverall I think it is a good paper, with a very nice theoretical contribution that is also practically very useful. I think it would be stronger without the sparsity assumption, but the authors also say that investigating asynchronosy without sparsity for nonsmooth functions is an open problem, so that is acknowledged.\n\npotentially major comment:\n - Theorem 2 relies heavily on a result from leblond 2017, which assumes smooth g's. It is not clear in the proof that the parts borrowed from Leblond 2017 does not require this assumption. \n\nminor comments about paper: \n - line 61, semicolon after \"sparse\"\n - line 127: \"a a\"\n\nminor comments about proof (mostly theorem 1):\n - eq. (14) is backwards (x-z, not z-x)\n - line 425: do you mean x+ = x - gamma vi?\n - Lemma 7 can divide RHS by 2 (triangle inequality in line 433 is loose)\n - line 435: not the third term (4th term)\n - line 444: I am not entirely clear why EDi = I. Di has to do with problem structure, not asynchronasy?\n - eq (33) those two are identical, typo somewhere with missing Ti?\n - line 447: Not sure where that split is coming from, and the relationship between alpha i, alpha i +, and grad fi.\n - line 448: there is some mixup in the second line, a nabla f_i(x*) became nabla f_i(x).\nSome of these seem pretty necessary to fix, but none of these seem fatal, especially since the result is very close to a previously proven result.\n\nI mostly skimmed theorem 2 proof; it looks reasonable, apart from the concern of borrowing results from Leblond (assumptions on g need to be clearer)", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
