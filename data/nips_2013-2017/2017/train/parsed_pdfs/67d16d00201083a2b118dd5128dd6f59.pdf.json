{
  "name" : "67d16d00201083a2b118dd5128dd6f59.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Testing and Learning on Distributions with Symmetric Noise Invariance",
    "authors" : [ "Ho Chung", "Leon Law", "Christopher Yau", "Dino Sejdinovic" ],
    "emails" : [ "hlaw@stats.ox.ac.uk", "c.yau@bham.ac.uk", "dino.sejdinovic@stats.ox.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "There are many sources of variability in data, and not all of them are pertinent to the questions that a data analyst may be interested in. Consider, for example, a nonparametric two-sample testing problem, which has recently been attracting significant research interest, especially in the context of kernel embeddings of distributions [2, 5, 7]. We observe samples {X1j}N1j=1 and {X2j} N2 j=1 from two data generating processes P1 and P2, respectively, and would like to test the null hypothesis that P1 = P2 without making any parametric assumptions on these distributions. With a large sample-size, the minutiae of the two data generating processes are uncovered (e.g. slightly different calibration of the data collecting equipment, different numerical precision), and we ultimately reject the null hypothesis, even if the sources of variation across the two samples may be irrelevant for the analysis.\nSimilarly, we may be interested in learning on distributions [14, 23, 24], where the appropriate level of granularity in the data is distributional. For example, each label yi in supervised learning is associated to a whole bag of observations Bi = {Xij}Nij=1 – assumed to come from a probability distribution Pi, or we may be interested in clustering such bags of observations. Again, nonparametric distances used in such contexts to facilitate a learning algorithm on distributions, such as Maximum Mean Discrepancy (MMD) [5], can be sensitive to irrelevant sources of variation and may lead to suboptimal or even misleading results, in which case building predictors which are invariant to noise is of interest.\nWhile it may be tempting to revert back to a parametric setup and work with simple, easy to interpret models, we argue that a different approach is possible: we stay within a nonparametric framework, exploit the irregular and complicated nature of real life distributions and encode invariances to sources\nof variation assumed to be irrelevant. In this contribution, we focus on invariances to symmetric additive noise on each of the data generating distributions. Namely, assume that the i-th sample {Xij}Nij=1 we observe does not follow the distribution Pi of interest but instead its convolution Pi ?Ei with some unknown noise distributions Ei assumed to be symmetric about 0 (we also require that it has a positive characteristic function). We would like to assess the differences between Pi and Pi′ while allowing Ei and Ei′ to differ in an arbitrary way. We investigate two approaches to this problem: (1) measuring the degree of asymmetry of the paired differences {Xij −Xi′j}, and (2) comparing the phase functions of the corresponding samples. While the first approach is simpler and presents a sensible solution for the two-sample testing problem, we demonstrate that phase functions give a much better gauge on the relative comparisons between bags of observations, as required for learning on distributions.\nThe paper is outlined as follows. In section 2, we provide an overview of the background. In section 3, we provide details of the construction and implementation of phase features. In section 4, we discuss the approach based on asymmetry in paired differences for two sample testing with invariances. Section 5 provides experiments on synthetic and real data, before concluding in section 6."
    }, {
      "heading" : "2 Background and Setup",
      "text" : "We will say that a random vector E on Rd is a symmetric positive definite (SPD) component if its characteristic function is positive, i.e. ϕE(ω) = EX∼E [ exp(iω>E) ] > 0, ∀ω ∈ Rd. This means that E is (1) symmetric about zero, i.e. E and −E have the same distribution and (2) if it has a density, this density must be a positive definite function [20]. Note that many distributions used to model additive noise, including the spherical zero-mean Gaussian distribution, as well as multivariate Laplace, Cauchy or Student’s t (but not uniform), are all SPD components.\nFollowing the terminology similar to that of [3], we will say that a random vector X on Rd is decomposable if its characteristic function can be written as ϕX = ϕX0ϕE , with ϕE > 0. Thus, if X can be written in the form X = X0 + E, where X0 and E are independent and E is an SPD noise component, then X is decomposable. We will say that X is indecomposable if it is not decomposable. In this paper, we will assume that mostly the indecomposable components of distributions are of interest and will construct tools to directly measure differences between these indecomposable components, encoding invariance to other sources of variability. The class of Borel Probability measures on Rd will be denotedM1+(Rd), while the class of indecomposable probability measures will be denoted by I(Rd) ⊆M1+(Rd)."
    }, {
      "heading" : "2.1 Kernel Embeddings, Fourier Features and learning on distributions",
      "text" : "For any positive definite function k : X × X 7→ R, there exists a unique reproducing kernel Hilbert space (RKHS)Hk of real-valued functions on X . Function k(·, x) is an element ofHk and represents evaluation at x, i.e. 〈f, k(·, x)〉H = f(x), ∀f ∈ Hk, ∀x ∈ X . The kernel mean embedding (cf. [15] for a recent review) of a probability measure P is defined by µP = EX∼P [k(·, X)] =∫ X k(·, x)dP (x). The Maximum Mean Discrepancy (MMD) between probability measures P and Q is then given by ‖µP −µQ‖Hk . For shift-invariant kernels on Rd, using Bochner’s characterisation of positive definiteness [26, 6.2], the squared MMD can be written as a weighted L2-distance between characteristic functions [22, Corollary 4]\n‖µP − µQ‖2Hk = ∫ Rd |ϕP (ω)− ϕQ (ω)|2 dΛ (ω) , (1)\nwhere Λ is the non-negative spectral measure (inverse Fourier transform) of kernel k as a function of x− y, while ϕP (ω) and ϕQ(ω) are the characteristic functions of probability measures P and Q. Bochner’s theorem is also used to construct random Fourier features (RFF) [19] for fast approximations to kernel methods in order to approximate a pre-specified shift-invariant kernel by a finite dimensional explicit feature map. If we can draw samples from its spectral measure Λ, we can\napproximate k by1\nk̂(x, y) = 1\nm m∑ j=1 [ cos(ωTj x) cos(ω T j y) + sin(ω T j x) sin(ω T j y) ] = 〈φ(x), φ(y)〉R2m\nwhere ω1, . . . , ωm ∼ Λ and φ(x) := √ 1 m [ cos ( ω>1 x ) , sin ( ω>1 x ) . . . , cos ( ω>mx ) , sin ( ω>mx )] . Thus, the explicit computation of the kernel matrix is not needed and the computational complexity is reduced. This also allows computation with the approximate, finite-dimensional embeddings µ̃P = Φ(P ) = EX∼Pφ(X) ∈ R2m, which can be understood as the evaluations (real and complex part stacked together) of the characteristic function ϕP at frequencies ω1, . . . , ωm. We will refer to the approximate embeddings Φ(P ) as Fourier features of distribution P .\nKernel embeddings can be used for supervised learning on distributions. Assume we have a training set {Bi, yi}ni=1, where input Bi = {xij} Ni j=1 is a bag of samples taking values in X , and yi is a response. Given a kernel k : X × X → R, we first map each Bi to the empirical embedding µP̂i = 1 Ni ∑Ni j=1 k(·, xij) ∈ Hk and then can apply any positive definite kernel onHk as the kernel on bag inputs, e.g. linear kernel K̃(Bi, B′i) = 〈µP̂i , µP̂i′ 〉Hk , in order to perform classification [14] or regression [24]. Approximate kernel embeddings have also been applied in this context [23]."
    }, {
      "heading" : "3 Phase Discrepancy and Phase Features",
      "text" : "While MMD and kernel embeddings are related to characteristic functions, and indeed the same connection forms a basis for fast approximations to kernel methods using random Fourier features [19], the relevant notion in our context is the phase function of a probability measure, recently used for nonparametric deconvolution by [3]. In this section, we overview this formalism. Based on the empirical phase functions, we will then derive and investigate hypothesis testing and learning framework using phase features of distributions.\nIn nonparametric deconvolution [3], the goal is to estimate the density function f0 of a univariate r.v. X0, but in general we only have noisy data samples X1, . . . , Xn\niid∼ X = X0 +E, where E denotes an independent noise term. Even though the distribution of E is unknown, making the assumption that E is an SPD noise component, and that X0 is indecomposable, i.e. X0 itself does not contain any SPD noise components, [3] show that it is possible to obtain consistent estimates of f0.\nThey distinguish between the symmetric noise and the underlying indecomposable component by matching phase functions, defined as\nρX (ω) = ϕX (ω)\n|ϕX (ω)|\nwhere ϕX (ω) denotes the characteristic function of X . Observe that |ρX (ω)| = 1, and thus we are effectively removing the amplitude information from the characteristic function. For a SPD noise component E, the phase function is ρE(ω) ≡ 1. But then since ϕX = ϕX0ϕE , we have that ρX0 = ρX = ϕX/|ϕX |, i.e. the phase function is invariant to additive SPD noise components. This motivates us to construct explicit feature maps of distributions with the same property and similarly to the motivation of [3], we argue that real-world distributions of interest often exhibit certain amount of irregularity and it is exactly this irregularity which is exploited in our methodology.\nIn analogy to the MMD, we first define the phase discrepancy (PhD) as a weighted L2-distances between the phase functions:\nPhD(X,Y ) = ∫ Rd |ρX (ω)− ρY (ω)|2 dΛ (ω) (2)\nfor some non-negative measure Λ (w.l.o.g. a probability measure). Now suppose we write X = X0 + U , Y = Y0 + V , where U and V are SPD noise components. This then implies ρX = ρX0 and ρY = ρY0 Λ-everywhere, so that PhD(X,Y ) = PhD(X0, Y0). It is clear then that the PhD is\n1a complex feature map φ(x) = √\n1 m\n[ exp ( iω>1 x ) , . . . , exp ( iω>mx )] can also be used, but we follow the con-\nvention of real-valued Fourier features, since kernels of interest are typically real-valued.\nnot affected by additive SPD noise components, so it captures desired invariance. However, the PhD for Λ supported everywhere is in fact not a proper metric on the indecomposable probability measures I(Rd), as one can find indecomposable random variables X and Y s.t. ρX = ρY and thus PhD(X,Y ) = 0. An example is given in Appendix A.\nWhile such cases appear contrived, we hence restrict attention to a subset of indecomposable probability measures P(Rd) ⊂ I(Rd), which are uniquely determined by phase functions, i.e. ∀P,Q ∈ P(Rd) : ρP = ρQ ⇒ P = Q. We now have the two following propositions (proofs are given in Appendix B).\nProposition 1.\nPhD(X,Y ) = 2− 2 ∫ ( Eξω(X) ‖Eξω(X)‖ )> ( Eξω(Y ) ‖Eξω(Y )‖ ) dΛ(ω)\nwhere ξω (x) = [ cos ( ω>x ) , sin ( ω>x )]> and ‖ · ‖ denotes the standard L2 norm.\nProposition 2.\nK (PX , PY ) = ∫ ( Eξω(X) ‖Eξω(X)‖ )> ( Eξω(Y ) ‖Eξω(Y )‖ ) dΛ(ω)\nis a positive definite kernel on probability measures.\nNow, we can construct an approximate explicit feature map for kernelK. Taking a sample {ωi}mi=1 ∼ Λ, we define Ψ : PX 7→ R2m given by Ψ(PX) = √ 1 m [ Eξω1 (X) ‖Eξω1 (X)‖ , . . . , Eξωm (X) ‖Eξωm (X)‖ ] . We will refer to Ψ(·) as the phase features. Note that these are very similar to Fourier features, but the cos, sin-pair corresponding to each frequency is normalised to have unit L2 norm. In other words, Ψ(·) can be thought of as evaluations of the phase function at the selected frequencies. By construction, phase features are invariant to additive SPD noise components. For an empirical measure, we simply have the following:\nΨ(P̂X) = √ 1 m [ Êξω1 (X) ‖Êξω1 (X)‖ , . . . , Êξωm (X) ‖Êξωm (X)‖ ] (3)\nwhere we have replaced the expectations by their empirical estimates. Because ∥∥∥Ψ(P̂X)∥∥∥ = 1, we\ncan construct\nP̂hD(P̂X , P̂Y ) = ∥∥∥Ψ(P̂X)−Ψ(P̂Y )∥∥∥2 = 2− 2Ψ(P̂X)>Ψ(P̂Y ), (4)\nwhich is a Monte Carlo estimator of PhD(P̂X , P̂Y ). In summary, Ψ(P̂ ) ∈ R2m is an explicit feature vector of the empirical distribution which encodes invariance to additive SPD noise components present in P 2, as demonstrated in Figure F.1 in the Appendix. It can now be directly applied to (1) two-sample testing up to SPD components, where the distance between the phase features, i.e. an estimate (4) of the PhD, can be used as a test statistic, with details given in section 5.1 and (2) learning on distributions, where we use phase features as the explicit feature map for a bag of samples.\nAlthough we have assumed an indecomposable underlying distribution so far, this assumption is not strict. For distribution regression, if the indecomposable assumption is invalid, given that the underlying distribution is irregular, it may still be useful to encode invariance as long as the benefit of removing the SPD components irrelevant for learning outweighs the signal in the SPD part of the distribution, i.e. there is a trade off between SPD noise and SPD signal. In practice, the phase features we propose can be used to encode such invariance where appropriate or in conjunction with other features which do not encode invariance.\nIn order to construct the approximate mean embeddings for learning, we first compute an explicit feature map by taking averages of the Fourier features, as given by Φ(P̂X) =√\n1 m [ Êξω1(X), . . . , Êξωm(X) ] . For phase features, we need to compute an additional normalisation term over each frequency as in (3). To obtain the set of frequencies {wi}mi=1, we can draw\n2Note that, unlike the population expression Ψ(P ), the empirical estimator Ψ(P̂ ) will in general have a distribution affected by the noise components and is thus only approximately invariant, but we observe that it captures invariance very well as long as the signal-to-noise regime remains relatively high (Section 5.1).\nsamples from a probability measure Λ corresponding to an inverse Fourier transform of a shiftinvariant kernel, e.g. Gaussian Kernel. However, given a supervised signal, we can also optimise a set of frequencies {wi}mi=1 that will give us a useful representation and good discriminative performance. In other words, we no longer focus on a specific shift-invariant kernel k, but are learning discriminative Fourier/phase features. To do this, we can construct a neural network (NN) with special activation functions, pooling layers as shown in Algorithm D.1 and Figure D.1 in the Appendix."
    }, {
      "heading" : "4 Asymmetry in Paired Differences",
      "text" : "We now consider a separate approach to nonparametric two-sample test, where we wish to test the null hypothesis that H0 : P d =Q vs. the general alternative, but we only have iid samples arising from X ∼ P ? E1 and Y ∼ Q ? E2. i.e. X = X0 + U Y = Y0 + V\nwhere X0 ∼ P , Y0 ∼ Q lie in the space of P(Rd) of indecomposable distributions uniquely determined by phase functions and U and V are SPD noise components. With this setting (proof in Appendix B):\nProposition 3. Under the null hypothesis H0, X − Y is SPD ⇐⇒ X0 d =Y0.\nThis motivates us to simply perform a two-sample test on X−Y and Y −X since its rejection would imply rejection of X0 d =Y0, as it tests for symmetry. However, note that this is a test for symmetry only and that for consistency against all alternatives, positivity of characteristic function would need to be checked separately. Now, given two i.i.d. samples {Xi}ni=1 and {Yi}ni=1 with n even, we split the two samples into two halves and compute Wi = Xi − Yi on one half and Zi = Yi −Xi on the other half, and perform a nonparametric two sample test on W and Z (which are, by construction, independent of each other). The advantage of this regime is that we can use any two-sample test – in particular in this paper, we will focus on the linear time mean embedding (ME) test [7], which was found to have performance similar to or better than the original MMD two-sample test [5], and explicitly formulates a criterion which maximises the test power. We will refer to the resulting test on paired differences as the Symmetric Mean Embedding (SME).\nAlthough we have assumed here that X0, Y0 lie in the space P(Rd) of indecomposable distributions, in practice, the SME test would not reject if the underlying distributions of interest differ only in the symmetric components (or in the SPD components for the PhD test). We argue this to be unlikely due to real life distributions being complex in nature with interesting differences often having a degree of asymmetry. In practice, we recommend the use of the ME and SME or PhD test together to provide an exploratory tool to understand the underlying differences, as demonstrated in the Higgs Data experiment in section 5.1.\nIt is tempting to also consider learning on distributions with invariances using this formalism. However note that the MMD on paired differences is not invariant to the additive SPD noise components under the alternative, i.e. in general MMD(X − Y, Y −X) 6= MMD(X0 − Y0, Y0 −X0). This means that the paired differences approach to learning is sensitive to the actual type and scale of the additive SPD noise components, hence not suitable for learning. The mathematical details and empirical experiments to show this are presented in Appendix C and F.1."
    }, {
      "heading" : "5 Experimental Results",
      "text" : ""
    }, {
      "heading" : "5.1 Two-Sample Tests with Invariances",
      "text" : "In this section, we demonstrate the performance of the SME test and the PhD test on both artificial and real-world data for testing the hypothesis H0 : X0 d =Y0 based on samples {Xi}Ni=1 from X0 + U and {Yi}Ni=1 from Y0 + V , where U and V are arbitrary SPD noise components (we assume the same number of samples for simplicity). SME test follows the setup in [7] but applied to {Xi−Yi}N/2i=1 and {Yi −Xi}Ni=N/2+1. For the PhD test, we use as the test statistic the estimate P̂hD(P̂X , P̂Y ) of (2). It is unclear what the exact form of the null distribution is, so we use a permutation test, by recomputing this statistic on the samples which are first merged and then randomly split in the original proportions.\nWhile we are combining samples with different distributions, the permutation test is still justified since, under the null hypothesis X0 d =Y0, the resulting characteristic function ϕnull of the mixture can be written as ϕnull = 1\n2 ϕX0ϕU +\n1 2 ϕX0ϕV = ϕX0( 1 2 ϕU + 1 2 ϕV )\nand since the mixture of the SPD noise terms is also SPD, we have that ρnull = ρX0 = ρY0 . For our experiments, we denote by N the sample size, d the dimension of the samples, and we take α = 0.05 to be the significance level. In the SME test, we take the number of test locations J to be 10, and use 20% of the samples to optimise the test locations. All experimental results are averaged over 1000 runs, where each run repeats the simulation or randomly samples without replacement from the dataset."
    }, {
      "heading" : "5.1.1 Synthetic example: Noisy χ2",
      "text" : "We start by demonstrating our tests with invariances on a simulated dataset where X0 and Y0 are random vectors with d = 5, each dimension is the same in distribution and follows χ2(4)/4 and χ2(8)/8 respectively, i.e. chi-squared random variables, with different degrees of freedom, rescaled to have the same mean 1 (but have different variances, 1/2 and 1/4 respectively). An illustration of the true and empirical phase and characteristic function with noise for these two distributions can be found in Appendix F.2. We construct samples {Xn1,i}Ni=1 and {Yn2,i}Ni=1 such that Xn1 ∼ X0 +U , where U ∼ N (0, σ21I) and similarly Yn2 ∼ Y0 + V , where V ∼ N (0, σ22I), ni denotes the noise-to-signal ratio given by the ratio of variances in each dimension, i.e. n1 = 2σ21 and n2 = 4σ 2 2 .\nWe first verify that Type I error is indeed controlled at our design level of α = 0.05 up to various additive SPD noise components. This is shown in Figure 1 (left), where X0 d =Y0, both constructed using χ2(4)/4, with the noiseless case found in Figure F.6 in the Appendix. It is noted here that the ME test rejects the null hypothesis for even a small difference in noise levels, hence it is unable to let us target the underlying distributions we are concerned with. This is unlike the SME test which controls the Type I error even for large differences in noise levels. The PhD test, on the other hand, while correctly controlling Type I at small noise levels, was found to have inflated Type I error rates for large noise, with more results and explanation provided in Figure F.6 in the Appendix. Namely, the test relies on the invariance to SPD of the population expression of PhD, but the estimator of the null distribution of the corresponding test statistic will in general be affected by the differing noise levels.\nNext, we investigate the power, shown in Figure 1 (right). For a fair comparison, we have included the PhD test power only for small noise levels, in which the Type I error is controlled at the design level. In these cases, the PhD test has better power than the SME test. This is not surprising, as for the SME we have to halve the sample size in order to construct a valid test. However, recall that the PhD test has an inflated Type I error for large noises, which means that its results should be considered with caution in practice. ME test rejects at all levels at all sample sizes as it picks up all possible\ndifferences. SME and PhD are by construction more conservative tests whose rejection provides a much stronger statement: two samples differ even when all arbitrary additive SPD components have been stripped off."
    }, {
      "heading" : "5.1.2 Higgs Dataset",
      "text" : "The UCI Higgs dataset [1, 11] is a dataset with 11 million observations, where the problem is to distinguish between the signal process where Higgs bosons are found, versus the background process that do not produce Higgs bosons. In particular, we will consider a two-sample test with the ME and SME test on the high level features derived by physicists, as well as a two-sample test on four extremely low level features (azimuthal angular momentum φ measured by four particle jets in the detector). The high level features here (in R7) have been shown to have good discriminative properties in [1]. Thus, we expect them to have different distributions across two processes. Denoting by X the high level features of the process without Higgs Boson, and Y as the corresponding distribution for the processes where Higgs bosons are produced, we test the null hypothesis that the indecomposable parts of X and Y agree. The results can be found in Table F.1 in the Appendix, which shows that the high level features differ even up to additive SPD components, with a high power for the SME and ME test even at small sample sizes (rejection rate of 0.94 at N = 500). Now we perform the same experiment, but with the low level features ∈ R4, commented in [1] to carry very little discriminating information, using the setup from [2].\nThe results for the ME and SME test can be found in Figure 2. Here we observe that while ME test clearly rejects and finds the difference between the two distributions, there is no evidence that the indecomposable parts of the joint distributions of the angular momentum actually differ. In fact, the test rejection rate remains around the chosen design level of α = 0.05 for all sample sizes. This highlights the significance in using the SME test, suggesting that the nature of the difference between the two processes can potentially be explained by some additive symmetric noise components which may be irrelevant for discrimination, providing an insight into the dataset. Furthermore, this also highlights the argument that given two samples from complex data collection and generation processes, a nonparametric two sample test like ME will likely reject given sufficient sample sizes, even if the discovered difference may not be of interest. With the SME test however, we can ask a much more subtle question about the differences between the assumed true underlying processes. Figures showing that the Type I error is controlled at the design level of α = 0.05 for both low and high level features can be found in Figure F.7 in the Appendix."
    }, {
      "heading" : "5.2 Learning with Phase Features",
      "text" : ""
    }, {
      "heading" : "5.2.1 Aerosol Dataset",
      "text" : "To demonstrate the phase features invariance to SPD noise component, we use the Aerosol MISR1 dataset also studied by [24] and [25] and consider a situation with covariate shift [18] on distribution inputs: the testing data is impaired by additive SPD components different to that in the training data.\nHere, we have an aerosol optical depth (AOD) multi-instance learning problem with 800 bags, where each bag contains 100 randomly selected multispectral (potentially cloudy) pixels within 20km radius around an AOD sensor. The label yi for each bag is given by the AOD sensor measurements and each sample xi is 16-dimensional. This can be understood as a distribution regression problem where each bag is treated as a set of samples from some distribution.\nWe use 640 bags for training and 160 bags for testing. Here in the bags for testing only, we add varying levels of Gaussian noise ∼ N (0, Z) to each bag, where Z is a diagonal matrix with diagonal components zi ∼ U [0, σvi] with vi being the empirical variance in dimension i across all samples, accounting for different scales across dimensions. For comparisons, we consider linear ridge regression on embeddings with respect to a Gaussian kernel, approximated with RFF (GLRR) as described in section 2.1 (i.e. a linear kernel is applied on approximate embeddings), linear ridge regression on phase features (PLRR) (i.e. normalisation step is applied to obtain (3)), and also the phase and Fourier neural networks (NN), described in Appendix D, tuning all hyperparameters with 3-fold cross validation. With the same model, we now measure Root Mean Square Error (RMSE) 100 times with various noise-corrupted test sets and results are shown in figure 3. It is also noted that a second level non-linear kernel K̃ does not improve performance significantly on this problem [24].\nWe see that GLRR and PLRR are competitive (see Appendix Table F.2) in the noiseless case, and these clearly outperform both the Fourier NN and Phase NN (likely due to the small size of the dataset). For increasing noise, the performance of GLRR degrades significantly, and while the performance of PLRR degrades also, the model is much more robust under additional SPD noise. In comparison, the Phase NN implementation is almost insensitive to covariate shift in the test sets, unlike the performance of PLRR, highlighting the importance of learning discriminative frequencies w in a very low signal-to-noise setting.\nIt is noted that the Fourier NN performs similarly to that of the Phase NN on this example. Interestingly, discriminative frequencies learnt on the training data correspond to Fourier features that are nearly normalised (i.e. they are close to unit norm - see Figure F.8 in the Appendix). This means that the Fourier NN has learned to be approximately invariant based on training data, indicating that the original Aerosol data potentially has irrelevant SPD noise components. This is reinforced by the nature of the dataset (each bag contains 100 randomly selected potentially cloudy pixels, known to be noisy [25]) and no loss of performance from going from GLRR to PLRR. The results highlights that phase features are stable under additive SPD noise."
    }, {
      "heading" : "5.2.2 Dark Matter Dataset",
      "text" : "We now study the use of phase features on the dark matter dataset, composing of a catalog of galaxy clusters. In this setting, we would like to predict the total mass of galaxy clusters, using the dispersion of velocities in the direction along our line of sight. In particular, we will use the ‘ML1’ dataset, as obtained from the authors of [16, 17], who constructed a catalog of massive halos from the MultiDark mdpl simulation [9]. The dataset contains 5028 bags, with each sample consisting of its sub-object velocity and its mass label in R. By viewing each galaxy cluster at multiple lines of sights, we obtain 15 000 bags, using the same experimental setup as in [10]. For experiments, we use approximately 9000 bags for training, and 3000 bags each for validation and testing, keeping those of multiple lines of sight in the same set. As before, we use GLRR and PLRR and we also include\nin comparisons methods with a second level Gaussian kernel (with RFF) applied to phase features (PGRR) and to approximate embeddings (GGRR). For a baseline, we also include a first level linear kernel (equivalent to representing each bag with its mean), before applying a second level gaussian kernel (LGRR). We use the same set of randomly sampled frequencies across the methods, tuning for the scale of the frequencies and for regularisation parameters.\nTable 1 shows the results of the methods across 10 different data splits, with 50 sets of randomised frequencies for each data split. We see that PLRR is significantly better than GLRR. This suggests that under this model structure, by removing SPD components from each bag, we can target the underlying signal and obtain superior performance, highlighting the applicability of phase features. Considering a second level gaussian kernel, we see that the GGRR has a slight advantage over PGRR, with PGRR performing similar to PLRR. This suggests that the SPD components of the distribution of sub-object velocity may be useful for predicting the mass of a galaxy cluster if an additional nonlinearity is applied to embeddings – whereas the benefits of removing them outweigh the signal present in them without this additional nonlinearity. To show that indeed the phase features are robust to SPD components, we perform the same covariate shift experiment as in the aerosol dataset, with results given in Figure 4. Note that LGRR is robust to noise, as each bag is represented by its mean."
    }, {
      "heading" : "6 Conclusion",
      "text" : "No dataset is immune from measurement noise and often this noise differs across different data generation and collection processes. When measuring distances between distributions, can we disentangle the differences in noise from the differences in the signal? We considered two different ways to encode invariances to additive symmetric noise in those distances, each with different strengths: a nonparametric measure of asymmetry in paired sample differences and a weighted distance between the empirical phase functions. The former was used to construct a hypothesis test on whether the difference between the two generating processes can be explained away by the difference in postulated noise, whereas the latter allowed us to introduce a flexible framework for invariant feature construction and learning algorithms on distribution inputs which are robust to measurement noise and target underlying signal distributions."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Dougal Sutherland for suggesting the use of of the dark matter dataset, Michelle Ntampaka for providing the catalog, as well as Ricardo Silva, Hyunjik Kim and Kaspar Martens for useful discussions. This work was supported by the EPSRC and MRC through the OxWaSP CDT programme (EP/L016710/1). C.Y. and H.C.L.L. also acknowledge the support of the MRC Grant No. MR/L001411/1.\nThe CosmoSim database used in this paper is a service by the Leibniz-Institute for Astrophysics Potsdam (AIP). The MultiDark database was developed in cooperation with the Spanish MultiDark Consolider Project CSD2009-00064. The authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) and the Partnership for Advanced Supercomputing in Europe (PRACE, www.prace-ri.eu) for funding the MultiDark simulation project by providing computing time on the GCS Supercomputer SuperMUC at Leibniz Supercomputing Centre (LRZ, www.lrz.de)."
    } ],
    "references" : [ {
      "title" : "Searching for exotic particles in high-energy physics with deep learning",
      "author" : [ "Pierre Baldi", "Peter Sadowski", "Daniel Whiteson" ],
      "venue" : "Nature communications,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Fast twosample testing with analytic representations of probability measures",
      "author" : [ "Kacper P Chwialkowski", "Aaditya Ramdas", "Dino Sejdinovic", "Arthur Gretton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Methodology for non-parametric deconvolution when the error distribution is unknown",
      "author" : [ "Aurore Delaigle", "Peter Hall" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Constructing summary statistics for approximate bayesian computation: semi-automatic approximate bayesian computation",
      "author" : [ "Paul Fearnhead", "Dennis Prangle" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Schölkopf", "Alexander Smola" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Interpretable distribution features with maximum testing power",
      "author" : [ "Wittawat Jitkrittum", "Zoltán Szabó", "Kacper P Chwialkowski", "Arthur Gretton" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "MultiDark simulations: the story of dark matter halo concentrations and density profiles",
      "author" : [ "Anatoly Klypin", "Gustavo Yepes", "Stefan Gottlober", "Francisco Prada", "Steffen Hess" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Bayesian approaches to distribution regression",
      "author" : [ "Ho Chung Leon Law", "Dougal J. Sutherland", "Dino Sejdinovic", "Seth Flaxman" ],
      "venue" : "arXiv preprint arXiv:1705.04293,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2017
    }, {
      "title" : "Decomposition of random variables and vectors",
      "author" : [ "Yu V Linnik", "IV Ostrovskii" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1977
    }, {
      "title" : "DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution Regression",
      "author" : [ "J. Mitrovic", "D. Sejdinovic", "Y.W. Teh" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Learning from distributions via support measure machines",
      "author" : [ "Krikamol Muandet", "Kenji Fukumizu", "Francesco Dinuzzo", "Bernhard Schölkopf" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Kernel mean embedding of distributions: A review and beyonds",
      "author" : [ "Krikamol Muandet", "Kenji Fukumizu", "Bharath Sriperumbudur", "Bernhard Schölkopf" ],
      "venue" : "arXiv preprint arXiv:1605.09522,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "A machine learning approach for dynamical mass measurements of galaxy clusters",
      "author" : [ "Michelle Ntampaka", "Hy Trac", "Dougal J. Sutherland", "Nicholas Battaglia", "Barnabás Póczos", "Jeff Schneider" ],
      "venue" : "The Astrophysical Journal,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Dynamical mass measurements of contaminated galaxy clusters using machine learning",
      "author" : [ "Michelle Ntampaka", "Hy Trac", "Dougal J. Sutherland", "S. Fromenteau", "B. Poczos", "Jeff Schneider" ],
      "venue" : "The Astrophysical Journal,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Dataset Shift in Machine Learning",
      "author" : [ "Joaquin Quinonero-Candela", "Masashi Sugiyama", "Anton Schwaighofer", "Neil D. Lawrence" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "Ali Rahimi", "Benjamin Recht" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "Positive definite probability densities and probability distributions",
      "author" : [ "H-J Rossberg" ],
      "venue" : "Journal of Mathematical Sciences,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1995
    }, {
      "title" : "Kernel embeddings of conditional distributions: A unified kernel framework for nonparametric inference in graphical models",
      "author" : [ "Le Song", "Kenji Fukumizu", "Arthur Gretton" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Hilbert space embeddings and metrics on probability measures",
      "author" : [ "Bharath K. Sriperumbudur", "Arthur Gretton", "Kenji Fukumizu", "Bernhard Schölkopf", "Gert R.G. Lanckriet" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Linear-time learning on distributions with approximate kernel embeddings",
      "author" : [ "Dougal J. Sutherland", "Junier B. Oliva", "Barnabás Póczos", "Jeff G. Schneider" ],
      "venue" : "In Proc. AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Two-stage sampled learning theory on distributions",
      "author" : [ "Zoltán Szabó", "Arthur Gretton", "Barnabás Póczos", "Bharath K. Sriperumbudur" ],
      "venue" : "In Proc. International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Mixture model for multiple instance regression and applications in remote sensing",
      "author" : [ "Z. Wang", "L. Lan", "S. Vucetic" ],
      "venue" : "IEEE Transactions on Geoscience and Remote Sensing,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Scattered Data Approximation",
      "author" : [ "H. Wendland" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Consider, for example, a nonparametric two-sample testing problem, which has recently been attracting significant research interest, especially in the context of kernel embeddings of distributions [2, 5, 7].",
      "startOffset" : 197,
      "endOffset" : 206
    }, {
      "referenceID" : 4,
      "context" : "Consider, for example, a nonparametric two-sample testing problem, which has recently been attracting significant research interest, especially in the context of kernel embeddings of distributions [2, 5, 7].",
      "startOffset" : 197,
      "endOffset" : 206
    }, {
      "referenceID" : 6,
      "context" : "Consider, for example, a nonparametric two-sample testing problem, which has recently been attracting significant research interest, especially in the context of kernel embeddings of distributions [2, 5, 7].",
      "startOffset" : 197,
      "endOffset" : 206
    }, {
      "referenceID" : 12,
      "context" : "Similarly, we may be interested in learning on distributions [14, 23, 24], where the appropriate level of granularity in the data is distributional.",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "Similarly, we may be interested in learning on distributions [14, 23, 24], where the appropriate level of granularity in the data is distributional.",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : "Similarly, we may be interested in learning on distributions [14, 23, 24], where the appropriate level of granularity in the data is distributional.",
      "startOffset" : 61,
      "endOffset" : 73
    }, {
      "referenceID" : 4,
      "context" : "Again, nonparametric distances used in such contexts to facilitate a learning algorithm on distributions, such as Maximum Mean Discrepancy (MMD) [5], can be sensitive to irrelevant sources of variation and may lead to suboptimal or even misleading results, in which case building predictors which are invariant to noise is of interest.",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 18,
      "context" : "E and −E have the same distribution and (2) if it has a density, this density must be a positive definite function [20].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "Following the terminology similar to that of [3], we will say that a random vector X on R is decomposable if its characteristic function can be written as φX = φX0φE , with φE > 0.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "[15] for a recent review) of a probability measure P is defined by μP = EX∼P [k(·, X)] = ∫ X k(·, x)dP (x).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "Bochner’s theorem is also used to construct random Fourier features (RFF) [19] for fast approximations to kernel methods in order to approximate a pre-specified shift-invariant kernel by a finite dimensional explicit feature map.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "linear kernel K̃(Bi, B′ i) = 〈μP̂i , P̂i′ 〉Hk , in order to perform classification [14] or regression [24].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 22,
      "context" : "linear kernel K̃(Bi, B′ i) = 〈μP̂i , P̂i′ 〉Hk , in order to perform classification [14] or regression [24].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 21,
      "context" : "Approximate kernel embeddings have also been applied in this context [23].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "While MMD and kernel embeddings are related to characteristic functions, and indeed the same connection forms a basis for fast approximations to kernel methods using random Fourier features [19], the relevant notion in our context is the phase function of a probability measure, recently used for nonparametric deconvolution by [3].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 2,
      "context" : "While MMD and kernel embeddings are related to characteristic functions, and indeed the same connection forms a basis for fast approximations to kernel methods using random Fourier features [19], the relevant notion in our context is the phase function of a probability measure, recently used for nonparametric deconvolution by [3].",
      "startOffset" : 328,
      "endOffset" : 331
    }, {
      "referenceID" : 2,
      "context" : "In nonparametric deconvolution [3], the goal is to estimate the density function f0 of a univariate r.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "X0 itself does not contain any SPD noise components, [3] show that it is possible to obtain consistent estimates of f0.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "This motivates us to construct explicit feature maps of distributions with the same property and similarly to the motivation of [3], we argue that real-world distributions of interest often exhibit certain amount of irregularity and it is exactly this irregularity which is exploited in our methodology.",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "The advantage of this regime is that we can use any two-sample test – in particular in this paper, we will focus on the linear time mean embedding (ME) test [7], which was found to have performance similar to or better than the original MMD two-sample test [5], and explicitly formulates a criterion which maximises the test power.",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 4,
      "context" : "The advantage of this regime is that we can use any two-sample test – in particular in this paper, we will focus on the linear time mean embedding (ME) test [7], which was found to have performance similar to or better than the original MMD two-sample test [5], and explicitly formulates a criterion which maximises the test power.",
      "startOffset" : 257,
      "endOffset" : 260
    }, {
      "referenceID" : 6,
      "context" : "SME test follows the setup in [7] but applied to {Xi−Yi} i=1 and {Yi −Xi}i=N/2+1.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "The UCI Higgs dataset [1, 11] is a dataset with 11 million observations, where the problem is to distinguish between the signal process where Higgs bosons are found, versus the background process that do not produce Higgs bosons.",
      "startOffset" : 22,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "The high level features here (in R(7)) have been shown to have good discriminative properties in [1].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "Now we perform the same experiment, but with the low level features ∈ R(4), commented in [1] to carry very little discriminating information, using the setup from [2].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "Now we perform the same experiment, but with the low level features ∈ R(4), commented in [1] to carry very little discriminating information, using the setup from [2].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 22,
      "context" : "To demonstrate the phase features invariance to SPD noise component, we use the Aerosol MISR1 dataset also studied by [24] and [25] and consider a situation with covariate shift [18] on distribution inputs: the testing data is impaired by additive SPD components different to that in the training data.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 23,
      "context" : "To demonstrate the phase features invariance to SPD noise component, we use the Aerosol MISR1 dataset also studied by [24] and [25] and consider a situation with covariate shift [18] on distribution inputs: the testing data is impaired by additive SPD components different to that in the training data.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : "To demonstrate the phase features invariance to SPD noise component, we use the Aerosol MISR1 dataset also studied by [24] and [25] and consider a situation with covariate shift [18] on distribution inputs: the testing data is impaired by additive SPD components different to that in the training data.",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 22,
      "context" : "It is also noted that a second level non-linear kernel K̃ does not improve performance significantly on this problem [24].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "This is reinforced by the nature of the dataset (each bag contains 100 randomly selected potentially cloudy pixels, known to be noisy [25]) and no loss of performance from going from GLRR to PLRR.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 14,
      "context" : "In particular, we will use the ‘ML1’ dataset, as obtained from the authors of [16, 17], who constructed a catalog of massive halos from the MultiDark mdpl simulation [9].",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : "In particular, we will use the ‘ML1’ dataset, as obtained from the authors of [16, 17], who constructed a catalog of massive halos from the MultiDark mdpl simulation [9].",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "In particular, we will use the ‘ML1’ dataset, as obtained from the authors of [16, 17], who constructed a catalog of massive halos from the MultiDark mdpl simulation [9].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "By viewing each galaxy cluster at multiple lines of sights, we obtain 15 000 bags, using the same experimental setup as in [10].",
      "startOffset" : 123,
      "endOffset" : 127
    } ],
    "year" : 2017,
    "abstractText" : "Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD), the resulting distance between distributions, are useful tools for fully nonparametric two-sample testing and learning on distributions. However, it is rare that all possible differences between samples are of interest – discovered differences can be due to different types of measurement noise, data collection artefacts or other irrelevant sources of variability. We propose distances between distributions which encode invariance to additive symmetric noise, aimed at testing whether the assumed true underlying processes differ. Moreover, we construct invariant features of distributions, leading to learning algorithms robust to the impairment of the input distributions with symmetric additive noise.",
    "creator" : null
  }
}