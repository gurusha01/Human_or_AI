{
  "name" : "82b8a3434904411a9fdc43ca87cee70c.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite Sum Structure",
    "authors" : [ "Alberto Bietti", "Julien Mairal" ],
    "emails" : [ "alberto.bietti@inria.fr", "julien.mairal@inria.fr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Many supervised machine learning problems can be cast as the minimization of an expected loss over a data distribution with respect to a vector x in Rp of model parameters. When an infinite amount of data is available, stochastic optimization methods such as SGD or stochastic mirror descent algorithms, or their variants, are typically used (see [5, 11, 24, 34]). Nevertheless, when the dataset is finite, incremental methods based on variance reduction techniques (e.g., [2, 8, 15, 17, 18, 27, 29]) have proven to be significantly faster at solving the finite-sum problem\nmin x∈Rp\n{ F (x) := f(x) + h(x) = 1\nn\nn ∑\ni=1\nfi(x) + h(x) } , (1)\nwhere the functions fi are smooth and convex, and h is a simple convex penalty that need not be differentiable such as the ℓ1 norm. A classical setting is fi(x) = ℓ(yi, x ⊤ξi) + (µ/2)‖x‖ 2, where (ξi, yi) is an example-label pair, ℓ is a convex loss function, and µ is a regularization parameter.\nIn this paper, we are interested in a variant of (1) where random perturbations of data are introduced, which is a common scenario in machine learning. Then, the functions fi involve an expectation over a random perturbation ρ, leading to the problem\nmin x∈Rp\n{ F (x) := 1\nn\nn ∑\ni=1\nfi(x) + h(x) } . with fi(x) = Eρ[f̃i(x, ρ)]. (2)\nUnfortunately, variance reduction methods are not compatible with the setting (2), since evaluating a single gradient ∇fi(x) requires computing a full expectation. Yet, dealing with random perturbations is of utmost interest; for instance, this is a key to achieve stable feature selection [23], improving the generalization error both in theory [33] and in practice [19, 32], obtaining stable and robust predictors [36], or using complex a priori knowledge about data to generate virtually\n∗Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nlarger datasets [19, 26, 30]. Injecting noise in data is also useful to hide gradient information for privacy-aware learning [10].\nDespite its importance, the optimization problem (2) has been littled studied and to the best of our knowledge, no dedicated optimization method that is able to exploit the problem structure has been developed so far. A natural way to optimize this objective when h=0 is indeed SGD, but ignoring the finite-sum structure leads to gradient estimates with high variance and slow convergence. The goal of this paper is to introduce an algorithm for strongly convex objectives, called stochastic MISO, which exploits the underlying finite sum using variance reduction. Our method achieves a faster convergence rate than SGD, by removing the dependence on the gradient variance due to sampling the data points i in {1, . . . , n}; the dependence remains only for the variance due to random perturbations ρ.\nTo the best of our knowledge, our method is the first algorithm that interpolates naturally between incremental methods for finite sums (when there are no perturbations) and the stochastic approximation setting (when n=1), while being able to efficiently tackle the hybrid case.\nRelated work. Many optimization methods dedicated to the finite-sum problem (e.g., [15, 29]) have been motivated by the fact that their updates can be interpreted as SGD steps with unbiased estimates of the full gradient, but with a variance that decreases as the algorithm approaches the optimum [15]; on the other hand, vanilla SGD requires decreasing step-sizes to achieve this reduction of variance, thereby slowing down convergence. Our work aims at extending these techniques to the case where each function in the finite sum can only be accessed via a first-order stochastic oracle.\nMost related to our work, recent methods that use data clustering to accelerate variance reduction techniques [3, 14] can be seen as tackling a special case of (2), where the expectations in fi are replaced by empirical averages over points in a cluster. While N-SAGA [14] was originally not designed for the stochastic context we consider, we remark that their method can be applied to (2). Their algorithm is however asymptotically biased and does not converge to the optimum. On the other hand, ClusterSVRG [3] is not biased, but does not support infinite datasets. The method proposed in [1] uses variance reduction in a setting where gradients are computed approximately, but the algorithm computes a full gradient at every pass, which is not available in our stochastic setting.\nPaper organization. In Section 2, we present our algorithm for smooth objectives, and we analyze its convergence in Section 3. For space limitation reasons, we present an extension to composite objectives and non-uniform sampling in Appendix A. Section 4 is devoted to empirical results."
    }, {
      "heading" : "2 The Stochastic MISO Algorithm for Smooth Objectives",
      "text" : "In this section, we introduce the stochastic MISO approach for smooth objectives (h = 0), which relies on the following assumptions:\n• (A1) global strong convexity: f is µ-strongly convex; • (A2) smoothness: f̃i(·, ρ) is L-smooth for all i and ρ (i.e., with L-Lipschitz gradients).\nNote that these assumptions are relaxed in Appendix A by supporting composite objectives and by exploiting different smoothness parameters Li on each example, a setting where non-uniform sampling of the training points is typically helpful to accelerate convergence (e.g., [35]).\nComplexity results. We now introduce the following quantity, which is essential in our analysis:\nσ2p := 1\nn\nn ∑\ni=1\nσ2i , with σ 2 i := Eρ\n[\n‖∇f̃i(x ∗, ρ)−∇fi(x ∗)‖2 ] ,\nwhere x∗ is the (unique) minimizer of f . The quantity σ2p represents the part of the variance of the gradients at the optimum that is due to the perturbations ρ. In contrast, another quantity of interest is the total variance σ2tot, which also includes the randomness in the choice of the index i, defined as\nσ2tot = Ei,ρ[‖∇f̃i(x ∗, ρ)‖2] = σ2p + Ei[‖∇fi(x ∗)‖2] (note that ∇f(x∗) = 0).\nThe relation between σ2tot and σ 2 p is obtained by simple algebraic manipulations.\nThe goal of our paper is to exploit the potential imbalance σ2p ≪ σ 2 tot, occurring when perturbations on input data are small compared to the sampling noise. The assumption is reasonable: given a data point, selecting a different one should lead to larger variation than a simple perturbation. From a theoretical point of view, the approach we propose achieves the iteration complexity presented in Table 1, see also Appendix D and [4, 5, 24] for the complexity analysis of SGD. The gain over SGD is of order σ2tot/σ 2 p, which is also observed in our experiments in Section 4. We also compare against the method N-SAGA; its convergence rate is similar to ours but suffers from a non-zero asymptotic error.\nMotivation from application cases. One clear framework of application is the data clustering scenario already investigated in [3, 14]. Nevertheless, we will focus on less-studied data augmentation settings that lead instead to true stochastic formulations such as (2). First, we consider learning a linear model when adding simple direct manipulations of feature vectors, via rescaling (multiplying each entry vector by a random scalar), Dropout, or additive Gaussian noise, in order to improve the generalization error [33] or to get more stable estimators [23]. In Table 2, we present the potential gain over SGD in these scenarios. To do that, we study the variance of perturbations applied to a feature vector ξ. Indeed, the gradient of the loss is proportional to ξ, which allows us to obtain good estimates of the ratio σ2tot/σ 2 p, as we observed in our empirical study of Dropout presented in Section 4. Whereas some perturbations are friendly for our method such as feature rescaling (a rescaling window of [0.9, 1.1] yields for instance a huge gain factor of 300), a large Dropout rate would lead to less impressive acceleration (e.g., a Dropout with δ = 0.5 simply yields a factor 2).\nSecond, we also consider more interesting domain-driven data perturbations such as classical image transformations considered in computer vision [26, 36] including image cropping, rescaling, brightness, contrast, hue, and saturation changes. These transformations may be used to train a linear\nAlgorithm 1 S-MISO for smooth objectives\nInput: step-size sequence (αt)t≥1; initialize x0 = 1\nn\n∑\ni z 0 i for some (z 0 i )i=1,...,n;\nfor t = 1, . . . do Sample an index it uniformly at random, a perturbation ρt, and update\nzti =\n{\n(1− αt)z t−1 i + αt(xt−1 − 1 µ ∇f̃it(xt−1, ρt)), if i = it zt−1i , otherwise. (3)\nxt = 1\nn\nn ∑\ni=1\nzti = xt−1 + 1\nn (ztit − z t−1 it ). (4)\nend for\nclassifier on top of an unsupervised multilayer image model such as unsupervised CKNs [22] or the scattering transform [6]. It may also be used for retraining the last layer of a pre-trained deep neural network: given a new task unseen during the full network training and given limited amount of training data, data augmentation may be indeed crucial to obtain good prediction and S-MISO can help accelerate learning in this setting. These scenarios are also studied in Table 2, where the experiment with ResNet-50 involving random cropping and rescaling produces 224× 224 images from 256× 256 ones. For these scenarios with realistic perturbations, the potential gain varies from 10 to 20.\nDescription of stochastic MISO. We are now in shape to present our method, described in Algorithm 1. Without perturbations and with a constant step-size, the algorithm resembles the MISO/Finito algorithms [9, 18, 21], which may be seen as primal variants of SDCA [28, 29]. Specifically, MISO is not able to deal with our stochastic objective (2), but it may address the deterministic finite-sum problem (1). It is part of a larger body of optimization methods that iteratively build a model of the objective function, typically a lower or upper bound on the objective that is easier to optimize; for instance, this strategy is commonly adopted in bundle methods [13, 25].\nMore precisely, MISO assumes that each fi is strongly convex and builds a model using lower bounds Dt(x) = 1\nn\n∑n\ni=1 d t i(x), where each d t i is a quadratic lower bound on fi of the form\ndti(x) = c t i,1 +\nµ 2 ‖x− zti‖ 2 = cti,2 − µ〈x, z t i〉+ µ 2 ‖x‖2. (5)\nThese lower bounds are updated during the algorithm using strong convexity lower bounds at xt−1 of the form lti(x) = fi(xt−1) + 〈∇fi(xt−1), x− xt−1〉+ µ 2 ‖x− xt−1‖ 2 ≤ fi(x):\ndti(x) =\n{\n(1− αt)d t−1 i (x) + αtl t i(x), if i = it dt−1i (x), otherwise, (6)\nwhich corresponds to an update of the quantity zti :\nzti =\n{\n(1− αt)z t−1 i + αt(xt−1 − 1 µ ∇fit(xt−1)), if i = it zt−1i , otherwise.\nThe next iterate is then computed as xt = argminx Dt(x), which is equivalent to (4). The original MISO/Finito algorithms use αt = 1 under a “big data” condition on the sample size n [9, 21], while the theory was later extended in [18] to relax this condition by supporting smaller constant steps αt = α, leading to an algorithm that may be interpreted as a primal variant of SDCA (see [28]).\nNote that when fi is an expectation, it is hard to obtain such lower bounds since the gradient ∇fi(xt−1) is not available in general. For this reason, we have introduced S-MISO, which can exploit approximate lower bounds to each fi using gradient estimates, by letting the step-sizes αt decrease appropriately as commonly done in stochastic approximation. This leads to update (3).\nSeparately, SDCA [29] considers the Fenchel conjugates of fi, defined by f ∗ i (y) = supx x ⊤y−fi(x). When fi is an expectation, f ∗ i is not available in closed form in general, nor are its gradients, and in fact exploiting stochastic gradient estimates is difficult in the duality framework. In contrast, [28] gives an analysis of SDCA in the primal, aka. “without duality”, for smooth finite sums, and our work extends this line of reasoning to the stochastic approximation and composite settings.\nRelationship with SGD in the smooth case. The link between S-MISO in the non-composite setting and SGD can be seen by rewriting the update (4) as\nxt = xt−1 + 1\nn (ztit − z t−1 it ) = xt−1 + αt n vt,\nwhere vt := xt−1 − 1\nµ ∇f̃it(xt−1, ρt)− z\nt−1 it . (7)\nNote that E[vt|Ft−1] = − 1 µ ∇f(xt−1), where Ft−1 contains all information up to iteration t; hence, the algorithm can be seen as an instance of the stochastic gradient method with unbiased gradients, which was a key motivation in SVRG [15] and later in other variance reduction algorithms [8, 28]. It is also worth noting that in the absence of a finite-sum structure (n=1), we have zt−1it =xt−1; hence our method becomes identical to SGD, up to a redefinition of step-sizes. In the composite case (see Appendix A), our approach yields a new algorithm that resembles regularized dual averaging [34].\nMemory requirements and handling of sparse datasets. The algorithm requires storing the vectors (zti)i=1,...,n, which takes the same amount of memory as the original dataset and which is therefore a reasonable requirement in many practical cases. In the case of sparse datasets, it is fair to assume that random perturbations applied to input data preserve the sparsity patterns of the original vectors, as is the case, e.g., when applying Dropout to text documents described with bag-ofwords representations [33]. If we further assume the typical setting where the µ-strong convexity comes from an ℓ2 regularizer: f̃i(x, ρ) = φi(x ⊤ξρi ) + (µ/2)‖x‖ 2, where ξρi is the (sparse) perturbed example and φi encodes the loss, then the update (3) can be written as\nzti =\n{\n(1− αt)z t−1 i − αt µ φ′i(x ⊤ t−1ξ ρt i )ξ ρt i , if i = it zt−1i , otherwise,\nwhich shows that for every index i, the vector zti preserves the same sparsity pattern as the examples ξ ρ i throughout the algorithm (assuming the initialization z0i = 0), making the update (3) efficient. The update (4) has the same cost since vt = z\nt it − zt−1it is also sparse.\nLimitations and alternative approaches. Since our algorithm is uniformly better than SGD in terms of iteration complexity, its main limitation is in terms of memory storage when the dataset cannot fit into memory (remember that the memory cost of S-MISO is the same as the input dataset). In these huge-scale settings, SGD should be preferred; this holds true in fact for all incremental methods when one cannot afford to perform more than one (or very few) passes over the data. Our paper focuses instead on non-huge datasets, which are those benefiting most from data augmentation.\nWe note that a different approach to variance reduction like SVRG [15] is able to trade off storage requirements for additional full gradient computations, which would be desirable in some situations. However, we were not able to obtain any decreasing step-size strategy that works for these methods, both in theory and practice, leaving us with constant step-size approaches as in [1, 14] that either maintain a non-zero asymptotic error, or require dynamically reducing the variance of gradient estimates. One possible way to explain this difficulty is that SVRG and SAGA [8] “forget” past gradients for a given example i, while S-MISO averages them in (3), which seems to be a technical key to make it suitable to stochastic approximation. Nevertheless, the question of whether it is possible to trade-off storage with computation in a setting like ours is open and of utmost interest."
    }, {
      "heading" : "3 Convergence Analysis of S-MISO",
      "text" : "We now study the convergence properties of the S-MISO algorithm. For space limitation reasons, all proofs are provided in Appendix B. We start by defining the problem-dependent quantities z∗i := x ∗ − 1 µ ∇fi(x ∗), and then introduce the Lyapunov function\nCt = 1\n2 ‖xt − x ∗‖2 + αt n2\nn ∑\ni=1\n‖zti − z ∗ i ‖ 2. (8)\nProposition 1 gives a recursion on Ct, obtained by upper-bounding separately its two terms, and finding coefficients to cancel out other appearing quantities when relating Ct to Ct−1. To this end, we borrow elements of the convergence proof of SDCA without duality [28]; our technical contribution is to extend their result to the stochastic approximation and composite (see Appendix A) cases.\nProposition 1 (Recursion on Ct). If (αt)t≥1 is a positive and non-increasing sequence satisfying\nα1 ≤ min\n{\n1 2 ,\nn\n2(2κ− 1)\n}\n, (9)\nwith κ = L/µ, then Ct obeys the recursion\nE[Ct] ≤ ( 1− αt n ) E[Ct−1] + 2 (αt n )2 σ2p µ2 . (10)\nWe now state the main convergence result, which provides the expected rate O(1/t) on Ct based on decreasing step-sizes, similar to [5] for SGD. Note that convergence of objective function values is directly related to that of the Lyapunov function Ct via smoothness:\nE[f(xt)− f(x ∗)] ≤\nL 2 E [ ‖xt − x ∗‖2 ] ≤ LE[Ct]. (11)\nTheorem 2 (Convergence of Lyapunov function). Let the sequence of step-sizes (αt)t≥1 be defined by αt = 2n γ+t with γ ≥ 0 such that α1 satisfies (9). For all t ≥ 0, it holds that\nE[Ct] ≤ ν\nγ + t+ 1 where ν := max\n{\n8σ2p µ2 , (γ + 1)C0\n}\n. (12)\nChoice of step-sizes in practice. Naturally, we would like ν to be small, in particular independent of the initial condition C0 and equal to the first term in the definition (12). We would like the dependence on C0 to vanish at a faster rate than O(1/t), as it is the case in variance reduction algorithms on finite sums. As advised in [5] in the context of SGD, we can initially run the algorithm with a constant step-size ᾱ and exploit this linear convergence regime until we reach the level of noise given by σp, and then start decaying the step-size. It is easy to see that by using a constant step-size ᾱ, Ct converges near a value C̄ := 2ᾱσ 2 p/nµ 2. Indeed, Eq. (10) with αt = ᾱ yields\nE[Ct − C̄] ≤ ( 1− ᾱ\nn\n)\nE[Ct−1 − C̄].\nThus, we can reach a precision C ′0 with E[C ′ 0] ≤ ǭ := 2C̄ in O( n ᾱ logC0/ǭ) iterations. Then, if we start decaying step-sizes as in Theorem 2 with γ large enough so that α1 = ᾱ, we have\n(γ + 1)E[C ′0] ≤ (γ + 1)ǭ = 8σ 2 p/µ 2,\nmaking both terms in (12) smaller than or equal to ν = 8σ2p/µ 2. Considering these two phases, with an initial step-size ᾱ given by (9), the final work complexity for reaching E[‖xt − x ∗‖2] ≤ ǫ is\nO\n((\nn+ L\nµ\n)\nlog C0 ǭ\n)\n+O\n(\nσ2p µ2ǫ\n)\n. (13)\nWe can then use (11) in order to obtain the complexity for reaching E[f(xt)− f(x ∗)] ≤ ǫ. Note that following this step-size strategy was found to be very effective in practice (see Section 4).\nAcceleration by iterate averaging. When one is interested in the convergence in function values, the complexity (13) combined with (11) yields O(Lσ2p/µ\n2ǫ), which can be problematic for illconditioned problems (large condition number L/µ). The following theorem presents an iterate averaging scheme which brings the complexity term down to O(σ2p/µǫ), which appeared in Table 1.\nTheorem 3 (Convergence under iterate averaging). Let the step-size sequence (αt)t≥1 be defined by\nαt = 2n\nγ + t for γ ≥ 1 s.t. α1 ≤ min\n{\n1 2 ,\nn\n4(2κ− 1)\n}\n.\nWe have\nE[f(x̄T )− f(x ∗)] ≤ 2µγ(γ − 1)C0 T (2γ + T − 1) + 16σ2p µ(2γ + T − 1) ,\nwhere\nx̄T := 2\nT (2γ + T − 1)\nT−1 ∑\nt=0\n(γ + t)xt.\nThe proof uses a similar telescoping sum technique to [16]. Note that if T ≫ γ, the first term, which depends on the initial condition C0, decays as 1/T\n2 and is thus dominated by the second term. Moreover, if we start averaging after an initial phase with constant step-size ᾱ, we can consider C0 ≈ 4ᾱσ 2 p/nµ\n2. In the ill-conditioned regime, taking ᾱ = α1 = 2n/(γ + 1) as large as allowed by (9), we have γ of the order of κ = L/µ ≫ 1. The full convergence rate then becomes\nE[f(x̄T )− f(x ∗)] ≤ O\n(\nσ2p µ(γ + T ) ( 1 + γ T )\n)\n.\nWhen T is large enough compared to γ, this becomes O(σ2p/µT ), leading to a complexity O(σ 2 p/µǫ)."
    }, {
      "heading" : "4 Experiments",
      "text" : "We present experiments comparing S-MISO with SGD and N-SAGA [14] on four different scenarios, in order to demonstrate the wide applicability of our method: we consider an image classification dataset with two different image representations and random transformations, and two classification tasks with Dropout regularization, one on genetic data, and one on (sparse) text data. Figures 1 and 3 show the curves for an estimate of the training objective using 5 sampled perturbations per example. The plots are shown on a logarithmic scale, and the values are compared to the best value obtained among the different methods in 500 epochs. The strong convexity constant µ is the regularization parameter. For all methods, we consider step-sizes supported by the theory as well as larger step-sizes that may work better in practice. Our C++/Cython implementation of all methods considered in this section is available at https://github.com/albietz/stochs.\nChoices of step-sizes. For both S-MISO and SGD, we use the step-size strategy mentioned in Section 3 and advised by [5], which we have found to be most effective among many heuristics\nwe have tried: we initially keep the step-size constant (controlled by a factor η ≤ 1 in the figures) for 2 epochs, and then start decaying as αt = C/(γ + t), where C = 2n for S-MISO, C = 2/µ for SGD, and γ is chosen large enough to match the previous constant step-size. For N-SAGA, we maintain a constant step-size throughout the optimization, as suggested in the original paper [14]. The factor η shown in the figures is such that η = 1 corresponds to an initial step-size nµ/(L− µ) for S-MISO (from (19) in the uniform case) and 1/L for SGD and N-SAGA (with L̄ instead of L in the non-uniform case when using the variant of Appendix A).\nImage classification with “data augmentation”. The success of deep neural networks is often limited by the availability of large amounts of labeled images. When there are many unlabeled images but few labeled ones, a common approach is to train a linear classifier on top of a deep network learned in an unsupervised manner, or pre-trained on a different task (e.g., on the ImageNet dataset). We follow this approach on the STL-10 dataset [7], which contains 5K training images from 10 classes and 100K unlabeled images, using a 2-layer unsupervised convolutional kernel network [22], giving representations of dimension 9 216. The perturbation consists of randomly cropping and scaling the input images. We use the squared hinge loss in a one-versus-all setting. The vector representations are ℓ2-normalized such that we may use the upper bound L = 1 + µ for the smoothness constant. We also present results on the same dataset using a scattering representation [6] of dimension 21 696, with random gamma corrections (raising all pixels to the power γ, where γ is chosen randomly around 1). For this representation, we add an ℓ1 regularization term and use the composite variant of S-MISO presented in Appendix A.\nFigure 1 shows convergence results on one training fold (500 images), for different values of µ, allowing us to study the behavior of the algorithms for different condition numbers. The low variance induced by data transformations allows S-MISO to reach suboptimality that is orders of magnitude smaller than SGD after the same number of epochs. Note that one unit on these plots corresponds to one order of magnitude in the logarithmic scale. N-SAGA initially reaches a smaller suboptimality than SGD, but quickly gets stuck due to the bias in the algorithm, as predicted by the theory [14], while S-MISO and SGD continue to converge to the optimum thanks to the decreasing step-sizes. The best validation accuracy for both representations is obtained for µ ≈ 10−4 (middle column), and we observed relative gains of up to 1% from using data augmentation. We computed empirical variances of the image representations for these two strategies, which are closely related to the variance in gradient estimates, and observed these transformations to account for about 10% of the total variance.\nFigure 2 shows convergence results when training the last layer of a 50-layer Residual network [12] that has been pre-trained on ImageNet. Here, we consider the common scenario of leveraging a deep model trained on a large dataset as a feature extractor in order to learn a new classifier on a different small dataset, where it would be difficult to train such a model from scratch. To simulate this setting, we consider a binary classification task on a small dataset of 100 images of size 256x256 taken from the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012, which we crop to\n224x224 before performing random adjustments to brightness, saturation, hue and contrast. As in the STL-10 experiments, the gains of S-MISO over other methods are of about one order of magnitude in suboptimality, as predicted by Table 2.\nDropout on gene expression data. We trained a binary logistic regression model on the breast cancer dataset of [31], with different Dropout rates δ, i.e., where at every iteration, each coordinate ξj of a feature vector ξ is set to zero independently with probability δ and to ξj/(1− δ) otherwise. The dataset consists of 295 vectors of dimension 8 141 of gene expression data, which we normalize in ℓ2 norm. Figure 3 (top) compares S-MISO with SGD and N-SAGA for three values of δ, as a way to control the variance of the perturbations. We include a Dropout rate of 0.01 to illustrate the impact of δ on the algorithms and study the influence of the perturbation variance σ2p, even though this value of δ is less relevant for the task. The plots show very clearly how the variance induced by the perturbations affects the convergence of S-MISO, giving suboptimality values that may be orders of magnitude smaller than SGD. This behavior is consistent with the theoretical convergence rate established in Section 3 and shows that the practice matches the theory.\nDropout on movie review sentiment analysis data. We trained a binary classifier with a squared hinge loss on the IMDB dataset [20] with different Dropout rates δ. We use the labeled part of the IMDB dataset, which consists of 25K training and 250K testing movie reviews, represented as 89 527-dimensional sparse bag-of-words vectors. In contrast to the previous experiments, we do not normalize the representations, which have great variability in their norms, in particular, the maximum Lipschitz constant across training points is roughly 100 times larger than the average one. Figure 3 (bottom) compares non-uniform sampling versions of S-MISO (see Appendix A) and SGD (see Appendix D) with their uniform sampling counterparts as well as N-SAGA. Note that we use a large step-size η = 10 for the uniform sampling algorithms, since η = 1 was significantly slower for all methods, likely due to outliers in the dataset. In contrast, the non-uniform sampling algorithms required no tuning and just use η = 1. The curves clearly show that S-MISO-NU has a much faster convergence in the initial phase, thanks to the larger step-size allowed by non-uniform sampling, and later converges similarly to S-MISO, i.e., at a much faster rate than SGD when the perturbations are small. The value of µ used in the experiments was chosen by cross-validation, and the use of Dropout gave improvements in test accuracy from 88.51% with no dropout to 88.68± 0.03% with δ = 0.1 and 88.86± 0.11% with δ = 0.3 (based on 10 different runs of S-MISO-NU after 400 epochs).\nFinally, we also study the effect of the iterate averaging scheme of Theorem 3 in Appendix E."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by a grant from ANR (MACARON project under grant number ANR14-CE23-0003-01), by the ERC grant number 714381 (SOLARIS project), and by the MSR-Inria joint center."
    } ],
    "references" : [ {
      "title" : "SGD with Variance Reduction beyond Empirical Risk Minimization",
      "author" : [ "M. Achab", "A. Guilloux", "S. Gaïffas", "E. Bacry" ],
      "venue" : "arXiv:1510.04822",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Katyusha: The first direct acceleration of stochastic gradient methods",
      "author" : [ "Z. Allen-Zhu" ],
      "venue" : "Symposium on the Theory of Computing (STOC)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters",
      "author" : [ "Z. Allen-Zhu", "Y. Yuan", "K. Sridharan" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Non-asymptotic analysis of stochastic approximation algorithms for machine learning",
      "author" : [ "F. Bach", "E. Moulines" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Optimization Methods for Large-Scale Machine Learning",
      "author" : [ "L. Bottou", "F.E. Curtis", "J. Nocedal" ],
      "venue" : "arXiv:1606.04838",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Invariant scattering convolution networks",
      "author" : [ "J. Bruna", "S. Mallat" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence (PAMI), 35(8):1872–1886",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "An Analysis of Single-Layer Networks in Unsupervised Feature Learning",
      "author" : [ "A. Coates", "H. Lee", "A.Y. Ng" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics (AISTATS)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "A. Defazio", "F. Bach", "S. Lacoste-Julien" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Finito: A faster",
      "author" : [ "A. Defazio", "J. Domke", "T.S. Caetano" ],
      "venue" : "permutable incremental gradient method for big data problems. In International Conference on Machine Learning (ICML)",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Privacy aware learning",
      "author" : [ "J.C. Duchi", "M.I. Jordan", "M.J. Wainwright" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Efficient online and batch learning using forward backward splitting",
      "author" : [ "J.C. Duchi", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research (JMLR), 10:2899–2934",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Convex analysis and minimization algorithms I: Fundamentals",
      "author" : [ "J.-B. Hiriart-Urruty", "C. Lemaréchal" ],
      "venue" : "Springer science & business media",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Variance Reduced Stochastic Gradient Descent with Neighbors",
      "author" : [ "T. Hofmann", "A. Lucchi", "S. Lacoste-Julien", "B. McWilliams" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method",
      "author" : [ "S. Lacoste-Julien", "M. Schmidt", "F. Bach" ],
      "venue" : "arXiv:1212.2002",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An optimal randomized incremental gradient method",
      "author" : [ "G. Lan", "Y. Zhou" ],
      "venue" : "Mathematical Programming",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "A Universal Catalyst for First-Order Optimization",
      "author" : [ "H. Lin", "J. Mairal", "Z. Harchaoui" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Training invariant support vector machines using selective sampling",
      "author" : [ "G. Loosli", "S. Canu", "L. Bottou" ],
      "venue" : "Large Scale Kernel Machines, pages 301–320. MIT Press, Cambridge, MA.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts" ],
      "venue" : "The 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 142–150. Association for Computational Linguistics",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning",
      "author" : [ "J. Mairal" ],
      "venue" : "SIAM Journal on Optimization, 25(2):829–855",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "End-to-End Kernel Learning with Supervised Convolutional Kernel Networks",
      "author" : [ "J. Mairal" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Stability selection",
      "author" : [ "N. Meinshausen", "P. Bühlmann" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(4):417–473",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Robust Stochastic Approximation Approach to Stochastic Programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization, 19(4):1574–1609",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Introductory Lectures on Convex Optimization",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Springer",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Transformation pursuit for image classification",
      "author" : [ "M. Paulin", "J. Revaud", "Z. Harchaoui", "F. Perronnin", "C. Schmid" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "M. Schmidt", "N. Le Roux", "F. Bach" ],
      "venue" : "Mathematical Programming, 162(1):83–112",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "SDCA without Duality",
      "author" : [ "S. Shalev-Shwartz" ],
      "venue" : "Regularization, and Individual Convexity. In International Conference on Machine Learning (ICML)",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research (JMLR), 14:567–599",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Transformation Invariance in Pattern Recognition — Tangent Distance and Tangent Propagation",
      "author" : [ "P.Y. Simard", "Y.A. LeCun", "J.S. Denker", "B. Victorri" ],
      "venue" : "G. B. Orr and K.-R. Müller, editors, Neural Networks: Tricks of the Trade, number 1524 in Lecture Notes in Computer Science, pages 239–274. Springer Berlin Heidelberg",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A Gene-Expression Signature as a Predictor of Survival in Breast Cancer",
      "author" : [ "M.J. van de Vijver" ],
      "venue" : "New England Journal of Medicine,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2002
    }, {
      "title" : "Learning with marginalized corrupted features",
      "author" : [ "L. van der Maaten", "M. Chen", "S. Tyree", "K.Q. Weinberger" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    }, {
      "title" : "Altitude Training: Strong Bounds for Single-layer Dropout",
      "author" : [ "S. Wager", "W. Fithian", "S. Wang", "P. Liang" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Dual averaging methods for regularized stochastic learning and online optimization",
      "author" : [ "L. Xiao" ],
      "venue" : "Journal of Machine Learning Research (JMLR), 11:2543–2596",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "L. Xiao", "T. Zhang" ],
      "venue" : "SIAM Journal on Optimization, 24(4):2057–2075",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Improving the robustness of deep neural networks via stability training",
      "author" : [ "S. Zheng", "Y. Song", "T. Leung", "I. Goodfellow" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "When an infinite amount of data is available, stochastic optimization methods such as SGD or stochastic mirror descent algorithms, or their variants, are typically used (see [5, 11, 24, 34]).",
      "startOffset" : 174,
      "endOffset" : 189
    }, {
      "referenceID" : 10,
      "context" : "When an infinite amount of data is available, stochastic optimization methods such as SGD or stochastic mirror descent algorithms, or their variants, are typically used (see [5, 11, 24, 34]).",
      "startOffset" : 174,
      "endOffset" : 189
    }, {
      "referenceID" : 23,
      "context" : "When an infinite amount of data is available, stochastic optimization methods such as SGD or stochastic mirror descent algorithms, or their variants, are typically used (see [5, 11, 24, 34]).",
      "startOffset" : 174,
      "endOffset" : 189
    }, {
      "referenceID" : 33,
      "context" : "When an infinite amount of data is available, stochastic optimization methods such as SGD or stochastic mirror descent algorithms, or their variants, are typically used (see [5, 11, 24, 34]).",
      "startOffset" : 174,
      "endOffset" : 189
    }, {
      "referenceID" : 1,
      "context" : ", [2, 8, 15, 17, 18, 27, 29]) have proven to be significantly faster at solving the finite-sum problem",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : ", [2, 8, 15, 17, 18, 27, 29]) have proven to be significantly faster at solving the finite-sum problem",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : ", [2, 8, 15, 17, 18, 27, 29]) have proven to be significantly faster at solving the finite-sum problem",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : ", [2, 8, 15, 17, 18, 27, 29]) have proven to be significantly faster at solving the finite-sum problem",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : ", [2, 8, 15, 17, 18, 27, 29]) have proven to be significantly faster at solving the finite-sum problem",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 26,
      "context" : ", [2, 8, 15, 17, 18, 27, 29]) have proven to be significantly faster at solving the finite-sum problem",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 28,
      "context" : ", [2, 8, 15, 17, 18, 27, 29]) have proven to be significantly faster at solving the finite-sum problem",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 22,
      "context" : "Yet, dealing with random perturbations is of utmost interest; for instance, this is a key to achieve stable feature selection [23], improving the generalization error both in theory [33] and in practice [19, 32], obtaining stable and robust predictors [36], or using complex a priori knowledge about data to generate virtually ∗Univ.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 32,
      "context" : "Yet, dealing with random perturbations is of utmost interest; for instance, this is a key to achieve stable feature selection [23], improving the generalization error both in theory [33] and in practice [19, 32], obtaining stable and robust predictors [36], or using complex a priori knowledge about data to generate virtually ∗Univ.",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 18,
      "context" : "Yet, dealing with random perturbations is of utmost interest; for instance, this is a key to achieve stable feature selection [23], improving the generalization error both in theory [33] and in practice [19, 32], obtaining stable and robust predictors [36], or using complex a priori knowledge about data to generate virtually ∗Univ.",
      "startOffset" : 203,
      "endOffset" : 211
    }, {
      "referenceID" : 31,
      "context" : "Yet, dealing with random perturbations is of utmost interest; for instance, this is a key to achieve stable feature selection [23], improving the generalization error both in theory [33] and in practice [19, 32], obtaining stable and robust predictors [36], or using complex a priori knowledge about data to generate virtually ∗Univ.",
      "startOffset" : 203,
      "endOffset" : 211
    }, {
      "referenceID" : 35,
      "context" : "Yet, dealing with random perturbations is of utmost interest; for instance, this is a key to achieve stable feature selection [23], improving the generalization error both in theory [33] and in practice [19, 32], obtaining stable and robust predictors [36], or using complex a priori knowledge about data to generate virtually ∗Univ.",
      "startOffset" : 252,
      "endOffset" : 256
    }, {
      "referenceID" : 13,
      "context" : "The complexity of N-SAGA [14] matches the first term of S-MISO but is asymptotically biased.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "Injecting noise in data is also useful to hide gradient information for privacy-aware learning [10].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : ", [15, 29]) have been motivated by the fact that their updates can be interpreted as SGD steps with unbiased estimates of the full gradient, but with a variance that decreases as the algorithm approaches the optimum [15]; on the other hand, vanilla SGD requires decreasing step-sizes to achieve this reduction of variance, thereby slowing down convergence.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 28,
      "context" : ", [15, 29]) have been motivated by the fact that their updates can be interpreted as SGD steps with unbiased estimates of the full gradient, but with a variance that decreases as the algorithm approaches the optimum [15]; on the other hand, vanilla SGD requires decreasing step-sizes to achieve this reduction of variance, thereby slowing down convergence.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 14,
      "context" : ", [15, 29]) have been motivated by the fact that their updates can be interpreted as SGD steps with unbiased estimates of the full gradient, but with a variance that decreases as the algorithm approaches the optimum [15]; on the other hand, vanilla SGD requires decreasing step-sizes to achieve this reduction of variance, thereby slowing down convergence.",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 2,
      "context" : "Most related to our work, recent methods that use data clustering to accelerate variance reduction techniques [3, 14] can be seen as tackling a special case of (2), where the expectations in fi are replaced by empirical averages over points in a cluster.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "Most related to our work, recent methods that use data clustering to accelerate variance reduction techniques [3, 14] can be seen as tackling a special case of (2), where the expectations in fi are replaced by empirical averages over points in a cluster.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "While N-SAGA [14] was originally not designed for the stochastic context we consider, we remark that their method can be applied to (2).",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, ClusterSVRG [3] is not biased, but does not support infinite datasets.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "The method proposed in [1] uses variance reduction in a setting where gradients are computed approximately, but the algorithm computes a full gradient at every pass, which is not available in our stochastic setting.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "ResNet-50 denotes a 50 layer network [12] pre-trained on the ImageNet dataset.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "R(2) tot (respectively, R 2 cluster) denotes the average squared distance between pairs of points in the dataset (respectively, in a given cluster), following [14].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 2,
      "context" : "Direct perturbation of linear model features Data clustering as in [3, 14] ≈ R(2) tot/R 2 cluster Additive Gaussian noise N (0, α(2)I) ≈ 1 + 1/α(2) Dropout with probability δ ≈ 1 + 1/δ Feature rescaling by s in U(1− w, 1 + w) ≈ 1 + 3/w(2)",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "Direct perturbation of linear model features Data clustering as in [3, 14] ≈ R(2) tot/R 2 cluster Additive Gaussian noise N (0, α(2)I) ≈ 1 + 1/α(2) Dropout with probability δ ≈ 1 + 1/δ Feature rescaling by s in U(1− w, 1 + w) ≈ 1 + 3/w(2)",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "Random image transformations ResNet-50 [12], color perturbation 21.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 21,
      "context" : "6 Unsupervised CKN [22], rescaling + crop 9.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "From a theoretical point of view, the approach we propose achieves the iteration complexity presented in Table 1, see also Appendix D and [4, 5, 24] for the complexity analysis of SGD.",
      "startOffset" : 138,
      "endOffset" : 148
    }, {
      "referenceID" : 4,
      "context" : "From a theoretical point of view, the approach we propose achieves the iteration complexity presented in Table 1, see also Appendix D and [4, 5, 24] for the complexity analysis of SGD.",
      "startOffset" : 138,
      "endOffset" : 148
    }, {
      "referenceID" : 23,
      "context" : "From a theoretical point of view, the approach we propose achieves the iteration complexity presented in Table 1, see also Appendix D and [4, 5, 24] for the complexity analysis of SGD.",
      "startOffset" : 138,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "One clear framework of application is the data clustering scenario already investigated in [3, 14].",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : "One clear framework of application is the data clustering scenario already investigated in [3, 14].",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 32,
      "context" : "First, we consider learning a linear model when adding simple direct manipulations of feature vectors, via rescaling (multiplying each entry vector by a random scalar), Dropout, or additive Gaussian noise, in order to improve the generalization error [33] or to get more stable estimators [23].",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 22,
      "context" : "First, we consider learning a linear model when adding simple direct manipulations of feature vectors, via rescaling (multiplying each entry vector by a random scalar), Dropout, or additive Gaussian noise, in order to improve the generalization error [33] or to get more stable estimators [23].",
      "startOffset" : 289,
      "endOffset" : 293
    }, {
      "referenceID" : 25,
      "context" : "Second, we also consider more interesting domain-driven data perturbations such as classical image transformations considered in computer vision [26, 36] including image cropping, rescaling, brightness, contrast, hue, and saturation changes.",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 35,
      "context" : "Second, we also consider more interesting domain-driven data perturbations such as classical image transformations considered in computer vision [26, 36] including image cropping, rescaling, brightness, contrast, hue, and saturation changes.",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 21,
      "context" : "classifier on top of an unsupervised multilayer image model such as unsupervised CKNs [22] or the scattering transform [6].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "classifier on top of an unsupervised multilayer image model such as unsupervised CKNs [22] or the scattering transform [6].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "Without perturbations and with a constant step-size, the algorithm resembles the MISO/Finito algorithms [9, 18, 21], which may be seen as primal variants of SDCA [28, 29].",
      "startOffset" : 104,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "Without perturbations and with a constant step-size, the algorithm resembles the MISO/Finito algorithms [9, 18, 21], which may be seen as primal variants of SDCA [28, 29].",
      "startOffset" : 104,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "Without perturbations and with a constant step-size, the algorithm resembles the MISO/Finito algorithms [9, 18, 21], which may be seen as primal variants of SDCA [28, 29].",
      "startOffset" : 104,
      "endOffset" : 115
    }, {
      "referenceID" : 27,
      "context" : "Without perturbations and with a constant step-size, the algorithm resembles the MISO/Finito algorithms [9, 18, 21], which may be seen as primal variants of SDCA [28, 29].",
      "startOffset" : 162,
      "endOffset" : 170
    }, {
      "referenceID" : 28,
      "context" : "Without perturbations and with a constant step-size, the algorithm resembles the MISO/Finito algorithms [9, 18, 21], which may be seen as primal variants of SDCA [28, 29].",
      "startOffset" : 162,
      "endOffset" : 170
    }, {
      "referenceID" : 12,
      "context" : "It is part of a larger body of optimization methods that iteratively build a model of the objective function, typically a lower or upper bound on the objective that is easier to optimize; for instance, this strategy is commonly adopted in bundle methods [13, 25].",
      "startOffset" : 254,
      "endOffset" : 262
    }, {
      "referenceID" : 24,
      "context" : "It is part of a larger body of optimization methods that iteratively build a model of the objective function, typically a lower or upper bound on the objective that is easier to optimize; for instance, this strategy is commonly adopted in bundle methods [13, 25].",
      "startOffset" : 254,
      "endOffset" : 262
    }, {
      "referenceID" : 8,
      "context" : "The original MISO/Finito algorithms use αt = 1 under a “big data” condition on the sample size n [9, 21], while the theory was later extended in [18] to relax this condition by supporting smaller constant steps αt = α, leading to an algorithm that may be interpreted as a primal variant of SDCA (see [28]).",
      "startOffset" : 97,
      "endOffset" : 104
    }, {
      "referenceID" : 20,
      "context" : "The original MISO/Finito algorithms use αt = 1 under a “big data” condition on the sample size n [9, 21], while the theory was later extended in [18] to relax this condition by supporting smaller constant steps αt = α, leading to an algorithm that may be interpreted as a primal variant of SDCA (see [28]).",
      "startOffset" : 97,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "The original MISO/Finito algorithms use αt = 1 under a “big data” condition on the sample size n [9, 21], while the theory was later extended in [18] to relax this condition by supporting smaller constant steps αt = α, leading to an algorithm that may be interpreted as a primal variant of SDCA (see [28]).",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 27,
      "context" : "The original MISO/Finito algorithms use αt = 1 under a “big data” condition on the sample size n [9, 21], while the theory was later extended in [18] to relax this condition by supporting smaller constant steps αt = α, leading to an algorithm that may be interpreted as a primal variant of SDCA (see [28]).",
      "startOffset" : 300,
      "endOffset" : 304
    }, {
      "referenceID" : 28,
      "context" : "Separately, SDCA [29] considers the Fenchel conjugates of fi, defined by f ∗ i (y) = supx x y−fi(x).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 27,
      "context" : "In contrast, [28] gives an analysis of SDCA in the primal, aka.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 14,
      "context" : "(7) Note that E[vt|Ft−1] = − 1 μ ∇f(xt−1), where Ft−1 contains all information up to iteration t; hence, the algorithm can be seen as an instance of the stochastic gradient method with unbiased gradients, which was a key motivation in SVRG [15] and later in other variance reduction algorithms [8, 28].",
      "startOffset" : 240,
      "endOffset" : 244
    }, {
      "referenceID" : 7,
      "context" : "(7) Note that E[vt|Ft−1] = − 1 μ ∇f(xt−1), where Ft−1 contains all information up to iteration t; hence, the algorithm can be seen as an instance of the stochastic gradient method with unbiased gradients, which was a key motivation in SVRG [15] and later in other variance reduction algorithms [8, 28].",
      "startOffset" : 294,
      "endOffset" : 301
    }, {
      "referenceID" : 27,
      "context" : "(7) Note that E[vt|Ft−1] = − 1 μ ∇f(xt−1), where Ft−1 contains all information up to iteration t; hence, the algorithm can be seen as an instance of the stochastic gradient method with unbiased gradients, which was a key motivation in SVRG [15] and later in other variance reduction algorithms [8, 28].",
      "startOffset" : 294,
      "endOffset" : 301
    }, {
      "referenceID" : 33,
      "context" : "In the composite case (see Appendix A), our approach yields a new algorithm that resembles regularized dual averaging [34].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 32,
      "context" : ", when applying Dropout to text documents described with bag-ofwords representations [33].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "We note that a different approach to variance reduction like SVRG [15] is able to trade off storage requirements for additional full gradient computations, which would be desirable in some situations.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "However, we were not able to obtain any decreasing step-size strategy that works for these methods, both in theory and practice, leaving us with constant step-size approaches as in [1, 14] that either maintain a non-zero asymptotic error, or require dynamically reducing the variance of gradient estimates.",
      "startOffset" : 181,
      "endOffset" : 188
    }, {
      "referenceID" : 13,
      "context" : "However, we were not able to obtain any decreasing step-size strategy that works for these methods, both in theory and practice, leaving us with constant step-size approaches as in [1, 14] that either maintain a non-zero asymptotic error, or require dynamically reducing the variance of gradient estimates.",
      "startOffset" : 181,
      "endOffset" : 188
    }, {
      "referenceID" : 7,
      "context" : "One possible way to explain this difficulty is that SVRG and SAGA [8] “forget” past gradients for a given example i, while S-MISO averages them in (3), which seems to be a technical key to make it suitable to stochastic approximation.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 27,
      "context" : "To this end, we borrow elements of the convergence proof of SDCA without duality [28]; our technical contribution is to extend their result to the stochastic approximation and composite (see Appendix A) cases.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "We now state the main convergence result, which provides the expected rate O(1/t) on Ct based on decreasing step-sizes, similar to [5] for SGD.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "As advised in [5] in the context of SGD, we can initially run the algorithm with a constant step-size ᾱ and exploit this linear convergence regime until we reach the level of noise given by σp, and then start decaying the step-size.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 15,
      "context" : "The proof uses a similar telescoping sum technique to [16].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 13,
      "context" : "We present experiments comparing S-MISO with SGD and N-SAGA [14] on four different scenarios, in order to demonstrate the wide applicability of our method: we consider an image classification dataset with two different image representations and random transformations, and two classification tasks with Dropout regularization, one on genetic data, and one on (sparse) text data.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "For both S-MISO and SGD, we use the step-size strategy mentioned in Section 3 and advised by [5], which we have found to be most effective among many heuristics",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "For N-SAGA, we maintain a constant step-size throughout the optimization, as suggested in the original paper [14].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "We follow this approach on the STL-10 dataset [7], which contains 5K training images from 10 classes and 100K unlabeled images, using a 2-layer unsupervised convolutional kernel network [22], giving representations of dimension 9 216.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 21,
      "context" : "We follow this approach on the STL-10 dataset [7], which contains 5K training images from 10 classes and 100K unlabeled images, using a 2-layer unsupervised convolutional kernel network [22], giving representations of dimension 9 216.",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 5,
      "context" : "We also present results on the same dataset using a scattering representation [6] of dimension 21 696, with random gamma corrections (raising all pixels to the power γ, where γ is chosen randomly around 1).",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "N-SAGA initially reaches a smaller suboptimality than SGD, but quickly gets stuck due to the bias in the algorithm, as predicted by the theory [14], while S-MISO and SGD continue to converge to the optimum thanks to the decreasing step-sizes.",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "Figure 2 shows convergence results when training the last layer of a 50-layer Residual network [12] that has been pre-trained on ImageNet.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 30,
      "context" : "We trained a binary logistic regression model on the breast cancer dataset of [31], with different Dropout rates δ, i.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "We trained a binary classifier with a squared hinge loss on the IMDB dataset [20] with different Dropout rates δ.",
      "startOffset" : 77,
      "endOffset" : 81
    } ],
    "year" : 2017,
    "abstractText" : "Stochastic optimization algorithms with variance reduction have proven successful for minimizing large finite sums of functions. Unfortunately, these techniques are unable to deal with stochastic perturbations of input data, induced for example by data augmentation. In such cases, the objective is no longer a finite sum, and the main candidate for optimization is the stochastic gradient descent method (SGD). In this paper, we introduce a variance reduction approach for these settings when the objective is composite and strongly convex. The convergence rate outperforms SGD with a typically much smaller constant factor, which depends on the variance of gradient estimates only due to perturbations on a single example.",
    "creator" : null
  }
}