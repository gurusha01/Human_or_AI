{
  "name" : "6f1d0705c91c2145201df18a1a0c7345.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Hierarchical Implicit Models and Likelihood-Free Variational Inference",
    "authors" : [ "Dustin Tran", "Rajesh Ranganath", "David M. Blei" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Consider a model of coin tosses. With probabilistic models, one typically posits a latent probability, and supposes each toss is a Bernoulli outcome given this probability [36, 15]. After observing a collection of coin tosses, Bayesian analysis lets us describe our inferences about the probability.\nHowever, we know from the laws of physics that the outcome of a coin toss is fully determined by its initial conditions (say, the impulse and angle of flip) [25, 9]. Therefore a coin toss’ randomness does not originate from a latent probability but in noisy initial parameters. This alternative model incorporates the physical system, better capturing the generative process. Furthermore the model is implicit, also known as a simulator: we can sample data from its generative process, but we may not have access to calculate its density [11, 20].\nCoin tosses are simple, but they serve as a building block for complex implicit models. These models, which capture the laws and theories of real-world physical systems, pervade fields such as population genetics [40], statistical physics [1], and ecology [3]; they underlie structural equation models in economics and causality [39]; and they connect deeply to generative adversarial networks (GANs) [18], which use neural networks to specify a flexible implicit density [35]. Unfortunately, implicit models, including GANs, have seen limited success outside specific domains. There are two reasons. First, it is unknown how to design implicit models for more general applications, exposing rich latent structure such as priors, hierarchies, and sequences. Second, existing methods for inferring latent structure in implicit models do not sufficiently scale to high-dimensional or large data sets. In this paper, we design a new class of implicit models and we develop a new algorithm for accurate and scalable inference.\nFor modeling, § 2 describes hierarchical implicit models, a class of Bayesian hierarchical models which only assume a process that generates samples. This class encompasses both simulators in the\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nclassical literature and those employed in GANs. For example, we specify a Bayesian GAN, where we place a prior on its parameters. The Bayesian perspective allows GANs to quantify uncertainty and improve data efficiency. We can also apply them to discrete data; this setting is not possible with traditional estimation algorithms for GANs [27]. For inference, § 3 develops likelihood-free variational inference (LFVI), which combines variational inference with density ratio estimation [49, 35]. Variational inference posits a family of distributions over latent variables and then optimizes to find the member closest to the posterior [23]. Traditional approaches require a likelihood-based model and use crude approximations, employing a simple approximating family for fast computation. LFVI expands variational inference to implicit models and enables accurate variational approximations with implicit variational families: LFVI does not require the variational density to be tractable. Further, unlike previous Bayesian methods for implicit models, LFVI scales to millions of data points with stochastic optimization.\nThis work has diverse applications. First, we analyze a classical problem from the approximate Bayesian computation (ABC) literature, where the model simulates an ecological system [3]. We analyze 100,000 time series which is not possible with traditional methods. Second, we analyze a Bayesian GAN, which is a GAN with a prior over its weights. Bayesian GANs outperform corresponding Bayesian neural networks with known likelihoods on several classification tasks. Third, we show how injecting noise into hidden units of recurrent neural networks corresponds to a deep implicit model for flexible sequence generation.\nRelated Work. This paper connects closely to three lines of work. The first is Bayesian inference for implicit models, known in the statistics literature as approximate Bayesian computation (ABC) [3, 33]. ABC steps around the intractable likelihood by applying summary statistics to measure the closeness of simulated samples to real observations. While successful in many domains, ABC has shortcomings. First, the results generated by ABC depend heavily on the chosen summary statistics and the closeness measure. Second, as the dimensionality grows, closeness becomes harder to achieve. This is the classic curse of dimensionality. The second is GANs [18]. GANs have seen much interest since their conception, providing an efficient method for estimation in neural network-based simulators. Larsen et al. [28] propose a hybrid of variational methods and GANs for improved reconstruction. Chen et al. [7] apply information penalties to disentangle factors of variation. Donahue et al. [12], Dumoulin et al. [13] propose to match on an augmented space, simultaneously training the model and an inverse mapping from data to noise. Unlike any of the above, we develop models with explicit priors on latent variables, hierarchies, and sequences, and we generalize GANs to perform Bayesian inference. The final thread is variational inference with expressive approximations [45, 48, 52]. The idea of casting the design of variational families as a modeling problem was proposed in Ranganath et al. [44]. Further advances have analyzed variational programs [42]—a family of approximations which only requires a process returning samples—and which has seen further interest [30]. Implicit-like variational approximations have also appeared in auto-encoder frameworks [32, 34] and message passing [24]. We build on variational programs for inferring implicit models."
    }, {
      "heading" : "2 Hierarchical Implicit Models",
      "text" : "Hierarchical models play an important role in sharing statistical strength across examples [16]. For a broad class of hierarchical Bayesian models, the joint distribution of the hidden and observed variables is\np(x, z,β) = p(β) N∏ n=1 p(xn | zn,β)p(zn |β), (1)\nwhere xn is an observation, zn are latent variables associated to that observation (local variables), and β are latent variables shared across observations (global variables). See Fig. 1 (left).\nWith hierarchical models, local variables can be used for clustering in mixture models, mixed memberships in topic models [4], and factors in probabilistic matrix factorization [47]. Global variables can be used to pool information across data points for hierarchical regression [16], topic models [4], and Bayesian nonparametrics [50].\nHierarchical models typically use a tractable likelihood p(xn | zn,β). But many likelihoods of interest, such as simulator-based models [20] and generative adversarial networks [18], admit high\nfidelity to the true data generating process and do not admit a tractable likelihood. To overcome this limitation, we develop hierarchical implicit models (HIMs). Hierarchical implicit models have the same joint factorization as Eq.1 but only assume that one can sample from the likelihood. Rather than define p(xn | zn,β) explicitly, HIMs define a function g that takes in random noise n ∼ s(·) and outputs xn given zn and β,\nxn = g( n | zn,β), n ∼ s(·). The induced, implicit likelihood of xn ∈ A given zn and β is\nP(xn ∈ A | zn,β) = ∫ {g( n | zn,β)=xn∈A} s( n) d n.\nThis integral is typically intractable. It is difficult to find the set to integrate over, and the integration itself may be expensive for arbitrary noise distributions s(·) and functions g. Fig. 1 (right) displays the graphical model for HIMs. Noise ( n) are denoted by triangles; deterministic computation (xn) are denoted by squares. We illustrate two examples.\nExample: Physical Simulators. Given initial conditions, simulators describe a stochastic process that generates data. For example, in population ecology, the Lotka-Volterra model simulates predator-prey populations over time via a stochastic differential equation [55]. For prey and predator populations x1, x2 ∈ R+ respectively, one process is\ndx1 dt = β1x1 − β2x1x2 + 1, 1 ∼ Normal(0, 10),\ndx2 dt = −β2x2 + β3x1x2 + 2, 2 ∼ Normal(0, 10),\nwhere Gaussian noises 1, 2 are added at each full time step. The simulator runs for T time steps given initial population sizes for x1, x2. Lognormal priors are placed over β. The Lotka-Volterra model is grounded by theory but features an intractable likelihood. We study it in § 4.\nExample: Bayesian Generative Adversarial Network. Generative adversarial networks (GANs) define an implicit model and a method for parameter estimation [18]. They are known to perform well on image generation [41]. Formally, the implicit model for a GAN is\nxn = g( n;θ), n ∼ s(·), (2) where g is a neural network with parameters θ, and s is a standard normal or uniform. The neural network g is typically not invertible; this makes the likelihood intractable.\nThe parameters θ in GANs are estimated by divergence minimization between the generated and real data. We make GANs amenable to Bayesian analysis by placing a prior on the parameters θ. We call this a Bayesian GAN. Bayesian GANs enable modeling of parameter uncertainty and are inspired by Bayesian neural networks, which have been shown to improve the uncertainty and data efficiency of standard neural networks [31, 37]. We study Bayesian GANs in § 4; Appendix B provides example implementations in the Edward probabilistic programming language [53]."
    }, {
      "heading" : "3 Likelihood-Free Variational Inference",
      "text" : "We described hierarchical implicit models, a rich class of latent variable models with local and global structure alongside an implicit density. Given data, we aim to calculate the model’s posterior p(z,β |x) = p(x, z,β)/p(x). This is difficult as the normalizing constant p(x) is typically\nintractable. With implicit models, the lack of a likelihood function introduces an additional source of intractability.\nWe use variational inference [23]. It posits an approximating family q ∈ Q and optimizes to find the member closest to p(z,β |x). There are many choices of variational objectives that measure closeness [42, 29, 10]. To choose an objective, we lay out desiderata for a variational inference algorithm for implicit models:\n1. Scalability. Machine learning hinges on stochastic optimization to scale to massive data [6]. The variational objective should admit unbiased subsampling with the standard technique,\nN∑ n=1 f(xn) ≈ N M M∑ m=1 f(xm),\nwhere some computation f(·) over the full data is approximated with a mini-batch of data {xm}. 2. Implicit Local Approximations. Implicit models specify flexible densities; this induces very com-\nplex posterior distributions. Thus we would like a rich approximating family for the per-data point approximations q(zn |xn,β). This means the variational objective should only require that one can sample zn ∼ q(zn |xn,β) and not evaluate its density.\nOne variational objective meeting our desiderata is based on the classical minimization of the Kullback-Leibler (KL) divergence. (Surprisingly, Appendix C details how the KL is the only possible objective among a broad class.)"
    }, {
      "heading" : "3.1 KL Variational Objective",
      "text" : "Classical variational inference minimizes the KL divergence from the variational approximation q to the posterior. This is equivalent to maximizing the evidence lower bound (ELBO),\nL = Eq(β,z |x)[log p(x, z,β)− log q(β, z |x)]. (3)\nLet q factorize in the same way as the posterior,\nq(β, z |x) = q(β) N∏\nn=1\nq(zn |xn,β),\nwhere q(zn |xn,β) is an intractable density and since the data x is constant during inference, we drop conditioning for the global q(β). Substituting p and q’s factorization yields\nL = Eq(β)[log p(β)− log q(β)] + N∑\nn=1\nEq(β)q(zn |xn,β)[log p(xn, zn |β)− log q(zn |xn,β)].\nThis objective presents difficulties: the local densities p(xn, zn |β) and q(zn |xn,β) are both intractable. To solve this, we consider ratio estimation."
    }, {
      "heading" : "3.2 Ratio Estimation for the KL Objective",
      "text" : "Let q(xn) be the empirical distribution on the observations x and consider using it in a “variational joint” q(xn, zn |β) = q(xn)q(zn |xn,β). Now subtract the log empirical log q(xn) from the ELBO above. The ELBO reduces to\nL ∝ Eq(β)[log p(β)− log q(β)] + N∑\nn=1\nEq(β)q(zn |xn,β) [ log\np(xn, zn |β) q(xn, zn |β)\n] . (4)\n(Here the proportionality symbol means equality up to additive constants.) Thus the ELBO is a function of the ratio of two intractable densities. If we can form an estimator of this ratio, we can proceed with optimizing the ELBO.\nWe apply techniques for ratio estimation [49]. It is a key idea in GANs [35, 54], and similar ideas have rearisen in statistics and physics [19, 8]. In particular, we use class probability estimation: given a sample from p(·) or q(·) we aim to estimate the probability that it belongs to p(·). We model\nthis using σ(r(·;θ)), where r is a parameterized function (e.g., neural network) taking sample inputs and outputting a real value; σ is the logistic function outputting the probability.\nWe train r(·;θ) by minimizing a loss function known as a proper scoring rule [17]. For example, in experiments we use the log loss,\nDlog = Ep(xn,zn |β)[− log σ(r(xn, zn,β;θ))] + Eq(xn,zn |β)[− log(1− σ(r(xn, zn,β;θ)))]. (5)\nThe loss is zero if σ(r(·;θ)) returns 1 when a sample is from p(·) and 0 when a sample is from q(·). (We also experiment with the hinge loss; see § 4.) If r(·;θ) is sufficiently expressive, minimizing the loss returns the optimal function [35],\nr∗(xn, zn,β) = log p(xn, zn |β)− log q(xn, zn |β).\nAs we minimize Eq.5, we use r(·;θ) as a proxy to the log ratio in Eq.4. Note r estimates the log ratio; it’s of direct interest and more numerically stable than the ratio.\nThe gradient of Dlog with respect to θ is\nEp(xn,zn |β)[∇θ log σ(r(xn, zn,β;θ))] + Eq(xn,zn |β)[∇θ log(1− σ(r(xn, zn,β;θ)))]. (6)\nWe compute unbiased gradients with Monte Carlo."
    }, {
      "heading" : "3.3 Stochastic Gradients of the KL Objective",
      "text" : "To optimize the ELBO, we use the ratio estimator,\nL = Eq(β |x)[log p(β)− log q(β)] + N∑\nn=1\nEq(β |x)q(zn |xn,β)[r(xn, zn, β)]. (7)\nAll terms are now tractable. We can calculate gradients to optimize the variational family q. Below we assume the priors p(β), p(zn |β) are differentiable. (We discuss methods to handle discrete global variables in the next section.)\nWe focus on reparameterizable variational approximations [26, 46]. They enable sampling via a differentiable transformation T of random noise, δ ∼ s(·). Due to Eq.7, we require the global approximation q(β;λ) to admit a tractable density. With reparameterization, its sample is\nβ = Tglobal(δglobal;λ), δglobal ∼ s(·),\nfor a choice of transformation Tglobal(·;λ) and noise s(·). For example, setting s(·) = N (0, 1) and Tglobal(δglobal) = µ+ σδglobal induces a normal distribution N (µ, σ2). Similarly for the local variables zn, we specify\nzn = Tlocal(δn,xn,β;φ), δn ∼ s(·).\nUnlike the global approximation, the local variational density q(zn |xn;φ) need not be tractable: the ratio estimator relaxes this requirement. It lets us leverage implicit models not only for data but also for approximate posteriors. In practice, we also amortize computation with inference networks, sharing parameters φ across the per-data point approximate posteriors.\nThe gradient with respect to global parameters λ under this approximating family is\n∇λL = Es(δglobal)[∇λ(log p(β)− log q(β))]] + N∑\nn=1\nEs(δglobal)sn(δn)[∇λr(xn, zn, β)]. (8)\nThe gradient backpropagates through the local sampling zn = Tlocal(δn,xn,β;φ) and the global reparameterization β = Tglobal(δglobal;λ). We compute unbiased gradients with Monte Carlo. The gradient with respect to local parameters φ is\n∇φL = N∑\nn=1\nEq(β)s(δn)[∇φr(xn, zn,β)]. (9)\nwhere the gradient backpropagates through Tlocal.1\nAlgorithm 1: Likelihood-free variational inference (LFVI)\nInput : Model xn, zn ∼ p(· |β), p(β) Variational approximation zn ∼ q(· |xn,β;φ), q(β |x;λ), Ratio estimator r(·;θ) Output: Variational parameters λ, φ Initialize θ, λ, φ randomly. while not converged do\nCompute unbiased estimate of∇θD (Eq.6), ∇λL (Eq.8), ∇φL (Eq.9). Update θ, λ, φ using stochastic gradient descent.\nend"
    }, {
      "heading" : "3.4 Algorithm",
      "text" : "Algorithm 1 outlines the procedure. We call it likelihood-free variational inference (LFVI). LFVI is black box: it applies to models in which one can simulate data and local variables, and calculate densities for the global variables. LFVI first updates θ to improve the ratio estimator r. Then it uses r to update parameters {λ,φ} of the variational approximation q. We optimize r and q simultaneously. The algorithm is available in Edward [53].\nLFVI is scalable: we can unbiasedly estimate the gradient over the full data set with mini-batches [22]. The algorithm can also handle models of either continuous or discrete data. The requirement for differentiable global variables and reparameterizable global approximations can be relaxed using score function gradients [43].\nPoint estimates of the global parameters β suffice for many applications [18, 46]. Algorithm 1 can find point estimates: place a point mass approximation q on the parameters β. This simplifies gradients and corresponds to variational EM."
    }, {
      "heading" : "4 Experiments",
      "text" : "We developed new models and inference. For experiments, we study three applications: a largescale physical simulator for predator-prey populations in ecology; a Bayesian GAN for supervised classification; and a deep implicit model for symbol generation. In addition, Appendix F, provides practical advice on how to address the stability of the ratio estimator by analyzing a toy experiment. We initialize parameters from a standard normal and apply gradient descent with ADAM.\nLotka-Volterra Predator-Prey Simulator. We analyze the Lotka-Volterra simulator of § 2 and follow the same setup and hyperparameters of Papamakarios and Murray [38]. Its global variables β govern rates of change in a simulation of predator-prey populations. To infer them, we posit a mean-field normal approximation (reparameterized to be on the same support) and run Algorithm 1 with both a log loss and hinge loss for the ratio estimation problem; Appendix D details the hinge loss. We compare to rejection ABC, MCMC-ABC, and SMC-ABC [33]. MCMC-ABC uses a spherical Gaussian proposal; SMC-ABC is manually tuned with a decaying epsilon schedule; all ABC methods are tuned to use the best performing hyperparameters such as the tolerance error.\nFig. 2 displays results on two data sets. In the top figures and bottom left, we analyze data consisting of a simulation for T = 30 time steps, with recorded values of the populations every 0.2 time units. The bottom left figure calculates the negative log probability of the true parameters over the tolerance error for ABC methods; smaller tolerances result in more accuracy but slower runtime. The top figures compare the marginal posteriors for two parameters using the smallest tolerance for the ABC methods. Rejection ABC, MCMC-ABC, and SMC-ABC all contain the true parameters in their 95% credible interval but are less confident than our methods. Further, they required 100, 000 simulations from the model, with an acceptance rate of 0.004% and 2.990% for rejection ABC and MCMC-ABC respectively.\n1The ratio r indirectly depends on φ but its gradient w.r.t. φ disappears. This is derived via the score function identity and the product rule (see, e.g., Ranganath et al. [43, Appendix]).\nThe bottom right figure analyzes data consisting of 100, 000 time series, each of the same size as the single time series analyzed in the previous figures. This size is not possible with traditional methods. Further, we see that with our methods, the posterior concentrates near the truth. We also experienced little difference in accuracy between using the log loss or the hinge loss for ratio estimation.\nBayesian Generative Adversarial Networks. We analyze Bayesian GANs, described in § 2. Mimicking a use case of Bayesian neural networks [5, 21], we apply Bayesian GANs for classification on small to medium-size data. The GAN defines a conditional p(yn |xn), taking a feature xn ∈ RD as input and generating a label yn ∈ {1, . . . ,K}, via the process\nyn = g(xn, n |θ), n ∼ N (0, 1), (10) where g(· |θ) is a 2-layer multilayer perception with ReLU activations, batch normalization, and is parameterized by weights and biases θ. We place normal priors, θ ∼ N (0, 1). We analyze two choices of the variational model: one with a mean-field normal approximation for q(θ |x), and another with a point mass approximation (equivalent to maximum a posteriori). We compare to a Bayesian neural network, which uses the same generative process as Eq.10 but draws from a Categorical distribution rather than feeding noise into the neural net. We fit it separately using a mean-field normal approximation and maximum a posteriori. Table 1 shows that Bayesian GANs generally outperform their Bayesian neural net counterpart. Note that Bayesian GANs can analyze discrete data such as in generating a classification label. Traditional GANs for discrete data is an open challenge [27]. In Appendix E, we compare Bayesian GANs with point estimation to typical GANs. Bayesian GANs are also able to leverage parameter uncertainty for analyzing these small to medium-size data sets.\nOne problem with Bayesian GANs is that they cannot work with very large neural networks: the ratio estimator is a function of global parameters, and thus the input size grows with the size of the\n· · · · · ·\nxt−1 xt xt+1\nzt−1 zt zt+1\n(a) A deep implicit model for sequences. It is a recurrent neural network (RNN) with noise injected into each hidden state. The hidden state is now an implicit latent variable. The same occurs for generating outputs.\n1 −x+x/x∗∗x∗//x∗x+ 2 x/x∗x+x∗x/x+x+x+ 3 /+x∗x+x∗x/x/x+x+ 4 /x+∗x+x∗x/x+x−x+ 5 x/x∗x/x∗x+x+x+x− 6 x+x+x/x∗x∗x+x/x+ (b) Generated symbols from the implicit model. Good\nsamples place arithmetic operators between the variable x. The implicit model learned to follow rules from the context free grammar up to some multiple operator repeats.\nneural network. One approach is to make the ratio estimator not a function of the global parameters. Instead of optimizing model parameters via variational EM, we can train the model parameters by backpropagating through the ratio objective instead of the variational objective. An alternative is to use the hidden units as input which is much lower dimensional [51, Appendix C].\nInjecting Noise into Hidden Units. In this section, we show how to build a hierarchical implicit model by simply injecting randomness into hidden units. We model sequences x = (x1, . . . ,xT ) with a recurrent neural network. For t = 1, . . . , T ,\nzt = gz(xt−1, zt−1, t,z), t,z ∼ N (0, 1), xt = gx(zt, t,x), t,x ∼ N (0, 1),\nwhere gz and gx are both 1-layer multilayer perceptions with ReLU activation and layer normalization. We place standard normal priors over all weights and biases. See Fig. 3a.\nIf the injected noise t,z combines linearly with the output of gz , the induced distribution p(zt |xt−1, zt−1) is Gaussian parameterized by that output. This defines a stochastic RNN [2, 14], which generalizes its deterministic connection. With nonlinear combinations, the implicit density is more flexible (and intractable), making previous methods for inference not applicable. In our method, we perform variational inference and specify q to be implicit; we use the same architecture as the probability model’s implicit priors.\nWe follow the same setup and hyperparameters as Kusner and Hernández-Lobato [27] and generate simple one-variable arithmetic sequences following a context free grammar,\nS → x‖S + S‖S − S‖S ∗ S‖S/S, where ‖ divides possible productions of the grammar. We concatenate the inputs and point estimate the global variables (model parameters) using variational EM. Fig. 3b displays samples from the inferred model, training on sequences with a maximum of 15 symbols. It achieves sequences which roughly follow the context free grammar."
    }, {
      "heading" : "5 Discussion",
      "text" : "We developed a class of hierarchical implicit models and likelihood-free variational inference, merging the idea of implicit densities with hierarchical Bayesian modeling and approximate posterior inference. This expands Bayesian analysis with the ability to apply neural samplers, physical simulators, and their combination with rich, interpretable latent structure.\nMore stable inference with ratio estimation is an open challenge. This is especially important when we analyze large-scale real world applications of implicit models. Recent work for genomics offers a promising solution [51].\nAcknowledgements. We thank Balaji Lakshminarayanan for discussions which helped motivate this work. We also thank Christian Naesseth, Jaan Altosaar, and Adji Dieng for their feedback and comments. DT is supported by a Google Ph.D. Fellowship in Machine Learning and an Adobe Research Fellowship. This work is also supported by NSF IIS-0745520, IIS-1247664, IIS1009542, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009, N66001-15-C-4032, Facebook, Adobe, Amazon, and the John Templeton Foundation."
    } ],
    "references" : [ {
      "title" : "The totem experiment at the CERN large Hadron collider",
      "author" : [ "G. Anelli", "G. Antchev", "P. Aspell", "V. Avati", "M. Bagliesi", "V. Berardi", "M. Berretti", "V. Boccone", "U. Bottigli", "M Bozzo" ],
      "venue" : "Journal of Instrumentation,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Learning stochastic recurrent networks. arXiv preprint arXiv:1411.7610",
      "author" : [ "J. Bayer", "C. Osendorfer" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Approximate Bayesian computation in evolution and ecology",
      "author" : [ "M.A. Beaumont" ],
      "venue" : "Annual Review of Ecology, Evolution and Systematics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Weight uncertainty in neural network",
      "author" : [ "C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "L. Bottou" ],
      "venue" : "In Proceedings of COMPSTAT’2010,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Approximating likelihood ratios with calibrated discriminative classifiers",
      "author" : [ "K. Cranmer", "J. Pavez", "G. Louppe" ],
      "venue" : "arXiv preprint arXiv:1506.02169",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Dynamical bias in the coin toss",
      "author" : [ "P. Diaconis", "S. Holmes", "R. Montgomery" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "The χ-Divergence for Approximate Inference",
      "author" : [ "A.B. Dieng", "D. Tran", "R. Ranganath", "J. Paisley", "D.M. Blei" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2017
    }, {
      "title" : "Monte Carlo methods of inference for implicit statistical models",
      "author" : [ "P.J. Diggle", "R.J. Gratton" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Methodological),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1984
    }, {
      "title" : "Adversarial feature learning",
      "author" : [ "J. Donahue", "P. Krähenbühl", "T. Darrell" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2017
    }, {
      "title" : "Adversarially learned inference",
      "author" : [ "V. Dumoulin", "I. Belghazi", "B. Poole", "A. Lamb", "M. Arjovsky", "O. Mastropietro", "A. Courville" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2017
    }, {
      "title" : "Sequential neural models with stochastic layers",
      "author" : [ "M. Fraccaro", "S.K. Sønderby", "U. Paquet", "O. Winther" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Bayesian data analysis. Texts in Statistical Science Series",
      "author" : [ "A. Gelman", "J.B. Carlin", "H.S. Stern", "D.B. Dunson", "A. Vehtari", "D.B. Rubin" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Data analysis using regression and multilevel/hierarchical models",
      "author" : [ "A. Gelman", "J. Hill" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2006
    }, {
      "title" : "Strictly proper scoring rules, prediction, and estimation",
      "author" : [ "T. Gneiting", "A.E. Raftery" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Statistical Inference of Intractable Generative Models via Classification",
      "author" : [ "M.U. Gutmann", "R. Dutta", "S. Kaski", "J. Corander" ],
      "venue" : "arXiv preprint arXiv:1407.4981",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Statistical inference for stochastic simulation models–theory and application",
      "author" : [ "F. Hartig", "J.M. Calabrese", "B. Reineking", "T. Wiegand", "A. Huth" ],
      "venue" : "Ecology Letters,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Black-box α-divergence minimization",
      "author" : [ "J.M. Hernández-Lobato", "Y. Li", "M. Rowland", "D. Hernández-Lobato", "T. Bui", "R.E. Turner" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Stochastic variational inference",
      "author" : [ "M.D. Hoffman", "D.M. Blei", "C. Wang", "J.W. Paisley" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul" ],
      "venue" : "Machine Learning",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1999
    }, {
      "title" : "Adversarial message passing for graphical models",
      "author" : [ "T. Karaletsos" ],
      "venue" : "In NIPS Workshop",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "The probability of heads",
      "author" : [ "J.B. Keller" ],
      "venue" : "The American Mathematical Monthly,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1986
    }, {
      "title" : "Auto-Encoding Variational Bayes",
      "author" : [ "D.P. Kingma", "M. Welling" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "GANs for sequences of discrete elements with the Gumbel-Softmax distribution",
      "author" : [ "M.J. Kusner", "J.M. Hernández-Lobato" ],
      "venue" : "In NIPS Workshop",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Autoencoding beyond pixels using a learned similarity metric",
      "author" : [ "A.B.L. Larsen", "S.K. Sønderby", "H. Larochelle", "O. Winther" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "Rényi Divergence Variational Inference",
      "author" : [ "Y. Li", "R.E. Turner" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Two methods for wild variational inference. arXiv preprint arXiv:1612.00081",
      "author" : [ "Q. Liu", "Y. Feng" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Bayesian methods for adaptive models",
      "author" : [ "D.J.C. MacKay" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1992
    }, {
      "title" : "Approximate Bayesian computational methods",
      "author" : [ "Marin", "J.-M", "P. Pudlo", "C.P. Robert", "R.J. Ryder" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2012
    }, {
      "title" : "Adversarial variational Bayes: Unifying variational autoencoders and generative adversarial networks. arXiv preprint arXiv:1701.04722",
      "author" : [ "L. Mescheder", "S. Nowozin", "A. Geiger" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2017
    }, {
      "title" : "Learning in implicit generative models. arXiv preprint arXiv:1610.03483",
      "author" : [ "S. Mohamed", "B. Lakshminarayanan" ],
      "venue" : null,
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2016
    }, {
      "title" : "Machine Learning: A Probabilistic Perspective",
      "author" : [ "K. Murphy" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2012
    }, {
      "title" : "Bayesian Learning for Neural Networks",
      "author" : [ "R.M. Neal" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1994
    }, {
      "title" : "Fast -free inference of simulation models with Bayesian conditional density estimation",
      "author" : [ "G. Papamakarios", "I. Murray" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2016
    }, {
      "title" : "Population growth of human Y chromosomes: a study of Y chromosome microsatellites",
      "author" : [ "J.K. Pritchard", "M.T. Seielstad", "A. Perez-Lezaun", "M.W. Feldman" ],
      "venue" : "Molecular Biology and Evolution,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 1999
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "A. Radford", "L. Metz", "S. Chintala" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2016
    }, {
      "title" : "Operator variational inference",
      "author" : [ "R. Ranganath", "J. Altosaar", "D. Tran", "D.M. Blei" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2016
    }, {
      "title" : "Black box variational inference",
      "author" : [ "R. Ranganath", "S. Gerrish", "D.M. Blei" ],
      "venue" : "In Artificial Intelligence and Statistics",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2014
    }, {
      "title" : "Hierarchical variational models",
      "author" : [ "R. Ranganath", "D. Tran", "D.M. Blei" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2016
    }, {
      "title" : "Variational inference with normalizing flows",
      "author" : [ "D.J. Rezende", "S. Mohamed" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2015
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "D.J. Rezende", "S. Mohamed", "D. Wierstra" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2014
    }, {
      "title" : "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo",
      "author" : [ "R. Salakhutdinov", "A. Mnih" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2008
    }, {
      "title" : "Markov chain Monte Carlo and variational inference: Bridging the gap",
      "author" : [ "T. Salimans", "D.P. Kingma", "M. Welling" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2015
    }, {
      "title" : "Density-ratio matching under the Bregman divergence: A unified framework of density-ratio estimation",
      "author" : [ "M. Sugiyama", "T. Suzuki", "T. Kanamori" ],
      "venue" : "Annals of the Institute of Statistical Mathematics",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2012
    }, {
      "title" : "Hierarchical Bayesian nonparametric models with applications",
      "author" : [ "Y.W. Teh", "M.I. Jordan" ],
      "venue" : "Bayesian Nonparametrics,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2010
    }, {
      "title" : "Implicit causal models for genome-wide association studies",
      "author" : [ "D. Tran", "D.M. Blei" ],
      "venue" : "arXiv preprint arXiv:1710.10742",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2017
    }, {
      "title" : "Copula variational inference",
      "author" : [ "D. Tran", "D.M. Blei", "E.M. Airoldi" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2015
    }, {
      "title" : "Edward: A library for probabilistic modeling, inference, and criticism",
      "author" : [ "D. Tran", "A. Kucukelbir", "A.B. Dieng", "M. Rudolph", "D. Liang", "D.M. Blei" ],
      "venue" : "arXiv preprint arXiv:1610.09787",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets from a density ratio estimation perspective",
      "author" : [ "M. Uehara", "I. Sato", "M. Suzuki", "K. Nakayama", "Y. Matsuo" ],
      "venue" : "arXiv preprint arXiv:1610.02920",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2016
    }, {
      "title" : "Stochastic modelling for systems biology",
      "author" : [ "D.J. Wilkinson" ],
      "venue" : "CRC press",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "With probabilistic models, one typically posits a latent probability, and supposes each toss is a Bernoulli outcome given this probability [36, 15].",
      "startOffset" : 139,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "With probabilistic models, one typically posits a latent probability, and supposes each toss is a Bernoulli outcome given this probability [36, 15].",
      "startOffset" : 139,
      "endOffset" : 147
    }, {
      "referenceID" : 23,
      "context" : "However, we know from the laws of physics that the outcome of a coin toss is fully determined by its initial conditions (say, the impulse and angle of flip) [25, 9].",
      "startOffset" : 157,
      "endOffset" : 164
    }, {
      "referenceID" : 7,
      "context" : "However, we know from the laws of physics that the outcome of a coin toss is fully determined by its initial conditions (say, the impulse and angle of flip) [25, 9].",
      "startOffset" : 157,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "Furthermore the model is implicit, also known as a simulator: we can sample data from its generative process, but we may not have access to calculate its density [11, 20].",
      "startOffset" : 162,
      "endOffset" : 170
    }, {
      "referenceID" : 18,
      "context" : "Furthermore the model is implicit, also known as a simulator: we can sample data from its generative process, but we may not have access to calculate its density [11, 20].",
      "startOffset" : 162,
      "endOffset" : 170
    }, {
      "referenceID" : 36,
      "context" : "These models, which capture the laws and theories of real-world physical systems, pervade fields such as population genetics [40], statistical physics [1], and ecology [3]; they underlie structural equation models in economics and causality [39]; and they connect deeply to generative adversarial networks (GANs) [18], which use neural networks to specify a flexible implicit density [35].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "These models, which capture the laws and theories of real-world physical systems, pervade fields such as population genetics [40], statistical physics [1], and ecology [3]; they underlie structural equation models in economics and causality [39]; and they connect deeply to generative adversarial networks (GANs) [18], which use neural networks to specify a flexible implicit density [35].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 2,
      "context" : "These models, which capture the laws and theories of real-world physical systems, pervade fields such as population genetics [40], statistical physics [1], and ecology [3]; they underlie structural equation models in economics and causality [39]; and they connect deeply to generative adversarial networks (GANs) [18], which use neural networks to specify a flexible implicit density [35].",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 16,
      "context" : "These models, which capture the laws and theories of real-world physical systems, pervade fields such as population genetics [40], statistical physics [1], and ecology [3]; they underlie structural equation models in economics and causality [39]; and they connect deeply to generative adversarial networks (GANs) [18], which use neural networks to specify a flexible implicit density [35].",
      "startOffset" : 313,
      "endOffset" : 317
    }, {
      "referenceID" : 32,
      "context" : "These models, which capture the laws and theories of real-world physical systems, pervade fields such as population genetics [40], statistical physics [1], and ecology [3]; they underlie structural equation models in economics and causality [39]; and they connect deeply to generative adversarial networks (GANs) [18], which use neural networks to specify a flexible implicit density [35].",
      "startOffset" : 384,
      "endOffset" : 388
    }, {
      "referenceID" : 25,
      "context" : "We can also apply them to discrete data; this setting is not possible with traditional estimation algorithms for GANs [27].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 45,
      "context" : "For inference, § 3 develops likelihood-free variational inference (LFVI), which combines variational inference with density ratio estimation [49, 35].",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 32,
      "context" : "For inference, § 3 develops likelihood-free variational inference (LFVI), which combines variational inference with density ratio estimation [49, 35].",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 21,
      "context" : "Variational inference posits a family of distributions over latent variables and then optimizes to find the member closest to the posterior [23].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "First, we analyze a classical problem from the approximate Bayesian computation (ABC) literature, where the model simulates an ecological system [3].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "The first is Bayesian inference for implicit models, known in the statistics literature as approximate Bayesian computation (ABC) [3, 33].",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 30,
      "context" : "The first is Bayesian inference for implicit models, known in the statistics literature as approximate Bayesian computation (ABC) [3, 33].",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 26,
      "context" : "[28] propose a hybrid of variational methods and GANs for improved reconstruction.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[7] apply information penalties to disentangle factors of variation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "[13] propose to match on an augmented space, simultaneously training the model and an inverse mapping from data to noise.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "The final thread is variational inference with expressive approximations [45, 48, 52].",
      "startOffset" : 73,
      "endOffset" : 85
    }, {
      "referenceID" : 44,
      "context" : "The final thread is variational inference with expressive approximations [45, 48, 52].",
      "startOffset" : 73,
      "endOffset" : 85
    }, {
      "referenceID" : 48,
      "context" : "The final thread is variational inference with expressive approximations [45, 48, 52].",
      "startOffset" : 73,
      "endOffset" : 85
    }, {
      "referenceID" : 38,
      "context" : "Further advances have analyzed variational programs [42]—a family of approximations which only requires a process returning samples—and which has seen further interest [30].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 28,
      "context" : "Further advances have analyzed variational programs [42]—a family of approximations which only requires a process returning samples—and which has seen further interest [30].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 31,
      "context" : "Implicit-like variational approximations have also appeared in auto-encoder frameworks [32, 34] and message passing [24].",
      "startOffset" : 87,
      "endOffset" : 95
    }, {
      "referenceID" : 22,
      "context" : "Implicit-like variational approximations have also appeared in auto-encoder frameworks [32, 34] and message passing [24].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 14,
      "context" : "Hierarchical models play an important role in sharing statistical strength across examples [16].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 43,
      "context" : "With hierarchical models, local variables can be used for clustering in mixture models, mixed memberships in topic models [4], and factors in probabilistic matrix factorization [47].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 14,
      "context" : "Global variables can be used to pool information across data points for hierarchical regression [16], topic models [4], and Bayesian nonparametrics [50].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 46,
      "context" : "Global variables can be used to pool information across data points for hierarchical regression [16], topic models [4], and Bayesian nonparametrics [50].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 18,
      "context" : "But many likelihoods of interest, such as simulator-based models [20] and generative adversarial networks [18], admit high",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 16,
      "context" : "But many likelihoods of interest, such as simulator-based models [20] and generative adversarial networks [18], admit high",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 51,
      "context" : "For example, in population ecology, the Lotka-Volterra model simulates predator-prey populations over time via a stochastic differential equation [55].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 16,
      "context" : "Generative adversarial networks (GANs) define an implicit model and a method for parameter estimation [18].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 37,
      "context" : "They are known to perform well on image generation [41].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 29,
      "context" : "Bayesian GANs enable modeling of parameter uncertainty and are inspired by Bayesian neural networks, which have been shown to improve the uncertainty and data efficiency of standard neural networks [31, 37].",
      "startOffset" : 198,
      "endOffset" : 206
    }, {
      "referenceID" : 34,
      "context" : "Bayesian GANs enable modeling of parameter uncertainty and are inspired by Bayesian neural networks, which have been shown to improve the uncertainty and data efficiency of standard neural networks [31, 37].",
      "startOffset" : 198,
      "endOffset" : 206
    }, {
      "referenceID" : 49,
      "context" : "We study Bayesian GANs in § 4; Appendix B provides example implementations in the Edward probabilistic programming language [53].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 38,
      "context" : "There are many choices of variational objectives that measure closeness [42, 29, 10].",
      "startOffset" : 72,
      "endOffset" : 84
    }, {
      "referenceID" : 27,
      "context" : "There are many choices of variational objectives that measure closeness [42, 29, 10].",
      "startOffset" : 72,
      "endOffset" : 84
    }, {
      "referenceID" : 8,
      "context" : "There are many choices of variational objectives that measure closeness [42, 29, 10].",
      "startOffset" : 72,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "Machine learning hinges on stochastic optimization to scale to massive data [6].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 45,
      "context" : "We apply techniques for ratio estimation [49].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 32,
      "context" : "It is a key idea in GANs [35, 54], and similar ideas have rearisen in statistics and physics [19, 8].",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 50,
      "context" : "It is a key idea in GANs [35, 54], and similar ideas have rearisen in statistics and physics [19, 8].",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "It is a key idea in GANs [35, 54], and similar ideas have rearisen in statistics and physics [19, 8].",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "It is a key idea in GANs [35, 54], and similar ideas have rearisen in statistics and physics [19, 8].",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 15,
      "context" : "We train r(·;θ) by minimizing a loss function known as a proper scoring rule [17].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 32,
      "context" : ") If r(·;θ) is sufficiently expressive, minimizing the loss returns the optimal function [35], r(xn, zn,β) = log p(xn, zn |β)− log q(xn, zn |β).",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 24,
      "context" : ") We focus on reparameterizable variational approximations [26, 46].",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 42,
      "context" : ") We focus on reparameterizable variational approximations [26, 46].",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 49,
      "context" : "The algorithm is available in Edward [53].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "LFVI is scalable: we can unbiasedly estimate the gradient over the full data set with mini-batches [22].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 39,
      "context" : "The requirement for differentiable global variables and reparameterizable global approximations can be relaxed using score function gradients [43].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 16,
      "context" : "Point estimates of the global parameters β suffice for many applications [18, 46].",
      "startOffset" : 73,
      "endOffset" : 81
    }, {
      "referenceID" : 42,
      "context" : "Point estimates of the global parameters β suffice for many applications [18, 46].",
      "startOffset" : 73,
      "endOffset" : 81
    }, {
      "referenceID" : 35,
      "context" : "We analyze the Lotka-Volterra simulator of § 2 and follow the same setup and hyperparameters of Papamakarios and Murray [38].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 30,
      "context" : "We compare to rejection ABC, MCMC-ABC, and SMC-ABC [33].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "Mimicking a use case of Bayesian neural networks [5, 21], we apply Bayesian GANs for classification on small to medium-size data.",
      "startOffset" : 49,
      "endOffset" : 56
    }, {
      "referenceID" : 19,
      "context" : "Mimicking a use case of Bayesian neural networks [5, 21], we apply Bayesian GANs for classification on small to medium-size data.",
      "startOffset" : 49,
      "endOffset" : 56
    }, {
      "referenceID" : 25,
      "context" : "Traditional GANs for discrete data is an open challenge [27].",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "This defines a stochastic RNN [2, 14], which generalizes its deterministic connection.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "This defines a stochastic RNN [2, 14], which generalizes its deterministic connection.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 25,
      "context" : "We follow the same setup and hyperparameters as Kusner and Hernández-Lobato [27] and generate simple one-variable arithmetic sequences following a context free grammar, S → x‖S + S‖S − S‖S ∗ S‖S/S, where ‖ divides possible productions of the grammar.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 47,
      "context" : "Recent work for genomics offers a promising solution [51].",
      "startOffset" : 53,
      "endOffset" : 57
    } ],
    "year" : 2017,
    "abstractText" : "Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model’s flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.",
    "creator" : null
  }
}