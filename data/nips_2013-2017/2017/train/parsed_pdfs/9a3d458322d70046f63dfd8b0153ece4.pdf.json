{
  "name" : "9a3d458322d70046f63dfd8b0153ece4.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network",
    "authors" : [ "Lixin Fan" ],
    "emails" : [ "lixin.fan@nokia.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Since early 1990s the integration of fuzzy logic and computational neural networks has given birth to the fuzzy neural networks (FNN) [1]. While the formal fuzzy set theory provides a strict mathematical framework in which vague conceptual phenomena can be precisely and rigorously studied [2, 3, 4, 5], application-oriented fuzzy technologies lag far behind theoretical studies. In particular, fuzzy neural networks have only demonstrated limited successes on some toy examples such as [6, 7]. In order to catch up with the rapid advances in recent neural network developments, especially those with deep layered structures, it is the goal of this paper to demonstrate the relevance of FNN, and moreover, to provide a novel view on its non-fuzzy counterparts.\nOur revisiting of FNN is not merely for the fond remembrances of the golden age of “soft computing” [8]. Instead it provides a novel and theoretically justified perspective of neural computing, in which we are able to re-examine and demystify some useful techniques that were proposed to improve either effectiveness or efficiency of neural networks training processes. Among many others, batch normalization (BN) [9] is probably the most influential yet mysterious trick, that significantly improved the training efficiency by adapting to the change in the distribution of layers’ inputs (coined as internal covariate shift). Such kind of adaptations, when viewed within the fuzzy neural network framework, can be interpreted as rectifications to the deficiencies of neuron outputs with respect to the rightful generalized hamming distance (see definition 1) between inputs and neuron weights. Once\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nthe appropriate rectification is applied , the ill effects of internal covariate shift are automatically eradicated, and consequently, one is able to enjoy the fast training process without resorting to a sophisticated learning method used by BN.\nAnother crucial component in neural computing, Rectified linear unit (ReLU), has been widely used due to its strong biological motivations and mathematical justifications [10, 11, 12]. We show that within the generalized hamming group endowed with generalized hamming distance, ReLU can be regarded as setting a minimal hamming distance threshold between network input and neuron weights. This novel view immediately leads us to an effective double-thresholding scheme to suppress fuzzy elements in the generalized hamming group.\nThe proposed generalized hamming network (GHN) forms its foundation on the cornerstone notion of generalized hamming distance (GHD), which is essentially defined as h(x,w) := x+ w − 2xw for any x,w ∈ R (see definition 1). Its connection with the inferencing rule in neural computing is obvious: the last term (−2xw) corresponds to element-wise multiplications of neuron inputs and weights, and since we aim to measure the GHD between inputs x and weights w, the bias term then should take the value x+w. In this article we define any network that has its neuron outputs fulfilling this requirement (3) as a generalized hamming network. Since the underlying GHD induces a fuzzy XOR logic, GHN lends itself to rigorous analysis within the fuzzy logics theory (see definition 4). Apart from its theoretical appeals, GHN also demonstrates appealing features in terms of fast learning speed, well-controlled behaviour and simple parameter settings (see Section 4)."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Fuzzy logic and fuzzy neural network: the notion of fuzzy logic is based on the rejection of the fundamental principle of bivalence of classical logic i.e. any declarative sentence has only two possible truth values, true and false. Although the earliest connotation of fuzzy logic was attributed to Aristotle, the founder of classical logic [13], it was Zadeh’s publication in 1965 that ignited the enthusiasm about the theory of fuzzy sets [2]. Since then mathematical developments have advanced to a very high standard and are still forthcoming to day [3, 4, 5]. Fuzzy neural networks were proposed to take advantages of the flexible knowledge acquiring capability of neural networks [1, 14]. In theory it was proved that fuzzy systems and certain classes of neural networks are equivalent and convertible with each other [15, 16]. In practice, however, successful applications of FNNs are limited to some toy examples only [6, 7].\nDemystifying neural networks: efforts of interpreting neural networks by means of propositional logic dated back to McCulloch & Pitts’ seminial paper [17]. Recent research along this line include [18] and the references therein, in which First Order Logic (FOL) rules are encoded using soft logic on continuous truth values from the interval [0, 1]. These interpretations, albeit interesting, seldom explain effective neural network techniques such as batch normalization or ReLU. Recently [19] provided an improvement (and explanation) to batch normalization by removing dependencies in weight normalization between the examples in a minibatch.\nBinary-valued neural network: Restricted Boltzmann Machine (RBM) was used to model an “ensemble of binary vectors” and rose to prominence in the mid-2000s after fast learning algorithms were demonstrated by Hinton et. al. [20, 21]. Recent binarized neural network [22, 23] approximated standard CNNs by binarizing filter weights and/or inputs, with the aim to reduce computational complexity and memory consumption. The XNOR operation employed in [23] is limited to binary hamming distance and not readily applicable to non-binary neuron weights and inputs.\nEnsemble of binary patterns: the distributive property of GHD described in (1) provides an intriguing view on neural computing – even though real-valued pattens are involved in the computation, the computed GHD is strictly equivalent to the mean of binary hamming distances across two ensembles of binary patterns! This novel view illuminates the connection between generalized hamming networks and efficient binary features, that have long been used in various computer vision tasks, for instance, the celebrated Adaboost face detection[24], numerous binary features for key-point matching [25, 26] and binary codes for large database hashing [27, 28, 29, 30].\n−3 −2 −1 0 1 2 3 4 −3 −2 −1 0\n1 2 3 4\n−20\n−10\n0\n10\n20\nh(a, b)\n(a)\n−3 −2 −1 0 1 2 3 4 −3 −2\n−1 0 1 2 3 4 −1200 −1000 −800 −600 −400 −200\n0\nF(h(a, b))\n(b)\n−3 −2 −1 0 1 2 3 4 −3 −2 −1 0\n1 2 3 4\n0.2\n0.4\n0.6\n0.8\nμ(h(a, b))\n(c)\n−3 −2 −1 0 1 2 3 4 −3 −2 −1 0\n1 2\n3 4\n−0.015 −0.010 −0.005 0.000 0.005 0.010 0.015\n∂μ ∂a (h(a, b))\n(d)"
    }, {
      "heading" : "2 Generalized Hamming Distance",
      "text" : "(vi) Distributive property: let X̄M = (x1 + . . .xM )/M ∈ HL be element-wise arithmetic mean of a set of members xm ∈ HL, and ȲN be defined in the same vein. Then GHD is distributive:\nGL(X̄M ⊕L ȲN ) = 1 L L∑ l=1 x̄l ⊕ ȳl = 1 M 1 N 1 L M∑ m=1 N∑ n=1 L∑ l=1 xml ⊕ ynl\n= 1\nMN M∑ m=1 N∑ n=1 GL(xm ⊕L yn).\n(1)\nRemark: in case that xml , y n l ∈ {0, 1} i.e. for two sets of binary patterns, the mean of binary hamming distance between two sets can be efficiently computed as the GHD between two realvalued patterns X̄M , ȲN . Conversely, a real-valued pattern can be viewed as the element-wise average of an ensemble of binary patterns."
    }, {
      "heading" : "3 Generalized Hamming Network",
      "text" : "Despite the recent progresses in deep learning, artificial neural networks has long been criticized for its “black box” nature: “they capture hidden relations between inputs and outputs with a highly accurate approximation, but no definitive answer is offered for the question of how they work” [16]. In this section we provide an interpretation on neural computing by showing that, if the condition specified in (3) is fulfilled, outputs of each neuron can be strictly defined as the generalized hamming distance between inputs and weights. Moreover, the computations of GHD induces fuzzy implication of XOR connective, and therefore, the inferencing of entire network can be regarded as a logical calculus in the same vein as described in McCulloch & Pitts’ seminial paper [17]."
    }, {
      "heading" : "3.1 New perspective on neural computing",
      "text" : "The bearing of generalized hamming distance on neural computing is elucidated by looking at the negative of generalized hamming distance, (GHD, see definition 1), between inputs x ∈ HL and weights w ∈ HL in which L denotes the length of neuron weights e.g. in convolution kernels:\n−GL(w ⊕L x) = 2 L w · x− 1 L L∑ l=1 wl − 1 L L∑ l=1 xl (2)\nDivide (2) by the constant 2L and let\nb = −1 2 ( L∑ l=1 wl + L∑ l=1 xl )\n(3)\nthen it becomes the familiar form (w · x + b) of neuron outputs save the non-linear activation function. By enforcing the bias term to take the given value in (3), standard neuron outputs measure negatives of GHD between inputs and weights. Note that, for each layer, the bias term ∑L l=1 xl is\naveraged over neighbouring neurons in individual input image. The bias term ∑L\nl=1 wl is computed separately for each filter in fully connected or convolution layers. When weights are updated during the optimization, ∑L l=1 wl changes accordingly to keep up with weights and maintain stable neuron outputs. We discuss below (re-)interpretations of neural computing in terms of GHD.\nFuzzy inference: As illustrated in definition 4 GHD induces a fuzzy XOR connective. Therefore the negative of GHD quantifies the degree of equivalence between inputs x and weights w (see definition 4 of fuzzy XOR), i.e. the fuzzy truth value of the statement “x ↔ w” where↔ denotes a fuzzy equivalence relation. For GHD with multiple layers stacked together, neighbouring neuron outputs from the previous layer are integrated to form composite statements e.g. “(x11 ↔ w11, . . . ,x1i ↔ w1i ) ↔ w2j” where superscripts correspond to two layers. Thus stacked layers will form more complex, and hopefully more powerful, statements as the layer depth increases.\nBatch normalization demystified: When a mini-batch of training samples X = {x1, . . . ,xM} is involved in the computation, due to the distributive property of GHD, the data-dependent bias term L∑\nl=1\nxl equals the arithmetic mean of corresponding bias terms computed for each sample in the\nmini-batch i.e. 1M M∑\nm=1 L∑ l=1 xml . It is almost impossible to maintain a constant scalar b that fulfils\nthis requirement when mini-batch changes, especially at deep layers of the network whose inputs are influenced by weights of incoming layers. The celebrated batch normalization (BN) technique therefore proposed a learning method to compensate for the input vector change, with additional parameters γ, β to be learnt during the training [9]. It is our conjecture that batch normalization is approximating these rightful bias through optimization, and this connection is empirically revealed in Figure 2 with very similar neuron outputs obtained by BN and GHD. Indeed they are highly correlated during the course of training (with Pearson correlation coefficient=0.97), confirming our view that BN is attempting to influence the bias term according to (3).\nOnce b is enforced to follow (3), neither the optimization of bias terms nor the sophisticated learning method of BN is needed. In the following section we will illustrate a rectified neural network designed as such.\nRectified linear units (ReLU) redesigned: Due to its strong biological motivations [10] and mathematical justifications [11], rectified linear unit (ReLu) is the most popular activation function used for deep neural network [31]. If neuron outputs are rectified as the generalized hamming distances, the activation function max(0, 0.5− h(x,w)) then simply sets a minimal hamming distance threshold of 0.5 (see Figure 1). Astute readers may immediately spot two limitations of this activation function: a) it only takes into account the negative confidence region while disregards positive confidence regions; b) it allows elements in the fuzzy regime near 0.5 to misguide the optimization with their non-negligible gradients.\nA straightforward remedy to ReLU is to suppress elements within the fuzzy region by setting outputs between [0.5− r, 0.5 + r] to 0.5, where r is a parameter to control acceptable fuzziness in neuron outputs. In particular, we may set thresholds adaptively e.g. [0.5 − r · O, 0.5 + r · O] where O is the maximal magnitude of neuron outputs and the threshold ratio r is adjusted by the optimizer. This double-thresholding strategy effectively prevents noisy gradients of fuzzy elements, since 0.5 is a fixed point and x ⊕ 0.5 = 0.5 for any x. Empirically we found this scheme, in tandem with the rectification (3), dramatically boosts the training efficiency for challenging tasks such as CIFAR10/100 image classification. It must be noted that, however, the use of non-linear activation as such is not essential for GHD-based neural computing. When the double-thresholding is switched-off (by fixing r = 0), the learning is prolonged for challenging CIFAR10/100 image classification but its influence on the simple MNIST classification is almost negligible (see Section 4 for experimental results)."
    }, {
      "heading" : "3.2 Ganeralized hamming network with induced fuzzy XOR",
      "text" : "Definition 2. A generalized hamming network (GHN) is any networks consisting of neurons, whose outputs h ∈ HL are related to neuron inputs x ∈ HL and weights w ∈ HL by h = x⊕L w .\nRemark: In case that the bias term is computed directly from (3) such that h = x⊕L w is fulfilled strictly, the network is called a rectified GHN or simply a GHN. In other cases where bias terms are approximating the rightful offsets (e.g. by batch normalization [9]), the trained network is called an approximated GHN.\nCompared with traditional neural networks, the optimization of bias terms is no longer needed in GHN. Empirically, it is shown that the proposed GHN benefits from a fast and robust learning process that is on par with that of the batch-normalization approach, yet without resorting to sophisticated learning process of additional parameters (see Section 4 for experimental results). On the other hand, GHN also benefits from the rapid developments of neural computing techniques, in particular, those employing parallel computing on GPUs. Due to this efficient implementation of GHNs, it is the first time that fuzzy neural networks have demonstrated state-of-the-art performances on learning tasks with large scale datasets.\nOften neuron outputs are clamped by a logistic activation function to within the range [0, 1], so that outputs can be compared with the target labels in supervised learning. As shown below, GHD followed by such a non-linear activation actually induces a fuzzy XOR connective. We briefly review basic notion of fuzzy set used in our work and refer readers to [2, 32, 13] for thorough treatments and review of the topic. Definition 3. Fuzzy Set: Let X be an universal set of elements x ∈ X , then a fuzzy set A is a set of pairs: A := { ( x, µA(x) ) |x ∈ X,µA(x) ∈ I}, in which µA : X → I is called the membership function (or grade membership).\nRemark: In this work we let X be a Cartesian product of two sets X = P × U where P are (2D or 3D) collection of neural nodes and U are real numbers in ⊆ I or ⊆ R. We define the membership function µX(x) := µU (xp),∀x = (p, xp) ∈ X such that it is dependent on xp only. For the sake of brevity we abuse the notation and use µ(x), µX(x) and µU (xp) interchangeably. Definition 4. Induced fuzzy XOR: let two fuzzy set elements a, b ∈ U be assigned with respective grade or membership by a membership function µ : U → I : µ(a) = i, µ(b) = j, then the generalized hamming distance h(a, b) : U ×U → U induces a fuzzy XOR connective E : I× I → I whose membership function is given by\nµR(i, j) = µ(h(µ −1(i), µ−1(j))). (4)\nRemark: For the restricted case U = I the membership function can be trivially defined as the identity function µ = idI as proved in [4].\nRemark: For the generalized case where U = R, the fuzzy membership µ can be defined by a sigmoid function such as logistic, tanh or any function : U → I . In this work we adopt the logistic function µ(a) = 11+exp(0.5−a) and the resulting fuzzy XOR connective is given by following membership function:\nµR(i, j) = 1 1 + exp ( 0.5− µ−1(i)⊕ µ−1(j) ) , (5) where µ−1(a) = − ln( 1a − 1) + 1 2 is the inverse of µ(a). Following this analysis, it is possible to rigorously formulate neuron computing of the entire network according to inference rules of fuzzy logic theory (in the same vein as illustrated in [17]). Nevertheless, research along this line is out of the scope of the present article and will be reported elsewhere."
    }, {
      "heading" : "4 Performance evaluation",
      "text" : ""
    }, {
      "heading" : "4.1 A case study with MNIST image classification",
      "text" : "Overall performance: we tested a simple four-layered GHN (cv[1,5,5,16]-pool-cv[16,5,5,64]-poolfc[1024]-fc[1024,10]) on the MNIST dataset with 99.0% test accuracy obtained. For this relatively simple dataset, GHN is able to reach test accuracies above 0.95 with 1000 mini-batches and a learning rate 0.1. This learning speed is on par with that of the batch normalization (BN), but without resorting to the learning of additional parameters in BN. It was also observed a wide range of large learning rates (from 0.01 to 0.1) all resulted in similar final accuracies (see below). We ascribe this well-controlled robust learning behaviour to rectified bias terms enforced in GHNs.\nInfluence of learning rate: This experiment compares performances with different learning rates and Figure 3 (middle,right) show that a very large learning rate (0.1) leads to much faster learning without the risk of divergences. A small learning rate (0.01) suffice to guarantee the comparable final test accuracy. Therefore we set the learning rate to a constant 0.1 for all experiments unless stated otherwise.\nInfluence of non-linear double-thresholding: The non-linear double-thresholding can be turned off by setting the threshold ratio r = 0 (see texts in Section 3.1). Optionally the parameter r is automatically optimized together with the optimization of neuron weights. Figure 3 (left) shows that the GHN without non-linear activation (by setting r = 0) performs equally well as compared with the case where r is optimized (in Figure 3 left, right). There are no significant differences between two settings for this relative simple task."
    }, {
      "heading" : "4.2 CIFAR10/100 image classification",
      "text" : "In this experiment, we tested a six-layered GHN (cv[3,3,3,64]-cv[64,5,5,256]-pool-cv[256,5,5,256]pool-fc[1024]-fc[1024,512]-fc[1024,nclass]) on both CIFAR10 (nclass=10) and CIFAR100 (nclass=100) datasets. Figure 4 shows that the double-thresholding scheme improves the learning efficiency dramatically for these challenging image classification tasks: when the parameter r is optimized for each feature filter the numbers of iterations required to reach the same level of test accuracy are reduced by 1 to 2 orders of magnitudes. It must be noted that performances of such a simple generalized hamming network (89.3% for CIFAR10 and 60.1% for CIFAR100) are on par with many sophisticated networks reported in [33]. In our view, the rectified bias enforced by (3) can be readily applied to these sophisticated networks, although resulting improvements may vary and remain to be tested."
    }, {
      "heading" : "4.3 Generative modelling with Variational Autoencoder",
      "text" : "In this experiment, we tested the effect of rectification in GHN applied to a generative modelling setting. One crucial difference is that the objective is now to minimize reconstruction error instead of classification error. It turns out the double-thresholding scheme is no longer relevant for this setting and thus not used in the experiment.\nThe baseline network (784-400-400-20) used in this experiment is an improved implementation [34] of the influential paper [35], trained on the MNIST dataset of images of handwritten digits. We have rectified the outputs following (3) and, instead of optimizing the lower bound of the log marginal likelihood as in [35], we directly minimize the reconstruction error. Also we did not include weights regularization terms for the optimization as it is unnecessary for GHN. Figure 5 (left) illustrates the reconstruction error with respect to number of training steps (mini-batches). It is shown that the rectified generalized hamming network converges to a lower minimal reconstruction error as compared to the baseline network, with about 28% reduction. The rectification also leads to a faster convergence, which is in accordance with our observations in other experiments."
    }, {
      "heading" : "4.4 Sentence classification",
      "text" : "A simple CNN has been used for sentence-level classification tasks and excellent results were demonstrated on multiple benchmarks [36]. The baseline network used in this experiment is a re-implementation of [36] made available from [37]. Figure 5 (right) plots accuracy curves from both networks. It was observed that the rectified GHN did improve the learning speed, but did not improve the final accuracy as compared with the baseline network: both networks yielded the final evaluation accuracy around 74% despite that the training accuracy were almost 100%. The over-fitting in this experiment is probably due to the relatively small Movie Review dataset size with 10,662 example review sentences, half positive and half negative."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In summary, we proposed a rectified generalized hamming network (GHN) architecture which materializes a re-emerging principle of fuzzy logic inferencing. This principle has been extensively studied from a theoretic fuzzy logic point of view, but has been largely overlooked in the practical research of ANN. The rectified neural network derives fuzzy logic implications with underlying generalized hamming distances computed in neuron outputs. Bearing this rectified view in mind, we proposed to compute bias terms analytically without resorting to sophisticated learning methods such as batch normalization. Moreover, we have shown that, the rectified linear units (ReLU) was theoretically non-essential and could be skipped for some easy tasks. While for challenging classification problems, the double-thresholding scheme did improve the learning efficiency significantly.\nThe simple architecture of GHN, on the one hand, lends itself to being analysed rigorously and this follow up research will be reported elsewhere. On the other hand, GHN is the first fuzzy neural network of its kind that has demonstrated fast learning speed, well-controlled behaviour and stateof-the-art performances on a variety of learning tasks. By cross-checking existing networks against GHN, one is able to grasp the most essential ingredient of deep learning. It is our hope that this kind of comparative study will shed light on future deep learning research and eventually open the “black box” of artificial neural networks [16]."
    }, {
      "heading" : "Acknowledgement",
      "text" : "I am grateful to anonymous reviewers for their constructive comments to improve the quality of this paper. I greatly appreciate valuable discussions and supports from colleagues at Nokia Technologies."
    } ],
    "references" : [ {
      "title" : "Invited Review on the principles of fuzzy neural networks",
      "author" : [ "M M Gupta", "D H Rao" ],
      "venue" : "Fuzzy Sets and Systems, 61:1–18",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Fuzzy sets",
      "author" : [ "L.A. Zadeh" ],
      "venue" : "Information Control, 8:338–353",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "Fuzzy Implications and Inference Process",
      "author" : [ "József Tick", "János Fodor", "John Von Neumann" ],
      "venue" : "Computing and Informatics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "Xor-Implications and E-Implications: Classes of Fuzzy Implications Based on Fuzzy Xor",
      "author" : [ "Benjamín C Bedregal", "Renata H S Reiser", "Graçaliz P Dimuro" ],
      "venue" : "Electronic Notes in Theoretical Computer Science,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "On Zadeh’s intuitionistic fuzzy disjunction and conjunction",
      "author" : [ "Krassimir Atanassov" ],
      "venue" : "NIFS, 17(1):1–4,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Training Artificial Neural Networks for Fuzzy Logic",
      "author" : [ "Abhay B Ulsari" ],
      "venue" : "Complex Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1992
    }, {
      "title" : "fXOR fuzzy logic networks",
      "author" : [ "Witold Pedrycz", "Giancarlo Succi" ],
      "venue" : "Soft Computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2002
    }, {
      "title" : "Fuzzy set theory review",
      "author" : [ "H.-J Zimmermann" ],
      "venue" : "Advanced Review",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "editors, ICML,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "M",
      "author" : [ "R. Hahnloser", "R. Sarpeshkar" ],
      "venue" : "Mahowald, R.J. Douglas, H.S.Seung. Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit. 405",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Permitted and forbidden sets in symmetric threshold-linear networks",
      "author" : [ "H.S. Seung R Hahnloser" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2001
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Fuzzy Logic and Mathematics: A Historical Perspective",
      "author" : [ "R. Belohlavek", "J.W. Dauben", "G.J. Klir" ],
      "venue" : "Oxford University Press",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Fuzzy Neural Network Theory and Application",
      "author" : [ "P. Liu", "H.X. Li" ],
      "venue" : "Series in machine perception and artificial intelligence. World Scientific",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Functional equivalence between radial basis function networks and fuzzy inference systems",
      "author" : [ "Jyh-Shing Roger Jang", "Chuen-Tsai Sun" ],
      "venue" : "IEEE Trans. Neural Networks,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1993
    }, {
      "title" : "Are artificial neural networks black boxes",
      "author" : [ "José Manuel Benítez", "Juan Luis Castro", "Ignacio Requena" ],
      "venue" : "IEEE Trans. Neural Networks,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1997
    }, {
      "title" : "A logical calculus of ideas immanent in nervous activity",
      "author" : [ "Warren Mcculloch", "Walter Pitts" ],
      "venue" : "Bulletin of Mathematical Biophysics,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1943
    }, {
      "title" : "Harnessing deep neural networks with logic rules. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016",
      "author" : [ "Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard H. Hovy", "Eric P. Xing" ],
      "venue" : "Long Papers,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
      "author" : [ "Tim Salimans", "Diederik P. Kingma" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "Geoffrey Hinton", "Ruslan Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E. Hinton" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Binarized neural network: Training deep neural networks with weights and activations constrained to +1 or -1",
      "author" : [ "Matthieu Courbariaux", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Xnor-net: Imagenet classification using binary convolutional neural networks",
      "author" : [ "Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Robust real-time face detection",
      "author" : [ "Paul Viola", "Michael J. Jones" ],
      "venue" : "Int. J. Comput. Vision,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2004
    }, {
      "title" : "Brief: Binary robust independent elementary features",
      "author" : [ "Michael Calonder", "Vincent Lepetit", "Christoph Strecha", "Pascal Fua" ],
      "venue" : "In Proceedings of the 11th European Conference on Computer Vision: Part IV,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "Orb: An efficient alternative to sift or surf",
      "author" : [ "Ethan Rublee", "Vincent Rabaud", "Kurt Konolige", "Gary Bradski" ],
      "venue" : "In Proceedings of the 2011 International Conference on Computer Vision,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Learning to hash with binary reconstructive embeddings",
      "author" : [ "Brian Kulis", "Trevor Darrell" ],
      "venue" : "In Proceedings of the 22Nd International Conference on Neural Information Processing Systems,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2009
    }, {
      "title" : "Minimal loss hashing for compact binary codes",
      "author" : [ "Mohammad Norouzi", "David M. Blei" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2011
    }, {
      "title" : "Deep learning of binary hash codes for fast image retrieval",
      "author" : [ "Kevin Lin", "Huei-Fang Yang", "Jen-Hao Hsiao", "Chu-Song Chen" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Hamming distance metric learning",
      "author" : [ "Mohammad Norouzi", "David J Fleet", "Ruslan R Salakhutdinov" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2012
    }, {
      "title" : "Fuzzy Set Theory — and Its Applications",
      "author" : [ "H.-J. Zimmermann" ],
      "venue" : "Kluwer Academic Publishers, Norwell, MA, USA",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P. Kingma", "Max Welling" ],
      "venue" : "CoRR, abs/1312.6114,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2013
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : "CoRR, abs/1408.5882,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Since early 1990s the integration of fuzzy logic and computational neural networks has given birth to the fuzzy neural networks (FNN) [1].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "While the formal fuzzy set theory provides a strict mathematical framework in which vague conceptual phenomena can be precisely and rigorously studied [2, 3, 4, 5], application-oriented fuzzy technologies lag far behind theoretical studies.",
      "startOffset" : 151,
      "endOffset" : 163
    }, {
      "referenceID" : 2,
      "context" : "While the formal fuzzy set theory provides a strict mathematical framework in which vague conceptual phenomena can be precisely and rigorously studied [2, 3, 4, 5], application-oriented fuzzy technologies lag far behind theoretical studies.",
      "startOffset" : 151,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "While the formal fuzzy set theory provides a strict mathematical framework in which vague conceptual phenomena can be precisely and rigorously studied [2, 3, 4, 5], application-oriented fuzzy technologies lag far behind theoretical studies.",
      "startOffset" : 151,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "While the formal fuzzy set theory provides a strict mathematical framework in which vague conceptual phenomena can be precisely and rigorously studied [2, 3, 4, 5], application-oriented fuzzy technologies lag far behind theoretical studies.",
      "startOffset" : 151,
      "endOffset" : 163
    }, {
      "referenceID" : 5,
      "context" : "In particular, fuzzy neural networks have only demonstrated limited successes on some toy examples such as [6, 7].",
      "startOffset" : 107,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "In particular, fuzzy neural networks have only demonstrated limited successes on some toy examples such as [6, 7].",
      "startOffset" : 107,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : "Our revisiting of FNN is not merely for the fond remembrances of the golden age of “soft computing” [8].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "Among many others, batch normalization (BN) [9] is probably the most influential yet mysterious trick, that significantly improved the training efficiency by adapting to the change in the distribution of layers’ inputs (coined as internal covariate shift).",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "Another crucial component in neural computing, Rectified linear unit (ReLU), has been widely used due to its strong biological motivations and mathematical justifications [10, 11, 12].",
      "startOffset" : 171,
      "endOffset" : 183
    }, {
      "referenceID" : 10,
      "context" : "Another crucial component in neural computing, Rectified linear unit (ReLU), has been widely used due to its strong biological motivations and mathematical justifications [10, 11, 12].",
      "startOffset" : 171,
      "endOffset" : 183
    }, {
      "referenceID" : 11,
      "context" : "Another crucial component in neural computing, Rectified linear unit (ReLU), has been widely used due to its strong biological motivations and mathematical justifications [10, 11, 12].",
      "startOffset" : 171,
      "endOffset" : 183
    }, {
      "referenceID" : 12,
      "context" : "Although the earliest connotation of fuzzy logic was attributed to Aristotle, the founder of classical logic [13], it was Zadeh’s publication in 1965 that ignited the enthusiasm about the theory of fuzzy sets [2].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "Although the earliest connotation of fuzzy logic was attributed to Aristotle, the founder of classical logic [13], it was Zadeh’s publication in 1965 that ignited the enthusiasm about the theory of fuzzy sets [2].",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 2,
      "context" : "Since then mathematical developments have advanced to a very high standard and are still forthcoming to day [3, 4, 5].",
      "startOffset" : 108,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "Since then mathematical developments have advanced to a very high standard and are still forthcoming to day [3, 4, 5].",
      "startOffset" : 108,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Since then mathematical developments have advanced to a very high standard and are still forthcoming to day [3, 4, 5].",
      "startOffset" : 108,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "Fuzzy neural networks were proposed to take advantages of the flexible knowledge acquiring capability of neural networks [1, 14].",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "Fuzzy neural networks were proposed to take advantages of the flexible knowledge acquiring capability of neural networks [1, 14].",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 14,
      "context" : "In theory it was proved that fuzzy systems and certain classes of neural networks are equivalent and convertible with each other [15, 16].",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : "In theory it was proved that fuzzy systems and certain classes of neural networks are equivalent and convertible with each other [15, 16].",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "In practice, however, successful applications of FNNs are limited to some toy examples only [6, 7].",
      "startOffset" : 92,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "In practice, however, successful applications of FNNs are limited to some toy examples only [6, 7].",
      "startOffset" : 92,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "Demystifying neural networks: efforts of interpreting neural networks by means of propositional logic dated back to McCulloch & Pitts’ seminial paper [17].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 17,
      "context" : "Recent research along this line include [18] and the references therein, in which First Order Logic (FOL) rules are encoded using soft logic on continuous truth values from the interval [0, 1].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : "Recently [19] provided an improvement (and explanation) to batch normalization by removing dependencies in weight normalization between the examples in a minibatch.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 21,
      "context" : "Recent binarized neural network [22, 23] approximated standard CNNs by binarizing filter weights and/or inputs, with the aim to reduce computational complexity and memory consumption.",
      "startOffset" : 32,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : "Recent binarized neural network [22, 23] approximated standard CNNs by binarizing filter weights and/or inputs, with the aim to reduce computational complexity and memory consumption.",
      "startOffset" : 32,
      "endOffset" : 40
    }, {
      "referenceID" : 22,
      "context" : "The XNOR operation employed in [23] is limited to binary hamming distance and not readily applicable to non-binary neuron weights and inputs.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 23,
      "context" : "Ensemble of binary patterns: the distributive property of GHD described in (1) provides an intriguing view on neural computing – even though real-valued pattens are involved in the computation, the computed GHD is strictly equivalent to the mean of binary hamming distances across two ensembles of binary patterns! This novel view illuminates the connection between generalized hamming networks and efficient binary features, that have long been used in various computer vision tasks, for instance, the celebrated Adaboost face detection[24], numerous binary features for key-point matching [25, 26] and binary codes for large database hashing [27, 28, 29, 30].",
      "startOffset" : 537,
      "endOffset" : 541
    }, {
      "referenceID" : 24,
      "context" : "Ensemble of binary patterns: the distributive property of GHD described in (1) provides an intriguing view on neural computing – even though real-valued pattens are involved in the computation, the computed GHD is strictly equivalent to the mean of binary hamming distances across two ensembles of binary patterns! This novel view illuminates the connection between generalized hamming networks and efficient binary features, that have long been used in various computer vision tasks, for instance, the celebrated Adaboost face detection[24], numerous binary features for key-point matching [25, 26] and binary codes for large database hashing [27, 28, 29, 30].",
      "startOffset" : 591,
      "endOffset" : 599
    }, {
      "referenceID" : 25,
      "context" : "Ensemble of binary patterns: the distributive property of GHD described in (1) provides an intriguing view on neural computing – even though real-valued pattens are involved in the computation, the computed GHD is strictly equivalent to the mean of binary hamming distances across two ensembles of binary patterns! This novel view illuminates the connection between generalized hamming networks and efficient binary features, that have long been used in various computer vision tasks, for instance, the celebrated Adaboost face detection[24], numerous binary features for key-point matching [25, 26] and binary codes for large database hashing [27, 28, 29, 30].",
      "startOffset" : 591,
      "endOffset" : 599
    }, {
      "referenceID" : 26,
      "context" : "Ensemble of binary patterns: the distributive property of GHD described in (1) provides an intriguing view on neural computing – even though real-valued pattens are involved in the computation, the computed GHD is strictly equivalent to the mean of binary hamming distances across two ensembles of binary patterns! This novel view illuminates the connection between generalized hamming networks and efficient binary features, that have long been used in various computer vision tasks, for instance, the celebrated Adaboost face detection[24], numerous binary features for key-point matching [25, 26] and binary codes for large database hashing [27, 28, 29, 30].",
      "startOffset" : 644,
      "endOffset" : 660
    }, {
      "referenceID" : 27,
      "context" : "Ensemble of binary patterns: the distributive property of GHD described in (1) provides an intriguing view on neural computing – even though real-valued pattens are involved in the computation, the computed GHD is strictly equivalent to the mean of binary hamming distances across two ensembles of binary patterns! This novel view illuminates the connection between generalized hamming networks and efficient binary features, that have long been used in various computer vision tasks, for instance, the celebrated Adaboost face detection[24], numerous binary features for key-point matching [25, 26] and binary codes for large database hashing [27, 28, 29, 30].",
      "startOffset" : 644,
      "endOffset" : 660
    }, {
      "referenceID" : 28,
      "context" : "Ensemble of binary patterns: the distributive property of GHD described in (1) provides an intriguing view on neural computing – even though real-valued pattens are involved in the computation, the computed GHD is strictly equivalent to the mean of binary hamming distances across two ensembles of binary patterns! This novel view illuminates the connection between generalized hamming networks and efficient binary features, that have long been used in various computer vision tasks, for instance, the celebrated Adaboost face detection[24], numerous binary features for key-point matching [25, 26] and binary codes for large database hashing [27, 28, 29, 30].",
      "startOffset" : 644,
      "endOffset" : 660
    }, {
      "referenceID" : 29,
      "context" : "Ensemble of binary patterns: the distributive property of GHD described in (1) provides an intriguing view on neural computing – even though real-valued pattens are involved in the computation, the computed GHD is strictly equivalent to the mean of binary hamming distances across two ensembles of binary patterns! This novel view illuminates the connection between generalized hamming networks and efficient binary features, that have long been used in various computer vision tasks, for instance, the celebrated Adaboost face detection[24], numerous binary features for key-point matching [25, 26] and binary codes for large database hashing [27, 28, 29, 30].",
      "startOffset" : 644,
      "endOffset" : 660
    }, {
      "referenceID" : 15,
      "context" : "Despite the recent progresses in deep learning, artificial neural networks has long been criticized for its “black box” nature: “they capture hidden relations between inputs and outputs with a highly accurate approximation, but no definitive answer is offered for the question of how they work” [16].",
      "startOffset" : 295,
      "endOffset" : 299
    }, {
      "referenceID" : 16,
      "context" : "Moreover, the computations of GHD induces fuzzy implication of XOR connective, and therefore, the inferencing of entire network can be regarded as a logical calculus in the same vein as described in McCulloch & Pitts’ seminial paper [17].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 8,
      "context" : "The celebrated batch normalization (BN) technique therefore proposed a learning method to compensate for the input vector change, with additional parameters γ, β to be learnt during the training [9].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 9,
      "context" : "Rectified linear units (ReLU) redesigned: Due to its strong biological motivations [10] and mathematical justifications [11], rectified linear unit (ReLu) is the most popular activation function used for deep neural network [31].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "Rectified linear units (ReLU) redesigned: Due to its strong biological motivations [10] and mathematical justifications [11], rectified linear unit (ReLu) is the most popular activation function used for deep neural network [31].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "by batch normalization [9]), the trained network is called an approximated GHN.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "We briefly review basic notion of fuzzy set used in our work and refer readers to [2, 32, 13] for thorough treatments and review of the topic.",
      "startOffset" : 82,
      "endOffset" : 93
    }, {
      "referenceID" : 30,
      "context" : "We briefly review basic notion of fuzzy set used in our work and refer readers to [2, 32, 13] for thorough treatments and review of the topic.",
      "startOffset" : 82,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "We briefly review basic notion of fuzzy set used in our work and refer readers to [2, 32, 13] for thorough treatments and review of the topic.",
      "startOffset" : 82,
      "endOffset" : 93
    }, {
      "referenceID" : 3,
      "context" : "Remark: For the restricted case U = I the membership function can be trivially defined as the identity function μ = idI as proved in [4].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "Following this analysis, it is possible to rigorously formulate neuron computing of the entire network according to inference rules of fuzzy logic theory (in the same vein as illustrated in [17]).",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 0,
      "context" : "1 A case study with MNIST image classification Overall performance: we tested a simple four-layered GHN (cv[1,5,5,16]-pool-cv[16,5,5,64]-poolfc[1024]-fc[1024,10]) on the MNIST dataset with 99.",
      "startOffset" : 107,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "1 A case study with MNIST image classification Overall performance: we tested a simple four-layered GHN (cv[1,5,5,16]-pool-cv[16,5,5,64]-poolfc[1024]-fc[1024,10]) on the MNIST dataset with 99.",
      "startOffset" : 107,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "1 A case study with MNIST image classification Overall performance: we tested a simple four-layered GHN (cv[1,5,5,16]-pool-cv[16,5,5,64]-poolfc[1024]-fc[1024,10]) on the MNIST dataset with 99.",
      "startOffset" : 107,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "1 A case study with MNIST image classification Overall performance: we tested a simple four-layered GHN (cv[1,5,5,16]-pool-cv[16,5,5,64]-poolfc[1024]-fc[1024,10]) on the MNIST dataset with 99.",
      "startOffset" : 107,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "1 A case study with MNIST image classification Overall performance: we tested a simple four-layered GHN (cv[1,5,5,16]-pool-cv[16,5,5,64]-poolfc[1024]-fc[1024,10]) on the MNIST dataset with 99.",
      "startOffset" : 125,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "1 A case study with MNIST image classification Overall performance: we tested a simple four-layered GHN (cv[1,5,5,16]-pool-cv[16,5,5,64]-poolfc[1024]-fc[1024,10]) on the MNIST dataset with 99.",
      "startOffset" : 125,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "1 A case study with MNIST image classification Overall performance: we tested a simple four-layered GHN (cv[1,5,5,16]-pool-cv[16,5,5,64]-poolfc[1024]-fc[1024,10]) on the MNIST dataset with 99.",
      "startOffset" : 125,
      "endOffset" : 136
    }, {
      "referenceID" : 9,
      "context" : "1 A case study with MNIST image classification Overall performance: we tested a simple four-layered GHN (cv[1,5,5,16]-pool-cv[16,5,5,64]-poolfc[1024]-fc[1024,10]) on the MNIST dataset with 99.",
      "startOffset" : 152,
      "endOffset" : 161
    }, {
      "referenceID" : 2,
      "context" : "In this experiment, we tested a six-layered GHN (cv[3,3,3,64]-cv[64,5,5,256]-pool-cv[256,5,5,256]pool-fc[1024]-fc[1024,512]-fc[1024,nclass]) on both CIFAR10 (nclass=10) and CIFAR100 (nclass=100) datasets.",
      "startOffset" : 51,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "In this experiment, we tested a six-layered GHN (cv[3,3,3,64]-cv[64,5,5,256]-pool-cv[256,5,5,256]pool-fc[1024]-fc[1024,512]-fc[1024,nclass]) on both CIFAR10 (nclass=10) and CIFAR100 (nclass=100) datasets.",
      "startOffset" : 51,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "In this experiment, we tested a six-layered GHN (cv[3,3,3,64]-cv[64,5,5,256]-pool-cv[256,5,5,256]pool-fc[1024]-fc[1024,512]-fc[1024,nclass]) on both CIFAR10 (nclass=10) and CIFAR100 (nclass=100) datasets.",
      "startOffset" : 51,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "In this experiment, we tested a six-layered GHN (cv[3,3,3,64]-cv[64,5,5,256]-pool-cv[256,5,5,256]pool-fc[1024]-fc[1024,512]-fc[1024,nclass]) on both CIFAR10 (nclass=10) and CIFAR100 (nclass=100) datasets.",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "In this experiment, we tested a six-layered GHN (cv[3,3,3,64]-cv[64,5,5,256]-pool-cv[256,5,5,256]pool-fc[1024]-fc[1024,512]-fc[1024,nclass]) on both CIFAR10 (nclass=10) and CIFAR100 (nclass=100) datasets.",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "In this experiment, we tested a six-layered GHN (cv[3,3,3,64]-cv[64,5,5,256]-pool-cv[256,5,5,256]pool-fc[1024]-fc[1024,512]-fc[1024,nclass]) on both CIFAR10 (nclass=10) and CIFAR100 (nclass=100) datasets.",
      "startOffset" : 84,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "In this experiment, we tested a six-layered GHN (cv[3,3,3,64]-cv[64,5,5,256]-pool-cv[256,5,5,256]pool-fc[1024]-fc[1024,512]-fc[1024,nclass]) on both CIFAR10 (nclass=10) and CIFAR100 (nclass=100) datasets.",
      "startOffset" : 84,
      "endOffset" : 97
    }, {
      "referenceID" : 31,
      "context" : "The baseline network (784-400-400-20) used in this experiment is an improved implementation [34] of the influential paper [35], trained on the MNIST dataset of images of handwritten digits.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 31,
      "context" : "We have rectified the outputs following (3) and, instead of optimizing the lower bound of the log marginal likelihood as in [35], we directly minimize the reconstruction error.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 32,
      "context" : "A simple CNN has been used for sentence-level classification tasks and excellent results were demonstrated on multiple benchmarks [36].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 32,
      "context" : "The baseline network used in this experiment is a re-implementation of [36] made available from [37].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "It is our hope that this kind of comparative study will shed light on future deep learning research and eventually open the “black box” of artificial neural networks [16].",
      "startOffset" : 166,
      "endOffset" : 170
    } ],
    "year" : 2017,
    "abstractText" : "We revisit fuzzy neural network with a cornerstone notion of generalized hamming distance, which provides a novel and theoretically justified framework to re-interpret many useful neural network techniques in terms of fuzzy logic. In particular, we conjecture and empirically illustrate that, the celebrated batch normalization (BN) technique actually adapts the “normalized” bias such that it approximates the rightful bias induced by the generalized hamming distance. Once the due bias is enforced analytically, neither the optimization of bias terms nor the sophisticated batch normalization is needed. Also in the light of generalized hamming distance, the popular rectified linear units (ReLU) can be treated as setting a minimal hamming distance threshold between network inputs and weights. This thresholding scheme, on the one hand, can be improved by introducing double-thresholding on both positive and negative extremes of neuron outputs. On the other hand, ReLUs turn out to be non-essential and can be removed from networks trained for simple tasks like MNIST classification. The proposed generalized hamming network (GHN) as such not only lends itself to rigorous analysis and interpretation within the fuzzy logic theory but also demonstrates fast learning speed, well-controlled behaviour and state-of-the-art performances on a variety of learning tasks.",
    "creator" : null
  }
}