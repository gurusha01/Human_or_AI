{
  "name" : "14ea0d5b0cf49525d1866cb1e95ada5d.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification",
    "authors" : [ "Bikash Joshi", "Massih-Reza Amini" ],
    "emails" : [ "bikash.joshi@imag.fr", "massih-reza.amini@imag.fr", "ipartalas@expedia.com", "franck.iutzeler@imag.fr", "yury@lanl.gov" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Large-scale multi-class or extreme classification involves problems with extremely large number of classes as it appears in text repositories such as Wikipedia, Yahoo! Directory (www.dir.yahoo.com), or Directory Mozilla DMOZ (www.dmoz.org); and it has recently evolved as a popular branch of machine learning with many applications in tagging, recommendation and ranking. The most common and popular baseline in this case is the one-versus-all approach (OVA) [18] where one independent binary classifier is learned per class. Despite its simplicity, this approach suffers from two main limitations; first, it becomes computationally intractable when the number of classes grow large, affecting at the same time the prediction. Second, it suffers from the class imbalance problem by construction.Recently, two main approaches have been studied to cope with these limitations. The first one, broadly divided in tree-based and embedding-based methods, have been proposed with the aim of reducing the effective space of labels in order to control the complexity of the learning problem. Tree-based methods [4, 3, 6, 7, 9, 21, 5, 15] rely on binary tree structures where each leaf corresponds to a class and inference is performed by traversing the tree from top to bottom; a binary classifier being used at each node to determine the child node to develop. These methods have logarithmic time complexity with the drawback that it is a challenging task to find a balanced tree structure which can partition the class labels. Further, even though different heuristics have been developed to address the unbalanced problem, these methods suffer from the drawback that they have to make several decisions prior to reaching a final category, which leads to error propagation and\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nthus a decrease in accuracy. On the other hand, label embedding approaches [11, 5, 19] first project the label-matrix into a low-dimensional linear subspace and then use an OVA classifier. However, the low-rank assumption of the label-matrix is generally transgressed in the extreme multi-class classification setting, and these methods generally lead to high prediction error.The second type of approaches aim at reducing the original multi-class problem into a binary one by first expanding the original training set using a projection of pairs of observations and classes into a low dimensional dyadic space, and then learning a single classifier to separate between pairs constituted with examples and their true classes and pairs constituted with examples with other classes [1, 28, 16]. Although prediction in the new representation space is relatively fast, the construction of the dyadic training observations is generally time consuming and prevails over the training and prediction times.\nContributions. In this paper, we propose a scalable multi-class classification method based on an aggressive double sampling of the dyadic output prediction problem. Instead of computing all possible dyadic examples, our proposed approach consists first in drawing a new training set of much smaller size from the original one by oversampling the most small size classes and by sub-sampling the few big size classes in order to avoid the curse of long-tailed class distributions common in the majority of large-scale multi-class classification problems [2]. The second goal is to reduce the number of constructed dyadic examples. Our reduction strategy brings inter-dependency between the pairs containing the same observation and its true class in the original training set. Thus, we derive new generalization bounds using local fractional Rademacher complexity showing that even with a shift in the original class distribution and also the inter-dependency between the pairs of example, the empirical risk minimization principle over the transformation of the sampled training set remains consistent. We validate our approach by conducting a series of experiments on subsets of DMOZ and the Wikipedia collections with up to 100,000 target categories."
    }, {
      "heading" : "2 A doubly-sampled multi-class to binary reduction strategy",
      "text" : "We address the problem of monolabel multi-class classification defined on joint space X × Y where X ⊆ Rd is the input space and Y = {1, . . . ,K} .= [K] the output space, made of K classes. Elements of X × Y are denoted as xy = (x, y). Furthermore, we assume the training set S = (xyii )mi=1 is made of m i.i.d examples/class pairs distributed according to a fixed but unknown probability distribution D, and we consider a class of predictor functions G = {g : X × Y → R}. We define the instantaneous loss for predictor g ∈ G on example xy as:\ne(g,xy) = 1 K − 1 ∑\ny′∈Y\\{y}\n1g(xy)≤g(xy′ ), (1)\nwhere 1π is the indicator function equal to 1 if the predicate π is true and 0 otherwise. Compared to the classical multi-class error, e′(g,xy) = 1y 6=argmaxy′∈Y g(xy′ ), the loss of (1) estimates the average number of classes, given any input data, that get a greater scoring by g than the correct class. The loss (1) is hence a ranking criterion, and the multi-class SVM of [29] and AdaBoost.MR [24] optimize convex surrogate functions of this loss. It is also used in label ranking [12]. Our objective is to find a function g ∈ G with a small expected risk R(g) = Exy∼D [e(g,xy)], by minimizing the empirical error defined as the average number of training examples xyii ∈ S which, in mean, are scored lower than xy ′\ni , for y ′ ∈ Y\\{yi} :\nR̃m(g,S) = 1\nm m∑ i=1 e(g,xyii ) = 1 m(K − 1) m∑ i=1 ∑ y′∈Y\\{yi} 1 g(x yi i )−g(x y′ i )≤0 . (2)"
    }, {
      "heading" : "2.1 Binary reduction based on dyadic representations of examples and classes",
      "text" : "In this work, we consider prediction functions of the form g = f ◦ φ, where φ : X × Y → Rp is a projection of the input and the output space into a joint feature space of dimension p; and f ∈ F = {f : Rp → R} is a function that measures the adequacy between an observation x and a class y using their corresponding representation φ(xy). The projection function φ is applicationdependent and it can either be learned [28], or defined using some heuristics [27, 16].\nFurther, consider the following dyadic transformation T (S) = ({ ( zj = ( φ(xki ), φ(x yi i ) ) , ỹj = −1 ) if k < yi(\nzj = ( φ(xyii ), φ(x k i ) ) , ỹj = +1 ) elsewhere ) j . =(i−1)(K−1)+k , (3)\nwhere j = (i− 1)(K − 1) + k with i ∈ [m], k ∈ [K − 1]; that expands a K-class labeled set S of size m into a binary labeled set T (S) of size N = m(K − 1) (e.g. Figure 1 over a toy problem). With the class of functions\nH = {h : Rp × Rp → R; (φ(xy), φ(xy ′ )) 7→ f(φ(xy))− f(φ(xy ′ )), f ∈ F}, (4)\nthe empirical loss (Eq. (2)) can be rewritten as :\nR̃T (S)(h) = 1\nN N∑ j=1 1ỹjh(zj)≤0. (5)\nHence, the minimization of Eq. (5) over the transformation T (S) of a training set S\ndefines a binary classification over the pairs of dyadic examples. However, this binary problem takes as examples dependent random variables, as for each original example xy ∈ S , the K − 1 pairs in {(φ(xy), φ(xy′)); ỹ} ∈ T (S) all depend on xy. In [16] this problem is studied by bounding the generalization error associated to (5) using the fractional Rademacher complexity proposed in [25]. In this work, we derive a new generalization bounds based on Local Rademacher Complexities introduced in [22]\nthat implies second-order (i.e. variance) information inducing faster convergence rates (Theorem 1). Our analysis relies on the notion of graph covering introduced in [14] and defined as : Definition 1 (Exact proper fractional cover of G, [14]). Let G = (V, E) be a graph. C = {(Ck, ωk)}k∈[J], for some positive integer J , with Ck ⊆ V and ωk ∈ [0, 1] is an exact proper fractional cover of G, if: i) it is proper: ∀k, Ck is an independent set, i.e., there is no connections between vertices in Ck; ii) it is an exact fractional cover of G: ∀v ∈ V, ∑ k:v∈Ck ωk = 1.\nThe weight W (C) of C is given by: W (C) .= ∑ k∈[J] ωk and the minimum weight χ∗(G) = minC∈K(G)W (C) over the set K(G) of all exact proper fractional covers of G is the fractional chromatic number of G. From this statement, [14] extended Hoeffding’s inequality and proposed large deviation bounds for sums of dependent random variables which was the precursor of new generalisation bounds, including a Talagrand’s type inequality for empirical processes in the dependent case presented in [22].\nWith the classes of functions G and H introduced previously, consider the parameterized familyHr which, for r > 0, is defined as: Hr = {h : h ∈ H,V[h] . = Vz,ỹ[1ỹh(z)] ≤ r},\nwhere V denotes the variance. The fractional Rademacher complexity introduced in [25] entails our analysis :\nRT (S)(H) . =\n2 N Eξ ∑\nk∈[K−1]\nωkECksup h∈H ∑ α∈Ck\nzα∈T (S)\nξαh(zα),\nwith (ξi)Ni=1 a sequence of independent Rademacher variables verifying P(ξn = 1) = P(ξn=−1) = 12 . If other is not specified explicitly we assume below all ωk = 1. Our first result that bounds the generalization error of a function h ∈ H; R(h) = ET (S)[R̃T (S)(h)], with respect\nto its empirical error R̃T (S)(h) over a transformed training set, T (S), and the fractional Rademacher complexity, RT (S)(H), is stated below.\nTheorem 1. Let S = (xyii )mi=1 ∈ (X × Y)m be a dataset of m examples drawn i.i.d. according to a probability distribution D over X × Y and T (S) = ((zi, ỹi))Ni=1 the transformed set obtained as in Eq. (3). Then for any 1 > δ > 0 and 0/1 loss ` : {−1,+1} × R → [0, 1], with probability at least (1− δ) the following generalization bound holds for all h ∈ Hr :\nR(h) ≤ R̃T (S)(h) + RT (S)(` ◦ Hr) + 5\n2\n(√ RT (S)(` ◦ Hr) + √ r\n2 )√ log 1δ m + 25 48 log 1δ m .\nThe proof is provided in the supplementary material, and it relies on the idea of splitting up the sum (5) into several parts, each part being a sum of independent variables."
    }, {
      "heading" : "2.2 Aggressive Double Sampling",
      "text" : "Even-though the previous multi-class to binary transformation T with a proper projection function φ allows to redefine the learning problem in a dyadic feature space of dimension p d, the increased number of examples can lead to a large computational overhead. In order to cope with this problem, we propose a (π, κ)-double subsampling of T (S), which first aims to balance the presence of classes by constructing a new training set Sπ from S with probabilities π = (πk)Kk=1.\nAlgorithm: (π, κ)-DS Input: Labeled training set S = (xyii )mi=1 initialization: Sπ ← ∅; Tκ(Sπ)← ∅ ; for k = 1..K do\nDraw randomly a set Sπk of examples of class k from S with probability πk; Sπ ← Sπ ∪ Sπk ;\nforall xy ∈ Sπ do Draw uniformly a set Yxy of κ classes from Y\\{y} . κ K; forall k ∈ Yxy do\nif k < y then Tκ(Sπ)← Tκ(Sπ) ∪ ( z = ( φ(xk), φ(xy) ) , ỹ = −1 ) ;\nelse Tκ(Sπ)← Tκ(Sπ) ∪ ( z = ( φ(xy), φ(xk) ) , ỹ = +1 ) ;\nreturn Tκ(Sπ) The idea here is to overcome the curse of long-tailed class distributions exhibited in majority of large-scale multiclass classification problems [2] by oversampling the most small size classes and by subsampling the few big size classes in S. The hyperparameters π are formally defined as ∀k, πk = P (xy ∈ Sπ|xy ∈ S). In practice we set them inversely proportional to the size of each class in the original training set; ∀k, πk ∝ 1/µk where µk is the proportion of class k in S . The second aim is to reduce the number of dyadic examples controlled by the hyperparameter κ. The pseudo-code of this aggressive double sampling procedure, referred to as (π, κ)-DS, is depicted above and it is composed of two main steps.\n1. For each class k ∈ {1, . . . ,K}, draw randomly a set Sπk of examples from S of that class with probability πk, and let Sπ = K⋃ k=1 Sπk ;\n2. For each example xy in Sπ , draw uniformly κ adversarial classes in Y\\{y}.\nAfter this double sampling, we apply the transformation T defined as in Eq. (3), leading to a set Tκ(Sπ) of size κ|Sπ| N . In Section 3, we will show that this procedure practically leads to dramatic improvements in terms of memory consumption, computational complexity, and a higher multi-class prediction accuracy when the number of classes is very large. The empirical loss over the transformation of the new subsampled training set Sπ of size M , outputted by the (π, κ)-DS algorithm is :\nR̃Tκ(Sπ)(h) = 1\nκM ∑ (ỹα,zα)∈Tκ(Sπ) 1ỹαh(zα)≤0 = 1 κM ∑ xy∈Sπ ∑ y′∈Yxy 1g(xy)−g(xy′ )≤0, (6)\nwhich is essentially the same empirical risk as the one defined in Eq. (2) but taken with respect to the training set outputted by the (π, κ)-DS algorithm. Our main result is the following theorem which bounds the generalization error of a function h ∈ H learned by minimizing R̃Tκ(Sπ).\nTheorem 2. Let S = (xyii )mi=1 ∈ (X × Y)m be a training set of size m i.i.d. according to a probability distribution D over X × Y , and T (S) = ((zi, ỹi))Ni=1the transformed set obtained with the transformation function T defined as in Eq. (3). Let Sπ ⊆ S, |Sπ| = M , be a training set outputted by the algorithm (π, κ)-DS and T (Sπ) ⊆ T (S) its corresponding transformation. Then for any 1 > δ > 0 with probability at least (1− δ) the following risk bound holds for all h ∈ H :\nR(h) ≤ αR̃Tκ(Sπ)(h) + αRTκ(Sπ)(` ◦ H) + α\n√ (K − 1) log 2δ\n2Mκ + √ 2α log 4Kδ β(m− 1) + 7β log 4Kδ 3(m− 1) .\nFurthermore, for all functions in the classHr, we have the following generalization bound that holds with probability at least (1− δ) :\nR(h) ≤αR̃Tκ(Sπ)(h) + αRTκ(Sπ)(` ◦ Hr) + √ 2α log 4Kδ β(m− 1) + 7β log 4Kδ 3(m− 1)\n+ 5α\n2\n(√ RTκ(Sπ)(` ◦ Hr) + √ r\n2\n)√ (K − 1) log 2δ\nMκ +\n25α\n48 log 2δ M ,\nwhere ` : {−1,+1} × R → [0, 1] 0/1 is an instantaneous loss, and α = maxy: 1≤y≤K ηy/πy, β = maxy: 1≤y≤K 1/πy and ηy > 0 is the proportion of class y in S.\nThe proof is provided in the supplementary material. This theorem hence paves the way for the consistency of the empirical risk minimization principle [26, Th. 2.1, p. 38] defined over the double sample reduction strategy we propose."
    }, {
      "heading" : "2.3 Prediction with Candidate Selection",
      "text" : "The prediction is carried out in the dyadic feature space, by first considering the pairs constituted by a test observation and all the classes, and then choosing the class that leads to the highest score by the learned classifier.\nAlgorithm: Prediction with Candidate Selection Algorithm Input: Unlabeled test set T ; Learned function f∗ : Rp → R; initialization: Ω← ∅; forall x ∈ T do\nSelect Yx ⊆ Y candidate set of q nearest-centroid classes; Ω← Ω ∪ argmaxk∈Yx f\n∗(φ(xk)) ; return predicted classes Ω In the large scale scenario, computing the feature representations for all classes may require a huge amount of time. To overcome this problem we sample over classes by choosing just those that are the nearest to a test example, based on its distance with class centroids. Here we propose to consider class centroids as the average of vectors within that class. Note that class centroids are computed once in the preliminary projection of training examples and classes in the dyadic feature space and thus represent no additional computation at this stage. The algorithm above presents the pseudocode of this candidate based selection strategy 1."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we provide an empirical evaluation of the proposed reduction approach with the (π, κ)DS sampling strategy for large-scale multi-class classification of document collections. First, we present the mapping φ : X × Y → Rp. Then, we provide a statistical and computational comparison of our method with state-of-the-art large-scale approaches on popular datasets."
    }, {
      "heading" : "3.1 a Joint example/class representation for text classification",
      "text" : "The particularity of text classification is that documents are represented in a vector space induced by the vocabulary of the corresponding collection [23]. Hence each class can be considered as a megadocument, constituted by the concatenation of all of the documents in the training set belonging to it,\n1The number of classes pre-selected can be tuned to offer a prediction time/accuracy tradeoff if the prediction time is more critical.\nand simple feature mapping of examples and classes can be defined over their common words. Here we used p = 10 features inspired from learning to rank [17] by resembling a class and a document to respectively a document and a query (Table 1). All features except feature 9, that is the distance of an example x to the centroid of all examples of a particular class y, are classical. In addition to its predictive interest, the latter is also used in prediction for performing candidate preselection. Note that for other large-scale multi-class classification applications like recommendation with extremely large number of offer categories or image classification, a same kind of mapping can either be learned or defined using their characteristics [27, 28]."
    }, {
      "heading" : "3.2 Experimental Setup",
      "text" : "Datasets. We evaluate the proposed method using popular datasets from the Large Scale Hierarchical Text Classification challenge (LSHTC) 1 and 2 [20]. These datasets are provided in a pre-processed format using stop-word removal and stemming. Various characteristics of these datesets including the statistics of train, test and heldout are listed in Table 2. Since, the datasets used in LSHTC2 challenge were in multi-label format, we converted them to multi-class format by replicating the instances belonging to different class labels. Also, for the largest dataset (WIKI-large) used in LSHTC2 challenge, we used samples with 50,000 and 100,000 classes. The smaller dataset of LSHTC2 challenge is named as WIKI-Small, whereas the two 50K and 100K samples of large dataset are named as WIKI-50K and WIKI-100K in our result section.\nBaselines. We compare the proposed approach,2 denoted as the sampling strategy by (π, κ)-DS, with popular baselines listed below:\n• OVA: LibLinear [10] implementation of one-vs-all SVM. • M-SVM: LibLinear implementation of multi-class SVM proposed in [8]. • RecallTree [9]: A recent tree based multi-class classifier implemented in Vowpal Wabbit.\n2Source code and datasets can be found in the following repository https://github.com/bikash617/AggressiveSampling-for-Multi-class-to-BinaryReduction\n• FastXML [21]: An extreme multi-class classification method which performs partitioning in the feature space for faster prediction.\n• PfastReXML [13]: Tree ensemble based extreme classifier for multi-class and multilabel problems.\n• PD-Sparse [30]: A recent approach which uses multi-class loss with `1-regularization.\nReferring to the work [30], we did not consider other recent methods SLEEC [5] and LEML [31] in our experiments, since they have been shown to be consistently outperformed by the above mentioned state-of-the-art approaches.\nPlatform and Parameters. In all of our experiments, we used a machine with an Intel Xeon 2.60GHz processor with 256 GB of RAM. Each of these methods require tuning of various hyper-parameters that influence their performance. For each methods, we tuned the hyperparameters over a heldout set and used the combination which gave best predictive performance. The list of used hyperparameters for the results we obtained are reported in the supplementary material (Appendix B).\nEvaluation Measures. Different approaches are evaluated over the test sets using accuracy and the macro F1 measure (MaF1), which is the harmonic average of macro precision and macro recall; higher MaF1thus corresponds to better performance. As opposed to accuracy, macro F1 measure is not affected by the class imbalance problem inherent to multi-class classification, and is commonly used as a robust measure for comparing predictive performance of classification methods."
    }, {
      "heading" : "4 Results",
      "text" : "The parameters of the datasets along with the results for compared methods are shown in Table 3. The results are provided in terms of train and predict times, total memory usage, and predictive performance measured with accuracy and macro F1-measure (MaF1). For better visualization and comparison, we plot the same results as bar plots in Fig. 3 keeping only the best five methods while comparing the total runtime and memory usage. First, we observe that the tree based approaches (FastXML, PfastReXML and RecallTree) have worse predictive performance compared to the other methods. This is due to the fact that the prediction error made at the top-level of the tree cannot be corrected at lower levels, also known as cascading effect. Even though they have lower runtime and memory usage, they suffer from this side effect.\nFor large scale collections (WIKI-Small, WIKI-50K and WIKI-100K), the solvers with competitive predictive performance are OVA, M-SVM, PD-Sparse and (π, κ)-DS. However, standard OVA and\nM-SVM have a complexity that grows linearly with K thus the total runtime and memory usage explodes on those datasets, making them impossible. For instance, on Wiki large dataset sample of 100K classes, the memory consumption of both approaches exceeds the Terabyte and they take several days to complete the training. Furthermore, on this data set and the second largest Wikipedia collection (WIKI-50K and WIKI-100K) the proposed approach is highly competitive in terms of Time, Total Memory and both performance measures comparatively to all the other approaches. These results suggest that the method least affected by long-tailed class distributions is (π, κ)-DS, mainly because of two reasons: first, the sampling tends to make the training set balanced and second, the reduced binary dataset contains similar number of positive and negative examples. Hence, for the proposed approach, there is an improvement in both accuracy and MaF1 measures. The recent PD-Sparse method also enjoys a competitive predictive performance but it requires to store intermediary weight vectors during optimization which prevents it from scaling well. The PD-Sparse solver provides an option for hashing leading to fewer memory usage during training which we used in the experiments; however, the memory usage is still significantly high for large datasets and at the same time this option slows down the training process considerably. In overall, among the methods with competitive predictive performance, (π, κ)-DS seems to present the best runtime and memory usage; its runtime is even competitive with most of tree-based methods, leading it to provide the best compromise among the compared methods over the three time, memory and performance measures."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We presented a new method for reducing a multiclass classification problem to binary classification. We employ similarity based feature representation for class and examples and a double sampling stochastic scheme for the reduction process. Even-though the sampling scheme shifts the distribution of classes and that the reduction of the original problem to a binary classification problem brings inter-dependency between the dyadic examples; we provide generalization error bounds suggesting that the Empirical Risk Minimization principle over the transformation of the sampled training set still remains consistent. Furthermore, the characteristics of the algorithm contribute for its excellent performance in terms of memory usage and total runtime and make the proposed approach highly suitable for large class scenario."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work has been partially supported by the LabEx PERSYVAL-Lab (ANR-11-LABX-0025-01) funded by the French program Investissement d’avenir, and by the U.S. Department of Energy’s Office of Electricity as part of the DOE Grid Modernization Initiative."
    } ],
    "references" : [ {
      "title" : "An iterative method for multi-class cost-sensitive learning",
      "author" : [ "Naoki Abe", "Bianca Zadrozny", "John Langford" ],
      "venue" : "In Proceedings of the 10 ACM SIGKDD,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "On power law distributions in large-scale taxonomies",
      "author" : [ "Rohit Babbar", "Cornelia Metzig", "Ioannis Partalas", "Eric Gaussier", "Massih R. Amini" ],
      "venue" : "SIGKDD Explorations,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Label embedding trees for large multi-class tasks",
      "author" : [ "Samy Bengio", "Jason Weston", "David Grangier" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Error-correcting tournaments",
      "author" : [ "Alina Beygelzimer", "John Langford", "Pradeep Ravikumar" ],
      "venue" : "In Proceedings of the 20th International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Sparse local embeddings for extreme multi-label classification",
      "author" : [ "Kush Bhatia", "Himanshu Jain", "Purushottam Kar", "Manik Varma", "Prateek Jain" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Extreme multi class classification",
      "author" : [ "Anna Choromanska", "Alekh Agarwal", "John Langford" ],
      "venue" : "In NIPS Workshop: eXtreme Classification,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Logarithmic time online multiclass prediction",
      "author" : [ "Anna Choromanska", "John Langford" ],
      "venue" : "CoRR, abs/1406.1822,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector machines",
      "author" : [ "Koby Crammer", "Yoram Singer" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2002
    }, {
      "title" : "Logarithmic time one-againstsome",
      "author" : [ "Hal Daume III", "Nikos Karampatziakis", "John Langford", "Paul Mineiro" ],
      "venue" : "arXiv preprint arXiv:1606.04988,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Liblinear: A library for large linear classification",
      "author" : [ "Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Multi-label prediction via compressed sensing",
      "author" : [ "Daniel J Hsu", "Sham M Kakade", "John Langford", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "On minimizing the position error in label ranking",
      "author" : [ "Eyke Hüllermeier", "Johannes Fürnkranz" ],
      "venue" : "In Machine Learning: ECML",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications",
      "author" : [ "Himanshu Jain", "Yashoteja Prabhu", "Manik Varma" ],
      "venue" : "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Large deviations for sums of partly dependent random variables",
      "author" : [ "S. Janson" ],
      "venue" : "Random Structures and Algorithms,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "Log-time and log-space extreme classification",
      "author" : [ "Kalina Jasinska", "Nikos Karampatziakis" ],
      "venue" : "arXiv preprint arXiv:1611.01964,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "On binary reduction of large-scale multiclass classification problems",
      "author" : [ "Bikash Joshi", "Massih-Reza Amini", "Ioannis Partalas", "Liva Ralaivola", "Nicolas Usunier", "Éric Gaussier" ],
      "venue" : "In Advances in Intelligent Data Analysis XIV - 14th International Symposium,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Letor: Benchmark dataset for research on learning to rank for information retrieval",
      "author" : [ "Tie-Yan Liu", "Jun Xu", "Tao Qin", "Wenying Xiong", "Hang Li" ],
      "venue" : "In Proceedings of SIGIR 2007 workshop on learning to rank for information retrieval,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "A review on the combination of binary classifiers in multiclass problems",
      "author" : [ "Ana Carolina Lorena", "André C. Carvalho", "João M. Gama" ],
      "venue" : "Artif. Intell. Rev.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Fast label embeddings via randomized linear algebra. In Machine Learning and Knowledge Discovery in Databases ",
      "author" : [ "Paul Mineiro", "Nikos Karampatziakis" ],
      "venue" : "European Conference, ECML PKDD",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "LSHTC: A Benchmark for Large-Scale Text Classification",
      "author" : [ "I. Partalas", "A. Kosmopoulos", "N. Baskiotis", "T. Artieres", "G. Paliouras", "E. Gaussier", "I. Androutsopoulos", "M.-R. Amini", "P. Galinari" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning",
      "author" : [ "Yashoteja Prabhu", "Manik Varma" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Entropy-based concentration inequalities for dependent variables",
      "author" : [ "Liva Ralaivola", "Massih-Reza Amini" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "A vector space model for automatic indexing",
      "author" : [ "G. Salton", "A. Wong", "C.S. Yang" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1975
    }, {
      "title" : "Improved boosting algorithms using confidence-rated predictions",
      "author" : [ "Robert E Schapire", "Yoram Singer" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1999
    }, {
      "title" : "Generalization error bounds for classifiers trained with interdependent data",
      "author" : [ "Nicolas Usunier", "Massih-Reza Amini", "Patrick Gallinari" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2005
    }, {
      "title" : "Collaborative ranking with 17 parameters",
      "author" : [ "Maksims Volkovs", "Richard S. Zemel" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Wsabie: Scaling up to large vocabulary image annotation",
      "author" : [ "Jason Weston", "Samy Bengio", "Nicolas Usunier" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2011
    }, {
      "title" : "Multi-class support vector machines",
      "author" : [ "Jason Weston", "Chris Watkins" ],
      "venue" : "Technical report, Technical Report CSD-TR-98-04,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1998
    }, {
      "title" : "Pd-sparse: A primal and dual sparse approach to extreme multiclass and multilabel classification",
      "author" : [ "Ian EH Yen", "Xiangru Huang", "Kai Zhong", "Pradeep Ravikumar", "Inderjit S Dhillon" ],
      "venue" : "In Proceedings of the 33nd International Conference on Machine Learning,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2016
    }, {
      "title" : "Large-scale multi-label learning with missing labels",
      "author" : [ "Hsiang-Fu Yu", "Prateek Jain", "Purushottam Kar", "Inderjit Dhillon" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "The most common and popular baseline in this case is the one-versus-all approach (OVA) [18] where one independent binary classifier is learned per class.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "Tree-based methods [4, 3, 6, 7, 9, 21, 5, 15] rely on binary tree structures where each leaf corresponds to a class and inference is performed by traversing the tree from top to bottom; a binary classifier being used at each node to determine the child node to develop.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "Tree-based methods [4, 3, 6, 7, 9, 21, 5, 15] rely on binary tree structures where each leaf corresponds to a class and inference is performed by traversing the tree from top to bottom; a binary classifier being used at each node to determine the child node to develop.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "Tree-based methods [4, 3, 6, 7, 9, 21, 5, 15] rely on binary tree structures where each leaf corresponds to a class and inference is performed by traversing the tree from top to bottom; a binary classifier being used at each node to determine the child node to develop.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "Tree-based methods [4, 3, 6, 7, 9, 21, 5, 15] rely on binary tree structures where each leaf corresponds to a class and inference is performed by traversing the tree from top to bottom; a binary classifier being used at each node to determine the child node to develop.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "Tree-based methods [4, 3, 6, 7, 9, 21, 5, 15] rely on binary tree structures where each leaf corresponds to a class and inference is performed by traversing the tree from top to bottom; a binary classifier being used at each node to determine the child node to develop.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 20,
      "context" : "Tree-based methods [4, 3, 6, 7, 9, 21, 5, 15] rely on binary tree structures where each leaf corresponds to a class and inference is performed by traversing the tree from top to bottom; a binary classifier being used at each node to determine the child node to develop.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "Tree-based methods [4, 3, 6, 7, 9, 21, 5, 15] rely on binary tree structures where each leaf corresponds to a class and inference is performed by traversing the tree from top to bottom; a binary classifier being used at each node to determine the child node to develop.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "Tree-based methods [4, 3, 6, 7, 9, 21, 5, 15] rely on binary tree structures where each leaf corresponds to a class and inference is performed by traversing the tree from top to bottom; a binary classifier being used at each node to determine the child node to develop.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "On the other hand, label embedding approaches [11, 5, 19] first project the label-matrix into a low-dimensional linear subspace and then use an OVA classifier.",
      "startOffset" : 46,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "On the other hand, label embedding approaches [11, 5, 19] first project the label-matrix into a low-dimensional linear subspace and then use an OVA classifier.",
      "startOffset" : 46,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "On the other hand, label embedding approaches [11, 5, 19] first project the label-matrix into a low-dimensional linear subspace and then use an OVA classifier.",
      "startOffset" : 46,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "The second type of approaches aim at reducing the original multi-class problem into a binary one by first expanding the original training set using a projection of pairs of observations and classes into a low dimensional dyadic space, and then learning a single classifier to separate between pairs constituted with examples and their true classes and pairs constituted with examples with other classes [1, 28, 16].",
      "startOffset" : 403,
      "endOffset" : 414
    }, {
      "referenceID" : 26,
      "context" : "The second type of approaches aim at reducing the original multi-class problem into a binary one by first expanding the original training set using a projection of pairs of observations and classes into a low dimensional dyadic space, and then learning a single classifier to separate between pairs constituted with examples and their true classes and pairs constituted with examples with other classes [1, 28, 16].",
      "startOffset" : 403,
      "endOffset" : 414
    }, {
      "referenceID" : 15,
      "context" : "The second type of approaches aim at reducing the original multi-class problem into a binary one by first expanding the original training set using a projection of pairs of observations and classes into a low dimensional dyadic space, and then learning a single classifier to separate between pairs constituted with examples and their true classes and pairs constituted with examples with other classes [1, 28, 16].",
      "startOffset" : 403,
      "endOffset" : 414
    }, {
      "referenceID" : 1,
      "context" : "Instead of computing all possible dyadic examples, our proposed approach consists first in drawing a new training set of much smaller size from the original one by oversampling the most small size classes and by sub-sampling the few big size classes in order to avoid the curse of long-tailed class distributions common in the majority of large-scale multi-class classification problems [2].",
      "startOffset" : 387,
      "endOffset" : 390
    }, {
      "referenceID" : 27,
      "context" : "The loss (1) is hence a ranking criterion, and the multi-class SVM of [29] and AdaBoost.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "MR [24] optimize convex surrogate functions of this loss.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : "The projection function φ is applicationdependent and it can either be learned [28], or defined using some heuristics [27, 16].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 25,
      "context" : "The projection function φ is applicationdependent and it can either be learned [28], or defined using some heuristics [27, 16].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "The projection function φ is applicationdependent and it can either be learned [28], or defined using some heuristics [27, 16].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "In [16] this problem is studied by bounding the generalization error associated to (5) using the fractional Rademacher complexity proposed in [25].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 24,
      "context" : "In [16] this problem is studied by bounding the generalization error associated to (5) using the fractional Rademacher complexity proposed in [25].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 21,
      "context" : "In this work, we derive a new generalization bounds based on Local Rademacher Complexities introduced in [22] that implies second-order (i.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "Our analysis relies on the notion of graph covering introduced in [14] and defined as : Definition 1 (Exact proper fractional cover of G, [14]).",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "Our analysis relies on the notion of graph covering introduced in [14] and defined as : Definition 1 (Exact proper fractional cover of G, [14]).",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "From this statement, [14] extended Hoeffding’s inequality and proposed large deviation bounds for sums of dependent random variables which was the precursor of new generalisation bounds, including a Talagrand’s type inequality for empirical processes in the dependent case presented in [22].",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 21,
      "context" : "From this statement, [14] extended Hoeffding’s inequality and proposed large deviation bounds for sums of dependent random variables which was the precursor of new generalisation bounds, including a Talagrand’s type inequality for empirical processes in the dependent case presented in [22].",
      "startOffset" : 286,
      "endOffset" : 290
    }, {
      "referenceID" : 24,
      "context" : "The fractional Rademacher complexity introduced in [25] entails our analysis : RT (S)(H) .",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "return Tκ(Sπ) The idea here is to overcome the curse of long-tailed class distributions exhibited in majority of large-scale multiclass classification problems [2] by oversampling the most small size classes and by subsampling the few big size classes in S.",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 22,
      "context" : "The particularity of text classification is that documents are represented in a vector space induced by the vocabulary of the corresponding collection [23].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : "Here we used p = 10 features inspired from learning to rank [17] by resembling a class and a document to respectively a document and a query (Table 1).",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 25,
      "context" : "Note that for other large-scale multi-class classification applications like recommendation with extremely large number of offer categories or image classification, a same kind of mapping can either be learned or defined using their characteristics [27, 28].",
      "startOffset" : 249,
      "endOffset" : 257
    }, {
      "referenceID" : 26,
      "context" : "Note that for other large-scale multi-class classification applications like recommendation with extremely large number of offer categories or image classification, a same kind of mapping can either be learned or defined using their characteristics [27, 28].",
      "startOffset" : 249,
      "endOffset" : 257
    }, {
      "referenceID" : 19,
      "context" : "We evaluate the proposed method using popular datasets from the Large Scale Hierarchical Text Classification challenge (LSHTC) 1 and 2 [20].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 9,
      "context" : "We compare the proposed approach,2 denoted as the sampling strategy by (π, κ)-DS, with popular baselines listed below: • OVA: LibLinear [10] implementation of one-vs-all SVM.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : "• M-SVM: LibLinear implementation of multi-class SVM proposed in [8].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "• RecallTree [9]: A recent tree based multi-class classifier implemented in Vowpal Wabbit.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 20,
      "context" : "• FastXML [21]: An extreme multi-class classification method which performs partitioning in the feature space for faster prediction.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 12,
      "context" : "• PfastReXML [13]: Tree ensemble based extreme classifier for multi-class and multilabel problems.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 28,
      "context" : "• PD-Sparse [30]: A recent approach which uses multi-class loss with `1-regularization.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 28,
      "context" : "Referring to the work [30], we did not consider other recent methods SLEEC [5] and LEML [31] in our experiments, since they have been shown to be consistently outperformed by the above mentioned state-of-the-art approaches.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "Referring to the work [30], we did not consider other recent methods SLEEC [5] and LEML [31] in our experiments, since they have been shown to be consistently outperformed by the above mentioned state-of-the-art approaches.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 29,
      "context" : "Referring to the work [30], we did not consider other recent methods SLEEC [5] and LEML [31] in our experiments, since they have been shown to be consistently outperformed by the above mentioned state-of-the-art approaches.",
      "startOffset" : 88,
      "endOffset" : 92
    } ],
    "year" : 2017,
    "abstractText" : "We address the problem of multi-class classification in the case where the number of classes is very large. We propose a double sampling strategy on top of a multi-class to binary reduction strategy, which transforms the original multi-class problem into a binary classification problem over pairs of examples. The aim of the sampling strategy is to overcome the curse of long-tailed class distributions exhibited in majority of large-scale multi-class classification problems and to reduce the number of pairs of examples in the expanded data. We show that this strategy does not alter the consistency of the empirical risk minimization principle defined over the double sample reduction. Experiments are carried out on DMOZ and Wikipedia collections with 10,000 to 100,000 classes where we show the efficiency of the proposed approach in terms of training and prediction time, memory consumption, and predictive performance with respect to state-of-the-art approaches.",
    "creator" : null
  }
}