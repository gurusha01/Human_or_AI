{
  "name" : "e94f63f579e05cb49c05c2d050ead9c0.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deep Supervised Discrete Hashing",
    "authors" : [ "Qi Li", "Zhenan Sun", "Ran He", "Tieniu Tan" ],
    "emails" : [ "qli@nlpr.ia.ac.cn", "znsun@nlpr.ia.ac.cn", "rhe@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Hashing has attracted much attention in recent years because of the rapid growth of image and video data on the web. It is one of the most popular techniques for image or video search due to its low computational cost and storage efficiency. Generally speaking, hashing is used to encode high dimensional data into a set of binary codes while preserving the similarity of images or videos. Existing hashing methods can be roughly grouped into two categories: data independent methods and data dependent methods.\nData independent methods rely on random projections to construct hash functions. Locality Sensitive Hashing (LSH) [3] is one of the representative methods, which uses random linear projections to map nearby data into similar binary codes. LSH is widely used for large scale image retrieval. In order to generalize LSH to accommodate arbitrary kernel functions, the Kenelized Locality Sensitive Hashing (KLSH) [7] is proposed to deal with high-dimensional kernelized data. Other variants of LSH are also proposed in recent years, such as super-bit LSH [5], non-metric LSH [14]. However, there are some limitations of data independent hashing methods, e.g., it makes no use of training data. The learning efficiency is low, and it requires longer hash codes to attain high accuracy. Due to the limitations of the data independent hashing methods, recent hashing methods try to exploit various machine learning techniques to learn more effective hash function based on a given dataset.\nData dependent methods refer to using training data to learn the hash functions. They can be further categorized into supervised and unsupervised methods. Unsupervised methods retrieve the neighbors under some kinds of distance metrics. Iterative Quantization (ITQ) [4] is one of the representative unsupervised hashing methods, in which the projection matrix is optimized by iterative projection and thresholding according to the given training samples. In order to utilize the semantic labels of data samples, supervised hashing methods are proposed. Supervised Hashing with Kernels (KSH) [13]\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nis a well-known method of this kind, which learns the hash codes by minimizing the Hamming distances between similar pairs, and at the same time maximizing the Hamming distances between dissimilar pairs. Binary Reconstruction Embedding (BRE) [6] learns the hash functions by explicitly minimizing the reconstruction error between the original distances and the reconstructed distances in Hamming space. Order Preserving Hashing (OPH) [17] learns the hash codes by preserving the supervised ranking list information, which is calculated based on the semantic labels. Supervised Discrete Hashing (SDH) [15] aims to directly optimize the binary hash codes using the discrete cyclic coordinate descend method.\nRecently, deep learning based hashing methods have been proposed to simultaneously learn the image representation and hash coding, which have shown superior performance over the traditional hashing methods. Convolutional Neural Network Hashing (CNNH) [20] is one of the early works to incorporate deep neural networks into hash coding, which consists of two stages to learn the image representations and hash codes. One drawback of CNNH is that the learned image representation can not give feedback for learning better hash codes. To overcome the shortcomings of CNNH, Network In Network Hashing (NINH) [8] presents a triplet ranking loss to capture the relative similarities of images. The image representation learning and hash coding can benefit each other within one stage framework. Deep Semantic Ranking Hashing (DSRH) [26] learns the hash functions by preserving semantic similarity between multi-label images. Other ranking-based deep hashing methods have also been proposed in recent years [18, 22]. Besides the triplet ranking based methods, some pairwise label based deep hashing methods are also exploited [9, 27]. A novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) is proposed to train very deep neural networks for supervised hashing in [25]. The classification information is used to learn hash codes. [25] relaxes the binary constraint to be continuous, then thresholds the obtained continuous variables to be binary codes.\nAlthough deep learning based methods have achieved great progress in image retrieval, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). Recent works try to divide the whole learning process into two streams under the multitask learning framework [11, 21, 22]. The hash stream is used to learn the hash function, while the classification stream is utilized to mine the semantic information. Although the two stream framework can improve the retrieval performance, the classification stream is only employed to learn the image representations, which does not have a direct impact on the hash function. In this paper, we use CNN to learn the image representation and hash function simultaneously. The last layer of CNN outputs the binary codes directly based on the pairwise label information and the classification information.\nThe contributions of this work are summarized as follows. 1) The last layer of our method is constrained to output the binary codes directly. The binary codes are learned to preserve the similarity relationship and keep the label consistent simultaneously. To the best of our knowledge, this is the first deep hashing method that uses both pairwise label information and classification information to learn the hash codes under one stream framework. 2) In order to reduce the quantization error, we keep the discrete nature of the hash codes during the optimization process. An alternating minimization method is proposed to optimize the objective function by using the discrete cyclic coordinate descend method. 3) Extensive experiments have shown that our method outperforms current state-of-the-art methods on benchmark datasets for image retrieval, which demonstrates the effectiveness of the proposed method."
    }, {
      "heading" : "2 Deep supervised discrete hashing",
      "text" : ""
    }, {
      "heading" : "2.1 Problem definition",
      "text" : "Given N image samples X = {xi}Ni=1 ∈ Rd×N , hash coding is to learn a collection of K-bit binary codes B ∈ {−1, 1}K×N , where the i-th column bi ∈ {−1, 1}K denotes the binary codes for the i-th sample xi. The binary codes are generated by the hash function h (·), which can be rewritten as [h1 (·) , ..., hK (·)]. For image sample xi, its hash codes can be represented as bi = h (xi) = [h1 (xi) , ..., hK (xi)]. Generally speaking, hashing is to learn a hash function to project image samples to a set of binary codes."
    }, {
      "heading" : "2.2 Similarity measure",
      "text" : "In supervised hashing, the label information is given as Y = {yi}Ni=1 ∈ Rc×N , where yi ∈ {0, 1} c corresponds to the sample xi, c is the number of categories. Note that one sample may belong to multiple categories. Given the semantic label information, the pairwise label information is derived as: S = {sij}, sij ∈ {0, 1}, where sij = 1 when xi and xj are semantically similar, sij = 0 when xi and xj are semantically dissimilar. For two binary codes bi and bj , the relationship between their Hamming distance distH (·, ·) and their inner product 〈·, ·〉 is formulated as follows: distH (bi, bj) = 1 2 (K − 〈bi, bj〉). If the inner product of two binary codes is small, their Hamming distance will be large, and vice versa. Therefore the inner product of different hash codes can be used to quantify their similarity.\nGiven the pairwise similarity relationship S = {sij}, the Maximum a Posterior (MAP) estimation of hash codes can be represented as:\np (B|S) ∝ p (S|B) p (B) = Π sij∈S p (sij |B) p (B) (1)\nwhere p (S|B) denotes the likelihood function, p (B) is the prior distribution. For each pair of the images, p (sij |B) is the conditional probability of sij given their hash codes B, which is defined as follows:\np (sij |B) = {\nσ (Φij) , sij = 1 1− σ (Φij) , sij = 0 (2)\nwhere σ (x) = 1/ (1 + e−x) is the sigmoid function, Φij = 12 〈bi, bj〉 = 1 2b T i bj . From Equation 2 we can see that, the larger the inner product 〈bi, bj〉 is, the larger p (1|bi, bj) will be, which implies that bi and bj should be classified as similar, and vice versa. Therefore Equation 2 is a reasonable similarity measure for hash codes."
    }, {
      "heading" : "2.3 Loss function",
      "text" : "In recent years, deep learning based methods have shown their superior performance over the traditional handcrafted features on object detection, image classification, image segmentation, etc. In this section, we take advantage of recent advances in CNN to learn the hash function. In order to have a fair comparison with other deep hashing methods, we choose the CNN-F network architecture [2] as a basic component of our algorithm. This architecture is widely used to learn the hash function in recent works [9, 18]. Specifically, there are two separate CNNs to learn the hash function, which share the same weights. The pairwise samples are used as the input for these two separate CNNs. The CNN model consists of 5 convolutional layers and 2 fully connected layers. The number of neurons in the last fully connected layer is equal to the number of hash codes.\nConsidering the similarity measure, the following loss function is used to learn the hash codes: J = − log p (S|B) = − ∑ sij∈S log p (sij |B) = − ∑ sij∈S ( sijΦij − log ( 1 + eΦij )) . (3)\nEquation 3 is the negative log likelihood function, which makes the Hamming distance of two similar points as small as possible, and at the same time makes the Hamming distance of two dissimilar points as large as possible.\nAlthough pairwise label information is used to learn the hash function in Equation 3, the label information is not fully exploited. Most of the previous works make use of the label information under a two stream multi-task learning framework [21, 22]. The classification stream is used to measure the classification error, while the hash stream is employed to learn the hash function. One basic assumption of our algorithm is that the learned binary codes should be ideal for classification. In order to take advantage of the label information directly, we expect the learned binary codes to be optimal for the jointly learned linear classifier.\nWe use a simple linear classifier to model the relationship between the learned binary codes and the label information: Y = WTB, (4) where W = [w1, w2,...,wC ] is the classifier weight, Y = [y1, y2,...,yN ] is the ground-truth label vector. The loss function can be calculated as:\nQ = L ( Y,WTB ) + λ ‖W‖2F = N∑ i=1 L ( yi,W T bi ) + λ ‖W‖2F , (5)\nwhere L (·) is the loss function, λ is the regularization parameter, ‖·‖F is the Frobenius norm of a matrix. Combining Equation 5 and Equation 3, we have the following formulation:\nF = J + µQ = − ∑ sij∈S ( sijΦij − log ( 1 + eΦij )) + µ N∑ i=1 L ( yi,W T bi ) + ν ‖W‖2F , (6)\nwhere µ is the trade-off parameters, ν = λµ. Suppose that we choose the l2 loss for the linear classifier, Equation 6 is rewritten as follows:\nF = − ∑ sij∈S ( sijΦij − log ( 1 + eΦij )) + µ N∑ i=1 ∥∥yi −WT bi∥∥22 + ν ‖W‖2F , (7) where ‖·‖2 is l2 norm of a vector. The hypothesis for Equation 7 is that the learned binary codes should make the pairwise label likelihood as large as possible, and should be optimal for the jointly learned linear classifier."
    }, {
      "heading" : "2.4 Optimization",
      "text" : "The minimization of Equation 7 is a discrete optimization problem, which is difficult to optimize directly. There are several ways to solve this problem. (1) In the training stage, the sigmoid or tanh activation function is utilized to replace the ReLU function after the last fully connected layer, and then the continuous outputs are used as a relaxation of the hash codes. In the testing stage, the hash codes are obtained by applying a thresholding function on the continuous outputs. One limitation of this method is that the convergence of the algorithm is slow. Besides, there will be a large quantization error. (2) The sign function is directly applied after the outputs of the last fully connected layer, which constrains the outputs to be binary variables strictly. However, the sign function is non-differentiable, which is difficult to back propagate the gradient of the loss function.\nBecause of the discrepancy between the Euclidean space and the Hamming space, it would result in suboptimal hash codes if one totally ignores the binary constraints. We emphasize that it is essential to keep the discrete nature of the binary codes. Note that in our formulation, we constrain the outputs of the last layer to be binary codes directly, thus Equation 7 is difficult to optimize directly. Similar to [9, 18, 22], we solve this problem by introducing an auxiliary variable. Then we approximate Equation 7 as:\nF = − ∑ sij∈S ( sijΨij − log ( 1 + eΨij )) + µ N∑ i=1 ∥∥yi −WT bi∥∥22 + ν ‖W‖2F , s.t. bi = sgn(hi), hi ∈ RK×1, (i = 1, ..., N) ,\n(8)\nwhere Ψij = 12hi Thj . hi (i = 1, ..., N) can be seen as the output of the last fully connected layer, which is represented as: hi = M TΘ (xi; θ) + n, (9)\nwhere θ denotes the parameters of the previous layers before the last fully connected layer, M ∈ R4096×K represents the weight matrix, n ∈ RK×1 is the bias term. According to the Lagrange multipliers method, Equation 8 can be reformulated as:\nF = − ∑ sij∈S ( sijΨij − log ( 1 + eΨij )) +µ\nN∑ i=1 ∥∥yi −WT bi∥∥22 + ν ‖W‖2F + η N∑ i=1 ‖bi − sgn (hi)‖22,\ns.t. bi ∈ {−1, 1}K , (i = 1, ..., N) ,\n(10)\nwhere η is the Lagrange Multiplier. Equation 10 can be further relaxed as: F = − ∑ sij∈S ( sijΨij − log ( 1 + eΨij )) +µ\nN∑ i=1 ∥∥yi −WT bi∥∥22 + ν ‖W‖2F + η N∑ i=1 ‖bi − hi‖22,\ns.t. bi ∈ {−1, 1}K , (i = 1, ..., N) .\n(11)\nThe last term actually measures the constraint violation caused by the outputs of the last fully connected layer. If the parameter η is set sufficiently large, the constraint violation is penalized severely. Therefore the outputs of the last fully connected layer are forced closer to the binary codes, which are employed for classification directly.\nThe benefit of introducing an auxiliary variable is that we can decompose Equation 11 into two sub optimization problems, which can be iteratively solved by using the alternating minimization method.\nFirst, when fixing bi, W , we have:\n∂F ∂hi = − 12 ∑\nj:sij∈S\n( sij − e Ψij\n1+eΨij\n) hj − 12 ∑ j:sji∈S ( sji − e Ψji 1+eΨji ) hj − 2η (bi − hi) (12)\nThen we update parameters M , n and Θ as follows:\n∂F ∂M = Θ (xi; θ) ( ∂F ∂hi )T , ∂F∂n = ∂F ∂hi , ∂F∂Θ(xi;θ) = M ∂F ∂hi . (13)\nThe gradient will propagate to previous layers by Back Propagation (BP) algorithm.\nSecond, when fixing M , n, Θ and bi, we solve W as:\nF = µ N∑ i=1 ∥∥yi −WT bi∥∥22 + ν ‖W‖2F . (14) Equation 14 is a least squares problem, which has a closed form solution:\nW = ( BBT + ν\nµ I\n)−1 BTY, (15)\nwhere B = {bi}Ni=1 ∈ {−1, 1} K×N , Y = {yi}Ni=1 ∈ RC×N .\nFinally, when fixing M , n, Θ and W , Equation 11 becomes:\nF = µ N∑ i=1 ∥∥yi −WT bi∥∥22 + η N∑ i=1 ‖bi − hi‖22,\ns.t. bi ∈ {−1, 1}K , (i = 1, ..., N) . (16)\nIn this paper, we use the discrete cyclic coordinate descend method to iteratively solve B row by row:\nmin B ∥∥WTB∥∥2 − 2 Tr (P ) , s.t. B ∈ {−1, 1}K×N , (17) where P = WY + ηµH . Let x\nT be the kth (k = 1, ...,K) row ofB,B1 be the matrix ofB excluding xT , pT be the kth column of matrix P , P1 be the matrix of P excluding p, wT be the kth column of matrix W , W1 be the matrix of W excluding w, then we can derive:\nx = sgn ( p−BT1 W1w ) . (18)\nIt is easy to see that each bit of the hash codes is computed based on the pre-learned K − 1 bits B1. We iteratively update each bit until the algorithm converges."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Experimental settings",
      "text" : "We conduct extensive experiments on two public benchmark datasets: CIFAR-10 and NUS-WIDE. CIFAR-10 is a dataset containing 60,000 color images in 10 classes, and each class contains 6,000 images with a resolution of 32x32. Different from CIFAR-10, NUS-WIDE is a public multi-label image dataset. There are 269,648 color images in total with 5,018 unique tags. Each image is annotated with one or multiple class labels from the 5,018 tags. Similar to [8, 12, 20, 24], we use a subset of 195,834 images which are associated with the 21 most frequent concepts. Each concept consists of at least 5,000 color images in this dataset.\nWe follow the previous experimental setting in [8, 9, 18]. In CIFAR-10, we randomly select 100 images per class (1,000 images in total) as the test query set, 500 images per class (5,000 images in\ntotal) as the training set. For NUS-WIDE dataset, we randomly sample 100 images per class (2,100 images in total) as the test query set, 500 images per class (10,500 images in total) as the training set. The similar pairs are constructed according to the image labels: two images will be considered similar if they share at least one common semantic label. Otherwise, they will be considered dissimilar. We also conduct experiments on CIFAR-10 and NUS-WIDE dataset under a different experimental setting. In CIFAR-10, 1,000 images per class (10,000 images in total) are selected as the test query set, the remaining 50,000 images are used as the training set. In NUS-WIDE, 100 images per class (2,100 images in total) are randomly sampled as the test query images, the remaining images (193,734 images in total) are used as the training set.\nAs for the comparison methods, we roughly divide them into two groups: traditional hashing methods and deep hashing methods. The compared traditional hashing methods consist of unsupervised and supervised methods. Unsupervised hashing methods include SH [19], ITQ [4]. Supervised hashing methods include SPLH [16], KSH [13], FastH [10], LFH [23], and SDH [15]. Both the hand-crafted features and the features extracted by CNN-F network architecture are used as the input for the traditional hashing methods. Similar to previous works, the handcrafted features include a 512-dimensional GIST descriptor to represent images of CIFAR-10 dataset, and a 1134-dimensional feature vector to represent images of NUS-WIDE dataset. The deep hashing methods include DQN [1], DHN [27], CNNH [20], NINH [8], DSRH [26], DSCH [24], DRCSH [24], DPSH [9], DTSH [18] and VDSH [25]. Note that DPSH, DTSH and DSDH are based on the CNN-F network architecture, while DQN, DHN, DSRH are based on AlexNet architecture. Both the CNN-F network architecture and AlexNet architecture consist of five convolutional layers and two fully connected layers. In order to have a fair comparison, most of the results are directly reported from previous works. Following [25], the pre-trained CNN-F model is used to extract CNN features on CIFAR-10, while a 500 dimensional bag-of-words feature vector is used to represent each image on NUS-WIDE for VDSH. Then we re-run the source code provided by the authors to obtain the retrieval performance. The parameters of our algorithm are set based on the standard cross-validation procedure. µ, ν and η in Equation 11 are set to 1, 0.1 and 55, respectively.\nSimilar to [8], we adopt four widely used evaluation metrics to evaluate the image retrieval quality: Mean Average Precision (MAP) for different number of bits, precision curves within Hamming distance 2, precision curves with different number of top returned samples and precision-recall curves. When computing MAP for NUS-WIDE dataset under the first experimental setting, we only consider the top 5,000 returned neighbors. While we consider the top 50,000 returned neighbors under the second experimental setting."
    }, {
      "heading" : "3.2 Empirical analysis",
      "text" : "In order to verify the effectiveness of our method, several variants of our method (DSDH) are also proposed. First, we only consider the pairwise label information while neglecting the linear classification information in Equation 7, which is named DSDH-A (similar to [9]). Then we design a two-stream deep hashing algorithm to learn the hash codes. One stream is designed based on the pairwise label information in Equation 3, and the other stream is constructed based on the classification information. The two streams share the same image representations except for the last"
    }, {
      "heading" : "SH 0.127 0.128 0.126 0.129 SH 0.454 0.406 0.405 0.400",
      "text" : "fully connected layer. We denote this method as DSDH-B. Besides, we also design another approach directly applying the sign function after the outputs of the last fully connected layer in Equation 7, which is denoted as DSDH-C. The loss function of DSDH-C can be represented as:\nF = − ∑ sij∈S ( sijΨij − log ( 1 + eΨij )) + µ N∑ i=1 ∥∥yi −WThi∥∥22 + ν ‖W‖2F + η N∑ i=1 ‖bi − sgn (hi)‖22, s.t. hi ∈ RK×1, (i = 1, ..., N) (19)\nThen we use the alternating minimization method to optimize DSDH-C. The results of different methods on CIFAR-10 under the first experimental setting are shown in Figure 1. From Figure 1 we can see that, (1) The performance of DSDH-C is better than DSDH-A. DSDH-B is better than DSDH-A in terms of precision with Hamming radius 2 and precision-recall curves. More information is exploited in DSDH-C than DSDH-A, which demonstrates the classification information is helpful for learning the hash codes. (2) The improvement of DSDH-C over DSDH-A is marginal. The reason is that the classification information in DSDH-C is only used to learn the image representations, which is not fully exploited. Due to violation of the discrete nature of the hash codes, DSDH-C has a large quantization loss. Note that our method further beats DSDH-B and DSDH-C by a large margin."
    }, {
      "heading" : "3.3 Results under the first experimental setting",
      "text" : "The MAP results of all methods on CIFAR-10 and NUS-WIDE under the first experimental setting are listed in Table 1. From Table 1 we can see that the proposed method substantially outperforms the traditional hashing methods on CIFAR-10 dataset. The MAP result of our method is more than twice as much as SDH, FastH and ITQ. Besides, most of the deep hashing methods perform better than the traditional hashing methods. In particular, DTSH achieves the best performance among all the other methods except DSDH on CIFAR-10 dataset. Compared with DTSH, our method further improves the performance by 3 ∼ 7 percents. These results verify that learning the hash function and classifier within one stream framework can boost the retrieval performance.\nThe gap between the deep hashing methods and traditional hashing methods is not very huge on NUS-WIDE dataset, which is different from CIFAR-10 dataset. For example, the average MAP result of SDH is 0.603, while the average MAP result of DTSH is 0.804. The proposed method is slightly superior to DTSH in terms of the MAP results on NUS-WIDE dataset. The main reasons are that there exits more categories in NUS-WIDE than CIFAR-10, and each of the image contains multiple labels. Compared with CIFAR-10, there are only 500 images per class for training, which may not be enough for DSDH to learn the multi-label classifier. Thus the second term in Equation 7 plays a limited role to learn a better hash function. In Section 3.4, we will show that our method will achieve\na better performance than other deep hashing methods with more training images per class for the multi-label dataset."
    }, {
      "heading" : "3.4 Results under the second experimental setting",
      "text" : "Deep hashing methods usually need many training images to learn the hash function. In this section, we compare with other deep hashing methods under the second experimental setting, which contains more training images. Table 2 lists MAP results for different methods under the second experimental setting. As shown in Table 2, with more training images, most of the deep hashing methods perform better than in Section 3.3. For CIFAR-10 dataset, the average MAP result of DRSCH is 0.624, and the average MAP results of DPSH, DTSH and VDSH are 0.787, 0.922 and 0.846, respectively. The average MAP result of our method is 0.938 on CIFAR-10 dataset. DTSH, DPSH and VDSH have a significant advantage over other deep hashing methods. Our method further outperforms DTSH, DPSH and VDSH by about 2 ∼ 3 percents. For NUS-WIDE dataset, our method still achieves the best performance in terms of MAP. The performance of VDSH on NUS-WIDE dataset drops severely. The possible reason is that VDSH uses the provided bag-of-words features instead of the learned features."
    }, {
      "heading" : "3.5 Comparison with traditional hashing methods using deep learned features",
      "text" : "In order to have a fair comparison, we also compare with traditional hashing methods using deep learned features extracted by the CNN-F network under the first experimental setting. The MAP results of different methods are listed in Table 3. As shown in Table 3, most of the traditional hashing methods obtain a better retrieval performance using deep learned features. The average MAP results of FastH+CNN and SDH+CNN on CIFAR-10 dataset are 0.604 and 0.553, respectively. And the average MAP result of our method on CIFAR-10 dataset is 0.787, which outperforms the traditional hashing methods with deep learned features. Besides, the proposed algorithm achieves a comparable performance with the best traditional hashing methods on NUS-WIDE dataset under the first experimental setting."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we have proposed a novel deep supervised discrete hashing algorithm. We constrain the outputs of the last layer to be binary codes directly. Both the pairwise label information and the classification information are used for learning the hash codes under one stream framework. Because of the discrete nature of the hash codes, we derive an alternating minimization method to optimize the loss function. Extensive experiments have shown that our method outperforms state-of-the-art methods on benchmark image retrieval datasets."
    }, {
      "heading" : "5 Acknowledgements",
      "text" : "This work was partially supported by the National Key Research and Development Program of China (Grant No. 2016YFB1001000) and the Natural Science Foundation of China (Grant No. 61622310)."
    } ],
    "references" : [ {
      "title" : "Deep quantization network for efficient image retrieval",
      "author" : [ "Y. Cao", "M. Long", "J. Wang", "H. Zhu", "Q. Wen" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Return of the devil in the details: Delving deep into convolutional nets",
      "author" : [ "K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman" ],
      "venue" : "In BMVC,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Similarity search in high dimensions via hashing",
      "author" : [ "A. Gionis", "P. Indyk", "R. Motwani" ],
      "venue" : "In VLDB,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1999
    }, {
      "title" : "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval",
      "author" : [ "Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin" ],
      "venue" : "IEEE TPAMI,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Super-bit locality-sensitive hashing",
      "author" : [ "J. Ji", "J. Li", "S. Yan", "B. Zhang", "Q. Tian" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Learning to hash with binary reconstructive embeddings",
      "author" : [ "B. Kulis", "T. Darrell" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Kernelized locality-sensitive hashing for scalable image search",
      "author" : [ "B. Kulis", "K. Grauman" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Simultaneous feature learning and hash coding with deep neural networks",
      "author" : [ "H. Lai", "Y. Pan", "Y. Liu", "S. Yan" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Feature learning based deep supervised hashing with pairwise labels",
      "author" : [ "W.-J. Li", "S. Wang", "W.-C. Kang" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Fast supervised hashing with decision trees for high-dimensional data",
      "author" : [ "G. Lin", "C. Shen", "Q. Shi", "A. van den Hengel", "D. Suter" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1963
    }, {
      "title" : "Deep learning of binary hash codes for fast image retrieval",
      "author" : [ "K. Lin", "H.-F. Yang", "J.-H. Hsiao", "C.-S. Chen" ],
      "venue" : "In CVPRW,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Hashing with graphs",
      "author" : [ "W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang" ],
      "venue" : "In ICML, pages",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Supervised hashing with kernels",
      "author" : [ "W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang" ],
      "venue" : "In CVPR, pages 2074–2081,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Non-metric locality-sensitive hashing",
      "author" : [ "Y. Mu", "S. Yan" ],
      "venue" : "In AAAI, pages 539–544,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Supervised discrete hashing",
      "author" : [ "F. Shen", "C. Shen", "W. Liu", "H. Tao Shen" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Sequential projection learning for hashing with compact codes",
      "author" : [ "J. Wang", "S. Kumar", "S.-F. Chang" ],
      "venue" : "In ICML,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Order preserving hashing for approximate nearest neighbor search",
      "author" : [ "J. Wang", "N. Yu", "S. Li" ],
      "venue" : "In ACM MM,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Deep supervised hashing with triplet labels",
      "author" : [ "X. Wang", "Y. Shi", "K.M. Kitani" ],
      "venue" : "In ACCV,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Spectral hashing",
      "author" : [ "Y. Weiss", "A. Torralba", "R. Fergus" ],
      "venue" : "In NIPS, pages 1753–1760,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Supervised hashing for image retrieval via image representation learning",
      "author" : [ "R. Xia", "Y. Pan", "H. Lai", "C. Liu", "S. Yan" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Supervised learning of semantics-preserving hash via deep convolutional neural networks",
      "author" : [ "H.F. Yang", "K. Lin", "C.S. Chen" ],
      "venue" : "IEEE TPAMI,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2017
    }, {
      "title" : "Deep semantic-preserving and ranking-based hashing for image retrieval",
      "author" : [ "T. Yao", "F. Long", "T. Mei", "Y. Rui" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2016
    }, {
      "title" : "Supervised hashing with latent factor models",
      "author" : [ "P. Zhang", "W. Zhang", "W.-J. Li", "M. Guo" ],
      "venue" : "In SIGIR,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Bit-scalable deep hashing with regularized similarity learning for image retrieval and person re-identification",
      "author" : [ "R. Zhang", "L. Lin", "W. Zuo", "L. Zhang" ],
      "venue" : "IEEE TIP,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Efficient training of very deep neural networks for supervised hashing",
      "author" : [ "Z. Zhang", "Y. Chen", "V. Saligrama" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "Deep semantic ranking based hashing for multi-label image retrieval",
      "author" : [ "F. Zhao", "Y. Huang", "L. Wang", "T. Tan" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    }, {
      "title" : "Deep hashing network for efficient similarity retrieval",
      "author" : [ "H. Zhu", "M. Long", "J. Wang", "Y. Cao" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Locality Sensitive Hashing (LSH) [3] is one of the representative methods, which uses random linear projections to map nearby data into similar binary codes.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "In order to generalize LSH to accommodate arbitrary kernel functions, the Kenelized Locality Sensitive Hashing (KLSH) [7] is proposed to deal with high-dimensional kernelized data.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 4,
      "context" : "Other variants of LSH are also proposed in recent years, such as super-bit LSH [5], non-metric LSH [14].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 13,
      "context" : "Other variants of LSH are also proposed in recent years, such as super-bit LSH [5], non-metric LSH [14].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 3,
      "context" : "Iterative Quantization (ITQ) [4] is one of the representative unsupervised hashing methods, in which the projection matrix is optimized by iterative projection and thresholding according to the given training samples.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 12,
      "context" : "Supervised Hashing with Kernels (KSH) [13]",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : "Binary Reconstruction Embedding (BRE) [6] learns the hash functions by explicitly minimizing the reconstruction error between the original distances and the reconstructed distances in Hamming space.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "Order Preserving Hashing (OPH) [17] learns the hash codes by preserving the supervised ranking list information, which is calculated based on the semantic labels.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 14,
      "context" : "Supervised Discrete Hashing (SDH) [15] aims to directly optimize the binary hash codes using the discrete cyclic coordinate descend method.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 19,
      "context" : "Convolutional Neural Network Hashing (CNNH) [20] is one of the early works to incorporate deep neural networks into hash coding, which consists of two stages to learn the image representations and hash codes.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "To overcome the shortcomings of CNNH, Network In Network Hashing (NINH) [8] presents a triplet ranking loss to capture the relative similarities of images.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 25,
      "context" : "Deep Semantic Ranking Hashing (DSRH) [26] learns the hash functions by preserving semantic similarity between multi-label images.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 17,
      "context" : "Other ranking-based deep hashing methods have also been proposed in recent years [18, 22].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 21,
      "context" : "Other ranking-based deep hashing methods have also been proposed in recent years [18, 22].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "Besides the triplet ranking based methods, some pairwise label based deep hashing methods are also exploited [9, 27].",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 26,
      "context" : "Besides the triplet ranking based methods, some pairwise label based deep hashing methods are also exploited [9, 27].",
      "startOffset" : 109,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "A novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) is proposed to train very deep neural networks for supervised hashing in [25].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 24,
      "context" : "[25] relaxes the binary constraint to be continuous, then thresholds the obtained continuous variables to be binary codes.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "Recent works try to divide the whole learning process into two streams under the multitask learning framework [11, 21, 22].",
      "startOffset" : 110,
      "endOffset" : 122
    }, {
      "referenceID" : 20,
      "context" : "Recent works try to divide the whole learning process into two streams under the multitask learning framework [11, 21, 22].",
      "startOffset" : 110,
      "endOffset" : 122
    }, {
      "referenceID" : 21,
      "context" : "Recent works try to divide the whole learning process into two streams under the multitask learning framework [11, 21, 22].",
      "startOffset" : 110,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "In order to have a fair comparison with other deep hashing methods, we choose the CNN-F network architecture [2] as a basic component of our algorithm.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "This architecture is widely used to learn the hash function in recent works [9, 18].",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "This architecture is widely used to learn the hash function in recent works [9, 18].",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "Most of the previous works make use of the label information under a two stream multi-task learning framework [21, 22].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : "Most of the previous works make use of the label information under a two stream multi-task learning framework [21, 22].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "Similar to [9, 18, 22], we solve this problem by introducing an auxiliary variable.",
      "startOffset" : 11,
      "endOffset" : 22
    }, {
      "referenceID" : 17,
      "context" : "Similar to [9, 18, 22], we solve this problem by introducing an auxiliary variable.",
      "startOffset" : 11,
      "endOffset" : 22
    }, {
      "referenceID" : 21,
      "context" : "Similar to [9, 18, 22], we solve this problem by introducing an auxiliary variable.",
      "startOffset" : 11,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "Similar to [8, 12, 20, 24], we use a subset of 195,834 images which are associated with the 21 most frequent concepts.",
      "startOffset" : 11,
      "endOffset" : 26
    }, {
      "referenceID" : 11,
      "context" : "Similar to [8, 12, 20, 24], we use a subset of 195,834 images which are associated with the 21 most frequent concepts.",
      "startOffset" : 11,
      "endOffset" : 26
    }, {
      "referenceID" : 19,
      "context" : "Similar to [8, 12, 20, 24], we use a subset of 195,834 images which are associated with the 21 most frequent concepts.",
      "startOffset" : 11,
      "endOffset" : 26
    }, {
      "referenceID" : 23,
      "context" : "Similar to [8, 12, 20, 24], we use a subset of 195,834 images which are associated with the 21 most frequent concepts.",
      "startOffset" : 11,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "We follow the previous experimental setting in [8, 9, 18].",
      "startOffset" : 47,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "We follow the previous experimental setting in [8, 9, 18].",
      "startOffset" : 47,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "We follow the previous experimental setting in [8, 9, 18].",
      "startOffset" : 47,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "Unsupervised hashing methods include SH [19], ITQ [4].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 3,
      "context" : "Unsupervised hashing methods include SH [19], ITQ [4].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : "Supervised hashing methods include SPLH [16], KSH [13], FastH [10], LFH [23], and SDH [15].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "Supervised hashing methods include SPLH [16], KSH [13], FastH [10], LFH [23], and SDH [15].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "Supervised hashing methods include SPLH [16], KSH [13], FastH [10], LFH [23], and SDH [15].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "Supervised hashing methods include SPLH [16], KSH [13], FastH [10], LFH [23], and SDH [15].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "Supervised hashing methods include SPLH [16], KSH [13], FastH [10], LFH [23], and SDH [15].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 0,
      "context" : "The deep hashing methods include DQN [1], DHN [27], CNNH [20], NINH [8], DSRH [26], DSCH [24], DRCSH [24], DPSH [9], DTSH [18] and VDSH [25].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 26,
      "context" : "The deep hashing methods include DQN [1], DHN [27], CNNH [20], NINH [8], DSRH [26], DSCH [24], DRCSH [24], DPSH [9], DTSH [18] and VDSH [25].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "The deep hashing methods include DQN [1], DHN [27], CNNH [20], NINH [8], DSRH [26], DSCH [24], DRCSH [24], DPSH [9], DTSH [18] and VDSH [25].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "The deep hashing methods include DQN [1], DHN [27], CNNH [20], NINH [8], DSRH [26], DSCH [24], DRCSH [24], DPSH [9], DTSH [18] and VDSH [25].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "The deep hashing methods include DQN [1], DHN [27], CNNH [20], NINH [8], DSRH [26], DSCH [24], DRCSH [24], DPSH [9], DTSH [18] and VDSH [25].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 23,
      "context" : "The deep hashing methods include DQN [1], DHN [27], CNNH [20], NINH [8], DSRH [26], DSCH [24], DRCSH [24], DPSH [9], DTSH [18] and VDSH [25].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 23,
      "context" : "The deep hashing methods include DQN [1], DHN [27], CNNH [20], NINH [8], DSRH [26], DSCH [24], DRCSH [24], DPSH [9], DTSH [18] and VDSH [25].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "The deep hashing methods include DQN [1], DHN [27], CNNH [20], NINH [8], DSRH [26], DSCH [24], DRCSH [24], DPSH [9], DTSH [18] and VDSH [25].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 17,
      "context" : "The deep hashing methods include DQN [1], DHN [27], CNNH [20], NINH [8], DSRH [26], DSCH [24], DRCSH [24], DPSH [9], DTSH [18] and VDSH [25].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 24,
      "context" : "The deep hashing methods include DQN [1], DHN [27], CNNH [20], NINH [8], DSRH [26], DSCH [24], DRCSH [24], DPSH [9], DTSH [18] and VDSH [25].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 24,
      "context" : "Following [25], the pre-trained CNN-F model is used to extract CNN features on CIFAR-10, while a 500 dimensional bag-of-words feature vector is used to represent each image on NUS-WIDE for VDSH.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 7,
      "context" : "Similar to [8], we adopt four widely used evaluation metrics to evaluate the image retrieval quality: Mean Average Precision (MAP) for different number of bits, precision curves within Hamming distance 2, precision curves with different number of top returned samples and precision-recall curves.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 8,
      "context" : "First, we only consider the pairwise label information while neglecting the linear classification information in Equation 7, which is named DSDH-A (similar to [9]).",
      "startOffset" : 159,
      "endOffset" : 162
    } ],
    "year" : 2017,
    "abstractText" : "With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefiting from recent advances in deep learning, deep hashing methods have achieved promising results for image retrieval. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a deep supervised discrete hashing algorithm based on the assumption that the learned binary codes should be ideal for classification. Both the pairwise label information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithm. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our method outperforms current state-of-the-art methods on benchmark datasets.",
    "creator" : null
  }
}