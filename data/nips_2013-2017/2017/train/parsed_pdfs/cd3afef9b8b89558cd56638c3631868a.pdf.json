{
  "name" : "cd3afef9b8b89558cd56638c3631868a.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Random Permutation Online Isotonic Regression",
    "authors" : [ "Wojciech Kotłowski", "Wouter M. Koolen", "Alan Malek" ],
    "emails" : [ "wkotlowski@cs.put.poznan.pl", "wmkoolen@cwi.nl", "amalek@mit.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A function f : R → R is called isotonic (non-decreasing) if x ≤ y implies f(x) ≤ f(y). Isotonic functions model monotonic relationships between input and output variables, like those between drug dose and response [25] or lymph node condition and survival time [24]. The problem of isotonic regression is to find the isotonic function that best explains a given data set or population distribution. The isotonic regression problem has been extensively studied in statistics [1, 24], which resulted in efficient optimization algorithms for fitting isotonic functions to the data [7, 16] and sharp convergence rates of estimation under various model assumptions [26, 29].\nIn online learning problems, the data arrive sequentially, and the learner is tasked with predicting each subsequent data point as it arrives [6]. In online isotonic regression, the natural goal is to predict the incoming data points as well as the best isotonic function in hindsight. Specifically, for time steps t = 1, . . . , T , the learner observes an instance xi ∈ R, makes a prediction ŷi of the true label yi, which is assumed to lie in [0, 1]. There is no restriction that the labels or predictions be isotonic. We evaluate a prediction ŷi by its squared loss (ŷi − yi)2. The quality of an algorithm is measured by its regret, ∑T t=1(ŷi − yi)2 − L∗T , where L∗T is the loss of the best isotonic function on the entire data sequence.\nIsotonic regression is nonparametric: the number of parameters grows linearly with the number of data points. It is thus natural to ask whether there are efficient, provably low regret algorithms for online isotonic regression. As of yet, the picture is still very incomplete in the online setting. The first online results were obtained in the recent paper [14] which considered linearly ordered domains in the adversarial fixed design model, i.e. a model in which all the inputs x1, . . . , xT are given to the learner before the start of prediction. The authors show that, due to the nonparametric nature of the problem, many textbook online learning algorithms fail to learn at all (including Online Gradient Descent, Follow the Leader and Exponential Weights) in the sense that their worst-case regret grows linearly with the number of data points. They prove a Ω(T 1 3 ) worst case regret lower bound, and develop a matching algorithm that achieves the optimal Õ(T 1 3 ) regret. Unfortunately, the fixed design assumption is often unrealistic. This leads us to our main question: Can we design methods for online isotonic regression that are practical (do not hinge on fixed design)?\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nOur contributions Our long-term goal is to design practical and efficient methods for online isotonic regression, and in this work we move beyond the fixed design model and study algorithms that do not depend on future instances. Unfortunately, the completely adversarial design model (in which the instances are selected by an adaptive adversary) is impossibly hard: every learner can suffer linear regret in this model [14]. So in order to drop the fixed design assumption, we need to constrain the adversary in some other way. In this paper we consider the natural random permutation model, in which all T instances and labels are chosen adversarially before the game begins but then are presented to the learner in a random order.\nThis model corresponds with the intuition that the data gathering process (which fixes the order) is independent of the underlying data generation mechanism (which fixes instances and labels). We will show that learning is possible in the random permutation model (in fact we present a reduction showing that it is not harder than adversarial fixed design) by proving an Õ(T 1 3 ) upper bound on regret for an online-to-batch conversion of the optimal fixed design algorithm from [14] (Section 3).\nOur main tool for analyzing the random permutation model is the leave-one-out loss, drawing interesting connections with cross-validation and calibration. The leave-one-out loss on a set of t labeled instances is the error of the learner predicting the i-th label after seeing all remaining t− 1 labels, averaged uniformly over i = 1, . . . , t. We begin by proving a general correspondence between regret and leave-one-out loss for the random permutation model in Section 2.1, which allows us to use excess leave-one-out loss as a proxy for regret. We then describe a version of online-to-batch conversion that relates the fixed design model with the random permutation model, resulting in an algorithm that attains the optimal Õ(T 1 3 ) regret.\nSection 4 then turns to the computationally efficient and natural class of forward algorithms that use an offline optimization oracle to form their prediction. This class contains most common online isotonic regression algorithms. We then show a O(T 1 2 ) upper bound on the regret for the entire class, which improves to O(T 1 3 ) for the well-specified case where the data are in fact generated from an isotonic function plus i.i.d. noise (the most common model in the statistics literature).\nWhile forward algorithms match the lower bound for the well-specified case, there is a factor T 1 6 gap in the random permutation case. Section 4.6 proposes a new algorithm that calls a weighted offline oracle with a large weight on the current instance. This algorithm can be efficiently computed via [16]. We prove necessary bounds on the weight.\nRelated work Offline isotonic regression has been extensively studied in statistics starting from work by [1, 4]. Applications range across statistics, biology, medicine, psychology, etc. [24, 15, 25, 22, 17]. In statistics, isotonic regression is studied in generative models [26, 3, 29]. In machine learning, isotonic regression is used for calibrating class probability estimates [28, 21, 18, 20, 27], ROC analysis [8], training Generalized Linear Models and Single Index Models[12, 11], data cleaning [13], and ranking [19]. Fast algorithms for partial ordering are developed in [16].\nIn the online setting, [5] bound the minimax regret for monotone predictors under logarithmic loss and [23, 10] study online nonparametric regression in general. Efficient algorithms and worst-cases regret bounds for fixed design online isotonic regression are studied in [14]. Finally, the relation between regret and leave-one-out loss was pioneered by [9] for linear regression."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "Given a finite set of instances {x1, . . . , xt} ⊂ R, a function f : {x1, . . . , xt} → [0, 1] is isotonic (non-decreasing) if xi ≤ xj implies f(xi) ≤ f(xj) for all i, j ∈ {1, . . . , t}. Given a set of labeled instances D = {(x1, y1), . . . , (xt, yt)} ⊂ R× [0, 1], let L∗(D) denote the total squared loss of the best isotonic function on D,\nL∗(D) := min isotonic f t∑ i=1 (yi − f(xi))2.\nThis convex optimization problem can be solved by the celebrated Pool Adjacent Violators Algorithm (PAVA) in time linear in t [1, 7]. The optimal solution, called the isotonic regression function, is piecewise constant and its value on any of its levels sets equals the average of labels within that set [24].\nOnline isotonic regression in the random permutation model is defined as follows. At the beginning of the game, the adversary chooses data instances x1 < . . . < xT 1 and labels y1, . . . , yT . A permutation σ = (σ1, . . . , σT ) of {1, . . . , T} is then drawn uniformly at random and used to determine the order in which the data will be revealed. In round t, the instance xσt is revealed to the learner who then predicts ŷσt . Next, the learner observes the true label yσt and incurs the squared loss (ŷσt − yσt)2. For a fixed permutation σ, we use the shorthand notation L∗t = L\n∗({(xσ1 , yσ1), . . . , (xσt , yσt)}) to denote the optimal isotonic regression loss of the first t labeled instances (L∗t will clearly depend on σ, except for the case t = T ). The goal of the learner is to minimize the expected regret,\nRT := Eσ [ T∑ t=1 (yσt − ŷσt)2 ] − L∗T = T∑ t=1 rt,\nwhere we have decomposed the regret into its per-round increase, rt := Eσ [ (yσt − ŷσt)2 − L∗t + L∗t−1 ] , (1)\nwith L∗0 := 0. To simplify the analysis, let us assume that the prediction strategy does not depend on the order in which the past data were revealed (which is true for all algorithms considered in this paper). Fix t and define D = {(xσ1 , yσ1), . . . , (xσt , yσt)} to be the set of first t labeled instances. Furthermore, let D−i = D\\{(xσi , yσi)} denote the set D with the instance from round i removed. Using this notation, the expression under the expectation in (1) can be written as( yσt − ŷσt(D−t)\n)2 − L∗(D) + L∗(D−t), where we made the dependence of ŷσt on D−t explicit (and used the fact that it only depends on the set of instances, not on their order). By symmetry of the expectation over permutations with respect to the indices, we have\nEσ [( yσt − ŷσt(D−t) )2] = Eσ [( yσi − ŷσi(D−i) )2] , and Eσ [ L∗(D−t) ] = Eσ [ L∗(D−i) ] ,\nfor all i = 1, . . . , t. Thus, (1) can as well be rewritten as: rt = Eσ [ 1\nt t∑ i=1 (( yσi − ŷσi(D−i) )2 + L∗(D−i) ) − L∗(D) ] .\nLet us denote the expression inside the expectation by rt(D) to stress its dependence on the set of instances D, but not on the order in which they were revealed. If we can show that rt(D) ≤ Bt holds for all t, then its expectation has the same bound, so RT ≤ ∑T t=1Bt."
    }, {
      "heading" : "2.1 Excess Leave-One-Out Loss and Regret",
      "text" : "Our main tool for analyzing the random permutation model is the leave-one-out loss. In the leave-one-out model, there is no sequential structure. The adversary picks a data set D = {(x1, y1), . . . , (xt, yt)} with x1 < . . . < xt. An index i is sampled uniformly at random, the learner is given D−i, the entire data set except (xi, yi), and predicts ŷi (as a function of D−i) on instance xi. We call the difference between the expected loss of the learner and L∗(D) the expected excess leave-one-out loss:\n`oot(D) := 1\nt (( t∑ i=1 ( yi − ŷi(D−i) )2)− L∗(D)). (2) The random permutation model has the important property that the bound on the excess leave-one-out loss of a prediction algorithm translates into a regret bound. A similar result has been shown by [9] for expected loss in the i.i.d. setting. Lemma 2.1. rt(D) ≤ `oot(D) for any t and any data set D = {(x1, y1), . . . , (xt, yt)}.\nProof. As x1 < . . . < xt, let (y∗1 , . . . , y ∗ t ) = argminf1≤...≤ft ∑t i=1(yi − fi)2 be the isotonic\nregression function on D. From the definition of L∗, we can see that L∗(D) = ∑t i=1(y ∗ i − yi)2 ≥ L∗(D−i) + (yi − y∗i )2. Thus, the regret increase rt(D) is bounded by\nrt(D) = t∑ i=1 (yi − ŷi)2 + L∗(D−i) t − L∗(D) ≤ t∑ i=1 (yi − ŷi)2 − (yi − y∗i )2 t = `oot(D).\n1 We assume all points xt are distinct as it will significantly simplify the presentation. All results in this paper are also valid for the case x1 ≤ . . . ≤ xT .\nHowever, we note that lower bounds for `oot(D) do not imply lower bounds on regret.\nIn what follows, our strategy will be to derive bounds `oot(D) ≤ Bt for various algorithms, from which the regret bound RT ≤ ∑T t=1Bt can be immediately obtained. From now on, we abbreviate `oot(D) to `oot, (as D is clear from the context); we will also consistently assume x1 < . . . < xt."
    }, {
      "heading" : "2.2 Noise free case",
      "text" : "As a warm-up, we analyze the noise-free case (when the labels themselves are isotonic) and demonstrate that analyzing `oot easily results in an optimal bound for this setting. Proposition 2.2. Assume that the labels satisfy y1 ≤ y2 ≤ . . . ≤ yt. The prediction ŷi that is the linear interpolation between adjacent labels ŷi = 12 (yi−1 + yi+1), has\n`oot ≤ 1\n2t , and thus RT ≤\n1 2 log(T + 1).\nProof. For δi := yi − yi−1, it is easy to check that `oot = 14t ∑t i=1(δi+1 − δi)2 because the L∗(D) term is zero. This expression is a convex function of δ1, . . . , δt+1. Note that δi ≥ 0 for each i = 1, . . . , t+ 1, and ∑t+1 i=1 δi = 1. Since the maximum of a convex function is at the boundary of the feasible region, the maximizer is given by δi = 1 for some i ∈ {1, . . . , t+ 1}, and δj = 0 for all j ∈ {1, . . . , t+ 1}, j 6= i. This implies that `oot ≤ (2t)−1."
    }, {
      "heading" : "2.3 General Lower Bound",
      "text" : "In [14], a general lower bound was derived showing that the regret of any online isotonic regression procedure is at least Ω(T 1 3 ) for the adversarial setup (when labels and the index order were chosen adversarially). This lower bound applies regardless of the order of outcomes, and hence it is also a lower bound for the random permutation model. This bound translates into `oot = Ω(t−2/3)."
    }, {
      "heading" : "3 Online-to-batch for fixed design",
      "text" : "Here, we describe an online-to-batch conversion that relates the adversarial fixed design model with the random permutation model considered in this paper. In the fixed design model with time horizon Tfd the learner is given the points x1, . . . , xTfd in advance (which is not the case in the random permutation model), but the adversary chooses the order σ in which the labels are revealed (as opposed to σ being drawn at random). We can think of an algorithm for fixed design as a prediction function\nŷfd ( xσt ∣∣yσ1 , . . . , yσt−1 , {x1, . . . , xTfd}),\nfor any order σ, any set {x1, . . . , xTfd} (and hence any time horizon Tfd), and any time t. This notation is quite heavy, but makes it explicit that the learner, while predicting at point xσt , knows the previously revealed labels and the whole set of instances.\nIn the random permutation model, at trial t, the learner only knows the previously revealed t − 1 labeled instances and predicts on the new instance. Without loss of generality, denote the past instances by D−i = {(x1, y1), . . . , (xi−1, yi−1), (xi+1, yi+1), . . . (xt, yt)}, and the new instance by xi, for some i ∈ {1, . . . , t}. Given an algorithm for fixed design ŷfd, we construct a prediction ŷt = ŷt(D−i, xi) of the algorithm in the random permutation model. The reduction goes through an online-to-batch conversion. Specifically, at trial t, given past labeled instances D−i, and a new point xi, the learner plays the expectation of the prediction of the fixed design algorithm with time horizon T fd = t and points {x1, . . . , xt} under a uniformly random time from the past j ∈ {1, . . . , t} and a random permutation σ on {1, . . . , t}, with σt = i, i.e.2\nŷt = E{σ:σt=i} [ 1\nt t∑ j=1 ŷfd(xi|yσ1 , . . . , yσj−1 , {x1, . . . , xt} )] . (3)\n2Choosing the prediction as an expectation is elegant but inefficient. However, the proof indicates that we might as well sample a single j and a single random permutation σ to form the prediction and the reduction would also work in expectation.\nNote that this is a valid construction, as the right hand side only depends on D−i and xi, which are known to the learner in a random permutation model at round t. We prove (in Appendix A) that the excess leave-one-out loss of ŷ at trial t is upper bounded by the expected regret (over all permutations) of ŷfd in trials 1, . . . , t divided by t:\nTheorem 3.1. Let D = {(x1, y1), . . . , (xt, yt)} be a set of t labeled instances. Fix any algorithm ŷfd for online adversarial isotonic regression with fixed design, and let Regt(ŷ\nfd |σ) denote its regret on D when the labels are revealed in order σ. The random permutation learner ŷ from (3) ensures `oot(D) ≤ 1tEσ[Regt(ŷ fd |σ)].\nThis constructions allows immediate transport of the Õ(T 1 3\nfd) fixed design regret result from [14].\nTheorem 3.2. There is an algorithm for the random-permutation model with excess leave-one-out loss `oot = Õ(t− 2 3 ) and hence expected regret RT ≤ ∑ t Õ(t − 23 ) = Õ(T 1 3 )."
    }, {
      "heading" : "4 Forward Algorithms",
      "text" : "For clarity of presentation, we use vector notation in this section: y = (y1, . . . , yt) is the label vector, y∗ = (y∗1 , . . . , y ∗ t ) is the isotonic regression function, and y−i = (y1, . . . , yi−1, yi+1, . . . , yt) is y with i-th label removed. Moreover, keeping in mind that x1 < . . . < xt, we can drop xi’s entirely from the notation and refer to an instance xi simply by its index i.\nGiven labels y−i and some index i to predict on, we want a good prediction for yi. Follow the Leader (FL) algorithms, which predict using the best isotonic function on the data seen so far, are not directly applicable to online isotonic regression: the best isotonic function is only defined at the observed data instances and can be arbitrary (up to monotonicity constraint) otherwise. Instead, we analyze a simple and natural class of algorithms which we dub forward algorithms3. We define a forward algorithm, or FA, to be any algorithm that estimates a label y′i ∈ [0, 1] (possibly dependent on i and y−i), and plays with the FL strategy on the sequence of past data including the new instance with the estimated label, i.e. performs offline isotonic regression on y′,\nŷ = argmin f1≤...≤ft { t∑ j=1 (y′j − fj)2 } , where y′ = (y1, . . . , yi−1, y′i, yi+1, . . . , yt).\nThen, FA predicts with ŷi, the value at index i of the offline function of the augmented data. Note that if the estimate turned out to be correct (y′i = yi), the forward algorithm would suffer no additional loss for that round.\nForward algorithms are practically important: we will show that many popular algorithms can be cast as FA with a particular estimate. FA automatically inherit any computational advances for offline isotonic regression; in particular, they scale efficiently to partially ordered data [16]. To our best knowledge, we are first to give bounds on the performance of these algorithms in the online setting.\nAlternative formulation We can describe a FA using a minimax representation of the isotonic regression [see, e.g., 24]: the optimal isotonic regression y∗ satisfies\ny∗i = min r≥i max `≤i y`,r = max `≤i min r≥i y`,r, (4)\nwhere y`,r = ∑r j=` yj r−`+1 . The “saddle point” (`i, ri) for which y ∗ i = y`i,ri , specifies the boundaries of the level set {j : y∗j = y∗i } of the isotonic regression function that contains i. It follows from (4) that isotonic regression is monotonic with respect to labels: for any two label sequences y and z such that yi ≤ zi for all i, we have y∗i ≤ z∗i for all i. Thus, if we denote the predictions for label estimates y′i = 0 and y ′ i = 1 by ŷ 0 i and ŷ 1 i , respectively, the monotonicity implies that any FA has ŷ0i ≤ ŷi ≤ ŷ1i . Conversely, using the continuity of isotonic regression y∗ as a function of y, (which follows from (4)), we can show that for any prediction ŷi with ŷ0i ≤ ŷi ≤ ŷ1i , there exists an estimate y′t ∈ [0, 1] that could generate this prediction. Hence, we can equivalently interpret FA as an algorithm which in each trial predicts with some ŷi in the range [ŷ0i , ŷ 1 i ].\n3The name highlights resemblance to the Forward algorithm introduced by [2] for exponential family models."
    }, {
      "heading" : "4.1 Instances",
      "text" : "With the above equivalence between forward algorithms and algorithms that predict in [ŷ0i , ŷ 1 i ], we can show that many of the well know isotonic regression algorithms are forward algorithms and thereby add weight to our next section where we prove regret bounds for the entire class.\nIsotonic regression with interpolation (IR-Int)[28] Given y−i and index i, the algorithm first computes f∗, the isotonic regression of y−i, and then predicts with ŷinti = 1 2 ( f∗i−1 + f ∗ i+1 ) , where we used f∗0 = 0 and f ∗ t+1 = 1. To see that this is a FA, note that if we use estimate y ′ i = ŷ int i , the isotonic regression of y′ = (y1, . . . , yi−1, y′i, yi+1, . . . , yt) is ŷ = (f ∗ 1 , . . . , f ∗ i−1, y ′ i, f ∗ i+1, . . . , f ∗ t ). This is because: i) ŷ is isotonic by construction; ii) f∗ has the smallest squared error loss for y−t among isotonic functions; and iii) the loss of ŷ on point y′i is zero, and the loss of ŷ on all other points is equal to the loss of f∗.\nDirect combination of ŷ0i and ŷ1i . It is clear from Section 4, that any algorithm that predicts ŷi = λiŷ 0 i + (1 − λi)ŷ1i for some λi ∈ [0, 1] is a FA. The weight λi can be set to a constant (e.g., λi = 1/2), or can be chosen depending on ŷ1i and ŷ 0 i . Such algorithms were considered by [27]:\nlog-IVAP : ŷlogi = ŷ1i\nŷ1i + 1− ŷ0i , Brier-IVAP : ŷBrieri =\n1 + (ŷ0i ) 2 − (1− ŷ1i )2\n2 .\nIt is straightforward to show that both algorithms satisfy ŷ0i ≤ ŷi ≤ ŷ1i and are thus instances of FA.\nLast-step minimax (LSM). LSM plays the minimax strategy with one round remaining,\nŷi = argmin ŷ∈[0,1] max yi∈[0,1]\n{ (ŷ − yi)2 − L∗(y) } ,\nwhere L∗(y) is the isotonic regression loss on y. Define L∗b = L ∗(y1, . . . , yi−1, b, yi+1, . . . , yt) for b ∈ {0, 1}, i.e. L∗b is the loss of isotonic regression function with label estimate y′i = b. In Appendix B we show that ŷi = 1+L∗0−L ∗ 1\n2 and it is also an instance of FA."
    }, {
      "heading" : "4.2 Bounding the leave-one-out loss",
      "text" : "We now give a O( √\nlog t t ) bound on the leave-one-out loss for forward algorithms. Interestingly, the\nbound holds no matter what label estimate the algorithm uses. The proof relies on the stability of isotonic regression with respect to a change of a single label. While the bound looks suboptimal in light of Section 2.3, we will argue in Section 4.5 that the bound is actually tight (up to a logarithmic factor) for one FA and experimentally verify that all other mentioned forward algorithms also have a tight lower bound of that form for the same sequence of outcomes.\nWe will bound `oot by defining δi = ŷi − y∗i and using the following simple inequality:\n`oot = 1\nt t∑ i=1 ( (ŷi − yi)2 − (y∗i − yi)2 ) = 1 t t∑ i=1 (ŷi − y∗i )(ŷi + y∗i − 2yi) ≤ 2 t t∑ i=1 |δi|.\nTheorem 4.1. Any forward algorithm has `oot = O (√ log t t ) .\nProof. Fix some forward algorithm. For any i, let {j : y∗j = y∗i } = {`i, . . . , ri}, for some `i ≤ i ≤ ri, be the level set of isotonic regression at level y∗i . We need the stronger version of the minimax representation, shown in Appendix C:\ny∗i = min r≥i y`i,r = max`≤i y`,ri . (5)\nWe partition the points {1, . . . , t} into K consecutive segments: Sk = { i : y∗i ∈ [ k−1 K , k K )} for\nk = 1, . . . ,K − 1 and SK = { i : y∗i ≥ K−1K } . Due to monotonicity of y∗, Sk are subsets of the form {`k, . . . , rk} (where we use rk = `k − 1 if Sk is empty). From the definition, every level set of y∗ is contained in Sk for some k, and each `k (rk) is a left-end (right-end) of some level set.\nNow, choose some index i, and let Sk be such that i ∈ Sk. Let y′i be the estimate of the FA, and let y′ = (y1, . . . , yi−1, y ′ i, yi+1, . . . , yt). The minimax representation (4) and definition of FA imply\nŷi = max `≤i min r≥i y′`,r ≥ min r≥i y′`k,r = minr≥i { y`k,r + y′i − yi r − `k + 1 } ≥ min\nr≥i y`k,r + minr≥i y′i − yi r − `k + 1 ≥ min r≥`k y`k,r + minr≥i y′i − yi r − `k + 1\nby (5) ≥ y∗`k + minr≥i −1 r − `k + 1 ≥ y∗`k − 1 i− `k + 1 ≥ y∗i − 1 K − 1 i− `k + 1 .\nA symmetric argument gives ŷi ≤ y∗i + 1K + 1 rk−i+1 . Hence, we can bound |δi| = |ŷi − y ∗ i | ≤\n1 K + max\n{ 1\ni−`k+1 , 1 rk−i+1 } . Summing over i ∈ Sk yields ∑ i∈Sk |δi| ≤ |Sk| K + 2 ( 1 + log |Sk| ) ,\nwhich allows the bound\n`oot ≤ 2\nt ∑ i |δi| ≤ 2 K + 4 K t (1 + log t).\nThe theorem follows from setting K = Θ( √ t/ log t)."
    }, {
      "heading" : "4.3 Forward algorithms for the well-specified case",
      "text" : "While the `oot upper bound of the previous section yields a regret bound RT ≤ ∑ tO( √ log t/t) = Õ(T 1 2 ) that is a factor O(T 1 6 ) gap from the lower bound in Section 2.3, there are two pieces of good news. First, forward algorithms do get the optimal rate in the well-specified setting, popular in the classical statistics literature, where the labels are generated i.i.d. such that E[yi] = µi with isotonic µ1 ≤ . . . ≤ µt.4 Second, there is a Ω(t− 1 2 ) lower bound for forward algorithms as proven in the next section. Together, these results imply that the random permutation model in indeed harder than the well-specified case: forward algorithms are sufficient for the latter but not the former. Theorem 4.2. For data generated from the well-specified setting (monotonic means with i.i.d. noise), any FA has `oot = Õ(t− 2 3 ), which translates to a Õ(T 1 3 ) bound on the regret.\nThe proof is given in Appendix D. Curiously, the proof makes use of the existence of the seemingly unrelated optimal algorithm with Õ(t− 2 3 ) excess leave-one-out loss from Theorem 3.2."
    }, {
      "heading" : "4.4 Entropic loss",
      "text" : "We now abandon the squared loss for a moment and analyze how a FA performs when the loss function is the entropic loss, defined as −y log ŷ − (1− y) log(1− ŷ) for y ∈ [0, 1]. Entropic loss (precisely: its binary-label version known as log-loss) is extensively used in the isotonic regression context for maximum likelihood estimation [14] or for probability calibration [28, 21, 27]. A surprising fact in isotonic regression is that minimizing entropic loss5 leads to exactly the same optimal solution as in the squared loss case, the isotonic regression function y∗ [24].\nNot every FA is appropriate for entropic loss, as recklessly choosing the label estimate might result in an infinite loss in just a single trial (as noted by [27]). Indeed, consider a sequence of outcomes with y1 = 0 and yi = 1 for i > 1. While predicting on index i = 1, choosing y′1 = 1 results in ŷ1 = 1, for which the entropic loss is infinite (as y1 = 0). Does there exists a FA which achieves a meaningful bound on `oot in the entropic loss setup?\nWe answer this question in the affirmative, showing that the log-IVAP predictor FA gets the same excess-leave-one-out loss bound as given in Theorem 4.1. As the reduction from the regret to leaveone-out loss (Lemma 2.1) does not use any properties of the loss function, this immediately implies a bound on the expected regret. Interestingly, the proof (given in Appendix G) uses as an intermediate step the bound on |δi| for the worst possible forward algorithm which always produces the estimate being the opposite of the actual label.\nTheorem 4.3. The log-IVAP algorithm has `oot = O (√ log t t ) for the entropic loss.\n4The Ω(T 1/3) regret lower bound in [14] uses a mixture of well-specified distributions and still applies. 5In fact, this statement applies to any Bregman divergence [24]."
    }, {
      "heading" : "4.5 Lower bound",
      "text" : "The last result of this section is that FA can be made to have `oot = Ω(t− 1 2 ). We show this by means of a counterexample. Assume t = n2 for some integer n > 0 and let the labels be binary, yi ∈ {0, 1}. We split the set {1, . . . , t} into n consecutive segments, each of size n = √ t. The proportion of ones (yi = 1) in the k-th segment is equal to kn , but within each segment all ones precede all zeros. For instance, when t = 25, the label sequence is:\n10000︸ ︷︷ ︸ 1/5 11000︸ ︷︷ ︸ 2/5 11100︸ ︷︷ ︸ 3/5 11110︸ ︷︷ ︸ 4/5 11111︸ ︷︷ ︸ 5/5 ,\nOne can use the minimax formulation (4) to verify that the segments will correspond to the level sets of the isotonic regression and that y∗i = k n for any i in the k-th segment. This sequence is hard: Lemma 4.4. The IR-Int algorithm run on the sequence described above has `oot = Ω(t− 1 2 ).\nWe prove the lower bound for IR-Int, since the presentation (in Appendix E) is clearest. Empirical simulations showing that the other forward algorithms also suffer this regret are in Appendix F."
    }, {
      "heading" : "4.6 Towards optimal forward algorithms",
      "text" : "An attractive feature of forward algorithms is that they generalize to partial orders, for which efficient offline optimization algorithms exist. However, in Section 4 we saw that FAs only give a Õ(t− 1 2 ) rate, while in Section 3 we saw that Õ(t− 2 3 ) is possible (with an algorithm that is not known to scale to partial orders). Is there any hope of an algorithm that both generalizes and has the optimal rate?\nIn this section, we propose the Heavy-γ algorithm, a slight modification of the forward algorithm that plugs in label estimate y′i = γ ∈ [0, 1] with weight c (with unit weight on all other points), then plays the value of the isotonic regression function. Implementation is straightforward for offline isotonic regression algorithms that permit the specification of weights (such as [16]). Otherwise, we might simulate such weighting by plugging in c copies of the estimated label γ at location xi.\nWhat label estimate γ and weight c should we use? We show that the choice of γ is not very sensitive, but it is crucial to tune the weight to c = Θ(t 1 3 ). Lemmas H.1 and H.2 show that higher and lower c are necessarily sub-optimal for `oot. This leaves only one choice for c, for which we believe Conjecture 4.5. Heavy-γ with weight c = Θ(t 13 ) has `oot = Õ(t− 2 3 ).\nWe cannot yet prove this conjecture, although numerical experiments strongly suggest it. We do not believe that picking a constant label γ is special. For example, we might alternatively predict with the average of the predictions of Heavy-1 and Heavy-0. Yet not any label estimate works. In particular, if we estimate the label that would be predicted by IR-Int (see 4.1) and the discussion below it), and we plug that in with any weight c ≥ 0, then the isotonic regression function will still have that same label estimate as its value. This means that the Ω(t− 1 2 ) lower bound of Section 4.5 applies."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We revisit the problem of online isotonic regression and argue that we need a new perspective to design practical algorithms. We study the random permutation model as a novel way to bypass the stringent fixed design requirement of previous work. Our main tool in the design and analysis of algorithms is the leave-one-out loss, which bounds the expected regret from above. We start by observing that the adversary from the adversarial fixed design setting also provides a lower bound here. We then show that this lower bound can be matched by applying online-to-batch conversion to the optimal algorithm for fixed design. Next we provide an online analysis of the natural, popular and practical class of Forward Algorithms, which are defined in terms of an offline optimization oracle. We show that Forward algorithms achieve a decent regret rate in all cases, and match the optimal rate in special cases. We conclude by sketching the class of practical Heavy algorithms and conjecture that a specific parameter setting might guarantee the correct regret rate.\nOpen problem The next major challenge is the design and analysis of efficient algorithms for online isotonic regression on arbitrary partial orders. Heavy-γ is our current best candidate. We pose deciding if it in fact even guarantees Õ(T 1 3 ) regret on linear orders as an open problem."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Wojciech Kotłowski acknowledges support from the Polish National Science Centre (grant no. 2016/22/E/ST6/00299). Wouter Koolen acknowledges support from the Netherlands Organization for Scientific Research (NWO) under Veni grant 639.021.439. This work was done in part while Koolen was visiting the Simons Institute for the Theory of Computing."
    } ],
    "references" : [ {
      "title" : "An empirical distribution function for sampling with incomplete information",
      "author" : [ "M. Ayer", "H.D. Brunk", "G.M. Ewing", "W.T. Reid", "E. Silverman" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1955
    }, {
      "title" : "Relative loss bounds for on-line density estimation with the exponential family of distributions",
      "author" : [ "K. Azoury", "M. Warmuth" ],
      "venue" : "Journal of Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Rates of convergence for minimum contrast estimators",
      "author" : [ "Lucien Birgé", "Pascal Massart" ],
      "venue" : "Probability Theory and Related Fields,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1993
    }, {
      "title" : "Maximum likelihood estimates of monotone parameters",
      "author" : [ "H.D. Brunk" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1955
    }, {
      "title" : "Worst-case bounds for the logarithmic loss of predictors",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Isotone optimization in R: Pool-adjacent-violators algorithm (PAVA) and active set methods",
      "author" : [ "Jan de Leeuw", "Kurt Hornik", "Patrick Mair" ],
      "venue" : "Journal of Statistical Software,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "PAV and the ROC convex hull",
      "author" : [ "Tom Fawcett", "Alexandru Niculescu-Mizil" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Relative expected instantaneous loss bounds",
      "author" : [ "Jürgen Forster", "Manfred K Warmuth" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2002
    }, {
      "title" : "A chaining algorithm for online nonparametric regression",
      "author" : [ "Pierre Gaillard", "Sébastien Gerchinovitz" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Efficient learning of generalized linear and single index models with isotonic regression",
      "author" : [ "Sham M Kakade", "Varun Kanade", "Ohad Shamir", "Adam Kalai" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "The isotron algorithm: High-dimensional isotonic regression",
      "author" : [ "Adam Tauman Kalai", "Ravi Sastry" ],
      "venue" : "In COLT,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Rule learning with monotonicity constraints",
      "author" : [ "Wojciech Kotłowski", "Roman Słowiński" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Online isotonic regression",
      "author" : [ "Wojciech Kotłowski", "Wouter M. Koolen", "Alan Malek" ],
      "venue" : "Proceedings of the 29th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis",
      "author" : [ "J.B. Kruskal" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1964
    }, {
      "title" : "Fast, provable algorithms for isotonic regression in all `p-norms",
      "author" : [ "Rasmus Kyng", "Anup Rao", "Sushant Sachdeva" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Efficient regularized isotonic regression with application to gene–gene interaction search",
      "author" : [ "Ronny Luss", "Saharon Rosset", "Moni Shahar" ],
      "venue" : "Annals of Applied Statistics,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Predicting accurate probabilities with a ranking loss",
      "author" : [ "Aditya Krishna Menon", "Xiaoqian Jiang", "Shankar Vembu", "Charles Elkan", "Lucila Ohno- Machado" ],
      "venue" : "In Interantional Conference on Machine Learning (ICML),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Intervalrank: Isotonic regression with listwise and pairwise constraint",
      "author" : [ "T. Moon", "A. Smola", "Y. Chang", "Z. Zheng" ],
      "venue" : "In WSDM,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "On the relationship between binary classification, bipartite ranking, and binary class probability estimation",
      "author" : [ "Harikrishna Narasimhan", "Shivani Agarwal" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Predicting good probabilities with supervised learning",
      "author" : [ "Alexandru Niculescu-Mizil", "Rich Caruana" ],
      "venue" : "In ICML,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2005
    }, {
      "title" : "Consistent probabilistic outputs for protein function prediction",
      "author" : [ "G. Obozinski", "C.E. Grant", "G.R.G. Lanckriet", "M.I. Jordan", "W.W. Noble" ],
      "venue" : "Genome Biology,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "Online nonparametric regression",
      "author" : [ "Alexander Rakhlin", "Karthik Sridharan" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Order Restricted Statistical Inference",
      "author" : [ "T. Robertson", "F.T. Wright", "R.L. Dykstra" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1998
    }, {
      "title" : "Dose finding using the biased coin up-and-down design and isotonic regression",
      "author" : [ "Mario Stylianou", "Nancy Flournoy" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2002
    }, {
      "title" : "Estimating a regression function",
      "author" : [ "Sara Van de Geer" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1990
    }, {
      "title" : "Large-scale probabilistic predictors with and without guarantees of validity",
      "author" : [ "Vladimir Vovk", "Ivan Petej", "Valentina Fedorova" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Transforming classifier scores into accurate multiclass probability estimates",
      "author" : [ "Bianca Zadrozny", "Charles Elkan" ],
      "venue" : "In International Conference on Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2002
    }, {
      "title" : "Risk bounds in isotonic regression",
      "author" : [ "Cun-Hui Zhang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Isotonic functions model monotonic relationships between input and output variables, like those between drug dose and response [25] or lymph node condition and survival time [24].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 23,
      "context" : "Isotonic functions model monotonic relationships between input and output variables, like those between drug dose and response [25] or lymph node condition and survival time [24].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "The isotonic regression problem has been extensively studied in statistics [1, 24], which resulted in efficient optimization algorithms for fitting isotonic functions to the data [7, 16] and sharp convergence rates of estimation under various model assumptions [26, 29].",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 23,
      "context" : "The isotonic regression problem has been extensively studied in statistics [1, 24], which resulted in efficient optimization algorithms for fitting isotonic functions to the data [7, 16] and sharp convergence rates of estimation under various model assumptions [26, 29].",
      "startOffset" : 75,
      "endOffset" : 82
    }, {
      "referenceID" : 6,
      "context" : "The isotonic regression problem has been extensively studied in statistics [1, 24], which resulted in efficient optimization algorithms for fitting isotonic functions to the data [7, 16] and sharp convergence rates of estimation under various model assumptions [26, 29].",
      "startOffset" : 179,
      "endOffset" : 186
    }, {
      "referenceID" : 15,
      "context" : "The isotonic regression problem has been extensively studied in statistics [1, 24], which resulted in efficient optimization algorithms for fitting isotonic functions to the data [7, 16] and sharp convergence rates of estimation under various model assumptions [26, 29].",
      "startOffset" : 179,
      "endOffset" : 186
    }, {
      "referenceID" : 25,
      "context" : "The isotonic regression problem has been extensively studied in statistics [1, 24], which resulted in efficient optimization algorithms for fitting isotonic functions to the data [7, 16] and sharp convergence rates of estimation under various model assumptions [26, 29].",
      "startOffset" : 261,
      "endOffset" : 269
    }, {
      "referenceID" : 28,
      "context" : "The isotonic regression problem has been extensively studied in statistics [1, 24], which resulted in efficient optimization algorithms for fitting isotonic functions to the data [7, 16] and sharp convergence rates of estimation under various model assumptions [26, 29].",
      "startOffset" : 261,
      "endOffset" : 269
    }, {
      "referenceID" : 5,
      "context" : "In online learning problems, the data arrive sequentially, and the learner is tasked with predicting each subsequent data point as it arrives [6].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 13,
      "context" : "The first online results were obtained in the recent paper [14] which considered linearly ordered domains in the adversarial fixed design model, i.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "Unfortunately, the completely adversarial design model (in which the instances are selected by an adaptive adversary) is impossibly hard: every learner can suffer linear regret in this model [14].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 13,
      "context" : "We will show that learning is possible in the random permutation model (in fact we present a reduction showing that it is not harder than adversarial fixed design) by proving an Õ(T 1 3 ) upper bound on regret for an online-to-batch conversion of the optimal fixed design algorithm from [14] (Section 3).",
      "startOffset" : 287,
      "endOffset" : 291
    }, {
      "referenceID" : 15,
      "context" : "This algorithm can be efficiently computed via [16].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "Related work Offline isotonic regression has been extensively studied in statistics starting from work by [1, 4].",
      "startOffset" : 106,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "Related work Offline isotonic regression has been extensively studied in statistics starting from work by [1, 4].",
      "startOffset" : 106,
      "endOffset" : 112
    }, {
      "referenceID" : 25,
      "context" : "In statistics, isotonic regression is studied in generative models [26, 3, 29].",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "In statistics, isotonic regression is studied in generative models [26, 3, 29].",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 28,
      "context" : "In statistics, isotonic regression is studied in generative models [26, 3, 29].",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 27,
      "context" : "In machine learning, isotonic regression is used for calibrating class probability estimates [28, 21, 18, 20, 27], ROC analysis [8], training Generalized Linear Models and Single Index Models[12, 11], data cleaning [13], and ranking [19].",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 20,
      "context" : "In machine learning, isotonic regression is used for calibrating class probability estimates [28, 21, 18, 20, 27], ROC analysis [8], training Generalized Linear Models and Single Index Models[12, 11], data cleaning [13], and ranking [19].",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : "In machine learning, isotonic regression is used for calibrating class probability estimates [28, 21, 18, 20, 27], ROC analysis [8], training Generalized Linear Models and Single Index Models[12, 11], data cleaning [13], and ranking [19].",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "In machine learning, isotonic regression is used for calibrating class probability estimates [28, 21, 18, 20, 27], ROC analysis [8], training Generalized Linear Models and Single Index Models[12, 11], data cleaning [13], and ranking [19].",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 26,
      "context" : "In machine learning, isotonic regression is used for calibrating class probability estimates [28, 21, 18, 20, 27], ROC analysis [8], training Generalized Linear Models and Single Index Models[12, 11], data cleaning [13], and ranking [19].",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : "In machine learning, isotonic regression is used for calibrating class probability estimates [28, 21, 18, 20, 27], ROC analysis [8], training Generalized Linear Models and Single Index Models[12, 11], data cleaning [13], and ranking [19].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "In machine learning, isotonic regression is used for calibrating class probability estimates [28, 21, 18, 20, 27], ROC analysis [8], training Generalized Linear Models and Single Index Models[12, 11], data cleaning [13], and ranking [19].",
      "startOffset" : 191,
      "endOffset" : 199
    }, {
      "referenceID" : 10,
      "context" : "In machine learning, isotonic regression is used for calibrating class probability estimates [28, 21, 18, 20, 27], ROC analysis [8], training Generalized Linear Models and Single Index Models[12, 11], data cleaning [13], and ranking [19].",
      "startOffset" : 191,
      "endOffset" : 199
    }, {
      "referenceID" : 12,
      "context" : "In machine learning, isotonic regression is used for calibrating class probability estimates [28, 21, 18, 20, 27], ROC analysis [8], training Generalized Linear Models and Single Index Models[12, 11], data cleaning [13], and ranking [19].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 18,
      "context" : "In machine learning, isotonic regression is used for calibrating class probability estimates [28, 21, 18, 20, 27], ROC analysis [8], training Generalized Linear Models and Single Index Models[12, 11], data cleaning [13], and ranking [19].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 15,
      "context" : "Fast algorithms for partial ordering are developed in [16].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "In the online setting, [5] bound the minimax regret for monotone predictors under logarithmic loss and [23, 10] study online nonparametric regression in general.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 22,
      "context" : "In the online setting, [5] bound the minimax regret for monotone predictors under logarithmic loss and [23, 10] study online nonparametric regression in general.",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "In the online setting, [5] bound the minimax regret for monotone predictors under logarithmic loss and [23, 10] study online nonparametric regression in general.",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "Efficient algorithms and worst-cases regret bounds for fixed design online isotonic regression are studied in [14].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "Finally, the relation between regret and leave-one-out loss was pioneered by [9] for linear regression.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "This convex optimization problem can be solved by the celebrated Pool Adjacent Violators Algorithm (PAVA) in time linear in t [1, 7].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "This convex optimization problem can be solved by the celebrated Pool Adjacent Violators Algorithm (PAVA) in time linear in t [1, 7].",
      "startOffset" : 126,
      "endOffset" : 132
    }, {
      "referenceID" : 23,
      "context" : "The optimal solution, called the isotonic regression function, is piecewise constant and its value on any of its levels sets equals the average of labels within that set [24].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 8,
      "context" : "A similar result has been shown by [9] for expected loss in the i.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "In [14], a general lower bound was derived showing that the regret of any online isotonic regression procedure is at least Ω(T 1 3 ) for the adversarial setup (when labels and the index order were chosen adversarially).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "This constructions allows immediate transport of the Õ(T 1 3 fd) fixed design regret result from [14].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "FA automatically inherit any computational advances for offline isotonic regression; in particular, they scale efficiently to partially ordered data [16].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "(3)The name highlights resemblance to the Forward algorithm introduced by [2] for exponential family models.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 27,
      "context" : "Isotonic regression with interpolation (IR-Int)[28] Given y−i and index i, the algorithm first computes f∗, the isotonic regression of y−i, and then predicts with ŷ i = 1 2 ( f∗ i−1 + f ∗ i+1 ) , where we used f∗ 0 = 0 and f ∗ t+1 = 1.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 26,
      "context" : "Such algorithms were considered by [27]:",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "Entropic loss (precisely: its binary-label version known as log-loss) is extensively used in the isotonic regression context for maximum likelihood estimation [14] or for probability calibration [28, 21, 27].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 27,
      "context" : "Entropic loss (precisely: its binary-label version known as log-loss) is extensively used in the isotonic regression context for maximum likelihood estimation [14] or for probability calibration [28, 21, 27].",
      "startOffset" : 195,
      "endOffset" : 207
    }, {
      "referenceID" : 20,
      "context" : "Entropic loss (precisely: its binary-label version known as log-loss) is extensively used in the isotonic regression context for maximum likelihood estimation [14] or for probability calibration [28, 21, 27].",
      "startOffset" : 195,
      "endOffset" : 207
    }, {
      "referenceID" : 26,
      "context" : "Entropic loss (precisely: its binary-label version known as log-loss) is extensively used in the isotonic regression context for maximum likelihood estimation [14] or for probability calibration [28, 21, 27].",
      "startOffset" : 195,
      "endOffset" : 207
    }, {
      "referenceID" : 23,
      "context" : "A surprising fact in isotonic regression is that minimizing entropic loss5 leads to exactly the same optimal solution as in the squared loss case, the isotonic regression function y∗ [24].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 26,
      "context" : "Not every FA is appropriate for entropic loss, as recklessly choosing the label estimate might result in an infinite loss in just a single trial (as noted by [27]).",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 13,
      "context" : "(4)The Ω(T ) regret lower bound in [14] uses a mixture of well-specified distributions and still applies.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 23,
      "context" : "(5)In fact, this statement applies to any Bregman divergence [24].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : "Implementation is straightforward for offline isotonic regression algorithms that permit the specification of weights (such as [16]).",
      "startOffset" : 127,
      "endOffset" : 131
    } ],
    "year" : 2017,
    "abstractText" : "We revisit isotonic regression on linear orders, the problem of fitting monotonic functions to best explain the data, in an online setting. It was previously shown that online isotonic regression is unlearnable in a fully adversarial model, which lead to its study in the fixed design model. Here, we instead develop the more practical random permutation model. We show that the regret is bounded above by the excess leave-one-out loss for which we develop efficient algorithms and matching lower bounds. We also analyze the class of simple and popular forward algorithms and recommend where to look for algorithms for online isotonic regression on partial orders.",
    "creator" : "pdftk 2.02 - www.pdftk.com"
  }
}