{
  "name" : "50cf0763d8eb871776d4f28b39deb564.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On Separability of Loss Functions, and Revisiting Discriminative Vs Generative Models",
    "authors" : [ "Adarsh Prasad", "Alexandru Niculescu-Mizil", "Pradeep Ravikumar" ],
    "emails" : [ "adarshp@andrew.cmu.edu", "alex@nec-labs.com", "pradeepr@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Consider the classical conditional generative model setting, where we have a binary random response Y 2 {0, 1}, and a random covariate vector X 2 Rp, such that X|(Y = i) ⇠ P\n✓i for i 2 {0, 1}. Assuming that we know P (Y ) and {P\n✓i}1 i=0 , we can use the Bayes rule to predict the response Y given covariates X . This is said to be the generative model approach to classification. Alternatively, consider the conditional distribution P (Y |X) as specified by the Bayes rule, also called the discriminative model corresponding to the generative model specified above. Learning this conditional model directly is said to be the discriminative model approach to classification. In a classical paper [8], the authors provided theoretical justification for the common wisdom regarding generative and discriminative models: when the generative model assumptions hold, the generative model estimators initially converge faster as a function of the number of samples, but have the same asymptotic error rate as discriminative models. And when the generative model assumptions do not hold, the discriminative model estimators eventually overtake the generative model estimators. Their analysis however was for the specific generative-discriminative model pair of Naive Bayes, and logistic regression models, and moreover, was not under a high-dimensional sampling regime, when the number of samples could even be smaller than the number of parameters. In this paper, we aim to extend their analysis to these more general settings.\nDoing so however required some novel technical and conceptual developments. To motivate the machinery we develop, consider why the Naive Bayes model estimator might initially converge faster. The Naive Bayes model makes the conditional independence assumption that P (X|Y ) = Q\np\ns=1\nP (X s |Y ), so that the parameters of each of the conditional distributions P (X s |Y ) for s 2 {1, . . . , p} could be estimated independently. The corresponding log-likelihood loss function is thus fully “separable” into multiple components. The logistic regression log-likelihood on the other hand is seemingly much less “separable”, and in particular, it does not split into multiple components each of which can be estimated independently. In general, we do not expect the loss functions underlying statistical estimators to be fully separable into multiple components, so that we need a more flexible notion of separability, where different losses could be shown to be separable to differing degrees. In a very related note, though it might seem unrelated at first, the analysis of `1 convergence rates of\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nstatistical estimators considerably lags that of say ` 2 rates (see for instance, the unified framework of [7], which is suited to `\n2 rates but is highly sub-optimal for `1 rates). In part, the analysis of `1 rates is harder because it implicitly requires analysis at the level of individual coordinates of the parameter vector. While this is thus harder than an `\n2 error analysis, intuitively this would be much easier if the loss function were to split into independent components involving individual coordinates. While general loss functions might not be so “fully separable”, they might perhaps satisfy a softer notion of separability motivated above. In a contribution that would be of independent interest, we develop precisely such a softer notion of separability for general loss functions. We then use this notion of separability to derive `1 convergence rates for general M -estimators.\nGiven this machinery, we are then able to contrast generative and discriminative models. We focus on the case where the generative models are specified by exponential family distributions, so that the corresponding discriminative models are logistic regression models with the generative model sufficient statistics as feature functions. To compare the convergence rates of the two models, we focus on the difference of the two generative model parameters, since this difference is also precisely the model parameter for the discriminative model counterpart of the generative model, via an application of the Bayes rule. Moreover, as Li et al. [3] and others show, the `\n2 convergence rates of the difference of the two parameters is what drives the classification error rates of both generative as well as discriminative model classifiers. Incidentally, such a difference of generative model parameters has also attracted interest outside the context of classification, where it is called differential parameter learning [1, 14, 6]. We thus analyze the `1 as well as `2 rates for both the generative and discriminative models, focusing on this parameter difference. As we show, unlike the case of Naive Bayes and logistic regression in low-dimensions as studied in [8], this general highdimensional setting is more nuanced, and in particular depends on the separability of the generative models. As we show, under some conditions on the models, generative and discriminative models not only have potentially different `1 rates, but also differing “burn in” periods in terms of the minimum number of samples required in order for the convergence rates to hold. The choice of a generative vs discriminative model, namely that with a better sample complexity, thus depends on their corresponding separabilities. As a minor note, we also show how generative model M -estimators are not directly suitable in high-dimensions, and provide a simple methodological fix in order to obtain better `\n2 rates. We instantiate our results with two running examples of isotropic and non-isotropic Gaussian generative models, and also corroborate our theory with instructive simulations."
    }, {
      "heading" : "2 Background and Setup.",
      "text" : "We consider the problem of differential parameter estimation under the following generative model. Let Y 2 {0, 1} denote a binary response variable, and let X = (X\n1 , . . . , X p ) 2 Rp be the covariates. For simplicity, we assume P[Y = 1] = P[Y = 0] = 1\n2 . We assume that conditioned on the response variable, the covariates belong to an exponential family, X|Y ⇠ P\n✓ ⇤ Y (·), where:\nP ✓ ⇤ Y (X|Y ) = h(X) exp(h✓⇤ Y , (X)i A(✓⇤ Y )). (1)\nHere, ✓⇤ Y is the vector of the true canonical parameters, A(✓) is the log-partition function and (X) is the sufficient statistic. We assume access to two sets of samples X n\n0 = {x(0) i }n i=1 ⇠ P ✓ ⇤ 0 and X n\n1 = {x(1) i }n i=1 ⇠ P ✓ ⇤ 1 . Given these samples, as noted in the introduction, we are particularly\ninterested in estimating the differential parameter ✓⇤diff := ✓ ⇤ 1 ✓⇤ 0 , since this is also the model parameter corresponding to the discriminative model, as we show below. In high dimensional sampling settings, we additionally assume that ✓⇤diff is at most s-sparse, i.e. ||✓⇤diff||\n0  s. We will be using the following two exponential family generative models as running examples: isotropic and non-isotropic multivariate Gaussian models.\nIsotropic Gaussians (IG) Let X = (X 1 , . . . , X p ) ⇠ N (µ, I p ) be an isotropic gaussian random variable; it’s density can be written as:\nP µ (x) = 1 p\n(2⇡)p exp\n✓\n1 2\n(x µ)T(x µ) ◆ . (2)\nGaussian MRF (GMRF). Let X = (X 1 , . . . , X p ) denote a zero-mean gaussian random vector; it’s density is fully-parametrized as by the inverse covariance or concentration matrix ⇥ = (⌃) 1 0\nand can be written as:\nP ⇥ (x) = 1 r\n(2⇡)pdet ⇣ (⇥) 1 ⌘\nexp\n✓\n1 2 xT⇥x\n◆\n. (3)\nLet d ⇥ = max j2[p] ⇥ (:,j)\n0\nis the maximum number non-zeros in a row of ⇥. Let  ⌃ ⇤ =\n(⇥⇤) 1 1, where |||M |||1 is the `1/`1 operator norm given by |||M |||1 = max j=1,2,...,p P p k=1 |M jk |.\nGenerative Model Estimation. Here, we proceed by estimating the two parameters {✓⇤ i }1 i=0 individually. Letting b✓\n1 and b✓ 0 be the corresponding estimators, we can then estimate the difference of the parameters as b✓diff = b✓1 b✓0. The most popular class of estimators for the individual parameters is based on Maximum likelihood Estimation (MLE), where we maximize the likelihood of the given data. For isotropic gaussians, the negative log-likelihood function can be written as:\nL nIG(✓) =\n✓T ✓\n2\n✓T bµ, (4)\nwhere bµ = 1 n\nP\nn\ni=1\nx i . In the case of GGMs the negative log-likelihood function can be written as:\nL nGGM(⇥) =\nDD ⇥, b⌃ EE log(det(⇥)), (5)\nwhere b⌃ = 1 n\nP\nn\ni=1\nx i xT i\nis the sample covariance matrix and hhU, V ii = P\ni,j\nU ij V ij denotes the trace inner product on the space of symmetric matrices. In high-dimensional sampling regimes (n << p), regularized MLEs, for instance with `\n1 -regularization under the assumption of sparse model parameters, have been widely used [11, 10, 2]. Discriminative Model Estimation. Using Bayes rule, we have that:\nP[Y = 1|X] = P[X|Y = 1]P[Y = 1]P[X|Y = 0]P[Y = 0] + P[X|Y = 1]P[Y = 1]\n=\n1\n1 + exp ( (h✓⇤ 1 ✓⇤ 0\n, (x)i + c⇤)) (6)\nwhere c⇤ = A(✓⇤ 0 ) A(✓⇤ 1 ). The conditional distribution is simply a logistic regression model, with the generative model sufficient statistics as the features, and with optimal parameters being precisely the difference ✓⇤diff := ✓ ⇤ 1 ✓⇤ 0\nof the generative model parameters. The corresponding negative log-likelihood function can be written as\nLlogistic(✓, c) = 1\nn\nn\nX\ni=1\n( y i (h✓, (x i )i + c) + (h✓, (x i )i + c)) (7)\nwhere (t) = log(1 + exp(t)). In high dimensional sampling regimes, under the assumption that the model parameters are sparse, we would use the `\n1 -penalized version b✓diff of the MLE (7) to estimate ✓⇤diff.\nOutline. We proceed by studying the more general problem of `1 error for parameter estimation for any loss function L\nn (·). Specifically, consider the general M -estimation problem, where we are given n i.i.d samples Zn\n1\n= {z 1 , z 2 , . . . , z n }, z i 2 Z from some distribution P, and we are interested in estimating some parameter ✓⇤ of the distribution P. Let ` : Rp ⇥ Z 7! R be a twice differentiable and convex function which assigns a loss `(✓; z) to any parameter ✓ 2 Rp, for a given observation z. Also assume that the loss is Fisher consistent so that ✓⇤ 2 argmin\n✓ ¯L(✓) where ¯L(✓) def= E\nz⇠P[`(✓; z)] is the population loss. We are then interested in analyzing the M -estimators ✓⇤ that minimize the empirical loss i.e. b✓ 2 argmin\n✓\nL n (✓), or regularized versions thereof, where L\nn (✓) = 1 n\nP\nn\ni=1\nL(✓; Z i ).\nWe introduce a notion of the separability of a loss function, and show how more separable losses require fewer samples to establish convergence for b✓ ✓⇤\n1 . We then instantiate our separability\nresults from this general setting for both generative and discriminative models. We calculate the number of samples required for generative and discriminative approaches to estimate the differential parameter ✓⇤diff, for consistent convergence rates with respect to `1 and `2 norm. We also discuss the consequences of these results for high dimensional classification for Gaussian Generative models."
    }, {
      "heading" : "3 Separability",
      "text" : "Let R( ; ✓⇤) = rL n (✓⇤+ ) rL n (✓⇤) r2L n (✓⇤) be the error in the first order approximation of the gradient at ✓⇤. Let B1(r) = {✓| ||✓||1  r} be an `1 ball of radius r. We begin by analyzing the low dimensional case, and then extend it to high dimensions."
    }, {
      "heading" : "3.1 Low Dimensional Sampling Regimes",
      "text" : "In low dimensional sampling regimes, we assume that the number of samples n p. In this setting, we make the standard assumption that the empirical loss function L\nn (·) is strongly convex. Let b✓ = argmin\n✓\nL n (✓) denote the unique minimizer of the empirical loss function. We begin by defining a notion of separability for any such empirical loss function L\nn . Definition 1. L\nn\nis (↵, , ) locally separable around ✓⇤ if the remainder term R( ; ✓⇤) satisfies:\n||R( ; ✓⇤)||1  1 || ||↵1 8 2 B1( )\nThis definition might seem a bit abstract, but for some general intuition, indicates the region where it is separable, ↵ indicates the conditioning of the loss, while it is that quantifies the degree of separability: the larger it is, the more separable the loss function. Next, we provide some additional intuition on how a loss function’s separability is connected to (↵, , ). Using the mean-value theorem, we can write ||R( , ✓⇤)||1 = r2L n (✓⇤ + t ) r2L n (✓⇤)\n1 for some t 2 (0, 1). This can be further simplified as ||R( , ✓⇤)||1  r2L n (✓⇤ + t ) r2L n (✓⇤)\n1 || ||1. Hence, ↵ and 1/ measure the smoothness of Hessian (w.r.t. the `1/`1 matrix norm) in the neighborhood of ✓⇤, with ↵ being the smoothness exponent, and 1/ being the smoothness constant. Note that the Hessian of the loss function r2L\nn (✓) is a random matrix and can vary from being a diagonal matrix for a fully-separable loss function to a dense matrix for a heavily-coupled loss function. Moreover, from standard concentration arguments, the `1/`1 matrix norm for a diagonal (\"separable\") subgaussian random matrix has at most logarithmic dimension dependence1, but for a dense (\"non-separable\") random matrix, the `1/`1 matrix norm could possibly scale linearly in the dimension. Thus, the scaling of `1/`1 matrix norm gives us an indication how “separable” the matrix is. This intuition is captured by (↵, , ), which we further elaborate in future sections by explicitly deriving (↵, , ) for different loss functions and use them to derive `\n2 and `1 convergence rates. Theorem 1. Let L\nn\nbe a strongly convex loss function which is (↵, , ) locally separable function\naround ✓⇤. Then, if ||rL n (✓⇤)||1  min{\n2\n, 1\n2\n↵ ↵ 1\n1 ↵ 1 }\nb✓ ✓⇤\n1  2 ||rL n (✓⇤)||1\nwhere  = r2L n (✓⇤) 1\n1.\nProof. (Proof Sketch). The proof begins by constructing a suitable continuous function F , for which b = b✓ ✓⇤ is the unique fixed point. Next, we show that F (B1(r)) ✓ B1(r) for r = 2 ||rL\nn (✓⇤)||1. Since F is continuous and `1-ball is convex and compact, the contraction property coupled with Brouwer’s fixed point theorem [9], shows that there exists some fixed point of F , such that || ||1  2 ||rLn(✓⇤)||1. By uniqueness of the fixed point, we then establish our result. See Figure 1 for a geometric description and Section A for more details"
    }, {
      "heading" : "3.2 High Dimensional Sampling Regimes",
      "text" : "In high dimensional sampling regimes (n << p), estimation of model parameters is typically an under-determined problem. It is thus necessary to impose additional assumptions on the true model parameter ✓⇤. We will focus on the popular assumption of sparsity, which entails that the number of non-zero coefficients of ✓⇤ is small, so that ||✓⇤||\n0  s. For this setting, we will be focusing in particular on `\n1\n-regularized empirical loss minimization:\n1Follows from the concentration of subgaussian maxima [12]\nb✓ n = argmin\n✓\nL n (✓) + n ||✓|| 1\n(8)\nLet S = {i | ✓⇤ i 6= 0} be the support set of the true parameter and M(S) = {v|v S c = 0} be the corresponding subspace. Note that under a high-dimensional sampling regime, we can no longer assume that the empirical loss L\nn (·) is strongly convex. Accordingly, we make the following set of assumptions:\n• Assumption 1 (A1): Positive Definite Restricted Hessian. r2 SS L n (✓⇤) % min I • Assumption 2 (A2): Irrepresentability. There exists some 2 (0, 1] such that\nr2 S c S L n\n(✓⇤)\nr2 SS L n\n(✓⇤) 1\n1  1 • Assumption 3 (A3). Unique Minimizer. When restricted to the true support, the solution to the ` 1\npenalized loss minimization problem is unique, which we denote by: ˜✓ n = argmin\n✓2M(S) {L n (✓) + n ||✓|| 1 } . (9)\nAssumptions 1 and 2 are common in high dimensional analysis. We verify that Assumption 3 holds for different loss functions individually. We refer the reader to [13, 5, 11, 10] for further details on these assumptions. For this high dimensional sampling regime, we also modify our separability notion to a restricted separability, which entails that the remainder term be separable only over the model subspace M(S). Definition 2. L\nn is (↵, , ) restricted locally separable around ✓⇤ over the subspace M(S) if the remainder term R( ; ✓⇤) satisfies:\n||R( ; ✓⇤)||1  1 || ||↵1 8 2 B1( ) \\ M(S)\nWe present our main deterministic result in high dimensions. Theorem 2. Let L\nn be a (↵, , ) locally separable function around ✓⇤. If ( n , rL n (✓⇤)) are such that,\n• 8 n ||rL n (✓⇤)||1. • ||rL\nn (✓⇤)||1 + n  min n\n2\n, 1\n2\n↵ ↵ 1\n1 ↵ 1\no\nThen we have that support(b✓ n) ✓ support(✓⇤) and\nb✓ n ✓⇤ 1  2 (||rL n (✓⇤)||1 + n)\nwhere  = r2 SS L n (✓⇤) 1\n1\nProof. (Proof Sketch). The proof invokes the primal-dual witness argument [13] which when combined with Assumption 1-3, gives b✓\nn 2 M(S) and that b✓ n is the unique solution of the restricted problem. The rest of the proof proceeds similar to Theorem 1, by constructing a suitable function F : R|S| 7! R|S| for which b = b✓\nn ✓⇤ is the unique fixed point, and showing that F is contractive over B1(r; ✓⇤) for r = 2 (||rLn(✓⇤)||1 + n).See Section B for more details.\nDiscussion. Theorems 1 and 2 provide a general recipe to estimate the number of samples required by any loss `(✓, z) to establish `1 convergence. The first step is to calculate the separability constants (↵, , ) for the corresponding empirical loss function L\nn . Next, since the loss ` is Fisher consistent, so that r ¯L(✓⇤) = 0, the upper bound on ||rL\nn (✓⇤)||1 can be shown to hold by analyzing the concentration of rL\nn (✓⇤) around its mean. We emphasize that we do not impose any restrictions on the values of (↵, , ). In particular, these can scale with the number of samples n; our results hold so long as the number of samples n satisfy the conditions of the theorem. As a rule of thumb, the smaller that either or get for any given loss `, the larger the required number of samples.\n4 `1-rates for Generative and Discriminative Model Estimation\nIn this section we study the `1 rates for differential parameter estimation for the discriminative and generative approaches. We do so by calculating the separability of discriminative and generative loss functions, and then instantiate our previously derived results."
    }, {
      "heading" : "4.1 Discriminative Estimation",
      "text" : "As discussed before, the discriminative approach uses ` 1 -regularized logistic regression with the sufficient statistic as features to estimate the differential parameter. In addition to A1-A3, we assume column normalization of the sufficient statistics, i.e. P n\ni=1\n([ (x i )] j ) 2  n. Let n =\nmax\ni || (x) i ||1, ⌫n = maxi ||( (x)i)S ||2. Firstly, we characterize the separability of the logistic loss.\nLemma 1. The logistic regression negative log-likelihood L n\nLogistic\nfrom (7) is ⇣\n2, 1 s n⌫ 2 n , 1\n⌘\nre-\nstricted local separable around ✓⇤.\nCombining Lemma 1 with Theorem 2, we get the following corollary. Corollary 3. (Logistic Regression) Consider the model in (1), then there exist universal positive constants C\n1 , C 2 and C 3 such that for n C 1 2s2 2 n ⌫4 n log p and n = C 2\nq\nlog p\nn\n, the discriminative\ndifferential estimate b✓ diff , satisfies\nsupport(b✓ diff ) ✓ support(✓⇤ diff ) and b✓ diff ✓⇤ diff 1  C 3\nr\nlog p\nn ."
    }, {
      "heading" : "4.2 Generative Estimation",
      "text" : "We characterize the separability of Generative Exponential Families. The negative log-likelihood function can be written as:\nL n (✓) = A(✓) h✓, n i , where\nn\n=\n1\nn\nP\nn\ni=1\n(x i ). In this setting, the remainder term is independent of the data and can be written as R( ) = rA(✓⇤ + ) rA(✓⇤) r2A(✓⇤) and rL\nn (✓⇤) = E[ (x)] 1 n (x i ). Hence, ||rL\nn (✓⇤)||1 is a measure of how well the sufficient statistics concentrate around their mean. Next, we show the separability of our running examples Isotropic Gaussians and Gaussian Graphical Models. Lemma 2. The isotropic Gaussian negative log-likelihood L\nn\nIG from (4) is (·, 1, 1) locally separable around ✓⇤.\nLemma 3. The Gaussian MRF negative log-likelihood L n\nGGM\nfrom (5) is ⇣\n2, 2 3d\n⇤ ⇥ 3 ⌃⇤\n, 1 3d\n⇤ ⇥⌃⇤\n⌘\nrestricted locally separable around ⇥⇤.\nComparing Lemmas 1, 2 and 3, we see that the separability of the discriminative model loss depends weakly on the feature functions. On the other hand, the separability for the generative model loss depends critically on the underlying sufficient statistics. This has consequences for their differing sample complexities for differential parameter estimation, as we show next. Corollary 4. (Isotropic Gaussians) Consider the model in (2). Then there exist universal constants C\n1 , C 2 , C 3 such that if the number of samples scale as n C 1 log p, then with probability atleast 1 1/pC2 , the generative estimate of the differential parameter b✓\ndiff\nsatisfies\nb✓ diff ✓⇤ diff 1  C 3\nr\nlog p\nn .\nComparing Corollary 3 and Corollary 4, we see that for isotropic gaussians, both the discriminative and generative approach achieve the same `1 convergence rates, but at different sample complexities. Specifically, the sample complexity for the generative method depends only logarithmically on the dimension p, and is independent of the differential sparsity s, while the sample complexity of the discriminative method depends on the differential sparsity s. Therefore in this case, the generative method is strictly better than its discriminative counterpart, assuming that the generative model assumptions hold. Corollary 5. (Gaussian MRF) Consider the model in (3), and suppose that the scaled covariates X\nk\n/ p ⌃⇤ kk\nare subgaussian with parameter 2. Then there exist universal positive constants C\n2 , C 3 , C 4 such that if the number of samples for the two generative models scale as\nn i\nC 2 2 i 6 (⇥\n⇤ i )\n1d2 ⇥ ⇤ i log p, for i 2 {0, 1}, then with probability at least 1 1/pC3 , the gen-\nerative estimate of the differential parameter, b⇥ diff = b⇥ 1 b⇥ 0 , satisfies\nb⇥ diff ⇥⇤ diff 1  C 4\nr\nlog p\nn ,\nand support( b⇥ i ) ✓ support(⇥⇤ i ) for i 2 {0, 1}.\nComparing Corollary 3 and Corollary 5, we see that for Gaussian Graphical Models, both the discriminative and generative approach achieve the same `1 convergence rates, but at different sample complexities. Specifically, the sample complexity for the generative method depends only on row-wise sparsity of the individual models d2\n⇥ ⇤ i , and is independent of sparsity s of the differential\nparameter ⇥⇤diff. In contrast, the sample complexity of the discriminative method depends only on the sparsity of the differential parameter, and is independent of the structural complexities of the individual model parameters. This suggests that in high dimensions, even when the generative model assumptions hold, generative methods might perform poorly if the underlying model is highly non-separable (e.g. d = ⌦(p)), which is in contrast to the conventional wisdom in low dimensions.\nRelated Work. Note that results similar to Corollaries 3 and 5 have been previously reported in [11, 5] separately. Under the same set of assumptions as ours, Li et al. [5] provide a unified analysis for support recovery and `1-bounds for `1-regularized M-estimators. While they obtain the same rates as ours, their required sample complexities are much higher, since they do not exploit the separability of the underlying loss function. As one example, in the case of GMRFs, their results require the number of samples to scale as n > k2 log p, where k is the total number of edges in the graph, which is sub-optimal, and in particular does not match the GMRF-specific analysis of [11]. On the other hand, our unified analysis is tighter, and in particular, does match the results of [11].\n5 `2-rates for Generative and Discriminative Model Estimation\nIn this section we study the ` 2 rates for differential parameter estimation for the discriminative and generative approaches."
    }, {
      "heading" : "5.1 Discriminative Approach",
      "text" : "The bounds for the discriminative approach are relatively straightforward. Corollary 3 gives bounds on the `1 error and establishes that support(b✓) ✓ support(✓⇤). Since the true model parameter is s-sparse, ||✓⇤||\n0\n s, the ` 2 error can be simply bounded as p s kb✓ ✓⇤k1."
    }, {
      "heading" : "5.2 Generative Approach",
      "text" : "In the previous section, we saw that the generative approach is able to exploit the inherent separability of the underlying model, and thus is able to get `1 rates for differential parameter estimation at a much lower sample complexity. Unfortunately, it does not have support consistency. Hence a naïve\ngenerative estimator will have an ` 2 error scaling with q p log p\nn , which in high dimensions, would make it unappealing. However, one can exploit the sparsity of ✓⇤diff and get better rates of convergence in `\n2 -norm by simply soft-thresholding the generative estimate. Moreover, soft-thresholding also leads to support consistency. Definition 3. We denote the soft-thresholding operator ST\nn (·), defined as:\nST n (✓) = argmin w\n1\n2\n||w ✓||2 2 + n ||w|| 1 .\nLemma 4. Suppose ✓ = ✓⇤ + ✏ for some s-sparse ✓⇤. Then there exists a universal constant C 1 such\nthat for n 2 ||✏||1,\n||ST n (✓) ✓⇤||\n2\n C 1 p s ||✏||1 and ||ST n (✓) ✓ ⇤|| 1  C 1 s ||✏||1 (10)\nNote that this is a completely deterministic result and has no sample complexity requirement. Motivated by this, we introduce a thresholded generative estimator that has two stages: (a) compute b✓diff using the generative model estimates, and (b) soft-threshold the generative estimate with n = c\nb✓diff ✓⇤diff 1 . An elementary application of Lemma 4 can then be shown to provide ` 2 error bounds for b✓diff given its `1 error bounds, and that the true parameter ✓⇤diff is s-sparse. We instantiate these `\n2 -bounds via corollaries for our running examples of Isotropic Gaussians, and Gaussian MRFs. Lemma 5. (Isotropic Gaussians) Consider the model in (2). Then there exist universal constants C\n1 , C 2 , C 3 such that if the number of samples scale as n C 1 log p, then with probability atleast\n1 1/pC2 , the soft-thresholded generative estimate of the differential parameter ST n\n⇣\nb✓ diff\n⌘\n, with\nthe soft-thresholding parameter set as n\n= c q log p\nn\nfor some constant c, satisfies:\nST n\n⇣\nb✓ diff\n⌘\n✓⇤ diff\n2\n C 3\nr\ns log p\nn .\nLemma 6. (Gaussian MRF) Consider the model in Equation 3, and suppose that the covariates X\nk\n/ p ⌃⇤ kk\nare subgaussian with parameter 2. Then there exist universal positive constants C\n2 , C 3 , C 4 such that if the number of samples for the two generative models scale as\nn i C 2 2 i 6 (⇥\n⇤ i )\n1d2 ⇥ ⇤ i log p, for i 2 {0, 1}, for i 2 {0, 1}, then with probability at least 1 1/pC3 ,\nthe soft-thresholded generative estimate of the differential parameter, ST n\n⇣\nb⇥ diff\n⌘\n, with the soft-\nthresholding parameter set as n\n= c q log p\nn\nfor some constant c, satisfies:\nST n\n⇣\nb⇥ diff\n⌘\n⇥⇤ diff\n2\n C 4\nr\ns log p\nn .\nComparing Lemmas 5 and 6 to Section 5.1, we can see that the additional soft-thresholding step allows the generative methods to achieve the same `\n2 -error rates as the discriminative methods, but at different sample complexities. The sample complexities of the generative estimates depend on the separabilities of the individual models, and is independent of the differential sparsity s, where as the sample complexity of the discriminative estimate depends only on the differential sparsity s."
    }, {
      "heading" : "6 Experiments: High Dimensional Classification",
      "text" : "In this section, we corroborate our theoretical results on ` 2 -error rates for generative and discriminative model estimators, via their consequences for high dimensional classification. We focus on the case of isotropic Gaussian generative models X|Y ⇠ N (µ\nY , I p ), where µ 0 , µ 1 2 Rp are unknown\nand µ 1 µ 0 is s-sparse. Here, we are interested in a classifier C : Rp 7! {0, 1} that achieves low classification error E\nX,Y [1 {C(X) 6= Y }]. Under this setting, it can be shown that the Bayes classifier, that achieves the lowest possible classification error, is given by the linear discriminant classifier C⇤(x) = 1 xT w⇤ + b⇤ > 0 , where w⇤ = (µ 1 µ 0 ) and b⇤ = µ T 0 µ0 µ T 1 µ1\n2 . Thus, the coefficient w⇤ of the linear discriminant is precisely the differential parameter, which can be estimated via both generative and discriminative approaches as detailed in the previous section. Moreover, the classification error can also be related to the `\n2 error of the estimates. Under some mild assumptions, Li et al. [3] showed that for any linear classifier bC(x) = 1 n xT bw +bb > 0 o\n, the excess classification error can be bounded as:\nE( bC)  C 1\n✓\n|| bw w⇤||2 2 + bb b⇤\n2\n2\n◆\n,\nfor some constant C 1 > 0, and where E(C) = E X,Y [1 {C(X) 6= Y }] E X,Y [1 {C⇤(X) 6= Y }] is the excess 0-1 error. In other words, the excess classification error is bounded by a constant times the ` 2 error of the differential parameter estimate.\nMethods. In this setting, as discussed in previous sections, the discriminative model is simply a logistic regression model with linear features (6), so that the discriminative estimate of the differential parameter bw as well as the constant bias term bb can be simply obtained via `\n1 -regularized logistic regression. For the generative estimate, we use our two stage estimator from Section 5, which proceeds by estimating bµ\n0 , bµ 1 using the empirical means, and then estimating the differential parameter by soft-thresholding the difference of the generative model parameter estimates bw\nT = ST n (bµ1 bµ0)\nwhere n = C 1\nq\nlog p\nn\nfor some constant C 1 . The corresponding estimate for b⇤ is given by ˆb T\n= 1 2\nh bw T , bµ 1 + bµ 0 i. Experimental Setup. For our experimental setup, we consider isotropic Gaussian models with\nmeans µ 0 = 1 p 1p s\n\n1\ns\n0\np s\n, µ 1 = 1 p + 1p s\n\n1\ns\n0\np s\n, and vary the sparsity level s. For both methods,\nwe set the regularization parameter 2 as n =\np\nlog(p)/n. We report the excess classification error for the two approaches, averaged over 20 trials, in Figure 2.\nResults. As can be seen from Figure 2, our two-staged thresholded generative estimator is always better than the discriminative estimator, across different sparsity levels s. Moreover, the sample complexity or “burn-in” period of the discriminative classifier strongly depends on the sparsity level, which makes it unsuitable when the true parameter is not highly sparse. For our two-staged generative estimator, we see that the sparsity s has no effect on the “burn-in” period of the classifier. These observations validate our theoretical results from Section 5.\n2See Appendix J for cross-validated plots."
    }, {
      "heading" : "Acknowledgements",
      "text" : "A.P. and P.R. acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1447574, DMS-1264033, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences."
    } ],
    "references" : [ {
      "title" : "From ‘differential expression’to ‘differential networking’–identification of dysfunctional regulatory networks in diseases",
      "author" : [ "Alberto de la Fuente" ],
      "venue" : "Trends in genetics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Introduction to high-dimensional statistics, volume 138",
      "author" : [ "Christophe Giraud" ],
      "venue" : "CRC Press,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Fast classification rates for high-dimensional gaussian generative models",
      "author" : [ "Tianyang Li", "Adarsh Prasad", "Pradeep K Ravikumar" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Minimax gaussian classification & clustering",
      "author" : [ "Tianyang Li", "Xinyang Yi", "Constantine Carmanis", "Pradeep Ravikumar" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2017
    }, {
      "title" : "Sparsistency of 1-regularized m-estimators",
      "author" : [ "Yen-Huan Li", "Jonathan Scarlett", "Pradeep Ravikumar", "Volkan Cevher" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Direct learning of sparse changes in markov networks by density ratio estimation",
      "author" : [ "Song Liu", "John A Quinn", "Michael U Gutmann", "Taiji Suzuki", "Masashi Sugiyama" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "A unified framework for highdimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "Sahand Negahban", "Bin Yu", "Martin J Wainwright", "Pradeep K Ravikumar" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes",
      "author" : [ "Andrew Y Ng", "Michael I Jordan" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2002
    }, {
      "title" : "Iterative solution of nonlinear equations in several variables",
      "author" : [ "James M Ortega", "Werner C Rheinboldt" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "High-dimensional ising model selection using `1-regularized logistic regression",
      "author" : [ "Pradeep Ravikumar", "Martin J Wainwright", "John D Lafferty" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "High-dimensional covariance estimation by minimizing `1-penalized log-determinant divergence",
      "author" : [ "Pradeep Ravikumar", "Martin J Wainwright", "Garvesh Raskutti", "Bin Yu" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "High-dimensional statistics: A non-asymptotic viewpoint. preparation",
      "author" : [ "JM Wainwright" ],
      "venue" : "University of California, Berkeley,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Sharp thresholds for high-dimensional and noisy sparsity recovery using-constrained quadratic programming (lasso)",
      "author" : [ "Martin J Wainwright" ],
      "venue" : "IEEE transactions on information theory,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Direct estimation of differential networks",
      "author" : [ "Sihai Dave Zhao", "T Tony Cai", "Hongzhe Li" ],
      "venue" : "Biometrika, page asu009,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "In a classical paper [8], the authors provided theoretical justification for the common wisdom regarding generative and discriminative models: when the generative model assumptions hold, the generative model estimators initially converge faster as a function of the number of samples, but have the same asymptotic error rate as discriminative models.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "statistical estimators considerably lags that of say ` 2 rates (see for instance, the unified framework of [7], which is suited to ` 2 rates but is highly sub-optimal for `1 rates).",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "[3] and others show, the ` 2 convergence rates of the difference of the two parameters is what drives the classification error rates of both generative as well as discriminative model classifiers.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Incidentally, such a difference of generative model parameters has also attracted interest outside the context of classification, where it is called differential parameter learning [1, 14, 6].",
      "startOffset" : 181,
      "endOffset" : 191
    }, {
      "referenceID" : 13,
      "context" : "Incidentally, such a difference of generative model parameters has also attracted interest outside the context of classification, where it is called differential parameter learning [1, 14, 6].",
      "startOffset" : 181,
      "endOffset" : 191
    }, {
      "referenceID" : 5,
      "context" : "Incidentally, such a difference of generative model parameters has also attracted interest outside the context of classification, where it is called differential parameter learning [1, 14, 6].",
      "startOffset" : 181,
      "endOffset" : 191
    }, {
      "referenceID" : 7,
      "context" : "As we show, unlike the case of Naive Bayes and logistic regression in low-dimensions as studied in [8], this general highdimensional setting is more nuanced, and in particular depends on the separability of the generative models.",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "In high-dimensional sampling regimes (n << p), regularized MLEs, for instance with ` 1 -regularization under the assumption of sparse model parameters, have been widely used [11, 10, 2].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 9,
      "context" : "In high-dimensional sampling regimes (n << p), regularized MLEs, for instance with ` 1 -regularization under the assumption of sparse model parameters, have been widely used [11, 10, 2].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 1,
      "context" : "In high-dimensional sampling regimes (n << p), regularized MLEs, for instance with ` 1 -regularization under the assumption of sparse model parameters, have been widely used [11, 10, 2].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 8,
      "context" : "Since F is continuous and `1-ball is convex and compact, the contraction property coupled with Brouwer’s fixed point theorem [9], shows that there exists some fixed point of F , such that || ||1  2 ||rLn(✓)||1.",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 11,
      "context" : "For this setting, we will be focusing in particular on ` 1 -regularized empirical loss minimization: (1)Follows from the concentration of subgaussian maxima [12] 4",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 12,
      "context" : "We refer the reader to [13, 5, 11, 10] for further details on these assumptions.",
      "startOffset" : 23,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "We refer the reader to [13, 5, 11, 10] for further details on these assumptions.",
      "startOffset" : 23,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "We refer the reader to [13, 5, 11, 10] for further details on these assumptions.",
      "startOffset" : 23,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "We refer the reader to [13, 5, 11, 10] for further details on these assumptions.",
      "startOffset" : 23,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "The proof invokes the primal-dual witness argument [13] which when combined with Assumption 1-3, gives b ✓ n 2 M(S) and that b ✓ n is the unique solution of the restricted problem.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "Note that results similar to Corollaries 3 and 5 have been previously reported in [11, 5] separately.",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "Note that results similar to Corollaries 3 and 5 have been previously reported in [11, 5] separately.",
      "startOffset" : 82,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "[5] provide a unified analysis for support recovery and `1-bounds for `1-regularized M-estimators.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "As one example, in the case of GMRFs, their results require the number of samples to scale as n > k2 log p, where k is the total number of edges in the graph, which is sub-optimal, and in particular does not match the GMRF-specific analysis of [11].",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 10,
      "context" : "On the other hand, our unified analysis is tighter, and in particular, does match the results of [11].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "[3] showed that for any linear classifier b C(x) = 1 n",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2017,
    "abstractText" : "We revisit the classical analysis of generative vs discriminative models for general exponential families, and high-dimensional settings. Towards this, we develop novel technical machinery, including a notion of separability of general loss functions, which allow us to provide a general framework to obtain `1 convergence rates for general M -estimators. We use this machinery to analyze `1 and `2 convergence rates of generative and discriminative models, and provide insights into their nuanced behaviors in high-dimensions. Our results are also applicable to differential parameter estimation, where the quantity of interest is the difference between generative model parameters.",
    "creator" : null
  }
}