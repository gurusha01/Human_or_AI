{
  "name" : "a82d922b133be19c1171534e6594f754.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On Optimal Generalizability in Parametric Learning",
    "authors" : [ "Ahmad Beirami", "Meisam Razaviyayn", "Shahin Shahrampour", "Vahid Tarokh" ],
    "emails" : [ "beirami@seas.harvard.edu", "razaviya@usc.edu", "shahin@seas.harvard.edu", "vahid@seas.harvard.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider the parametric supervised/unsupervised learning problem, where the objective of the learner is to build a predictor based on a set of historical data. Let zn = {zi}ni=1, where zi ∈ Z denotes the data samples at the learner’s disposal that are assumed to be drawn i.i.d. from an unknown density function p(·), and Z is compact. We assume that the learner expresses the objective in terms of minimizing a parametric loss function `(z;θ), which is a function of the parameter vector θ. The learner solves for the unknown parameter vector θ ∈ Θ ⊆ Rk, where k denotes the number of parameters in the model class, and Θ is a convex, compact set.\nLet L(θ) , E{`(z;θ)} (1)\nbe the risk associated with the parameter vector θ, where the expectation is with respect to the density p(·) that is unknown to the learner. Ideally, the goal of the learner is to choose the parameter vector θ∗ such that θ∗ ∈ arg minθ∈Θ L(θ) = arg minθ∈ΘE{`(z;θ)}. Since the density function p(·) is unknown, the learner cannot compute θ∗ and hence cannot achieve the ideal performance of L(θ∗) = minθ∈Θ L(θ) associated with the model class Θ. Instead, one can consider the minimiza-\n∗School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138, USA. †Department of Industrial and Systems Engineering, University of Southern California, Los Angeles, CA\n90089, USA.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\ntion of the empirical version of the problem through the empirical risk minimization framework:\nθ̂(zn) ∈ arg min θ∈Θ ∑ i∈[n] `(zi;θ) + r(θ),\nwhere [n] , {1, 2, . . . , n} and r(θ) is some regularization function. While the learner can evaluate her performance on the training data samples (also called the in-sample empirical risk, i.e., 1 n ∑n i=1 `(zi; θ̂(z\nn))), it is imperative to assess the average performance of the learner on fresh test samples, i.e., L(θ̂(zn)), which is referred to as the out-of-sample risk. A simple and universal approach to measuring the out-of-sample risk is cross validation [1]. Leave-one-out cross validation (LOOCV), which is a popular exhaustive cross validation strategy, uses (n − 1) of the samples for training while one sample is left out for testing. This procedure is repeated on the n samples in a round-robin fashion, and the learner ends up with n estimates for the out-of-sample loss corresponding to each sample. These estimates together form a cross validation vector which can be used for the estimation of the out-of-sample performance, model selection, and tuning the model hyperparameters. While LOOCV provides a reliable estimate of the out-of-sample loss, it brings about an additional factor of n in terms of computational cost, which makes it practically impossible because of the high computational cost of training when the number of samples is large.\nContribution: Our first contribution is to provide an approximation for the cross validation vector, called ALOOCV, with much lower computational cost. We compare its performance with LOOCV in problems of reasonable size where LOOCV is tractable. We also test it on problems of large size where LOOCV is practically impossible to implement. We describe how to handle quasi-smooth loss/regularizer functions. We also show that ALOOCV is asymptotically equivalent to Takeuchi information criterion (TIC) under certain regularity conditions.\nOur second contribution is to use ALOOCV to develop a gradient descent algorithm for jointly optimizing the regularization hyperparameters as well as the unknown parameter vector θ. We show that multiple hyperparameters could be tuned using the developed algorithm. We emphasize that the second contribution would not have been possible without the developed estimator as obtaining the gradient of the LOOCV with respect to tuning parameters is computationally expensive. Our experiments show that the developed method handles quasi-smooth regularized loss functions as well as number of tuning parameters that is on the order of the training samples.\nFinally, it is worth mentioning that although the leave-one-out cross validation scenario is considered in our analyses, the results and the algorithms can be extended to the leave-q-out cross validation and bootstrap techniques.\nRelated work: A main application of cross validation (see [1] for a recent survey) is in model selection [2–4]. On the theoretical side, the proposed approximation on LOOCV is asymptotically equivalent to Takeuchi information criterion (TIC) [4–7], under certain regularity conditions (see [8] for a proof of asymptotic equivalence of AIC and LOOCV in autoregressive models). This is also related to Barron’s predicted square error (PSE) [9] and Moody’s effective number of parameters for nonlinear systems [10]. Despite these asymptotic equivalences our main focus is on the nonasymptotic performance of ALOOCV.\nALOOCV simplifies to the closed form derivation of the LOOCV for linear regression, called PRESS (see [11, 12]). Hence, this work can be viewed as an approximate extension of this closed form derivation for an arbitrary smooth regularized loss function. This work is also related to the concept of influence functions [13], which has recently received renewed interest [14]. In contrast to methods based on influence functions that require large number of samples due to their asymptotic nature, we empirically show that the developed ALOOCV works well even when the number of samples and features are small and comparable to each other. In particular, ALOOCV is capable of predicting overfitting and hence can be used for model selection and choosing the regularization hyperparameter. Finally, we expect that the idea of ALOOCV can be extended to derive computationally efficient approximate bootstrap estimators [15].\nOur second contribution is a gradient descent optimization algorithm for tuning the regularization hyperparameters in parametric learning problems. A similar approach has been taken for tuning the single parameter in ridge regression where cross validation can be obtained in closed form [16]. Most of the existing methods, on the other hand, ignore the response and carry out the optimization solely based on the features, e.g., Stein unbiased estimator of the risk for multiple parameter selection [17, 18].\nBayesian optimization has been used for tuning the hyperparameters in the model [19–23], which postulates a prior on the parameters and optimizes for the best parameter. Bayesian optimization methods are generally derivative free leading to slow convergence rate. In contrast, the proposed method is based on a gradient descent method. Other popular approaches to the tuning of the optimization parameters include grid search and random search [24–26]. These methods, by nature, also suffer from slow convergence. Finally, model selection has been considered as a bi-level optimization [27,28] where the training process is modeled as a second level optimization problem within the original problem. These formulations, similar to many other bi-level optimization problems, often lead to computationally intensive algorithms that are not scalable.\nWe remark that ALOOCV can also be used within Bayesian optimization, random search, and grid search methods. Further, resource allocation can be used for improving the optimization performance in all of such methods."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "To facilitate the presentation of the ideas, let us define the following concepts. Throughout, we assume that all the vectors are in column format.\nDefinition 1 (regularization vector/regularized loss function) We suppose that the learner is concerned with M regularization functions r1(θ), . . . , rM (θ) in addition to the main loss function `(z;θ). We define the regularization vector r(θ) as\nr(θ) , (r1(θ), . . . , rM (θ)) >.\nFurther, let λ = (λ1, . . . , λM )> be the vector of regularization parameters. We call wn(z;θ,λ) the regularized loss function given by\nwn(z;θ,λ) , `(z;θ) + 1 n λ>r(θ) = `(z;θ) + 1 n ∑ m∈[M ] λmrm(θ).\nThe above definition encompasses many popular learning problems. For example, elastic net regression [31] can be cast in this framework by setting r1(θ) = ‖θ‖1 and r2(θ) = 12‖θ‖ 2 2.\nDefinition 2 (empirical risk/regularized empirical risk) Let the empirical risk be defined as L̂zn(θ) = 1n ∑n i=1 `(zi;θ). Similarly, let the regularized empirical risk be defined as Ŵzn(θ,λ) = 1 n ∑n i=1{wn(zi;θ,λ)}.\nDefinition 3 (regularized empirical risk minimization) We suppose that the learner solves the empirical risk minimization problem by selecting θ̂λ(zn) as follows:\nθ̂λ(z n) ∈ arg min\nθ∈Θ\n{ Ŵzn(θ,λ) } = arg min\nθ∈Θ ∑ i∈[n] `(zi;θ) + λ >r(θ)  . (2) Once the learner solves for θ̂λ(zn), the empirical risk corresponding to θ̂λ(zn) can be readily computed by L̂zn(θ̂λ(zn)) = 1n ∑ i∈[n] `(zi; θ̂λ(z n)). While the learner can evaluate her performance on the observed data samples (also called the in-sample empirical risk, i.e., L̂zn(θ̂λ(zn))), it is imperative to assess the performance of the learner on unobserved fresh samples, i.e., L(θ̂λ(zn)) (see (1)), which is referred to as the out-of-sample risk. To measure the out-of-sample risk, it is a common practice to perform cross validation as it works outstandingly well in many practical situations and is conceptually universal and simple to implement.\nLeave-one-out cross validation (LOOCV) uses all of the samples but one for training, which is left out for testing, leading to an n-dimensional cross validation vector of out-of-sample estimates. Let us formalize this notion. Let zn\\i , (z1, . . . , zi−1, zi+1, . . . , zn) denote the set of the training examples excluding zi.\nDefinition 4 (LOOCV empirical risk minimization/cross validation vector) Let θ̂λ(zn\\i) be the estimated parameter over the training set zn\\i, i.e.,\nθ̂λ(z n\\i) ∈ arg min\nθ∈Rk\n{ Ŵzn\\i(θ,λ) } = arg min\nθ∈Rk  ∑ j∈[n]\\i `(zj ;θ) + λ >r(θ)  . (3) The cross validation vector is given by {CVλ,i(zn)}i∈[n] where CVλ,i(zn) , `(zi; θ̂λ(zn\\i)), and the cross validation out-of-sample estimate is given by CVλ(zn) , 1n ∑n i=1 CVλ,i(z n).\nThe empirical mean and the empirical variance of the n-dimensional cross validation vector are used by practitioners as surrogates on assessing the out-of-sample performance of a learning method.\nThe computational cost of solving the problem in (3) is n times that of the original problem in (2). Hence, while LOOCV provides a simple yet powerful tool to estimate the out-of-sample performance, the additional factor of n in terms of the computational cost makes LOOCV impractical in large-scale problems. One common solution to this problem is to perform validation on fewer number of samples, where the downside is that the learner would obtain a much more noisy and sometimes completely unstable estimate of the out-of-sample performance compared to the case where the entire LOOCV vector is at the learner’s disposal. On the other hand, ALOOCV described next will provide the benefits of LOOCV with negligible additional computational cost.\nWe emphasize that the presented problem formulation is general and includes a variety of parametric machine learning tasks, where the learner empirically solves an optimization problem to minimize some loss function."
    }, {
      "heading" : "3 Approximate Leave-One-Out Cross Validation (ALOOCV)",
      "text" : "We assume that the regularized loss function is three times differentiable with continuous derivatives (see Assumption 1). This includes many learning problems, such as the L2 regularized logistic loss function. We later comment on how to handle the `1 regularizer function in LASSO. To proceed, we need one more definition.\nDefinition 5 (Hessian/empirical Hessian) LetH(θ) denote the Hessian of the risk function defined as H(θ) , ∇2θL(θ). Further, let Ĥzn(θ,λ) denote the empirical Hessian of the regularized loss function, defined as Ĥzn(θ,λ) , Êzn { ∇2θwn(z;θ,λ) } = 1n ∑n i=1∇2θwn(zi;θ,λ). Similarly, we\ndefine Ĥzn(θ,λ) , Êzn\\i { ∇2θwn(z;θ,λ) } = 1n−1 ∑ i∈[n]\\i∇2θwn(zi;θ,λ).\nNext we present the set of assumptions we need to prove the main result of the paper.\nAssumption 1 We assume that\n(a) There exists θ∗ ∈ Θ◦,3 such that ‖θ̂λ(zn)− θ∗‖∞ = op(1).4\n(b) wn(z;θ) is of class C3 as a function of θ for all z ∈ Z .\n(c) H(θ∗) 0 is positive definite.\nTheorem 1 Under Assumption 1, let\nθ̃ (i)\nλ (z n) , θ̂λ(z\nn) + 1\nn− 1\n( Ĥzn\\i ( θ̂λ(z n),λ ))−1 ∇θ`(zi; θ̂λ(zn)), (4)\nassuming the inverse exists. Then,\nθ̂λ(z n\\i)− θ̃\n(i) λ (z n) = 1\nn− 1\n( Ĥzn\\i ( θ̂λ(z n),λ ))−1 ε (i) λ,n, (5)\n3(·)◦ denotes the interior operator. 4Xn = op(an) implies that Xn/an approaches 0 in probability with respect to the density function p(·).\nwith high probability where\nε (i) λ,n = ε (i),1 λ,n − ε (i),2 λ,n , (6)\nand ε(i),1λ,n is defined as\nε (i),1 λ,n ,\n1\n2 ∑ j∈[n]\\i ∑ κ∈[k] (θ̂λ(z n)− θ̂λ(zn\\i))> ( ∂ ∂θκ ∇2θwn−1(zj ; ζi,j,1λ,κ (z n),λ) ) (θ̂λ(z n)− θ̂λ(zn\\i))êκ,\n(7) where êκ is κ-th standard unit vector, and such that for all κ ∈ [k], ζi,j,1λ,κ (zn) = αi,j,1κ θ̂λ(zn) + (1− αi,j,1κ )θ̂λ(zn\\i) for some 0 ≤ αi,j,1κ ≤ 1. Further, ε (i),2 λ,n is defined as\nε (i),2 λ,n , ∑ j∈[n]\\i ∑ κ,ν∈[k]̂ e>ν (θ̂λ(z n)−θ̂λ(zn\\i)) ( ∂2 ∂θκ∂θν ∇>θ wn−1(zj ; ζi,j,2λ,κ,ν(z n),λ) ) (θ̂λ(z n)−θ̂λ(zn\\i))êκ, (8) such that for κ, ν ∈ [k], ζ(i),2λ,κ,ν(zn) = αi,j,2κ,ν θ̂λ(zn)+(1−αi,j,2κ,ν )θ̂λ(zn\\i) for some 0 ≤ αi,j,2κ,ν ≤ 1. Further, we have5\n‖θ̂λ(zn)− θ̂λ(zn\\i))‖∞ = Op ( 1\nn\n) , (9)\n‖θ̂λ(zn\\i)− θ̃ (i) λ (z n)‖∞ = Op\n( 1\nn2\n) . (10)\nSee the appendix for the proof. Inspired by Theorem 1, we provide an approximation on the cross validation vector.\nDefinition 6 (approximate cross validation vector) Let ACVλ,i(zn) = ` ( zi; θ̃ (i) λ (z n) ) . We call\n{ACVλ,i(zn)}i∈[n] the approximate cross validation vector. We further call\nACVλ(zn) , 1\nn n∑ i=1 ACVλ,i(zn) (11)\nthe approximate cross validation estimator of the out-of-sample loss.\nWe remark that the definition can be extended to leave-q-out and q-fold cross validation by replacing the index i to an index set S with |S| = q, comprised of the q left-out samples in (4).\nThe cost of the computation of {θ̃ (i)\nλ (z n)}i∈[n] is upper bounded byO(np+C(n, p)) where C(n, p)\nis the computational cost of solving for θ̂λ(zn) in (2); see [14]. Note that the empirical risk minimization problem posed in (2) requires time at least Ω(np). Hence, the overall cost of computation of {θ̃ (i)\nλ (z n)}i∈[n] is dominated by solving (2). On the other hand, the cost of computing the true\ncross validation performance by naively solving n optimization problems {θ̂λ(zn\\i)}i∈[n] posed in (3) would be O(nC(n, p)) which would necessarily be Ω(n2p) making it impractical for largescale problems.\nCorollary 2 The approximate cross validation vector is exact for kernel ridge regression. That is, given that the regularized loss function is quadratic in θ, we have θ̃ (i)\nλ (z n) = θ̂λ(z n\\i) for all i ∈ [n] .\nProof We notice that the error term ε(i)λ,n in (6) only depends on the third derivative of the loss function in a neighborhood of θ̂λ(zn). Hence, provided that the regularized loss function is quadratic in θ, ε(i)λ,n = 0 for all i ∈ [n].\n5Xn = Op(an) implies that Xn/an is stochastically bounded with respect to the density function p(·).\nThe fact that the cross validation vector could be obtained for kernel ridge regression in closed form without actually performing cross validation is not new, and the method is known as PRESS [11]. In a sense, the presented approximation could be thought of as an extension of this idea to more general loss and regularizer functions while losing the exactness property. We remark that the idea of ALOOCV is also related to that of the influence functions. In particular, influence functions have been used in [14] to derive an approximation on LOOCV for neural networks with large sample sizes. However, we notice that methods based on influence functions usually underestimate overfitting making them impractical for model selection. In contrast, we empirically demonstrate the effectiveness of ALOOCV in capturing overfitting and model selection.\nIn the case of `1 regularizer we assume that the support set of θ̂λ(zn) and θ̂λ(zn\\i) are the same. Although this would be true for large enough n under Assumption 1, it is not necessarily true for a given sample zn when sample i is left out. Provided that the support set of θ̂λ(zn\\i) is known we use the developed machinery in Theorem 1 on the subset of parameters that are non-zero. Further, we ignore the `1 regularizer term in the regularized loss function as it does not contribute to the Hessian matrix locally, and we assume that the regularized loss function is otherwise smooth in the sense of Assumption 1. In this case, the cost of calculating ALOOCV would scale with O(npa log(1/ )) where pa denotes the number of non-zero coordinates in the solution θ̂λ(zn).\nWe remark that although the nature of guarantees in Theorem 1 are asymptotic, we have experimentally observed that the estimator works really well even for n and p as small as 50 in elastic net regression, logistic regression, and ridge regression. Next, we also provide an asymptotic characterization of the approximate cross validation.\nLemma 3 Under Assumption 1, we have ACVλ(zn) = L̂zn(θ̂λ(zn)) + R̂zn(θ̂λ(zn),λ) +Op ( 1\nn2\n) , (12)\nwhere R̂zn(θ,λ) ,\n1 n(n− 1) ∑ i∈[n] ∇>θ `(zi;θ) [ Ĥzn\\i(θ,λ) ]−1 ∇θ`(zi;θ). (13)\nNote that in contrast to the ALOOCV (in Theorem 1), the Op(1/n2) error term here depends on the second derivative of the loss function with respect to the parameters, consequently leading to worse performance, and underestimation of overfitting."
    }, {
      "heading" : "4 Tuning the Regularization Parameters",
      "text" : "Thus far, we presented an approximate cross validation vector that closely follows the predictions provided by the cross validation vector, while being computationally inexpensive. In this section, we use the approximate cross validation vector to tune the regularization parameters for the optimal out-of-sample performance. We are interested in solving minλ ( CVλ(zn) = 1n ∑n i=1 ` ( zi; θ̂λ ( zn\\i ))) . To this end, we need to calculate the gradient of\nθ̂λ(z n) with respect to λ, which is given in the following lemma. Lemma 4 We have∇λθ̂λ(zn) = − 1n [ Ĥzn ( θ̂λ(z n),λ )]−1 ∇θr(θ̂λ(zn)). Corollary 5 We have∇λθ̂λ(zn\\i) = − 1n−1 [ Ĥzn\\i ( θ̂λ(z n\\i),λ )]−1 ∇θr(θ̂λ(zn\\i)).\nIn order to apply first order optimization methods for minimizing CVλ(zn), we need to compute its gradient with respect to the tuning parameter vector λ. Applying the simple chain rule implies\n∇λCVλ(zn) = 1\nn n∑ i=1 ∇>λ θ̂λ(zn\\i) ∇θ` ( zi; θ̂λ ( zn\\i )) (14)\n=− 1 n(n− 1) n∑ i=1 ∇>θ r ( θ̂λ(z n\\i) ) [ Ĥzn\\i ( θ̂λ(z n\\i) )]−1 ∇θ` ( zi; θ̂λ ( zn\\i )) , (15)\nwhere (15) follows by substituting∇λθ̂λ(zn\\i) from Corollary 5. However, (15) is computationally expensive and almost impossible in practice even for medium size datasets. Hence, we use the ALOOCV from (4) (Theorem 1) in (14) to approximate the gradient.\nLet\ng (i) λ (z n) , − 1 n− 1\n∇>θ r ( θ̃ (i) λ (z n) ) [ Ĥzn\\i ( θ̃ (i) λ (z n) )]−1 ∇θ` ( zi; θ̃ (i) λ (z n) ) . (16)\nFurther, motivated by the suggested ALOOCV, let us define the approximate gradient gλ(z n) as gλ(z n) , 1n ∑ i∈[n] g (i) λ (z\nn) . Based on our numerical experiments, this approximate gradient closely follows the gradient of the cross validation, i.e., ∇λCVλ(zn) ≈ gλ(zn). Note that this approximation is straightforward to compute. Therefore, using this approximation, we can apply the first order optimization algorithm 1 to optimize the tuning parameter λ. Although Algorithm 1 is\nAlgorithm 1 Approximate gradient descent algorithm for tuning λ\nInitialize the tuning parameter λ0, choose a step-size selection rule, and set t = 0 for t = 0, 1, 2, . . . do\ncalculate the approximate gradient gλt(z n)\nset λt+1 = λt − αtgλt(zn) end for\nmore computationally efficient compared to LOOCV (saving a factor of n), it might still be computationally expensive for large values of n as it still scales linearly with n. Hence, we also present an online version of the algorithm using the stochastic gradient descent idea; see Algorithm 2.\nAlgorithm 2 Stochastic (online) approximate gradient descent algorithm for tuning λ\nInitialize the tuning parameter λ0 and set t = 0 for t = 0, 1, 2, . . . do\nchoose a random index it ∈ {1, . . . , n} calculate the stochastic gradient g(it)\nλt (zn) using (16)\nset λt+1 = λt − αtg(it) λt (zn) end for"
    }, {
      "heading" : "5 Numerical Experiments",
      "text" : "Ridge regression with diagonal regressors: We consider the following regularized loss function:\nwn(z;θ,λ) = `(z;θ) + 1 n λ>r(θ) = 1 2 (y − θ>x)2 + 1 2n θ> diag(λ)θ.\nn pλ L̂zn L ACV 1e5 0.6578 0.6591 0.6578 (0.0041) 1e4 0.5810 0.6069 0.5841 (0.0079) 1e3 0.5318 0.5832 0.5444 (0.0121) 1e2 0.5152 0.5675 0.5379 (0.0146) 1e1 0.4859 0.5977 0.5560 (0.0183) 1e0 0.4456 0.6623 0.6132 (0.0244)\nTable 2: The results of logistic regression (insample loss, out-of-sample loss, CV, ACV) on CIFAR-10 dataset with n = 9600 and p = 3072.\n`(zi; θ̂λ(z n)) CV ACV IF\n0.0872 8.5526 8.6495 0.2202 0.0920 2.1399 2.1092 0.2081 0.0926 10.8783 9.4791 0.2351 0.0941 3.5210 3.3162 0.2210 0.0950 5.7753 6.1859 0.2343 0.0990 5.2626 5.0554 0.2405 0.1505 12.0483 11.5281 0.3878\nTable 3: Comparison of the leave-one-out estimates on the 8 outlier samples with highest in-sample loss in the MNIST dataset.\nIn other words, we consider one regularization parameter per each model parameter. To validate the proposed optimization algorithm, we consider a scenario with p = 50 where x is drawn i.i.d. from N (0, Ip). We let y = θ∗>x + where θ1 = . . . = θ40 = 0 and θ41, . . . , θ50 ∼ N (0, 1) i.i.d, and ∼ N (0, 0.1). We draw n = 150 samples from this model, and apply Algorithm 1 to optimize for λ = (λ1, . . . , λ50). The problem is designed in such a way that out of 50 features, the first 40 are irrelevant while the last 10 are important. We initialize the algorithm with λ11 = . . . = λ 1 50 = 1/3 and compute ACV using Theorem 1. Recall that in this case, ACV is exactly equivalent to CV (see Corollary 2). Figure 1 plots ALOOCV, the out-of-sample loss, and the mean value of λ calculated over the irrelevant and relevant features respectively. As expected, the λ for an irrelevant feature is set to a larger number, on the average, compared to that of a relevant feature. Finally, we remark that the optimization of 50 tuning parameters in 800 iterations took a mere 28 seconds on a PC.\nLogistic regression: The second example that we consider is logistic regression:\nwn(z;θ,λ) = `(z;θ) + 1 n λ>r(θ) = H(y|| sigmoid(θ0 + θ>x)) + 1 2n λ‖θ‖22.\nwhere H(·||·) for any u ∈ [0, 1] and v ∈ (0, 1) is given by H(u||v) := u log 1v + (1 − u) log 1 1−v , and denotes the binary cross entropy function, and sigmoid(x) := 1/(1 + e−x) denotes the sigmoid function. In this case, we only consider a single regularization parameter. Since the loss and regularizer are smooth, we resort to Theorem 1 to compute ACV. We applied logistic regression on MNIST and CIFAR-10 image datasets where we used each pixel in the image as a feature according to the aforementioned loss function. In MNIST, we classify the digits 2 and 3 while in CIFAR-10, we classify “bird” and “cat.” As can be seen in Tables 1 and 2, ACV closely follows CV on the MNIST dataset. On the other hand, the approximation of LOOCV based on influence functions [14] performs poorly in the regime where the model is significantly overfit and hence it cannot be used for effective model selection. On CIFAR-10, ACV takes ≈1s to run per each sample, whereas CV takes ≈60s per each sample requiring days to run for each λ even for this medium sized problem. The histogram of the normalized difference between CV and ACV vectors is plotted in Figure 3 for 5 runs of the algorithm for each λ in Table 1. As can be seen, CV and ACV are almost always within 5% of each other. We have also plotted the loss for the eight outlier samples with the highest in-sample loss in the MNIST dataset in Table 3. As can be seen, ALOOCV closely follows LOOCV even when the leave-one-out loss is two orders of magnitude larger than the in-sample loss for these outliers. On the other hand, the approximation based on the influence functions fails to capture the out-of-sample performance and the outliers in this case.\nElastic net regression: Finally, we consider the popular elastic net regression problem [31]:\nwn(z;θ,λ) = `(z;θ) + 1 n λ>r(θ) = 1 2 (y − θ>x)2 + 1 n λ1‖θ‖1 + 1 2n λ2‖θ‖22.\nIn this case, there are only two regularization parameters to be optimized for the quasi-smooth regularized loss. Similar to the previous case, we consider y = θ∗>x+ where θκ = κρκψκ where ρκ is a Bernoulli(1/2) RV and ψκ ∼ N (0, 1). Hence, the features are weighted non-uniformly in y and half of them are zeroed out on the average. We apply both Algorithms 1 and 2 where we used the approximation in Theorem 1 and the explanation on how to handle `1 regularizers to compute ACV. We initialized with λ1 = λ2 = 0. As can be seen on the left panel (Figure 4), ACV closely follows CV in this case. Further, we see that both algorithms are capable of significantly reducing the loss after only a few iterations. The right panel compares the run-time of the algorithms vs. the number of samples. This confirms our analysis that the run-time of CV scales quadratically with O(n2) as opposed to O(n) in ACV. This impact is more signified in the inner panel where the run-time ratio is plotted."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This work was supported in part by DARPA under Grant No. W911NF-16-1-0561. The authors are thankful to Jason D. Lee (USC) who brought to their attention the recent work [14] on influence functions for approximating leave-one-out cross validation."
    } ],
    "references" : [ {
      "title" : "A survey of cross-validation procedures for model selection",
      "author" : [ "Sylvain Arlot", "Alain Celisse" ],
      "venue" : "Statistics surveys,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "The predictive sample reuse method with applications",
      "author" : [ "Seymour Geisser" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1975
    }, {
      "title" : "Smoothing noisy data with spline functions",
      "author" : [ "Peter Craven", "Grace Wahba" ],
      "venue" : "Numerische Mathematik,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1978
    }, {
      "title" : "Model selection and multimodel inference: a practical information-theoretic approach",
      "author" : [ "Kenneth P Burnham", "David R Anderson" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Statistical predictor identification",
      "author" : [ "Hirotugu Akaike" ],
      "venue" : "Annals of the Institute of Statistical Mathematics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1970
    }, {
      "title" : "Information theory and an extension of the maximum likelihood principle",
      "author" : [ "Hirotogu Akaike" ],
      "venue" : "In Selected Papers of Hirotugu Akaike,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Distribution of informational statistics and a criterion of model fitting",
      "author" : [ "K Takeuchi" ],
      "venue" : "suri-kagaku (mathematical sciences)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1976
    }, {
      "title" : "Cross-validation and multinomial prediction",
      "author" : [ "Mervyn Stone" ],
      "venue" : "Biometrika, pages 509–515,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1974
    }, {
      "title" : "Predicted squared error: a criterion for automatic model selection",
      "author" : [ "Andrew R Barron" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1984
    }, {
      "title" : "The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems",
      "author" : [ "John E Moody" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1992
    }, {
      "title" : "The relationship between variable selection and data agumentation and a method for prediction",
      "author" : [ "David M Allen" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1974
    }, {
      "title" : "Plane answers to complex questions: the theory of linear models",
      "author" : [ "Ronald Christensen" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Characterizations of an empirical influence function for detecting influential cases in regression",
      "author" : [ "R Dennis Cook", "Sanford Weisberg" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1980
    }, {
      "title" : "Understanding black-box predictions via influence functions",
      "author" : [ "Pang Wei Koh", "Percy Liang" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2017
    }, {
      "title" : "Better bootstrap confidence intervals",
      "author" : [ "Bradley Efron" ],
      "venue" : "Journal of the American statistical Association,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1987
    }, {
      "title" : "Generalized cross-validation as a method for choosing a good ridge",
      "author" : [ "Gene H Golub", "Michael Heath", "Grace Wahba" ],
      "venue" : "parameter. Technometrics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1979
    }, {
      "title" : "Regularization parameter selection for nonlinear iterative image restoration and MRI reconstruction using GCV and SURE-based methods",
      "author" : [ "Sathish Ramani", "Zhihao Liu", "Jeffrey Rosen", "Jon-Fredrik Nielsen", "Jeffrey A Fessler" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Application of Bayesian approach to numerical methods of global and stochastic optimization",
      "author" : [ "Jonas Močkus" ],
      "venue" : "Journal of Global Optimization,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1994
    }, {
      "title" : "On Bayesian methods for seeking the extremum",
      "author" : [ "Jonas Močkus" ],
      "venue" : "In Optimization Techniques IFIP Technical Conference,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1975
    }, {
      "title" : "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
      "author" : [ "Eric Brochu", "Vlad M Cora", "Nando De Freitas" ],
      "venue" : "arXiv preprint arXiv:1012.2599,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Sequential model-based optimization for general algorithm configuration",
      "author" : [ "Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown" ],
      "venue" : "In International Conference on Learning and Intelligent Optimization,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Practical Bayesian optimization of machine learning algorithms",
      "author" : [ "Jasper Snoek", "Hugo Larochelle", "Ryan P Adams" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Random search for hyper-parameter optimization",
      "author" : [ "James Bergstra", "Yoshua Bengio" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Auto-weka: Combined selection and hyperparameter optimization of classification algorithms",
      "author" : [ "Chris Thornton", "Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown" ],
      "venue" : "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Towards an empirical foundation for assessing Bayesian optimization of hyperparameters",
      "author" : [ "Katharina Eggensperger", "Matthias Feurer", "Frank Hutter", "James Bergstra", "Jasper Snoek", "Holger Hoos", "Kevin Leyton-Brown" ],
      "venue" : "In NIPS workshop on Bayesian Optimization in Theory and Practice,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    }, {
      "title" : "Bilevel model selection for support vector machines",
      "author" : [ "Gautam Kunapuli", "K Bennett", "Jing Hu", "Jong-Shi Pang" ],
      "venue" : "In CRM Proceedings and Lecture Notes,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2008
    }, {
      "title" : "Model selection via bilevel optimization",
      "author" : [ "Kristin P Bennett", "Jing Hu", "Xiaoyun Ji", "Gautam Kunapuli", "Jong-Shi Pang" ],
      "venue" : "In 2006 Intl. Joint Conf. on Neural Networks",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1929
    }, {
      "title" : "Multi-task Bayesian optimization. In Advances in neural information processing",
      "author" : [ "Kevin Swersky", "Jasper Snoek", "Ryan P Adams" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2004
    }, {
      "title" : "Hyperband: Bandit-based configuration evaluation for hyperparameter optimization",
      "author" : [ "Lisha Li", "Kevin Jamieson", "Giulia DeSalvo", "Afshin Rostamizadeh", "Ameet Talwalkar" ],
      "venue" : "Proc. of ICLR,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2017
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "Hui Zou", "Trevor Hastie" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "A simple and universal approach to measuring the out-of-sample risk is cross validation [1].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "Related work: A main application of cross validation (see [1] for a recent survey) is in model selection [2–4].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "On the theoretical side, the proposed approximation on LOOCV is asymptotically equivalent to Takeuchi information criterion (TIC) [4–7], under certain regularity conditions (see [8] for a proof of asymptotic equivalence of AIC and LOOCV in autoregressive models).",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 8,
      "context" : "This is also related to Barron’s predicted square error (PSE) [9] and Moody’s effective number of parameters for nonlinear systems [10].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "This is also related to Barron’s predicted square error (PSE) [9] and Moody’s effective number of parameters for nonlinear systems [10].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "ALOOCV simplifies to the closed form derivation of the LOOCV for linear regression, called PRESS (see [11, 12]).",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "ALOOCV simplifies to the closed form derivation of the LOOCV for linear regression, called PRESS (see [11, 12]).",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 12,
      "context" : "This work is also related to the concept of influence functions [13], which has recently received renewed interest [14].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "This work is also related to the concept of influence functions [13], which has recently received renewed interest [14].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 14,
      "context" : "Finally, we expect that the idea of ALOOCV can be extended to derive computationally efficient approximate bootstrap estimators [15].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "A similar approach has been taken for tuning the single parameter in ridge regression where cross validation can be obtained in closed form [16].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : ", Stein unbiased estimator of the risk for multiple parameter selection [17, 18].",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "Finally, model selection has been considered as a bi-level optimization [27,28] where the training process is modeled as a second level optimization problem within the original problem.",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "Finally, model selection has been considered as a bi-level optimization [27,28] where the training process is modeled as a second level optimization problem within the original problem.",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 29,
      "context" : "For example, elastic net regression [31] can be cast in this framework by setting r1(θ) = ‖θ‖1 and r2(θ) = 12‖θ‖ 2 2.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "The cost of the computation of {θ̃ (i) λ (z )}i∈[n] is upper bounded byO(np+C(n, p)) where C(n, p) is the computational cost of solving for θ̂λ(z) in (2); see [14].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : "The fact that the cross validation vector could be obtained for kernel ridge regression in closed form without actually performing cross validation is not new, and the method is known as PRESS [11].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 13,
      "context" : "In particular, influence functions have been used in [14] to derive an approximation on LOOCV for neural networks with large sample sizes.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "On the other hand, the approximation of LOOCV based on influence functions [14] performs poorly in the regime where the model is significantly overfit and hence it cannot be used for effective model selection.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 29,
      "context" : "Elastic net regression: Finally, we consider the popular elastic net regression problem [31]:",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "Lee (USC) who brought to their attention the recent work [14] on influence functions for approximating leave-one-out cross validation.",
      "startOffset" : 57,
      "endOffset" : 61
    } ],
    "year" : 2017,
    "abstractText" : "We consider the parametric learning problem, where the objective of the learner is determined by a parametric loss function. Employing empirical risk minimization with possibly regularization, the inferred parameter vector will be biased toward the training samples. Such bias is measured by the cross validation procedure in practice where the data set is partitioned into a training set used for training and a validation set, which is not used in training and is left to measure the outof-sample performance. A classical cross validation strategy is the leave-one-out cross validation (LOOCV) where one sample is left out for validation and training is done on the rest of the samples that are presented to the learner, and this process is repeated on all of the samples. LOOCV is rarely used in practice due to the high computational complexity. In this paper, we first develop a computationally efficient approximate LOOCV (ALOOCV) and provide theoretical guarantees for its performance. Then we use ALOOCV to provide an optimization algorithm for finding the regularizer in the empirical risk minimization framework. In our numerical experiments, we illustrate the accuracy and efficiency of ALOOCV as well as our proposed framework for the optimization of the regularizer.",
    "creator" : null
  }
}