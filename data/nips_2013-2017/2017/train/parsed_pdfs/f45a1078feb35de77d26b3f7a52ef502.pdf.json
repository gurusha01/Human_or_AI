{
  "name" : "f45a1078feb35de77d26b3f7a52ef502.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Wasserstein Learning of Deep Generative Point Process Models",
    "authors" : [ "Shuai Xiao", "Mehrdad Farajtabar", "Xiaojing Ye", "Junchi Yan", "Hongyuan Zha" ],
    "emails" : [ "benjaminforever@sjtu.edu.cn,", "mehrdad@gatech.edu", "xye@gsu.edu,", "yanjc@cn.ibm.com", "lsong@cc.gatech.edu", "zha@cc.gatech.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Event sequences are ubiquitous in areas such as e-commerce, social networks, and health informatics. For example, events in e-commerce are the times a customer purchases a product from an online vendor such as Amazon. In social networks, event sequences are the times a user signs on or generates posts, clicks, and likes. In health informatics, events can be the times when a patient exhibits symptoms or receives treatments. Bidding and asking orders also comprise events in the stock market. In all of these applications, understanding and predicting user behaviors exhibited by the event dynamics are of great practical, economic, and societal interest.\nTemporal point processes [1] is an effective mathematical tool for modeling events data. It has been applied to sequences arising from social networks [2, 3, 4], electronic health records [5], ecommerce [6], and finance [7]. A temporal point process is a random process whose realization consists of a list of discrete events localized in (continuous) time. The point process representation of sequence data is fundamentally different from the discrete time representation typically used in time series analysis. It directly models the time period between events as random variables, and allows temporal events to be modeled accurately, without requiring the choice of a time window to aggregate events, which may cause discretization errors. Moreover, it has a remarkably extensive theoretical foundation [8].\n∗Authors contributed equally. Work completed at Georgia Tech.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nHowever, conventional point process models often make strong unrealistic assumptions about the generative processes of the event sequences. In fact, a point process is characterized by its conditional intensity function – a stochastic model for the time of the next event given all the times of previous events. The functional form of the intensity is often designed to capture the phenomena of interests [9]. Some examples are homogeneous and non-homogeneous Poisson processes [10], self-exciting point processes [11], self-correcting point process models [12], and survival processes [8]. Unfortunately, they make various parametric assumptions about the latent dynamics governing the generation of the observed point patterns. As a consequence, model misspecification can cause significantly degraded performance using point process models, which is also shown by our experimental results later.\nTo address the aforementioned problem, the authors in [13, 14, 15] propose to learn a general representation of the underlying dynamics from the event history without assuming a fixed parametric form in advance. The intensity function of the temporal point process is viewed as a nonlinear function of the history of the process and is parameterized using a recurrent neural network. Attenional mechanism is explored to discover the underlying structure [16]. Apparently this line of work still relies on explicit modeling of the intensity function. However, in many tasks such as data generation or event prediction, knowledge of the whole intensity function is unnecessary. On the other hand, sampling sequences from intensity-based models is usually performed via a thinning algorithm [17], which is computationally expensive; many sample events might be rejected because of the rejection step, especially when the intensity exhibits high variation. More importantly, most of the methods based on intensity function are trained by maximizing log likelihood or a lower bound on it. They are asymptotically equivalent to minimizing the Kullback-Leibler (KL) divergence between the data and model distributions, which suffers serious issues such as mode dropping [18, 19]. Alternatively, Generative Adversarial Networks (GAN) [20] have proven to be a promising alternative to traditional maximum likelihood approaches [21, 22].\nIn this paper, for the first time, we bypass the intensity-based modeling and likelihood-based estimation of temporal point processes and propose a neural network-based model with a generative adversarial learning scheme for point processes. In GANs, two models are used to solve a minimax game: a generator which samples synthetic data from the model, and a discriminator which classifies the data as real or synthetic. Theoretically speaking, these models are capable of modeling an arbitrarily complex probability distribution, including distributions over discrete events. They achieve state-of-the-art results on a variety of generative tasks such as image generation, image super-resolution, 3D object generation, and video prediction [23, 24].\nThe original GAN in [20] minimizes the Jensen-Shannon (JS) and is regarded as highly unstable and prone to miss modes. Recently, Wasserstein GAN (WGAN) [25] is proposed to use the Earth Moving distance (EM) as an objective for training GANs. Furthermore it is shown that the EM objective, as a metric between probability distributions [26] has many advantages as the loss function correlates with the quality of the generated samples and reduces mode dropping [27]. Moreover, it leverages the geometry of the space of event sequences in terms of their distance, which is not the case for an MLE-based approach. In this paper we extend the notion of WGAN for temporal point processes and adopt a Recurrent Neural Network (RNN) for training. Importantly, we are able to demonstrate that Wasserstein distance training of RNN point process models outperforms the same architecture trained using MLE.\nIn a nutshell, the contributions of the paper are: i) We propose the first intensity-free generative model for point processes and introduce the first (to our best knowledge) likelihood-free corresponding learning methods; ii) We extend WGAN for point processes with Recurrent Neural Network architecture for sequence generation learning; iii) In contrast to the usual subjective measures of evaluating GANs we use a statistical and a quantitative measure to compare the performance of the model to the conventional ones. iv) Extensive experiments involving various types of point processes on both synthetic and real datasets show the promising performance of our approach."
    }, {
      "heading" : "2 Proposed Framework",
      "text" : "In this section, we define Point Processes in a way that is suitable to be combined with the WGANs."
    }, {
      "heading" : "2.1 Point Processes",
      "text" : "Let S be a compact space equipped with a Borel σ-algebra B. Take Ξ as the set of counting measures on S with C as the smallest σ-algebra on it. Let (Ω,F ,P) be a probability space. A point process on S is a measurable map ξ : Ω→ Ξ from the probability space (Ω,F ,P) to the measurable space (Ξ, C). Figure 1-a illustrates this mapping.\nEvery realization of a point process ξ can be written as ξ = ∑n i=1 δXi where δ is the Dirac measure, n is an integer-valued random variable and Xi’s are random elements of S or events. A point process can be equivalently represented by a counting process: N(B) := ∫ B ξ(x)dx, which basically is the number of events in each Borel subset B ∈ B of S. The mean measure M of a point process ξ is a measure on S that assigns to every B ∈ B the expected number of events of ξ in B, i.e., M(B) := E[N(B)] for all B ∈ B. For inhomogeneous Poisson process, M(B) = ∫ B λ(x)dx, where the intensity function λ(x) yields a positive measurable function on S. Intuitively speaking, λ(x)dx is the expected number of events in the infinitesimal dx. For the most common type of point process, a Homogeneous Poisson process, λ(x) = λ and M(B) = λ|B|, where | · | is the Lebesgue measure on (S,B). More generally, in Cox point processes, λ(x) can be a random density possibly depending on the history of the process. For any point process, given λ(·), N(B) ∼ Poisson( ∫ B λ(x)dx). In addition, if B1, . . . , Bk ∈ B are disjoint, then N(B1), . . . , N(Bk) are independent conditioning on λ(·). For the ease of exposition, we will present the framework for the case where the events are happening in the real half-line of time. But the framework is easily extensible to the general space."
    }, {
      "heading" : "2.2 Temporal Point Processes",
      "text" : "A particularly interesting case of point processes is given when S is the time interval [0, T ), which we will call a temporal point process. Here, a realization is simply a set of time points: ξ = ∑n i=1 δti . With a slight notation abuse we will write ξ = {t1, . . . , tn} where each ti is a random time before T . Using a conditional intensity (rate) function is the usual way to characterize point processes.\nFor Inhomogeneous Poisson process (IP), the intensity λ(t) is a fixed non-negative function supported in [0, T ). For example, it can be a multi-modal function comprised of k Gaussian kernels: λ(t) =∑k i=1 αi(2πσ 2 i ) −1/2 exp ( −(t− ci)2/σ2i ) , for t ∈ [0, T ), where ci and σi are fixed center and standard deviation, respectively, and αi is the weight (or importance) for kernel i.\nA self-exciting (Hawkes) process (SE) is a cox process where the intensity is determined by previous (random) events in a special parametric form: λ(t) = µ+β ∑ ti<t\ng(t− ti), where g is a nonnegative kernel function, e.g., g(t) = exp(−ωt) for some ω > 0. This process has an implication that the occurrence of an event will increase the probability of near future events and its influence will (usually) decrease over time, as captured by (the usually) decaying fixed kernel g. µ is the exogenous rate of firing events and α is the coefficient for the endogenous rate.\nIn contrast, in self-correcting processes (SC), an event will decrease the probability of an event: λ(t) = exp(ηt− ∑ ti<t\nγ). The exp ensures that the intensity is positive, while η and γ are exogenous and endogenous rates.\nWe can utilize more flexible ways to model the intensity, e.g., by a Recurrent Neural Network (RNN): λ(t) = gw(t, hti), where hti is the feedback loop capturing the influence of previous events (last updated at the latest event) and is updated by hti = hv(ti, hti−1). Here w, v are network weights."
    }, {
      "heading" : "2.3 Wasserstein-Distance for Temporal Point Processes",
      "text" : "Given samples from a point process, one way to estimate the process is to find a model (Ωg,Fg,Pg)→ (Ξ, C) that is close enough to the real data (Ωr,Fr,Pr)→ (Ξ, C). As mentioned in the introduction, Wasserstein distance [25] is our choice as the proximity measure. The Wasserstein distance between distribution of two point processes is:\nW (Pr,Pg) = inf ψ∈Ψ(Pr,Pg) E(ξ,ρ)∼ψ[‖ξ − ρ‖?], (1)\nwhere Ψ(Pr,Pg) denotes the set of all joint distributions ψ(ξ, ρ) whose marginals are Pr and Pg . The distance between two sequences ‖ξ − ρ‖?, is tricky and need further attention. Take ξ = {x1, x2, . . . , xn} and ρ = {y1, . . . , ym}, where for simplicity we first consider the case m = n. The two sequences can be thought as discrete distributions µξ = ∑n i=1 1 nδxi and µ ρ = ∑n i=1 1 nδyi . Then, the distance between these two is an optimal transport problem argminπ∈Σ〈π,C〉, where Σ is the set of doubly stochastic matrices (rows and columns sum up to one), 〈·, ·〉 is the Frobenius dot product, and C is the cost matrix. Cij captures the energy needed to move a probability mass from xi to yj . We take Cij = ‖xi − yj‖◦ where ‖ · ‖◦ is the norm in S. It can be seen that the optimal solution is attained at extreme points and, by Birkhoff’s theorem, the extreme points of the set of doubly stochastic matrices is a permutation [28]. In other words, the mass is transfered from a unique source\nevent to a unique target event. Therefore, we have: ‖ξ − ρ‖? = minσ\n∑n\ni=1 ‖xi − yσ(i)‖◦, where the minimum is taken among all n! permutations of 1 . . . n. For the case m 6= n, without loss of generality we assume n ≤ m and define the distance as follows:\n‖ξ − ρ‖? = min σ ∑n i=1 ‖xi − yσ(i)‖◦ + ∑m i=n+1 ‖s− yσ(i)‖, (2)\nwhere s is a fixed limiting point in border of the compact space S and the minimum is over all permutations of 1 . . .m. The second term penalizes unmatched points in a very special way which will be clarified later. Appendix B proves that it is indeed a valid distance measure.\nInterestingly, in the case of temporal point process in [0, T ) the distance between ξ = {t1, . . . , tn} and ρ = {τ1, . . . , τm} is reduced to\n‖ξ − ρ‖? = ∑n\ni=1 |ti − τi|+ (m− n)× T − ∑m i=n+1 τi, (3)\nwhere the time points are ordered increasingly, s = T is chosen as the anchor point, and | · | is the Lebesgue measure in the real line. A proof is given in Appendix C. This choice of distance is significant in two senses. First, it is computationally efficient and no excessive computation is involved. Secondly, in terms of point processes, it is interpreted as the volume by which the two counting measures differ. Figure 1-b demonstrates this intuition and justifies our choice of metric in Ξ and Appendix D contains the proof. The distance used in our current work is the simplest yet effective distance that exhibits high interpretability and efficient computability. More robust distance like local alignment distance and dynamic time warping [29] should be more robust and are great venues for future work.\nEquation (1) is computationally highly intractable and its dual form is usually utilized [25]:\nW (Pr,Pg) = sup ‖f‖L≤1 Eξ∼Pr [f(ξ)]− Eρ∼Pg [f(ρ)], (4)\nwhere the supremum is taken over all Lipschitz functions f : Ξ → R, i.e., functions that assign a value to a sequence of events (points) and satisfy |f(ξ)− f(ρ)| ≤ ‖ξ − ρ‖? for all ξ and ρ. However, solving the dual form is still highly nontrivial. Enumerating all Lipschitz functions over point process realizations is impossible. Instead, we choose a parametric family of functions to approximate the search space fw and consider solving the problem\nmax w∈W,‖fw‖L≤1 Eξ∼Pr [fw(ξ)]− Eρ∼Pg [fw(ρ)] (5)\nwhere w ∈ W is the parameter. The more flexible fw, the more accurate will be the approximation. It is notable that W-distance leverages the geometry of the space of event sequences in terms of their distance, which is not the case for MLE-based approach. It in turn requires functions of event sequences f(x1, x2, ...), rather than functions of the time stamps f(xi). Furthermore, Stein’s method to approximate Poisson processes [30, 31] is also relevant as they are defining distances between a Poisson process and an arbitrary point process."
    }, {
      "heading" : "2.4 WGAN for Temporal Point Processes",
      "text" : "Equipped with a way to approximately compute the Wasserstein distance, we will look for a model Pr that is close to the distribution of real sequences. Again, we choose a sufficiently flexible parametric family of models, gθ parameterized by θ. Inspired by GAN [20], this generator takes a noise and turns it into a sample to mimic the real samples. In conventional GAN or WGAN, Gaussian or uniform distribution is chosen. In point processes, a homogeneous Poisson process plays the role of a\nnon-informative and uniform-like distribution: the probability of events in every region is independent of the rest and is proportional to its volume. Define the noise process as (Ωz,Fz,Pz)→ (Ξ, C), then ζ ∼ Pz is a sample from a Poisson process on S = [0, T ) with constant rate λz > 0. Therefore, gθ : Ξ→ Ξ is a transformation in the space of counting measures. Note that λz is part of the prior knowledge and belief about the problem domain. Therefore, the objective of learning the generative model can be written as minW (Pr,Pg) or equivalently:\nmin θ max w∈W,‖fw‖L≤1 Eξ∼Pr [fw(ξ)]− Eζ∼Pz [fw(gθ(ζ))] (6)\nIn GAN terminology fw is called the discriminator and gθ is known as the generator model. We estimate the generative model by enforcing that the sample sequences from the model have the same distribution as training sequences. Given L samples sequences from real data Dr = {ξ1, . . . , ξL} and from the noise Dz = {ζ1, . . . , ζL} the two expectations are estimated empirically: Eξ∼Pr [fw(ξ)] = 1 L ∑L l=1 fw(ξl) and Eζ∼Pz [fw(gθ(ζ))] = 1 L ∑L l=1 fw(gθ(ζl))."
    }, {
      "heading" : "2.5 Ingredients of WGANTPP",
      "text" : "To proceed with our point process based WGAN, we need the generator function gθ : Ξ→ Ξ, the discriminator function fw : Ξ→ R, and enforce Lipschitz constraint on fw. Figure 4 in Appendix A illustrates the data flow for WGANTPP.\nThe generator transforms a given sequence to another sequence. Similar to [32, 33] we use Recurrent Neural Networks (RNN) to model the generator. For clarity, we use the vanilla RNN to illustrate the computational process as below. The LSTM is used in our experiments for its capacity to capture long-range dependency. If the input and output sequences are ζ = {z1, . . . , zn} and ρ = {t1, . . . , tn} then the generator gθ(ζ) = ρ works according to\nhi = φ h g (A h gzi +B h g hi−1 + b h g ), ti = φ x g(B x ghi + b x g) (7)\nHere hi is the k-dimensional history embedding vector and φhg and φ x g are the activation functions. The parameter set of the generator is θ = {( Ahg ) k×1 , ( Bhg ) k×k , ( bhg ) k×1 , ( Bxg ) 1×k , ( bxg ) 1×1 } .\nSimilarly, we define the discriminator function who assigns a scalar value fw(ρ) = ∑n i=1 ai to the sequence ρ = {t1, . . . , tn} according to hi = φ h d(A h dti +B h g hi−1 + b h g ) ai = φ a d(B a dhi + b a d) (8)\nwhere the parameter set is comprised of w = {( Ahd ) k×1 , ( Bhd ) k×k , ( bhd ) k×1 , (B a d )1×k , (b a d)1×1 } . Note that both generator and discriminator RNNs are causal networks. Each event is only influenced by the previous events. To enforce the Lipschitz constraints the original WGAN paper [18] adopts weight clipping. However, our initial experiments shows an inferior performance by using weight clipping. This is also reported by the same authors in their follow-up paper [27] to the original work. The poor performance of weight clipping for enforcing 1-Lipschitz can be seen theoretically as well: just consider a simple neural network with one input, one neuron, and one output: f(x) = σ(wx+ b) and the weight clipping w < c. Then,\n|f ′(x)| ≤ 1⇐⇒ |wσ′(wx+ b)| ≤ 1⇐⇒ |w| ≤ 1/|σ′(wx+ b)| (9) It is clear that when 1/|σ′(wx+ b)| < c, which is quite likely to happen, the Lipschitz constraint is not necessarily satisfied. In our work, we use a novel approach for enforcing the Lipschitz constraints, avoiding the computation of the gradient which can be costly and difficult for point processes. We add the Lipschitz constraint as a regularization term to the empirical loss of RNN.\nmin θ max w∈W,‖fw‖L≤1\n1\nL L∑ l=1 fw(ξl)− L∑ l=1 fw(gθ(ζl))− ν L∑ l,m=1 | |fw(ξl)− fw(gθ(ζm))| |ξl − gθ(ζm)|? − 1| (10)\nWe can take each of the (\n2L 2\n) pairs of real and generator sequences, and regularize based on them;\nhowever, we have seen that only a small portion of pairs (O(L)), randomly selected, is sufficient. The procedure of WGANTPP learning is given in Algorithm 1\nRemark The significance of Lipschitz constraint and regularization (or more generally any capacity control) is more apparent when we consider the connection of W-distance and optimal transport problem [28]. Basically, minimizing the W-distance between the empirical distribution and the model distribution is equivalent to a semidiscrete optimal transport [28]. Without capacity control for the generator and discriminator, the optimal solution simply maps a partition of the sample space to the set of data points, in effect, memorizing the data points.\nAlgorithm 1 WGANTPP for Temporal Point Process. The default values α = 1e − 4, β1 = 0.5, β2 = 0.9, m = 256, ncritic = 5. Require: : the regularization coefficient ν for direct Lipschitz constraint. the batch size, m. the\nnumber of iterations of the critic per generator iteration, ncritic. Adam hyper-parameters α, β1, β2. Require: : w0, initial critic parameters. θ0, initial generator’s parameters.\n1: set prior λz to the expectation of event rate for real data. 2: while θ has not converged do 3: for t = 0, ..., ncritic do 4: Sample point process realizations {ξ(i)}mi=1 ∼ Pr from real data. 5: Sample {ζ(i)}mi=1 ∼ Pz from a Poisson process with rate λz . 6: L′ ← [ 1 m ∑m i=1 fw(gθ(ζ (i)))− 1m ∑m i=1 fw(ξ (i)) ] + ν ∑m i,j=1 | |fw(ξi)−fw(gθ(ζj))| |ξi−gθ(ζj)|? − 1| 7: w ← Adam(∇wL′, w, α, β1, β2) 8: end for 9: Sample {ζ(i)}mi=1 ∼ Pz from a Poisson process with rate λz .\n10: θ ← Adam(−∇θ 1m ∑m i=1 fw(gθ(ζ\n(i))), θ, α, β1, β2) 11: end while"
    }, {
      "heading" : "3 Experiments",
      "text" : "The current work aims at exploring the feasibility of modeling point process without prior knowledge of its underlying generating mechanism. To this end, most widely-used parametrized point processes, e.g., self-exciting and self-correcting, and inhomogeneous Poisson processes and one flexible neural network model, neural point process are compared. In this work we use the most general forms for simpler and clear exposition to the reader and propose the very first model in adversarial training of point processes in contrast to likelihood based models."
    }, {
      "heading" : "3.1 Datasets and Protocol",
      "text" : "Synthetic datasets. We simulate 20,000 sequences over time [0, T ) where T = 15, for inhomogeneous process (IP), self-exciting (SE), and self-correcting process (SC), recurrent neural point process (NN). We also create another 4 (= C34 ) datasets from the above 4 synthetic data by a uniform mixture\nfrom the triplets. The new datasets IP+SE+SC, IP+SE+NN, IP+SC+NN, SE+SC+NN are created to testify the mode dropping problem of learning a generative model. The parameter setting follows:\ni) Inhomogeneous process. The intensity function is independent from history and given in Sec. 2.2, where k = 3, α = [3, 7, 11], c = [1, 1, 1], σ = [2, 3, 2]. ii) Self-exciting process. The past events increase the rate of future events. The conditional intensity function is given in Sec. 2.2 where µ = 1.0, β = 0.8 and the decaying kernel g(t− ti) = e−(t−ti). iii) Self-correcting process. The conditional intensity function is defined in Sec. 2.2. It increases with time and decreases by events occurrence. We set η = 1.0, γ = 0.2. iv) Recurrent Neural Network process. The conditional intensity is given in Sec. 2.2, where the neural network’s parameters are set randomly and fixed. We first feed random variable from [0,1] uniform distribution, and then iteratively sample events from the intensity and feed the output into the RNN to get the new intensity for the next step.\nReal datasets. We collect sequences separately from four public available datasets, namely, healthcare MIMIC-III, public media MemeTracker, NYSE stock exchanges, and publications citations. The time scale for all real data are scaled to [0,15], and the details are as follows:\ni) MIMIC. MIMIC-III (Medical Information Mart for Intensive Care III) is a large, publicly available dataset, which contains de-identified health-related data during 2001 to 2012 for more than 40,000 patients. We worked with patients who appear at least 3 times, which renders 2246 patients. Their visiting timestamps are collected as the sequences. ii) Meme. MemeTracker tracks the meme diffusion over public media, which contains more than 172 million news articles or blog posts. The memes are sentences, such as ideas, proverbs, and the time is recorded when it spreads to certain websites. We randomly sample 22,000 cascades. iii) MAS. Microsoft Academic Search provides access to its data, including publication venues, time, citations, etc. We collect citations records for 50,000 papers. iv) NYSE. We use 0.7 million high-frequency transaction records from NYSE for a stock in one day. The transactions are evenly divided into 3,200 sequences with equal durations."
    }, {
      "heading" : "3.2 Experimental Setup",
      "text" : "Details. We can feed the temporal sequences to generator and discriminator directly. In practice, all temporal sequences are transformed into time duration between two consecutive events, i.e., transforming the sequence ξ = {t1, . . . , tn} into {τ1, . . . , τn−1}, where τi = ti+1−ti. This approach makes the model train easily and perform robustly. The transformed sequences are statistically identical to the original sequences, which can be used as the inputs of our neural network. To make sure we that the times are increasing we use elu + 1 activation function to produce positive inter arrival times for the generator and accumulate the intervals to get the sequence. The Adam optimization method with learning rate 1e-4, β1 = 0.5, β2 = 0.9, is applied. The code is available 2.\nBaselines. We compare the proposed method of learning point processes (i.e., minimizing sample distance) with maximum likelihood based methods for point process. To use MLE inference for point process, we have to specify its parametric model. The used parametric model are inhomogeneous Poisson process (mixture of Gaussian), self-exciting process, self-correcting process, and RNN. For\n2https://github.com/xiaoshuai09/Wasserstein-Learning-For-Point-Process\neach data, we use all the above solvers to learn the model and generate new sequences, and then we compare the generated sequences with real ones.\nEvaluation metrics. Although our model is an intensity-free approach we will evaluate the performance by metrics that are computed via intensity. For all models, we work with the empirical intensity instead. Note that our objective measures are in sharp contrast with the best practices in GANs in which the performance is usually evaluated subjectively, e.g., by visual quality assessment. We evaluate the performance of different methods to learn the underlying processes via two measures: 1) The first one is the well-known QQ plot of sequences generated from learned model. The quantile-quantile (q-q) plot is the graphical representation of the quantiles of the first data set against the quantiles of the second data set. From the time change property [10] of point processes, if the sequences come from the point process λ(t) , then the integral Λ = ∫ tt+1 ti\nλ(s)ds between consecutive events should be exponential distribution with parameter 1. Therefore, the QQ plot of Λ against exponential distribution with rate 1 should fall approximately along a 45-degree reference line. The evaluation procedure is as follows: i) The ground-truth data is generated from a model, say IP; ii) All 5 methods are used to learn the unknown process using the ground-truth data; iii) The learned model is used to generate a sequence; iv) The sequence is used against the theoretical quantiles from the model to see if the sequence is really coming from the ground-truth generator or not; v) The deviation from slope 1 is visualized or reported as a performance measure. 2) The second metric is the deviation between empirical intensity from the learned model and the ground truth intensity. We can estimate empirical intensity λ′(t) = E(N(t + δt) − N(t))/δt from sufficient number of realizations of point process through counting the average number of events during [t, t+ δt], where N(t) is the count process for λ(t). The L1 distance between the ground-truth empirical intensity and the learned empirical intensity is reported as a performance measure."
    }, {
      "heading" : "3.3 Results and Discussion",
      "text" : "Synthetic data. Figure 2 presents the learning ability of WGANTPP when the ground-truth data is generated via different types of point process. We first compare the QQ plots in the top row from the micro perspective view, where QQ plot describes the dependency between events. Red dots legend-ed with Real are the optimal QQ distribution, where the intensity function generates the sequences are known. We can observe that even though WGANTPP has no prior information about the ground-truth point process, it can estimate the model better except for the estimator that knows the parametric form of data. This is quite expected: When we are training a model and we know the parametric form of the generating model we can find it better. However, whenever the model is misspecified (i.e., we don’t know the parametric from a priori) WGANTPP outperforms other parametric forms and RNN approach. The middle row of figure 2 compares the empirical intensity. The Real line is the optimal empirical intensity estimated from the real data. The estimator can recover the empirical intensity well in the case that we know the parametric form where the data comes from. Otherwise, estimated intensity degrades considerably when the model is misspecified. We can observe our WGANTPP produces the empirical intensity better and performs robustly across different types of point process data. For MLE-IP, different number of kernels are tested and the empirical intensity results improves but QQ plot results degrade when the number of kernels increases, so only result of 3 kernels is shown mainly for clarity of presentation. The fact that the empirical intensity estimated from MLE-IP method are good and QQ plots are very bad indicates the inhomogeneous Poisson process can capture the average intensity (Macro dynamics) accurately but incapable of capturing the dependency between events (Micro dynamics). To testify that WGANTPP can cope with mode dropping, we generate mixtures of data from three different point processes and use this data to train different models. Models with specified form can handle limited types of data and fail to learn from diverse data sources. The last row of figure 2 shows the learned intensity from mixtures of data. WGANTPP produces better empirical intensity than alternatives, which fail to capture the heterogeneity in data. To verify the robustness of WGANTPP, we randomly initialize the generator parameters and run 10 rounds to get the mean and std of deviations for both empirical intensity and QQ plot from ground truth. For empirical intensity, we compute the integral of difference of learned intensity and ground-truth intensity. Table 1 reports the mean and std of deviations for intensity deviation. For each estimators, we obtain the slope from the regression line for its QQ plot. Table 1 reports the mean and std of deviations for slope of the QQ plot. Compared to the MLE-estimators, WGANTPP consistently outperforms even without prior knowledge about the parametric form of the true underlying generative point process. Note that for mixture models QQ-plot is not feasible. Real-world data. We evaluate WGANTPP on a diverse real-world data process from health-care, public media, scientific activities and stock exchange. For those real world data, the underlying\ngenerative process is unknown, previous works usually assume that they are certain types of point process from their domain knowledge. Figure 3 shows the intensity learned from different models, where Real is estimated from the real-world data itself. Table 2 reports the intensity deviation. When all models have no prior knowledge about the true generative process, WGANTPP recovers intensity better than all the other models across the data sets.\nAnalysis. We have observed that when the generating model is misspecified WGANTPP outperforms the other methods without leveraging the a priori knowledge of the parametric form. However, when the exact parametric form of data is known and when it is utilized to learn the parameters, MLE with this full knowledge performs better. However, this is generally a strong assumption. As we have observed from the real-world experiments WGANTPP is superior in terms of performance. Somewhat surprising is the observation that WGANTPP tends to outperform the MLE-NN approach which basically uses the same RNN architecture but trained using MLE. The superior performance of our approach compared to MLE-NN is another witness of the the benefits of using W-distance in finding a generator that fits the observed sequences well. Even though the expressive power of the estimators is the same for WGANTPP and MLE-NN, MLE-NN may suffer from mode dropping or get stuck in an inferior local minimum since maximizing likelihood is asymptotically equivalent to minimizing the Kullback-Leibler (KL) divergence between the data and model distribution. The inherent weakness of KL divergence [25] renders MLE-NN perform unstably, and the large variances of deviations empirically demonstrate this point."
    }, {
      "heading" : "4 Conclusion and Future Work",
      "text" : "We have presented a novel approach for Wasserstein learning of deep generative point processes which requires no prior knowledge about the underlying true process and can estimate it accurately across a wide scope of theoretical and real-world processes. For the future work, we would like to explore the connection of the WGAN with the optimal transport problem. We will also explore other possible distance metrics over the realizations of point processes, and more sophisticated transforms of point processes, particularly those that are causal. Extending the current work to marked point processes and processes over structured spaces is another interesting venue for future work.\nAcknowledgements. This project was supported in part by NSF (IIS-1639792, IIS-1218749, IIS1717916, CMMI-1745382), NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF CNS-1704701, ONR N00014-15-1-2340, NSFC 61602176, Intel ISTC, NVIDIA and Amazon AWS."
    } ],
    "references" : [ {
      "title" : "An introduction to the theory of point processes",
      "author" : [ "DJ Daley", "D Vere-Jones" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Discovering latent network structure in point process data",
      "author" : [ "Scott W Linderman", "Ryan P Adams" ],
      "venue" : "In ICML,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Shaping social activity by incentivizing users",
      "author" : [ "Mehrdad Farajtabar", "Nan Du", "Manuel Gomez-Rodriguez", "Isabel Valera", "Hongyuan Zha", "Le Song" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Multistage campaigning in social networks",
      "author" : [ "Mehrdad Farajtabar", "Xiaojing Ye", "Sahar Harati", "Hongyuan Zha", "Le Song" ],
      "venue" : "In NIPS 2016",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "A multitask point process predictive model",
      "author" : [ "Wenzhao Lian", "Ricardo Henao", "Vinayak Rao", "Joseph E Lucas", "Lawrence Carin" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2030
    }, {
      "title" : "Path to purchase: A mutually exciting point process model for online advertising and conversion",
      "author" : [ "Lizhen Xu", "Jason A Duan", "Andrew Whinston" ],
      "venue" : "Management Science,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Hawkes processes in finance",
      "author" : [ "Emmanuel Bacry", "Iacopo Mastromatteo", "Jean-François Muzy" ],
      "venue" : "Market Microstructure and Liquidity,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Survival and event history analysis: a process point of view",
      "author" : [ "Odd Aalen", "Ornulf Borgan", "Hakon Gjessing" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Coevolve: A joint point process model for information diffusion and network co-evolution",
      "author" : [ "Mehrdad Farajtabar", "Yichen Wang", "Manuel Gomez-Rodriguez", "Shuang Li", "Hongyuan Zha", "Le Song" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Spectra of some self-exciting and mutually exciting point processes",
      "author" : [ "Alan G Hawkes" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1971
    }, {
      "title" : "A self-correcting point process",
      "author" : [ "Valerie Isham", "Mark Westcott" ],
      "venue" : "Stochastic Processes and Their Applications,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1979
    }, {
      "title" : "Recurrent marked temporal point processes: Embedding event history to vector",
      "author" : [ "Nan Du", "Hanjun Dai", "Rakshit Trivedi", "Utkarsh Upadhyay", "Manuel Gomez-Rodriguez", "Le Song" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Modeling the Intensity Function of Point Process Via Recurrent Neural Networks",
      "author" : [ "Shuai Xiao", "Junchi Yan", "Xiaokang Yang", "Hongyuan Zha", "Stephen M. Chu" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2017
    }, {
      "title" : "Eisne The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process",
      "author" : [ "Hongyuan Mei", "Jason" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2017
    }, {
      "title" : "Joint Modeling of Event Sequence and Time Series with Attentional Twin Recurrent Neural Networks",
      "author" : [ "Shuai Xiao", "Junchi Yan", "Mehrdad Farajtabar", "Le Song", "Xiaokang Yang", "Hongyuan Zha" ],
      "venue" : "arXiv preprint arXiv:1703.08524,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2017
    }, {
      "title" : "On lewis’ simulation method for point processes",
      "author" : [ "Yosihiko Ogata" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1981
    }, {
      "title" : "Towards principled methods for training generative adversarial networks",
      "author" : [ "Martin Arjovsky", "Léon Bottou" ],
      "venue" : "In NIPS 2016 Workshop on Adversarial Training. In review for ICLR,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2017
    }, {
      "title" : "Nips 2016 tutorial: Generative adversarial networks",
      "author" : [ "Ian Goodfellow" ],
      "venue" : "arXiv preprint:1701.00160,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "How (not) to train your generative model: Scheduled sampling, likelihood, adversary",
      "author" : [ "Ferenc Huszár" ],
      "venue" : "arXiv preprint arXiv:1511.05101,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "A note on the evaluation of generative models",
      "author" : [ "Lucas Theis", "Aäron van den Oord", "Matthias Bethge" ],
      "venue" : "arXiv preprint arXiv:1511.01844,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Plug & play generative networks: Conditional iterative generation of images in latent space",
      "author" : [ "Anh Nguyen", "Jason Yosinski", "Yoshua Bengio", "Alexey Dosovitskiy", "Jeff Clune" ],
      "venue" : "arXiv preprint:1612.00005,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2016
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "A new mallows distance based metric for comparing clusterings",
      "author" : [ "Ding Zhou", "Jia Li", "Hongyuan Zha" ],
      "venue" : "In ICML,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2005
    }, {
      "title" : "Improved training of wasserstein gans",
      "author" : [ "Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Vincent Dumoulin", "Aaron Courville" ],
      "venue" : "arXiv preprint arXiv:1704.00028,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2017
    }, {
      "title" : "Optimal transport: old and new, volume 338",
      "author" : [ "Cédric Villani" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2008
    }, {
      "title" : "Soft-DTW: a Differentiable Loss Function for Time-Series",
      "author" : [ "Marco Cuturi", "Mathieu Blondel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2017
    }, {
      "title" : "A new metric between distributions of point processes",
      "author" : [ "Dominic Schuhmacher", "Aihua Xia" ],
      "venue" : "Advances in applied probability,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "Functional poisson approximation in kantorovich–rubinstein distance with applications to u-statistics and stochastic geometry",
      "author" : [ "Laurent Decreusefond", "Matthias Schulte", "Christoph Thäle" ],
      "venue" : "The Annals of Probability,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2016
    }, {
      "title" : "C-rnn-gan: Continuous recurrent neural networks with adversarial training",
      "author" : [ "Olof Mogren" ],
      "venue" : "arXiv preprint arXiv:1611.09904,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "Contextual rnn-gans for abstract reasoning diagram generation",
      "author" : [ "Arnab Ghosh", "Viveka Kulharia", "Amitabha Mukerjee", "Vinay Namboodiri", "Mohit Bansal" ],
      "venue" : "arXiv preprint arXiv:1609.09444,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Temporal point processes [1] is an effective mathematical tool for modeling events data.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 1,
      "context" : "It has been applied to sequences arising from social networks [2, 3, 4], electronic health records [5], ecommerce [6], and finance [7].",
      "startOffset" : 62,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "It has been applied to sequences arising from social networks [2, 3, 4], electronic health records [5], ecommerce [6], and finance [7].",
      "startOffset" : 62,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "It has been applied to sequences arising from social networks [2, 3, 4], electronic health records [5], ecommerce [6], and finance [7].",
      "startOffset" : 62,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "It has been applied to sequences arising from social networks [2, 3, 4], electronic health records [5], ecommerce [6], and finance [7].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "It has been applied to sequences arising from social networks [2, 3, 4], electronic health records [5], ecommerce [6], and finance [7].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : "It has been applied to sequences arising from social networks [2, 3, 4], electronic health records [5], ecommerce [6], and finance [7].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 7,
      "context" : "Moreover, it has a remarkably extensive theoretical foundation [8].",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "The functional form of the intensity is often designed to capture the phenomena of interests [9].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 9,
      "context" : "Some examples are homogeneous and non-homogeneous Poisson processes [10], self-exciting point processes [11], self-correcting point process models [12], and survival processes [8].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : "Some examples are homogeneous and non-homogeneous Poisson processes [10], self-exciting point processes [11], self-correcting point process models [12], and survival processes [8].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "Some examples are homogeneous and non-homogeneous Poisson processes [10], self-exciting point processes [11], self-correcting point process models [12], and survival processes [8].",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 11,
      "context" : "To address the aforementioned problem, the authors in [13, 14, 15] propose to learn a general representation of the underlying dynamics from the event history without assuming a fixed parametric form in advance.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "To address the aforementioned problem, the authors in [13, 14, 15] propose to learn a general representation of the underlying dynamics from the event history without assuming a fixed parametric form in advance.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "To address the aforementioned problem, the authors in [13, 14, 15] propose to learn a general representation of the underlying dynamics from the event history without assuming a fixed parametric form in advance.",
      "startOffset" : 54,
      "endOffset" : 66
    }, {
      "referenceID" : 14,
      "context" : "Attenional mechanism is explored to discover the underlying structure [16].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "On the other hand, sampling sequences from intensity-based models is usually performed via a thinning algorithm [17], which is computationally expensive; many sample events might be rejected because of the rejection step, especially when the intensity exhibits high variation.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 16,
      "context" : "They are asymptotically equivalent to minimizing the Kullback-Leibler (KL) divergence between the data and model distributions, which suffers serious issues such as mode dropping [18, 19].",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 17,
      "context" : "They are asymptotically equivalent to minimizing the Kullback-Leibler (KL) divergence between the data and model distributions, which suffers serious issues such as mode dropping [18, 19].",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 18,
      "context" : "Alternatively, Generative Adversarial Networks (GAN) [20] have proven to be a promising alternative to traditional maximum likelihood approaches [21, 22].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "Alternatively, Generative Adversarial Networks (GAN) [20] have proven to be a promising alternative to traditional maximum likelihood approaches [21, 22].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "Alternatively, Generative Adversarial Networks (GAN) [20] have proven to be a promising alternative to traditional maximum likelihood approaches [21, 22].",
      "startOffset" : 145,
      "endOffset" : 153
    }, {
      "referenceID" : 21,
      "context" : "They achieve state-of-the-art results on a variety of generative tasks such as image generation, image super-resolution, 3D object generation, and video prediction [23, 24].",
      "startOffset" : 164,
      "endOffset" : 172
    }, {
      "referenceID" : 22,
      "context" : "They achieve state-of-the-art results on a variety of generative tasks such as image generation, image super-resolution, 3D object generation, and video prediction [23, 24].",
      "startOffset" : 164,
      "endOffset" : 172
    }, {
      "referenceID" : 18,
      "context" : "The original GAN in [20] minimizes the Jensen-Shannon (JS) and is regarded as highly unstable and prone to miss modes.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : "Furthermore it is shown that the EM objective, as a metric between probability distributions [26] has many advantages as the loss function correlates with the quality of the generated samples and reduces mode dropping [27].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 24,
      "context" : "Furthermore it is shown that the EM objective, as a metric between probability distributions [26] has many advantages as the loss function correlates with the quality of the generated samples and reduces mode dropping [27].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 25,
      "context" : "It can be seen that the optimal solution is attained at extreme points and, by Birkhoff’s theorem, the extreme points of the set of doubly stochastic matrices is a permutation [28].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 26,
      "context" : "More robust distance like local alignment distance and dynamic time warping [29] should be more robust and are great venues for future work.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 27,
      "context" : "Furthermore, Stein’s method to approximate Poisson processes [30, 31] is also relevant as they are defining distances between a Poisson process and an arbitrary point process.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 28,
      "context" : "Furthermore, Stein’s method to approximate Poisson processes [30, 31] is also relevant as they are defining distances between a Poisson process and an arbitrary point process.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "Inspired by GAN [20], this generator takes a noise and turns it into a sample to mimic the real samples.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 29,
      "context" : "Similar to [32, 33] we use Recurrent Neural Networks (RNN) to model the generator.",
      "startOffset" : 11,
      "endOffset" : 19
    }, {
      "referenceID" : 30,
      "context" : "Similar to [32, 33] we use Recurrent Neural Networks (RNN) to model the generator.",
      "startOffset" : 11,
      "endOffset" : 19
    }, {
      "referenceID" : 16,
      "context" : "To enforce the Lipschitz constraints the original WGAN paper [18] adopts weight clipping.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "This is also reported by the same authors in their follow-up paper [27] to the original work.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "Remark The significance of Lipschitz constraint and regularization (or more generally any capacity control) is more apparent when we consider the connection of W-distance and optimal transport problem [28].",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 25,
      "context" : "Basically, minimizing the W-distance between the empirical distribution and the model distribution is equivalent to a semidiscrete optimal transport [28].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 2,
      "context" : "2, where k = 3, α = [3, 7, 11], c = [1, 1, 1], σ = [2, 3, 2].",
      "startOffset" : 20,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "2, where k = 3, α = [3, 7, 11], c = [1, 1, 1], σ = [2, 3, 2].",
      "startOffset" : 20,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "2, where k = 3, α = [3, 7, 11], c = [1, 1, 1], σ = [2, 3, 2].",
      "startOffset" : 20,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "2, where k = 3, α = [3, 7, 11], c = [1, 1, 1], σ = [2, 3, 2].",
      "startOffset" : 36,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "2, where k = 3, α = [3, 7, 11], c = [1, 1, 1], σ = [2, 3, 2].",
      "startOffset" : 36,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "2, where k = 3, α = [3, 7, 11], c = [1, 1, 1], σ = [2, 3, 2].",
      "startOffset" : 36,
      "endOffset" : 45
    }, {
      "referenceID" : 1,
      "context" : "2, where k = 3, α = [3, 7, 11], c = [1, 1, 1], σ = [2, 3, 2].",
      "startOffset" : 51,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "2, where k = 3, α = [3, 7, 11], c = [1, 1, 1], σ = [2, 3, 2].",
      "startOffset" : 51,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "2, where k = 3, α = [3, 7, 11], c = [1, 1, 1], σ = [2, 3, 2].",
      "startOffset" : 51,
      "endOffset" : 60
    } ],
    "year" : 2017,
    "abstractText" : "Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model’s expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.",
    "creator" : null
  }
}