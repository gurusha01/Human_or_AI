{
  "name" : "27ed0fb950b856b06e1273989422e7d3.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deep Dynamic Poisson Factorization Model",
    "authors" : [ "Chengyue Gong" ],
    "emails" : [ "cygong@pku.edu.cn", "huangwb@pku.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "There has been growing interest in analyzing sequentially observed count vectors x1, x2,. . . , xT . Such data appears in many real world applications, such as recommend systems, text analysis, network analysis and time series analysis. Analyzing such data should conquer the computational or statistical challenges, since they are often high-dimensional, sparse, and with complex dependence across the time steps. For example, when analyzing the dynamic word count matrix of research papers, the amount of words used is large and many words appear only few times. Although we know the trend that one topic may encourage researchers to write papers about related topics in the following year, the relationship among each time step and each topic is still hard to analyze completely.\nBayesian factor analysis model has recently reached success in modeling sequentially observed count matrix. They assume the data is Poisson distributed, and model the data under Poisson Factorize Analysis (PFA). PFA factorizes a count matrix, where Φ ∈ RV×K+ is the loading matrix and Θ ∈ RT×K+ is the factor score matrix. The assumption that θt ∼ Gamma(θt−1,βt) is then included [1, 2] to smooth the transition through time. With property of the Gamma-Poisson distribution and Gamma-NB process, inference via MCMC is used in these models. Considering the lack of ability to capture the relationship between factors, a transition matrix is included in Poisson-Gamma Dynamical System (PGDS) [2]. However, these models may still have some shortcomings in exploring the long-time dependence among the time steps, as the independence assumption is made on θt−1 and θt+1 if θt is given. In text analysis problem, temporal Dirichlet process [3] is used to catch the time dependence on each topic using a given decay rate. This method may have weak points in analyzing other data with different pattern long-time dependence, such as fanatical data and disaster data [3].\nDeep models, which are also called hierarchical models in Bayesian learning field, are widely used in Bayesian models to fit the deep relationship between latent variables. Examples of this include the nested Chinese Restaurant Process [4], nest hierarchical Dirichlet process [5], deep Gaussian process [6, 7] and so on. Some models based on neural network structure or recurrent structure is also used, such as the Deep Exponential Families [8], the Deep Poisson Factor Analysis based on RBM or SBN [9, 10], the Neural Autoregressive Density Estimator based on neural networks [11], Deep Poisson\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nFactor Modeling with a recurrent structure based on PFA using a Bernoulli-Poisson link [12], Deep Latent Dirichlet Allocation uses stochastic gradient MCMC [23]. These models capture the deep relationship among the shallow models, and often outperform shallow models.\nIn this paper, we present the Deep Dynamic Poisson Factor Analysis (DDPFA) model. Based on PFA, our model includes recurrent neural networks to represent implicit distributions, in order to learn complicated relationship between different factors among short time. Deep structure is included in order to capture the long-time dependence. An inference algorithm based on variational inference is used for inferring the latent variables. Parameters in the neural networks are learnt according to a loss function based on the variational distributions. Finally, the DDPFA model is used on several synthetic and real-world datasets, and excellent results are obtained in prediction and fitting tasks."
    }, {
      "heading" : "2 Deep Dynamic Poisson Factorization Model",
      "text" : "Assume that V -dimensional sequentially observed count data x1, x2,. . . , xT are represented as a V × T count matrix X, a count data xvt ∈ {0, 1, . . .} is generated by the proposed DDPFA model as follows:\nxvt ∼ Poisson( ∑K k=1 θtkφvkλkψv) (1)\nwhere the latent variables θtk, φvk, λk and ψv are all positive variables. φk represents the strength of the kth component and is treated as factor. θtk represents the strength of the kth component at the tth time step. Feature-wise variable ψv captures the sparsity of the vth feature and λk recognizes the importance of the kth component. According to the regular setting in [2, 13-16], the factorization is regarded as X ∼ Poisson(ΦΘT ). Λ and Ψ can be absorbed into Θ. In this paper, in order to extract the sparsity of vth feature or kth component and impose a feature-wise or temporal smoothness constraint, Ψ and Λ are included in our model. The additive property of the Poisson distribution is used to decompose the observed count of xvt as K latent counts xvtk, k ∈ {0, . . . ,K}. In this way, the model is rewritten as:\nxvt = ∑K k=1 xvtk and xvtk ∼ Poisson(θtkφvkλkψv) (2)\nCapturing the complicated temporal dependence of Θ is the major purpose in this paper. In the previous work, transition via Gamma-Gamma-Poisson distribution structure is used, where θt ∼ Gamma(θt−1,βt) [1]. Non-homogeneous Poisson process over time to model the stochastic transition over different features is exploited in Poisson process models [17-19]. These models are then trained via MCMC or variational inference. However, it is rough for these models to catch complicated time dependence because of the weak points in their shallow structure in time dimension. In order to capture the complex time dependence over Θ, a deep and long-time dependence model with a dynamic structure over time steps is proposed. The first layer over Θ is as follows:\nθt ∼ p(θt|h(0)t−c, ...,h (0) t ) (3)\nwhere c is the size of a window for analysis, and the latent variables in the nth layer, n ≤ N , are indicated as follows:\nh (n) t ∼ p(h (n) t |h (n+1) t−c , ...,h (n+1) t ) and h (N) t ∼ p(h (N) t |h (N) t−c−1, ...,h (N) t−1) (4)\nwhere the implicit probability distribution p(h(n)t |·) is modeled as a recurrent neural network. Probability AutoEncoder with an auxiliary posterior distribution p(h(n)t |h (n−1) t , . . . ,h (n−1) t+c ), also modeled as a neural network, is exploited in our training phase. h(n)t is a K-dimensional latent variable in the nth layer at the tth time step. Specially, in the nth layer, h(N)t is generated from a Gamma distribution with h(N)t−c−1:t−1 as the prior information. This structure is illustrated in Figure 1.\nFinally, prior parameters are placed over other latent variables for Bayesian inference. These variables are generated as: φvk ∼ Gamma(αφ, βφ) andλk ∼ Gamma(αλ, βλ) and ψv ∼ Gamma(αψ, βψ). Although Dirichlet distribution is often used as prior distribution [13, 14, 15] over φvk in previous works, a Gamma distribution is exploited in our model due to the including of feature-wise parameter ψv and the purpose for obtaining feasible factor strength of φk.\nIn real world applications, like recommendation systems, the observed binary count data can be formulated by the proposed DDPFA model with a Bernoulli-Poisson link [1]. The distribution of b given λ is called Bernoulli-Poisson distribution as: b = 1(x > 1), x ∼ Poisson(λ) and the linking distribution is rewritten as: f(b|x, λ) = e−λ(1−b)(1− e−λ)b. The conditional posterior distribution is then (x|b, λ) ∼ b · Poisson+(λ), where Poisson+(λ) is a truncated Poisson distribution, so the MCMC or VI methods can be used to do inference. Non-count real-valued matrix can also be linked to a latent count matrix via a compound Poisson distribution or a Gamma belief network [20]."
    }, {
      "heading" : "3 Inference",
      "text" : "There are many classical inference approaches for Bayesian probabilistic model, such as Monte Carlo methods and variational inference. In the proposed method, variational inference is exploited because the implicit distribution is regarded as prior distribution over Θ. Two stages of inference in our model are adopted: the first stage updates latent variables by the coordinate-ascent method with a fixed implicit distribution, and the parameters in neural networks are learned in the second one.\nMean-field Approximation: In order to obtain mean-field VI, all variables are independent and governed by its own variational distribution. The joint distribution of the variational distribution is written as:\nq(Θ,Φ,Ψ,Λ,H) = ∏ v,t,n,k q(φvk|φ∗vk)q(ψk|ψ∗k)q(θtk|θ∗tk)q(λk|λ∗k)q(h (n) tk |h (n)∗ tk ) (5)\nwhere y∗ represents the prior variational parameter of the variable y. The variational parameters ν are fitted to minimize the KL divergence:\nν = argminν∗KL(p(Θ,Φ,Ψ,Λ,H|X)||q(Θ,Φ,Ψ,Λ,H|ν)) (6)\nThe variational distribution q(·|ν∗) is then used as a proxy for the posterior. The objective actually is equal to maximize the evidence low bound (ELBO) [19]. The optimization can be performed by a coordinate-ascent method or a variational-EM method. As a result, each variational parameter can be optimized iteratively while the remaining parameters of the model are set to fixed value. Due to Eq. 2, the conditional distribution of (xvt1, . . . , xvtk) is a multinomial while its parameter is normalized set of rates [19] and formulated as:\n(xvt1, . . . , xvtk)|θt,φv,λ, ψv ∼Mult(xvt·;θtφvλφv/ ∑ k θtkφvkλkψv) (7)\nGiven the auxiliary variables xvtk, the Poisson factorization model is a conditional conjugate model. The complete conditional of the latent variables is Gamma distribution and shown as:\nφvk|Θ,Λ,Ψ, α, β,X ∼ Gamma(αφ + xv·k, βφ + λkψvθ·k) λk|Θ,Φ,Φ, α, β,X ∼ Gamma(αλ + x··k, βθ + θ·k ∑ v ψvφvk)\nψv|Θ,Λ,Φ, α, β,X ∼ Gamma(αψ + xv··, βψ + ∑ k λkφvkθ·k)\n(8)\nGenerally, these distributions are derived from conjugate properties between Poisson and Gamma distribution. The posterior distribution of θtk described in Eq. 3 can be a Gamma distribution while the prior h(0)t−c:t is given as:\nθtk|Ψ,Λ,Φ,h(0), β,X ∼ Gamma(αθtk + xv·k, βθ + λk ∑ v ψvφvk) (9)\nwhere αθtk is calculated through a recurrent neural network with (h (0) t−c, ...,h (0) t ) as its inputs. Then the posterior distribution of h(0)tk described in Eq. 4 is given as:\nh(0)tk |Θ,h(1), β,X ∼ Gamma(αh(0)tk + γh(0)tk , βh) (10)\nwhere α h (n) tk is the prior information given by the (n+ 1)th layer, γ h (n) tk is the posterior information given by the (n − 1)th layer. Here, the notation h(−1)tk is equal to θtk. αh(n)tk is calculated through a recurrent neural network using (h(n+1)t−c , ...,h (n+1) t ) as its inputs. γh(n)tk is calculated through a recurrent neural network using (h(n−1)t+c , ...,h (n−1) t ) as its inputs. Therefore, the distribution mentioned in Eq. 9 can be regarded as an implicit conditional distribution of θtk given (h (0) t−c, ...,h (0) t ).\nAnd the distribution in Eq. 10 is an implicit distribution of α h (n) tk\ngiven (h(n+1)t−c , ...,h (n+1) t ) and\n(h (n−1) t+c , ...,h (n−1) t ).\nVariational Inference: Mean field variational inference can approximate the latent variables while all parameters of a neural network are given. If the observed data satisfies xvt > 0, the auxiliary variables xvtk can be updated by:\nxvtk ∝ exp{Ψ(θshptk )− logθ rte tk + Ψ(λ shp k )− logλ rte k\n+ Ψ(φshpvk )− logφ rte vk + Ψ(ψ shp v )− logψrtev }\n(11)\nwhere Ψ(·) is the digamma function. Variables with the superscript “shp” indicate the shape parameter of Gamma distribution, and those with the superscript “rte” are the rate parameter of it. This update comes from the expectation of the logarithm of a Gamma variable as 〈logθ〉 = Ψ(θshp)− log(θrte). Here, θ is generated from a Gamma distribution and 〈·〉 represents the expectation of the variable. Calculation of the expectation of the variable, obeyed Gamma distribution, is noted as 〈θ〉 = θshp/θrte. Variables can be updated by mean-field method as:\nφvk ∼ Gamma(αφ + 〈xv·k〉, βφ + 〈λk〉〈ψv〉〈θ·k〉) λk ∼ Gamma(αλ + 〈x··k〉, βθ + 〈θ·k〉 ∑ v 〈ψv〉〈φvk〉)\nψv ∼ Gamma(αψ + 〈xv··〉, βψ + ∑ k 〈λk〉〈φvk〉〈θ·k〉)\n(12)\nThe latent variables in the deep structure can also be updated by mean-field method: θtk ∼ Gamma(αθtk + 〈xv·k〉, βθ + 〈λk〉 ∑ v 〈ψv〉〈φvk〉) (13)\nh(n)tk ∼ Gamma(αh(n)tk + γh(n)tk , βh) (14)\nwhereα h (n) t = ffeed(〈hn+1〉),αh(N)t = ffeed(〈h N t−c−1:t−1〉) and γh(n)t = fback(〈h n−1〉), γ h (N) t = fback(〈hNt+c+1:t+1〉). ffeed(·) is the neural network constructing the prior distribution and fback(·) is the neural network constructing the posterior distribution.\nProbability AutoEncoder: This stage of the inference is to update the parameters of the neural networks. The bottom layer is used by us as an example. Given all latent variables, these parameters can be approximated by p(θt|h(0)t−c, ...,h (0) t ) and p(h (0) t |θt+c, ...,θt). p(θ (n) t |h (0) t−c, ...,h (0) t ) = Gamma(αθt ,βh) is modeled by a RNN with the inputs (h (0) t−c, ...,h (0) t ) and the outputs, αθt . The\np(h (0) t |θt+c, ...,θt) is also modeled as a RNN with the inputs (θt+c, ...,θt) and the outputs ,γh(0)t . With the posterior distribution from Θ to H(0) and the prior distribution from H(0) to Θ, the probability of Θ should be maximized. The loss function of these two neural networks is as follows:\nmax W { ∫ p(Θ|H(0))p(H(0)|Θ)dH(0)} (15)\nwhere W represents the parameters in neural networks. Because the integration in Eq. 15 is intractable, a new loss function should include auxiliary variational variables H(0)′. Assume that H(0)′ is generated by Θ, the optimization can be regarded as maximizing the probability of Θ with minimal difference between H(0)′ and H(0) as max\nW {p(Θ|H(0))} and\nmin W {KL(p(H(0)′|Θ)||p(H(0)|H(1))}\nThen approximating the variables generated from a distribution by its expectation, the loss function, similar to variational AutoEncoder [21], can be simplified to:\nmin W {‖〈p(H(0)′|Θ)〉 − 〈p(H(0)|H(1))〉‖2 + ‖Θ− 〈p(Θ|H(0))〉‖2} (16)\nSince only a few samples are drawn from one certain distrbution, which means sampling all latent variables is high-cost and useless, differentiable variational Bayes is not suitable. As a result, we focus more on fitting data than generating data. In our objective, the first term, a regularization, encourages the data to be reconstructed from the latent variables, and the second term encourages the decoder to fit the data.\nThe parameters in the networks for nth and (n+ 1)th layer are trained by the loss function:\nmin W {‖〈p(H(n+1)′|H(n))〉 − 〈p(H(n)|H(n+1))〉‖2 + ‖H(n) − 〈p(H(n)|H(n+1))〉‖2}\n(17)\nIn order to make the convergence more stable, the term of Θ in the first layer is collapsed intoX by using the fixed latent variables approximated by mean-field VI, and the loss function is as follows:\nmin W {‖〈p(H(0)′|Θ)〉 − 〈p(H(0)|H(1))〉‖2 + ‖X − 〈Ψ〉〈Λ〉〈Φ〉〈p(Θ|H(0))〉‖2} (18)\nAfter the layer-wise training, all the parameters in neural networks are jointly trained by the fine-tuning trick in stacked AutoEncoder [22]."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, four multi-dimensional synthetic datasets and five real-world datasets are exploited to examine the performance of the proposed model. Besides, the results of three existed methods, PGDS, LSTM, and PFA, are compared with results of our model. PGDS is a dynamic Poisson-Gamma system mentioned in Section 1, and LSTM is a classical time sequence model. In order to prove the deep relationship learnt by the deep structure can improve the performance, a simple PFA model is also included as a baseline.\nAll hyperparameters of PGDS set in [2] are used in this paper. 1000 times gibbs sampling iterations for PGDS is performed, 100 iterations used mean-field VI for PFA is performed, and 400 epochs is executed for LSTM. The parameters in the proposed DDPFA model are set as follows:α(λ,φ,ψ) = 1, β(λ,φ,ψ) = 2, α(θ,h) = 1, β(θ,h) = 1. The iterations is set to 100. The stochastic gradient descent for the neural networks is executed 10 epochs in each iteration. The size of the window is 4. Hyperparameters of PFA are set as the same to our model. Data in the last time step is exploited as the predicting target in a prediction task. Mean squared error (MSE) between the ground truth and the estimated value and the predicted mean squared error (PMSE) between the ground truth and the predicted value in next time step are exploited to evaluate the performance of each model."
    }, {
      "heading" : "4.1 Synthetic Datasets",
      "text" : "The multi-dimensional synthetic datasets are obtained by using the following functions where the subscript stands for the index of dimension:\nSDS1:f1(t) = f2(t) = t, f3(t) = f4(t) = t+ 1 on the interval t = [1 : 1 : 6].\nSDS2:f1(t) = t (mod 2), f2(t) = 2t (mod 2) + 2, f3(t) = t on the interval t = [1 : 1 : 20].\nSDS3:f1(t) = f2(t) = t, f3(t) = f4(t) = t + 1, f5(t) = I(4|t) on the interval t = [1 : 1 : 20], where I is an indicator function.\nSDS4:f1(t) = t (mod 2), f2(t) = 2t (mod 2) + 2, f3(t) = t (mod 10) on the interval t = [1 : 1 : 100].\nThe number of factor is set to K = 3, and the number of the layers is 2. Both fitting and predicting tasks are performed in each model. The hidden layer of LSTM is 4 and the size in each layer is 20. In Table 1, it is obviously that DDPFA has the best performance in fitting and prediction task of all the datasets. Note that the complex relationship learnt from the time steps helps the model catch more time patterns according to the results of DDPFA, PGDS and PFA. LSTM performs worse in SDS4 because the noise in the synthetic data and the long time steps make the neural network difficult to memorize enough information."
    }, {
      "heading" : "4.2 Real-world Datasets",
      "text" : "Five real-world datasets are used as follows:\nIntegrated Crisis Early Warning System (ICEWS): ICEWS is an international relations event data set extracted from news corpora used in [2]. We therefore treated undirected pairs of countries i↔ j as features and created a count matrix for the year 2003. The number of events for each pair during each day time step is counted, and all pairs with fewer than twenty-five total events is discarded, leaving T = 365, V = 6197, and 475646 events for the matrix.\nNIPS corpus (NIPS): NIPS corpus contains the text of every NIPS conference paper from the year 1987 to 2003. We created a single count matrix with one column per year. The dataset is downloaded from Gal’s page 1, with T = 17, V = 14036, with 3280697 events for the matrix.\nEbola corpus (EBOLA)2 : EBOLA corpus contains the data for the 2014 Ebola outbreak in West Africa every day from Mar 22th, 2014 to Jan 5th 2015, each column represents the cases or deaths in a West Africa country. After data cleaning, the dataset is with T = 122, V = 16.\nInternational Disaster(ID)3 : The International Disaster dataset contains essential core data on the occurrence and effects of over 22,000 mass disasters in the world from 1900 to the present day. A count matrix with T = 115 and V = 12 is built from the events of disasters occurred in Europe from the year 1902 to 2016, classified according to their disaster types.\nAnnual Sheep Population(ASP)4 : The Annual Sheep Population contains the sheep population in England & Wales from the year 1867 to 1939 yearly. The data matrix is with T = 73, V = 1.\n1http://ai.stanford.edu/gal/data.html 2https://github.com/cmrivers/ebola/blob/master/country_timeseries.csv 3http://www.emdat.be/ 4https://datamarket.com/data/list/?q=provider:tsdl\nWe set K = 3 for ID and ASP datasets, while set K = 10 for the others. The size of the hidden layers of the LSTM is 40. The settings of remainder parameters here are the same as those in the above experiment. The results of the experiment are shown in Table 2.\nTable 2 shows the results of four different models on the five datasets, and the proposed model DDPFA has satisfying performance in most experiments although the DDPFA’s result in ICEWS prediction task is not good enough. While smoothed data obtained from the transition matrix in PGDS performs well in this prediction task. However, In EBOLA and ASP datasets, PGDS fails in catching complicated time dependence. And it is a tough challenge for LSTM network to memorize enough useful patterns while its input data includes long-time patterns or the dimension of the data is particular high.\nAccording to the observation in Figure 2, it can be shown that the factors learnt by our model are not activated locally compared to PGDS. Natrually, in real-world data, it is impossible that only one factor happens in one time step. For example, in the ICEWS dataset, the connection between Israel and Occupied Palestinian Territory still remains strong during the Iraq War or other accidents. Figure 2(a) reveals that several factors at a certain time step are not captured by PGDS. In Figure 3, the changes of two meaningful factors in ICEWS is shown. These two factor, respectively, indicate Israel-Palestinian conflict and six-party talks. The long-time activation of factors is shown in thi figure, since DDPFA model can capture weak strength along time.\nIn Table 3, we show the performance of our model with different sizes. From the table, performance cannot be improved distinctly by adding more layers or adding more variables in upper layer. It is also noticed that expanding the dimension in bottom layer is more useful than in upper layers. The results reveal two problems of proposed DDPFA: \"pruning\" and uselessness of adding network layers.\n[25] notices hierarchical latent variable models do not take advantage of the structure, and gives such a conclusion that only using the bottom latent layer of hierarchical variational autoencoders should be enough. In order to solve this problem, the ladder-like architecture, in which each layer combines independent variables with latent variables depend on the upper layers, is used in our model. It is noticed that using ladder architecture could reach much better results from Table 3. Another problem, \"pruning\", is a phenomenon where the optimizer severs connections between most of the latent variables and the data [24]. In our experiments, it is noticed that some dimenisions in the latent layers only contain data noise. This problem is also found in differentiable variational Bayes and solved by using auxiliary MCMC strcuture [24]. Therefore, we believe this problem is caused by MF-variational inference used in our model and we hope it can be solved if we try other inference methods."
    }, {
      "heading" : "5 Summary",
      "text" : "A new model, called DDPFA, is proposed to obtain long-time and complicated dependence in time series count data. Inference in DDPFA is based on variational method for estimating the latent variables and approximating parameters in neural networks. In order to show the performance of the proposed model, four multi-dimensional synthetic datasets and five real-world datasets, ICEWS, NIPS corpus, EBOLA, International Disaster and Annual Sheep Population, are used, and the performance of three existed methods, PGDS, LSTM, and PFA, are compared. According to our experimental results, DDPFA has better effectivity and interpretability in sequential count analysis."
    } ],
    "references" : [ {
      "title" : "Nonparametric Bayesian Factor Analysis for Dynamic Count Matrices",
      "author" : [ "A. Ayan", "J. Ghosh", "M. Zhou" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Dynamic Non-Parametric Mixture Models and The Recurrent Chinese Restaurant Process",
      "author" : [ "A. Ahmed", "E. Xing" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Deep Gaussian Processes for Regression using Approximate Expectation Propagation",
      "author" : [ "T.D. Bui", "D. Hernándezlobato", "Y. Li" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Deep exponential families",
      "author" : [ "R. Ranganath", "L. Tang", "L. Charlin", "D.M. Blei" ],
      "venue" : "AISTATS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Scalable deep Poisson factor analysis for topic modeling",
      "author" : [ "Z. Gan", "C. Chen", "R. Henao", "D. Carlson", "L. Carin" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Learning deep sigmoid belief networks with data augmentation",
      "author" : [ "Z. Gan", "R. Henao", "D. Carlson", "L. Carin" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "A neural autoregressive topic model",
      "author" : [ "H. Larochelle", "S. Lauly" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Augment-and-conquer negative binomial processes",
      "author" : [ "M. Zhou", "L. Carin" ],
      "venue" : "NIPS, pages 2546–2554,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Beta-negative binomial process and Poisson factor analysis",
      "author" : [ "M. Zhou", "L. Hannah", "D. Dunson", "L. Carin" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Negative binomial process count and mixture modeling",
      "author" : [ "M. Zhou", "L. Carin" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Recurrent Poisson Factorization for Temporal Recommendation",
      "author" : [ "S.A. Hosseini", "K. Alizadeh", "A. Khodadadi" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2017
    }, {
      "title" : "Scalable Recommendation with Hierarchical Poisson Factorization",
      "author" : [ "P. Gopalan", "J.M. Hofman", "D.M. Blei" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Augmentable gamma belief networks",
      "author" : [ "M. Zhou", "Y. Cong", "B. Chen" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2016
    }, {
      "title" : "Auto-encoding variational Bayes",
      "author" : [ "D.P. Kingma", "W. Max" ],
      "venue" : "ICLR,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Greedy layer-wise training of deep networks",
      "author" : [ "Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2006
    }, {
      "title" : "Learning Hierarchical Features from Generative Models",
      "author" : [ "S. Zhao", "J. Song", "S. Ermon" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2017
    }, {
      "title" : "Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo",
      "author" : [ "M. Hoffman" ],
      "venue" : "ICML, 2017",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The assumption that θt ∼ Gamma(θt−1,βt) is then included [1, 2] to smooth the transition through time.",
      "startOffset" : 57,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "In text analysis problem, temporal Dirichlet process [3] is used to catch the time dependence on each topic using a given decay rate.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "This method may have weak points in analyzing other data with different pattern long-time dependence, such as fanatical data and disaster data [3].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 2,
      "context" : "Examples of this include the nested Chinese Restaurant Process [4], nest hierarchical Dirichlet process [5], deep Gaussian process [6, 7] and so on.",
      "startOffset" : 131,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "Some models based on neural network structure or recurrent structure is also used, such as the Deep Exponential Families [8], the Deep Poisson Factor Analysis based on RBM or SBN [9, 10], the Neural Autoregressive Density Estimator based on neural networks [11], Deep Poisson",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "Some models based on neural network structure or recurrent structure is also used, such as the Deep Exponential Families [8], the Deep Poisson Factor Analysis based on RBM or SBN [9, 10], the Neural Autoregressive Density Estimator based on neural networks [11], Deep Poisson",
      "startOffset" : 179,
      "endOffset" : 186
    }, {
      "referenceID" : 5,
      "context" : "Some models based on neural network structure or recurrent structure is also used, such as the Deep Exponential Families [8], the Deep Poisson Factor Analysis based on RBM or SBN [9, 10], the Neural Autoregressive Density Estimator based on neural networks [11], Deep Poisson",
      "startOffset" : 179,
      "endOffset" : 186
    }, {
      "referenceID" : 6,
      "context" : "Some models based on neural network structure or recurrent structure is also used, such as the Deep Exponential Families [8], the Deep Poisson Factor Analysis based on RBM or SBN [9, 10], the Neural Autoregressive Density Estimator based on neural networks [11], Deep Poisson",
      "startOffset" : 257,
      "endOffset" : 261
    }, {
      "referenceID" : 7,
      "context" : "According to the regular setting in [2, 13-16], the factorization is regarded as X ∼ Poisson(ΦΘ ).",
      "startOffset" : 36,
      "endOffset" : 46
    }, {
      "referenceID" : 8,
      "context" : "According to the regular setting in [2, 13-16], the factorization is regarded as X ∼ Poisson(ΦΘ ).",
      "startOffset" : 36,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "According to the regular setting in [2, 13-16], the factorization is regarded as X ∼ Poisson(ΦΘ ).",
      "startOffset" : 36,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "In the previous work, transition via Gamma-Gamma-Poisson distribution structure is used, where θt ∼ Gamma(θt−1,βt) [1].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "Non-homogeneous Poisson process over time to model the stochastic transition over different features is exploited in Poisson process models [17-19].",
      "startOffset" : 140,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "Non-homogeneous Poisson process over time to model the stochastic transition over different features is exploited in Poisson process models [17-19].",
      "startOffset" : 140,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "Although Dirichlet distribution is often used as prior distribution [13, 14, 15] over φvk in previous works, a Gamma distribution is exploited in our model due to the including of feature-wise parameter ψv and the purpose for obtaining feasible factor strength of φk.",
      "startOffset" : 68,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "Although Dirichlet distribution is often used as prior distribution [13, 14, 15] over φvk in previous works, a Gamma distribution is exploited in our model due to the including of feature-wise parameter ψv and the purpose for obtaining feasible factor strength of φk.",
      "startOffset" : 68,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "Although Dirichlet distribution is often used as prior distribution [13, 14, 15] over φvk in previous works, a Gamma distribution is exploited in our model due to the including of feature-wise parameter ψv and the purpose for obtaining feasible factor strength of φk.",
      "startOffset" : 68,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "In real world applications, like recommendation systems, the observed binary count data can be formulated by the proposed DDPFA model with a Bernoulli-Poisson link [1].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "Non-count real-valued matrix can also be linked to a latent count matrix via a compound Poisson distribution or a Gamma belief network [20].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 13,
      "context" : "Then approximating the variables generated from a distribution by its expectation, the loss function, similar to variational AutoEncoder [21], can be simplified to:",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 14,
      "context" : "After the layer-wise training, all the parameters in neural networks are jointly trained by the fine-tuning trick in stacked AutoEncoder [22].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "[25] notices hierarchical latent variable models do not take advantage of the structure, and gives such a conclusion that only using the bottom latent layer of hierarchical variational autoencoders should be enough.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Another problem, \"pruning\", is a phenomenon where the optimizer severs connections between most of the latent variables and the data [24].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : "This problem is also found in differentiable variational Bayes and solved by using auxiliary MCMC strcuture [24].",
      "startOffset" : 108,
      "endOffset" : 112
    } ],
    "year" : 2017,
    "abstractText" : "A new model, named as deep dynamic poisson factorization model, is proposed in this paper for analyzing sequential count vectors. The model based on the Poisson Factor Analysis method captures dependence among time steps by neural networks, representing the implicit distributions. Local complicated relationship is obtained from local implicit distribution, and deep latent structure is exploited to get the long-time dependence. Variational inference on latent variables and gradient descent based on the loss functions derived from variational distribution is performed in our inference. Synthetic datasets and real-world datasets are applied to the proposed model and our results show good predicting and fitting performance with interpretable latent structure.",
    "creator" : null
  }
}