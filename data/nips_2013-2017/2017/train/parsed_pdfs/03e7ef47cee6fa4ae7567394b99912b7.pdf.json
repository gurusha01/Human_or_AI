{
  "name" : "03e7ef47cee6fa4ae7567394b99912b7.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Query Complexity of Clustering with Side Information",
    "authors" : [ "Arya Mazumdar", "Barna Saha" ],
    "emails" : [ "arya@cs.umass.edu", "barna@cs.umass.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "2 logn H2(f+‖f−) ) whereH 2 denotes the squared Hellinger divergence. Moreover, this is also information-theoretic optimal within anO(log n) factor. Our algorithms are all efficient, and parameter free, i.e., they work without any knowledge of k, f+ and f−, and only depend logarithmically with n. Our lower bounds could be of independent interest, and provide a general framework for proving lower bounds for classification problems in the interactive setting. Along the way, our work also reveals intriguing connection to popular community detection models such as the stochastic block model and opens up many avenues for interesting future research."
    }, {
      "heading" : "1 Introduction",
      "text" : "Clustering is one of the most fundamental and popular methods for data classification. In this paper we provide a rigorous theoretical study of clustering with the help of an oracle, a model that saw a recent surge of popular heuristic algorithms.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nSuppose we are given a set of n points, that need to be clustered into k clusters where k is unknown to us. Suppose there is an oracle that either knows the true underlying clustering or can compute the best clustering under some optimization constraints. We are allowed to query the oracle whether any two points belong to the same cluster or not. How many such queries are needed to be asked at minimum to perform the clustering exactly? The motivation to this problem lies at the heart of modern machine learning applications where the goal is to facilitate more accurate learning from less data by interactively asking for labeled data, e.g., active learning and crowdsourcing. Specifically, automated clustering algorithms that rely just on a similarity matrix often return inaccurate results. Whereas, obtaining few labeled data adaptively can help in significantly improving its accuracy. Coupled with this observation, clustering with an oracle has generated tremendous interest in the last few years with increasing number of heuristics developed for this purpose [22, 40, 13, 42, 43, 18, 39, 12, 21, 29]. The number of queries is a natural measure of “efficiency” here, as it directly relates to the amount of labeled data or the cost of using crowd workers –however, theoretical guarantees on query complexity is lacking in the literature.\nOn the theoretical side, query complexity or the decision tree complexity is a classical model of computation that has been extensively studied for different problems [16, 4, 8]. For the clustering problem, one can obtain an upper bound of O(nk) on the query complexity easily and it is achievable even when k is unknown [40, 13]: to cluster an element at any stage of the algorithm, ask one query per existing cluster with this element (this is sufficient due to transitivity), and start a new cluster if all queries are negative. It turns out that Ω(nk) is also a lower bound, even for randomized algorithms (see, e.g., [13]). In contrast, the heuristics developed in practice often ask significantly less queries than nk. What could be a possible reason for this deviation between the theory and practice?\nBefore delving into this question, let us look at a motivating application that drives this work.\nA Motivating Application: Entity Resolution. Entity resolution (ER, also known as record linkage) is a fundamental problem in data mining and has been studied since 1969 [17]. The goal of ER is to identify and link/group different manifestations of the same real world object, e.g., different ways of addressing (names, email address, Facebook accounts) the same person, Web pages with different descriptions of the same business, different photos of the same object etc. (see the excellent survey by Getoor and Machanavajjhala [20]). However, lack of an ideal similarity function to compare objects makes ER an extremely challenging task. For example, DBLP, the popular computer science bibliography dataset is filled with ER errors [30]. It is common for DBLP to merge publication records of different persons if they share similar attributes (e.g. same name), or split the publication record of a single person due to slight difference in representation (e.g. Marcus Weldon vs Marcus K. Weldon). In recent years, a popular trend to improve ER accuracy has been to incorporate human wisdom. The works of [42, 43, 40] (and many subsequent works) use a computer-generated similarity matrix to come up with a collection of pair-wise questions that are asked interactively to a crowd. The goal is to minimize the number of queries to the crowd while maximizing the accuracy. This is analogous to our interactive clustering framework. But intriguingly, as shown by extensive experiments on various real datasets, these heuristics use far less queries than nk [42, 43, 40]–barring the Ω(nk) theoretical lower bound. On a close scrutiny, we find that all of these heuristics use some computer generated similarity matrix to guide in selecting the queries. Could these similarity matrices, aka side information, be the reason behind the deviation and significant reduction in query complexity?\nLet us call this clustering using side information, where the clustering algorithm has access to a similarity matrix. This can be generated directly from the raw data (e.g., by applying Jaccard similarity on the attributes), or using a crude classifier which is trained on a very small set of labelled samples. Let us assume the following generative model of side information: a noisy weighted upper-triangular similarity matrix W = {wi,j}, 1 ≤ i < j ≤ n, where wi,j is drawn from a probability distribution f+ if i, j, i 6= j, belong to the same cluster, and else from f−. However, the algorithm designer is given only the similarity matrix without any information on f+ and f−. In this work, one of our major contributions is to show the separation in query complexity of clustering with and without such side information. Indeed the recent works of [18, 33] analyze popular heuristic algorithms of [40, 43] where the probability distributions are obtained from real datasets which show that these heuristics are significantly suboptimal even for very simple distributions. To the best of our knowledge, before this work, there existed no algorithm that works for arbitrary unknown distributions f+ and f− with near-optimal performances. We develop a generic framework for proving information theoretic lower bounds for interactive clustering using side information, and design efficient algorithms for arbitrary\nf+ and f− that nearly match the lower bound. Moreover, our algorithms are parameter free, that is they work without any knowledge of f+, f− or k.\nConnection to popular community detection models. The model of side information considered in this paper is a direct and significant generalization of the planted partition model, also known as the stochastic block model (SBM) [28, 15, 14, 2, 1, 25, 24, 11, 36]. The stochastic block model is an extremely well-studied model of random graphs which is used for modeling communities in real world, and is a special case of a similarity matrix we consider. In SBM, two vertices within the same community share an edge with probability p, and two vertices in different communities share an edge with probability q, that is f+ is Bernoulli(p) and f− is Bernoulli(q). It is often assumed that k, the number of communities, is a constant (e.g. k = 2 is known as the planted bisection model and is studied extensively [1, 36, 15] or a slowly growing function of n (e.g. k = o(log n)). The points are assigned to clusters according to a probability distribution indicating the relative sizes of the clusters. In contrast, not only in our model f+ and f− can be arbitrary probability mass functions (pmfs), we do not have to make any assumption on k or the cluster size distribution, and can allow for any partitioning of the set of elements (i.e., adversarial setting). Moreover, f+ and f− are unknown. For SBM, parameter free algorithms are known relatively recently for constant number of linear sized clusters [3, 24].\nThere are extensive literature that characterize the threshold phenomenon in SBM in terms of p and q for exact and approximate recovery of clusters when relative cluster sizes are known and nearly balanced (e.g., see [2] and therein for many references). For k = 2 and equal sized clusters, sharp thresholds are derived in [1, 36] for a specific sparse region of p and q 1. In a more general setting, the vertices in the ith and the jth communities are connected with probability qij and threshold results for the sparse region has been derived in [2] - our model can be allowed to have this as a special case when we have pmfs fi,js denoting the distributions of the corresponding random variables. If an oracle gives us some of the pairwise binary relations between elements (whether they belong to the same cluster or not), the threshold of SBM must also change. But by what amount? This connection to SBM could be of independent interest to study query complexity of interactive clustering with side information, and our work opens up many possibilities for future direction.\nDeveloping lower bounds in the interactive setting appears to be significantly challenging, as algorithms may choose to get any deterministic information adaptively by querying, and standard lower bounding techniques based on Fano-type inequalities [9, 31] do not apply. One of our major contributions in this paper is to provide a general framework for proving information-theoretic lower bound for interactive clustering algorithms which holds even for randomized algorithms, and even with the full knowledge of f+, f− and k. In contrast, our algorithms are computationally efficient and are parameter free (works without knowing f+, f− and k). The technique that we introduce for our upper bounds could be useful for designing further parameter free algorithms which are extremely important in practice.\nOther Related works. The interactive framework of clustering model has been studied before where the oracle is given the entire clustering and the oracle can answer whether a cluster needs to be split or two clusters must be merged [7, 6]. Here we contain our attention to pair-wise queries, as in all practical applications that motivate this work [42, 43, 22, 40]. In most cases, an expert human or crowd serves as an oracle. Due to the scale of the data, it is often not possible for such an oracle to answer queries on large number of input data. Only recently, some heuristic algorithms with k-wise queries for small values of k but k > 2 have been proposed in [39], and a non-interactive algorithm that selects random triangle queries have been analyzed in [41]. Also recently, the stochastic block model with active label-queries have been studied in [19]. Perhaps conceptually closest to us is a recent work by [5] where they consider pair-wise queries for clustering. However, their setting is very different. They consider the specific NP-hard k-means objective with distance matrix which must be a metric and must satisfy a deterministic separation property. Their lower bounds are computational and not information theoretic; moreover their algorithm must know the parameters. There exists a significant gap between their lower and upper bounds:∼ log k vs k2, and it would be interesting if our techniques can be applied to improve this.\nHere we have assumed the oracle always returns the correct answer. To deal with the possibility that the crowdsourced oracle may give wrong answers, there are simple majority voting mechanisms or more complicated techniques [39, 12, 21, 29, 10, 41] to handle such errors. Our main objective is to\n1Most recent works consider the region of interest as p = a logn n and q = b logn n for some a > b > 0.\nstudy the power of side information, and we do not consider the more complex scenarios of handling erroneous oracle answers. The related problem of clustering with noisy queries is studied by us in a companion work [34]. Most of the results of the two papers are available online in a more extensive version [32].\nContributions. Formally the problem we study in this paper can be described as follows. Problem 1 (Query-Cluster with an Oracle). Consider a set of elements V ≡ [n] with k latent clusters Vi, i = 1, . . . , k, where k is unknown. There is an oracle O : V × V → {±1}, that when queried with a pair of elements u, v ∈ V × V , returns +1 iff u and v belong to the same cluster, and −1 iff u and v belong to different clusters. The queries Q ⊆ V × V can be done adaptively. Consider the side information W = {wu,v : 1 ≤ u < v ≤ n}, where the (u, v)th entry of W , wu,v is a random variable drawn from a discrete probability distribution f+ if u, v belong to the same cluster, and is drawn from a discrete2 probability distribution f−3 if u, v belong to different clusters. The parameters k, f+ and f− are unknown. Given V and W , find Q ⊆ V × V such that |Q| is minimum, and from the oracle answers and W it is possible to recover Vi, i = 1, 2, ..., k.\nWithout side information, as noted earlier, it is easy to see an algorithm with query complexity O(nk) for Query-Cluster. When no side information is available, it is also not difficult to have a lower bound of Ω(nk) on the query complexity. Our main contributions are to develop strong information theoretic lower bounds as well as nearly matching upper bounds when side information is available, and characterize the effect of side information on query complexity precisely.\nUpper Bound (Algorithms). We show that with side information W , a drastic reduction in query complexity of clustering is possible, even with unknown parameters f+, f−, and k. We propose a Monte Carlo randomized algorithm that reduces the number of queries from O(nk) toO( k\n2 logn H2(f+‖f−) ),\nwhereH(f‖g) is the Hellinger divergence between the probability distributions f , and g, and recovers the clusters accurately with high probability (with success probability 1− 1n ) without knowing f+, f− or k (see, Theorem 1). Depending on the value of k, this could be highly sublinear in n. Note that the squared Hellinger divergence between two pmfs f and g is defined to be,\nH2(f‖g) = 1 2 ∑ i (√ f(i)− √ g(i) )2 .\nWe also develop a Las Vegas algorithm, that is one which recovers the clusters with probability 1 (and not just with high probability), with query complexity O(n log n + k\n2 logn H2(f+‖f−) ). Since f+ and f−\ncan be arbitrary, not knowing the distributions provides a major challenge, and we believe, our recipe could be fruitful for designing further parameter-free algorithms. We note that all our algorithms are computationally efficient - in fact, the time required is bounded by the size of the side information matrix, i.e., O(n2). Theorem 1. Let the number of clusters k be unknown and f+ and f− be unknown discrete distributions with fixed cardinality of support. There exists an efficient (polynomial-time) Monte Carlo algorithm for Query-Cluster that has query complexity O(min (nk, k\n2 logn H2(f+‖f−) )) and recovers all\nthe clusters accurately with probability 1 − o( 1n ). Moreover there exists an efficient Las Vegas algorithm that with probability 1− o( 1n ) has query complexity O(n log n+ min (nk, k2 logn H2(f+‖f−) )).\nLower Bound. Our main lower bound result is information theoretic, and can be summarized in the following theorem. Note especially that, for lower bound we can assume the knowledge of k, f+, f− in contrast to upper bounds, which makes the results stronger. In addition, f+ and f− can be discrete or continuous distributions. Note that whenH2(f+‖f−) is close to 1, e.g., when the side information is perfect, no queries are required. However, that is not the case in practice, and we are interested in the region where f+ and f− are “close”, that isH2(f+‖f−) is small. Theorem 2. AssumeH2(f+‖f−) ≤ 118 . Any (possibly randomized) algorithm with the knowledge of f+, f−, and the number of clusters k, that does not perform Ω ( min {nk, k 2 H2(f+‖f−)} ) expected\n2Our lower bound holds for continuous distributions as well. 3For simplicity of expression, we treat the sample space to be of constant size. However, all our results\nextend to any finite sample space scaling linearly with its size.\nnumber of queries, will be unable to return the correct clustering with probability at least 16 − O( 1√\nk ). And to recover the clusters with probability 1, the number of queries must be Ω\n( n +\nmin {nk, k 2 H2(f+‖f−)} ) .\nThe lower bound therefore matches the query complexity upper bound within a logarithmic factor.\nNote that when no querying is allowed, this turns out exactly to be the setting of stochastic block model though with much general distributions. We have analyzed this case in Appendix C. To see how the probability of error must scale, we have used a generalized version of Fano’s inequality (e.g., [23]). However, when the number of queries is greater than zero, plus when queries can be adaptive, any such standard technique fails. Hence, significant effort has to be put forth to construct a setting where information theoretic minimax bounds can be applied. This lower bound could be of independent interest, and provides a general framework for deriving lower bounds for fundamental problems of classification, hypothesis testing, distribution testing etc. in the interactive learning setting. They may also lead to new lower bound proving techniques in the related multi-round communication complexity model where information again gets revealed adaptively.\nOrganization. The proof of the lower bound is provided in Section 2. The Monte Carlo algorithm is given in Section 3. The detailed proof of the Monte Carlo algorithm, and the Las Vegas algorithm and its proof are given in Appendix A and Appendix B respectively in the supplementary material for space constraint."
    }, {
      "heading" : "2 Lower Bound (Proof of Theorem 2)",
      "text" : "In this section, we develop our information theoretic lower bounds. We prove a more general result from which Theorem 2 follows. Lemma 1. Consider the case when we have k equally sized clusters of size a each (that is total number of elements is n = ka). Suppose we are allowed to make at most Q adaptive queries to the oracle. The probability of error for any algorithm for Query-Cluster is at least,\n1− 2 k\n( 1 + √ 4Q\nak )2 − 4Q ak(k − 1) − 2 √ aH(f+‖f−).\nThe main high-level technique to prove Lemma 1 is the following. Suppose, a node is to be assigned to a cluster. This situation is obviously akin to a k-hypothesis testing problem, and we want to use a lower bound on the probability of error. The side information and the query answers constitute a random vector whose distributions (among the k possible) must be far apart for us to successfully identify the clustering. But the main challenge comes from the interactive nature of the algorithm since it reveals deterministic information and into characterizing the set of elements that are not queried much by the algorithm.\nProof of Lemma 1. Since the total number of queries is Q, the average number of queries per element is at most 2Qak . Therefore there exist at least ak 2 elements that are queried at most T < 4Q ak times. Let x be one such element. We just consider the problem of assignment of x to a cluster (all other elements have been correctly assigned already), and show that any algorithm will make wrong assignment with positive probability.\nStep 1: Setting up the hypotheses. Note that the side information matrix W = (wi,j) is provided where the wi,js are independent random variables. Now assume the scenario when we use an algorithm ALG to assign x to one of the k clusters, Vu, u = 1, . . . , k. Therefore, given x, ALG takes as input the random variables wi,xs where i ∈ ttVt, makes some queries involving x and outputs a cluster index, which is an assignment for x. Based on the observations wi,xs, the task of ALG is thus a multi-hypothesis testing among k hypotheses. Let Hu, u = 1, . . . k denote the k different hypotheses Hu : x ∈ Vu. And let Pu, u = 1, . . . k denote the joint probability distributions of the random matrix W when x ∈ Vu. In short, for any event A, Pu(A) = Pr(A|Hu). Going forward, the subscript of probabilities or expectations will denote the appropriate conditional distribution.\nStep 2: Finding “weak” clusters. There must exist t ∈ {1, . . . , k} such that, k∑ v=1 Pt{ a query made by ALG involving cluster Vv} ≤ Et{Number of queries made by ALG} ≤ T.\nWe now find a subset of clusters, that are “weak,” i.e., not queried enough if Ht were true. Consider the set J ′ ≡ {v ∈ {1, . . . , k} : Pt{ a query made by ALG involving cluster Vv} < 2Tk(1−β)}, where β ≡ 1\n1+ √\n4Q ak\n. We must have, (k − |J ′|) · 2Tk(1−β) ≤ T, which implies, |J ′| ≥ (1+β)k2 .\nNow, to output a cluster without using the side information, ALG has to either make a query to the actual cluster the element is from, or query at least k − 1 times. In any other case, ALG must use the side information (in addition to using queries) to output a cluster. Let Eu denote the event that ALG outputs cluster Vu by using the side information. Let J ′′ ≡ {u ∈ {1, . . . , k} : Pt(Eu) ≤ 2βk}. Since ∑k u=1 Pt(Eu) ≤ 1, we must have, (k − |J ′′|) · 2 βk < 1, or |J ′′| > k − βk2 = (2−β)k 2 . We have, |J ′ ∩ J ′′| > (1+β)k2 + (2−β)k 2 − k = k 2 . This means, {Vu : u ∈ J\n′ ∩ J ′′} contains more than ak2 elements. Since there are ak 2 elements that are queried at most T times, these two sets must have nonzero intersection. Hence, we can assume that x ∈ V` for some ` ∈ J ′ ∩ J ′′, i.e., let H` be the true hypothesis. Now we characterize the error events of the algorithm ALG in assignment of x.\nStep 3: Characterizing error events for “x”. We now consider the following two events. E1 = {a query made by ALG involving cluster V`}; E2 = {k − 1 or more queries were made by ALG}. Note that if the algorithm ALG can correctly assign x to a cluster without using the side information then either of E1 or E2 must have to happen. Recall, E` denotes the event that ALG outputs cluster V` using the side information. Now consider the event E ≡ E` ⋃ E1 ⋃ E2. The probability of correct assignment is at most P`(E). We now bound this probability of correct recovery from above. Step 4: Bounding probability of correct recovery via Hellinger distance. We have,\nP`(E) ≤ Pt(E) + |P`(E)− Pt(E)| ≤ Pt(E) + ‖P` − Pt‖TV ≤ Pt(E) + √\n2H(P`‖Pt), where, ‖P − Q‖TV ≡ supA |P (A) − Q(A)| denotes the total variation distance between two probability distributions P and Q and in the last step we have used the relationship between total variation distance and the Hellinger divergence (see, for example, [38, Eq. (3)]). Now, recall that P` and Pt are the joint distributions of the independent random variables wi,x, i ∈ ∪uVu. Now, we use the fact that squared Hellinger divergence between product distribution of independent random variables are less than the sum of the squared Hellinger divergence between the individual distribution. We also note that the divergence between identical random variables are 0. We obtain√\n2H2(P`‖Pt) ≤ √ 2 · 2aH2(f+‖f−) = 2 √ aH(f+‖f−).\nThis is true because the only times when wi,x differs under Pt and under P` is when x ∈ Vt or x ∈ V`. As a result we have, P`(E) ≤ Pt(E) + 2 √ aH(f+‖f−). Now, using Markov inequality Pt(E2) ≤ Tk−1 ≤ 4Q ak(k−1) . Therefore,\nPt(E) ≤ Pt(E`) + Pt(E1) + Pt(E2) ≤ 2\nβk +\n8Q\nak2(1− β) +\n4Q\nak(k − 1) . Therefore, putting the value of β we get, P`(E) ≤ 2k ( 1 + √ 4Q ak )2 + 4Qak(k−1) + 2 √ aH(f+‖f−), which proves the lemma.\nProof of Theorem 2. Consider two cases. In the first case, suppose, nk < k 2\n9H2(f+‖f−) . Now consider the situation of Lemma 1, with a = nk . The probability of error of any algorithm must be at least,\n1− 2k ( 1 + √ 4Q ak )2 − 4Qak(k−1) − 2 3 ≥ 1 6 −O( 1√ k ), if the number of queries Q ≤ nk72 .\nIn the second case, suppose nk ≥ k 2\n9H2(f+‖f−) . Assume, a = b 1 9H2(f+‖f−)c. Then a ≥ 2, since H2(f+‖f−) ≤ 118 . We have nk ≥ k\n2a. Consider the situation when we are already given a complete cluster Vk with n− (k − 1)a elements, remaining (k − 1) clusters each has 1 element, and the rest (a− 1)(k − 1) elements are evenly distributed (but yet to be assigned) to the k − 1 clusters. Now we are exactly in the situation of Lemma 1 with k − 1 playing the role of k. If we have Q < ak 2\n72 , The probability of error is at least 1− ok(1)− 16 − 2 3 = 1 6 −O( 1√ k ). Therefore Q must be Ω( k 2\nH2(f+‖f−) ). Note that in this proof we have not in particular tried to optimize the constants.\nIf we want to recover the clusters with probability 1, then Ω(n) is a trivial lower bound. Hence, coupled with the above we get a lower bound of Ω(n+ min {nk, k 2\nH2(f+‖f−)}) in that case."
    }, {
      "heading" : "3 Algorithms",
      "text" : "We propose two algorithms (Monte Carlo and Las Vegas) both of which are completely parameter free that is they work without any knowledge of k, f+ and f−, and meet the respective lower bounds within an O(log n) factor. Here we present the Monte Carlo algorithm which drastically reduces the number of queries from O(nk) (no side information) to O( k\n2 logn H2(f+‖f−) ) and recovers the clusters\nexactly with probability at least 1−on(1). The detailed proof of it, as well as the Las Vegas algorithm are presented in Appendix A and Appendix B respectively in the supplementary material.\nOur algorithm uses a subroutine called Membership that takes as input an element v ∈ V and a subset of elements C ⊆ V \\ {v}. Assume that f+, f− are discrete distributions over fixed set of q points a1, a2, . . . , aq; that is wi,j takes value in the set {a1, a2, . . . , aq}. Define the empirical “inter” distribution pv,C for i = 1, . . . , q, pv,C(i) = |{u∈C:wu,v=ai}| |C| Also compute the “intra” distribution pC for i = 1, . . . , q, pC(i) = |{(u,v)∈C×C:u 6=v,wu,v=ai}|\n|C|(|C|−1) . Then we use Membership(v, C) = −H2(pv,C‖pC) as affinity of vertex v to C, where H(pv,C‖pC) denotes the Hellinger divergence between distributions. Note that since the membership is always negative, a higher membership implies that the ‘inter’ and ‘intra’ distributions are closer in terms of Hellinger distance.\nDesigning a parameter free Monte Carlo algorithm seems to be highly challenging as here, the number of queries depends only logarithmically with n. Intuitively, if an element v has the highest membership in some cluster C, then v should be queried with C first. Also an estimation from side information is reliable when the cluster already has enough members. Unfortunately, we know neither whether the current cluster size is reliable, nor we are allowed to make even one query per element.\nTo overcome this bottleneck, we propose an iterative-update algorithm which we believe will find more uses in developing parameter free algorithms. We start by querying a few points so that there is at least one cluster with Θ(log n) points. Now based on these queried memberships, we learn two empirical distributions p1+ from intra-cluster similarity values, and p 1 − from inter-cluster similarity values. Given an element v which has not been clustered yet, and a cluster C with the highest number of current members, we would like to consider the submatrix of side information pertaining to v and all u ∈ C and determine whether that side information is generated from f+ or f−. We know if the statistical distance between f+ and f− is small, then we would need more members in C to successfully do this test. Since we do not know f+ and f−, we compute the squared Hellinger divergence between p1+ and p 1 −, and use that to compute a threshold τ1 on the size of C. If C crosses this size threshold, we just use the side information to determine if v should belong to C. Otherwise, we query further until there is one cluster with size τ1, and re-estimate the empirical distributions p2+ and p2−. Again, we recompute a threshold τ2, and stop if the cluster under consideration crosses this new threshold. If not we continue. Interestingly, we can show when the process converges, we have a very good estimate ofH(f+‖f−) and, moreover it converges fast. Algorithm. Phase 1. Initialization. We initialize the algorithm by selecting any element v and creating a singleton cluster {v}. We then keep selecting new elements randomly and uniformly that have not yet been clustered, and query the oracle with it by choosing exactly one element from each of the clusters formed so far. If the oracle returns +1 to any of these queries then we include the element in the corresponding cluster, else we create a new singleton cluster with it. We continue this process until one cluster has grown to a size of dC log ne, where C is a constant. Phase 2. Iterative Update. Let C1, C2, ...Clx be the set of clusters formed after the xth iteration for some lx ≤ k, where we consider Phase 1 as the 0-th iteration. We estimate\np+,x = |{u, v ∈ Ci : u 6= v, wu,v = ai}|∑lx\ni=1 |Ci|(|Ci − 1|) ; p−,x = |{u ∈ Ci, v ∈ Cj , i < j, i, j ∈ [1, lx] : wu,v = ai}|∑lx i=1 ∑ i<j |Ci||Cj |\nDefine MEx = C logn H(p+,x‖p−,x)2 . If there is no cluster of size at least M E x formed so far, we select a new element yet to be clustered and query it exactly once with the existing clusters (that is by selecting one arbitrary point from every cluster and querying the oracle with it and the new element), and include it in an existing cluster or create a new cluster with it based on the query answer. We then set x = x+ 1 and move to the next iteration to get updated estimates of p+,x, p−,x,MEx and lx.\nElse if there is a cluster of size at least MEx , we stop and move to the next phase.\nPhase 3. Processing the grown clusters. Once Phase 2 has converged, let p+, p−,H(p+‖p−),ME and l be the final estimates. For every cluster C of size |C| ≥ME , call it grown and do the following.\n(3A.) For every unclustered element v, if Membership(v, C) ≥ −( 4H(p+‖p−)C − 2H(p+‖p−)2 C √ logn\n), then we include v in C without querying.\n(3B.) We create a new list Waiting(C), initially empty. If −( 4H(p+‖p−)C − 2H(p+‖p−)2 C √ logn ) > Membership(v, C) ≥ −( 4H(p+‖p−)C + 2H(p+‖p−)2 C √ logn\n), then we include v in Waiting(C). For every element in Waiting(C), we query the oracle with it by choosing exactly one element from each of the clusters formed so far starting with C. If oracle returns answer “yes” to any of these queries then we include the element in that cluster, else we create a new singleton cluster with it. We continue this until Waiting(C) is exhausted. We then call C completely grown, remove it from further consideration, and move to the next grown cluster. if there is no other grown cluster, then we move back to Phase 2.\nAnalysis. The main steps of the analysis are as follows (for full analysis see Appendix A).\n1. First, Lemma 3 shows with high probability H(p+‖p−) ∈ [H(f+‖f−) ± 4H(p+‖p−) 2\nB √ logn ] for a suitable constant B that depends on C. Using it, we can show the process converges whenever a cluster has grown to a size of 4C lognH2(f+‖f−) . The proof relies on adapting the Sanov’s Theorem (see Lemma 2) of information theory. We are measuring the distance between distributions via Hellinger distance, as opposed to KL divergence (which would have been a natural choice because of its presence in the rate function in Sanov’s therem), because Hellinger distance is a metric which proves to be crucial in our analysis. 2. Lemma 5 and Corollary 1 show that every element that is included in C in Phase (3A) truly belongs to C, and elements that are not in Waiting(C) can not be in C with high probability. Once Phase 2 has converged, if the condition of (3A) is satisfied, the element must belong to C. There is a small gray region of confidence interval (3B) such that if an element belongs there, we cannot be sure either way, but if an element does not satisfy either (3A) or 3B, it cannot be part of C. 3. Lemma 6 shows that size of Waiting(C) is constant showing an anti-concentration property. This coupled with the fact that the process converges when a cluster reaches size 4C lognH2(f+‖f−) gives the desired query complexity bound in Lemma 7."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "In this section, we report experimental results on a popular bibliographic dataset cora [35] consisting of 1879 nodes, 191 clusters and 1699612 edges out of which 62891 are intra-cluster edges. We remove any singleton node from the dataset – the final number of vertices that we classify is 1812 with 124 clusters. We use the similarity function computation used by [18] to compute f+ and f−. The two distributions are shown in Figure 1 on the left. The Hellinger square divergence between the two distributions is 0.6. In order to observe the dependency of the algorithm performance on the learnt distributions, we perturb the exact distributions to obtain two approximate distributions as shown in Figure 1 (middle) with Hellinger square divergence being 0.4587. We consider three strategies. Suppose the cluster in which a node v must be included has already been initialized and exists in the current solution. Moreover, suppose the algorithm decides to use queries to find membership of v. Then in the best strategy, only one query is needed to identify the cluster in which v belongs. In the worst strategy, the algorithm finds the correct cluster after querying all the existing clusters whose current membership is not enough to take a decision using side information. In the greedy strategy, the algorithm queries the clusters in non-decreasing order of Hellinger square divergence between f+ (or approximate version of it) and the estimated distribution from side information between v and each existing clusters. Note that, in practice, we will follow the greedy strategy. Figure 2 shows the performance of each strategy. We plot the number of queries vs F1 Score which computes the harmonic mean of precision and recall. We observe that the performance of greedy strategy is very close to that of best. With just 1136 queries, greedy achieves 80% precision and close to 90% recall. The best strategy would need 962 queries to achieve that performance. The performance of our algorithm on the exact and approximate distributions are also very close which indicates it is enough to learn a distribution that is close to exact. For example, using the approximate distributions,\nto achieve similar precision and recall, the greedy strategy just uses 1148 queries, that is 12 queries more than when we use when the distributions are known.\nDiscussion. This is the first rigorous theoretical study of interactive clustering with side information, and it unveils many interesting directions for future study of both theoretical and practical significance (see Appendix D for more details). Having arbitrary f+, f− is a generalization of SBM. Also it raises an important question about how SBM recovery threshold changes with queries. For sparse region of SBM, where f+ is Bernoulli(a ′ logn n ) and f− is Bernoulli( b′ logn n ), a\n′ > b′, Lemma 1 is not tight yet. However, it shows the following trend. Let us set a = nk in Lemma 1 with the above f+, f−. We conjecture by ignoring the lower order terms and a √ log n factor that with Q queries, the sharp recovery threshold of sparse SBM changes from ( √ a′− √ b′) ≥ √ k to ( √ a′− √ b′) ≥ √ k ( 1− Qnk )\n. Proving this bound remains an exciting open question.\nWe propose two computationally efficient algorithms that match the query complexity lower bound within log n factor and are completely parameter free. In particular, our iterative-update method to design Monte-Carlo algorithm provides a general recipe to develop any parameter-free algorithm, which are of extreme practical importance. The convergence result is established by extending Sanov’s theorem from the large deviation theory which gives bound only in terms of KL-divergence. Due to the generality of the distributions, the only tool we could use is Sanov’s theorem. However, Hellinger distance comes out to be the right measure both for lower and upper bounds. If f+ and f− are common distributions like Gaussian, Bernoulli etc., then other concentration results stronger than Sanov may be applied to improve the constants and a logarithm factor to show the trade-off between queries and thresholds as in sparse SBM. While some of our results apply to general fi,js, a full picture with arbitrary fi,js and closing the gap of log n between the lower and upper bound remain an important future direction.\nAcknowledgement. This work is supported in part by NSF awards CCF 1642658, CCF 1642550, CCF 1464310, CCF 1652303, a Yahoo ACE Award and a Google Faculty Research Award. We are particularly thankful to an anonymous reviewer whose comments led to notable improvement of the presentation of the paper."
    } ],
    "references" : [ {
      "title" : "Exact recovery in the stochastic block model",
      "author" : [ "E. Abbe", "A.S. Bandeira", "G. Hall" ],
      "venue" : "IEEE Trans. Information Theory, 62(1):471–487",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Community detection in general stochastic block models: Fundamental limits and efficient algorithms for recovery",
      "author" : [ "E. Abbe", "C. Sandon" ],
      "venue" : "IEEE 56th Annual Symposium on Foundations of Computer Science, FOCS 2015, Berkeley, CA, USA, 17-20 October, 2015, pages 670–688",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Recovering communities in the general stochastic block model without knowing the parameters",
      "author" : [ "E. Abbe", "C. Sandon" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 676–684",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deterministic selection in o (loglog n) parallel time",
      "author" : [ "M. Ajtai", "J. Komlos", "W.L. Steiger", "E. Szemerédi" ],
      "venue" : "Proceedings of the eighteenth annual ACM symposium on Theory of computing, pages 188–195. ACM",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Clustering with same-cluster queries",
      "author" : [ "H. Ashtiani", "S. Kushagra", "S. Ben-David" ],
      "venue" : "NIPS",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Local algorithms for interactive clustering",
      "author" : [ "P. Awasthi", "M.-F. Balcan", "K. Voevodski" ],
      "venue" : "ICML, pages 550–558",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Clustering with interactive feedback",
      "author" : [ "M.-F. Balcan", "A. Blum" ],
      "venue" : "International Conference on Algorithmic Learning Theory, pages 316–328. Springer",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Parallel selection with high probability",
      "author" : [ "B. Bollobás", "G. Brightwell" ],
      "venue" : "SIAM Journal on Discrete Mathematics, 3(1):21–31",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Spectral clustering of graphs with general degrees in the extended planted partition model",
      "author" : [ "K. Chaudhuri", "F.C. Graham", "A. Tsiatas" ],
      "venue" : "COLT, pages 35–1",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Community recovery in graphs with locality",
      "author" : [ "Y. Chen", "G. Kamath", "C. Suh", "D. Tse" ],
      "venue" : "Proceedings of The 33rd International Conference on Machine Learning, pages 689–698",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Stochastic block model and community detection in the sparse graphs: A spectral algorithm with optimal rate of recovery",
      "author" : [ "P. Chin", "A. Rao", "V. Vu" ],
      "venue" : "arXiv preprint arXiv:1501.05021",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Aggregating crowdsourced binary ratings",
      "author" : [ "N. Dalvi", "A. Dasgupta", "R. Kumar", "V. Rastogi" ],
      "venue" : "WWW, pages 285–294",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Top-k and clustering with noisy comparisons",
      "author" : [ "S.B. Davidson", "S. Khanna", "T. Milo", "S. Roy" ],
      "venue" : "ACM Trans. Database Syst., 39(4):35:1–35:39",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications",
      "author" : [ "A. Decelle", "F. Krzakala", "C. Moore", "L. Zdeborová" ],
      "venue" : "Physical Review E, 84(6):066106",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The solution of some random np-hard problems in polynomial expected time",
      "author" : [ "M.E. Dyer", "A.M. Frieze" ],
      "venue" : "Journal of Algorithms, 10(4):451–489",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Computing with noisy information",
      "author" : [ "U. Feige", "P. Raghavan", "D. Peleg", "E. Upfal" ],
      "venue" : "SIAM Journal on Computing, 23(5):1001–1018",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "A theory for record linkage",
      "author" : [ "I.P. Fellegi", "A.B. Sunter" ],
      "venue" : "Journal of the American Statistical Association, 64(328):1183–1210",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "Online entity resolution using an oracle",
      "author" : [ "D. Firmani", "B. Saha", "D. Srivastava" ],
      "venue" : "PVLDB, 9(5):384–395",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Active learning for community detection in stochastic block models",
      "author" : [ "A. Gadde", "E.E. Gad", "S. Avestimehr", "A. Ortega" ],
      "venue" : "Information Theory (ISIT), 2016 IEEE International Symposium on, pages 1889–1893. IEEE",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Entity resolution: theory",
      "author" : [ "L. Getoor", "A. Machanavajjhala" ],
      "venue" : "practice & open challenges. PVLDB, 5(12):2018–2019",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Who moderates the moderators?: crowdsourcing abuse detection in user-generated content",
      "author" : [ "A. Ghosh", "S. Kale", "P. McAfee" ],
      "venue" : "EC, pages 167–176",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Corleone: Hands-off crowdsourcing for entity matching",
      "author" : [ "C. Gokhale", "S. Das", "A. Doan", "J.F. Naughton", "N. Rampalli", "J. Shavlik", "X. Zhu" ],
      "venue" : "SIGMOD Conference, pages 601–612",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Lower bounds for the minimax risk using-divergences",
      "author" : [ "A. Guntuboyina" ],
      "venue" : "and applications. IEEE Transactions on Information Theory, 57(4):2386–2399",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Achieving exact cluster recovery threshold via semidefinite programming",
      "author" : [ "B. Hajek", "Y. Wu", "J. Xu" ],
      "venue" : "IEEE Transactions on Information Theory, 62(5):2788–2797",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Computational lower bounds for community detection on random graphs",
      "author" : [ "B.E. Hajek", "Y. Wu", "J. Xu" ],
      "venue" : "Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, pages 899–928",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Generalizing the fano inequality",
      "author" : [ "T.S. Han", "S. Verdu" ],
      "venue" : "IEEE Transactions on Information Theory, 40(4):1247–1251",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "W. Hoeffding" ],
      "venue" : "Journal of the American statistical association, 58(301):13–30",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1963
    }, {
      "title" : "Stochastic blockmodels: First steps",
      "author" : [ "P.W. Holland", "K.B. Laskey", "S. Leinhardt" ],
      "venue" : "Social networks, 5(2):109–137",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Iterative learning for reliable crowdsourcing systems",
      "author" : [ "D.R. Karger", "S. Oh", "D. Shah" ],
      "venue" : "NIPS, pages 1953–1961",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Evaluation of entity resolution approaches on real-world match problems",
      "author" : [ "H. Köpcke", "A. Thor", "E. Rahm" ],
      "venue" : "Proceedings of the VLDB Endowment, 3(1-2):484–493",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Clustering from labels and time-varying graphs",
      "author" : [ "S.H. Lim", "Y. Chen", "H. Xu" ],
      "venue" : "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 1188–1196. Curran Associates, Inc.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Clustering via crowdsourcing",
      "author" : [ "A. Mazumdar", "B. Saha" ],
      "venue" : "arXiv preprint arXiv:1604.01839",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A Theoretical Analysis of First Heuristics of Crowdsourced Entity Resolution",
      "author" : [ "A. Mazumdar", "B. Saha" ],
      "venue" : "The Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Clustering with noisy queries",
      "author" : [ "A. Mazumdar", "B. Saha" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 31",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Consistency thresholds for the planted bisection model",
      "author" : [ "E. Mossel", "J. Neeman", "A. Sly" ],
      "venue" : "Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 69–75. ACM",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Arimoto channel coding converse and rényi divergence",
      "author" : [ "Y. Polyanskiy", "S. Verdú" ],
      "venue" : "Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on, pages 1327–1333. IEEE",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "f divergence inequalities",
      "author" : [ "I. Sason", "S. Vérdu" ],
      "venue" : "IEEE Transactions on Information Theory, 62(11):5973–6006",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Entity resolution with crowd errors",
      "author" : [ "V. Verroios", "H. Garcia-Molina" ],
      "venue" : "31st IEEE International Conference on Data Engineering, ICDE 2015, Seoul, South Korea, April 13-17, 2015, pages 219–230",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Crowdsourcing algorithms for entity resolution",
      "author" : [ "N. Vesdapunt", "K. Bellare", "N. Dalvi" ],
      "venue" : "PVLDB, 7(12):1071–1082",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Crowdsourced clustering: Querying edges vs triangles",
      "author" : [ "R.K. Vinayak", "B. Hassibi" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1316–1324",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Crowder: Crowdsourcing entity resolution",
      "author" : [ "J. Wang", "T. Kraska", "M.J. Franklin", "J. Feng" ],
      "venue" : "PVLDB, 5(11):1483–1494",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Leveraging transitive relations for crowdsourced joins",
      "author" : [ "J. Wang", "G. Li", "T. Kraska", "M.J. Franklin", "J. Feng" ],
      "venue" : "SIGMOD Conference, pages 229–240",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Coupled with this observation, clustering with an oracle has generated tremendous interest in the last few years with increasing number of heuristics developed for this purpose [22, 40, 13, 42, 43, 18, 39, 12, 21, 29].",
      "startOffset" : 177,
      "endOffset" : 217
    }, {
      "referenceID" : 38,
      "context" : "Coupled with this observation, clustering with an oracle has generated tremendous interest in the last few years with increasing number of heuristics developed for this purpose [22, 40, 13, 42, 43, 18, 39, 12, 21, 29].",
      "startOffset" : 177,
      "endOffset" : 217
    }, {
      "referenceID" : 12,
      "context" : "Coupled with this observation, clustering with an oracle has generated tremendous interest in the last few years with increasing number of heuristics developed for this purpose [22, 40, 13, 42, 43, 18, 39, 12, 21, 29].",
      "startOffset" : 177,
      "endOffset" : 217
    }, {
      "referenceID" : 40,
      "context" : "Coupled with this observation, clustering with an oracle has generated tremendous interest in the last few years with increasing number of heuristics developed for this purpose [22, 40, 13, 42, 43, 18, 39, 12, 21, 29].",
      "startOffset" : 177,
      "endOffset" : 217
    }, {
      "referenceID" : 41,
      "context" : "Coupled with this observation, clustering with an oracle has generated tremendous interest in the last few years with increasing number of heuristics developed for this purpose [22, 40, 13, 42, 43, 18, 39, 12, 21, 29].",
      "startOffset" : 177,
      "endOffset" : 217
    }, {
      "referenceID" : 17,
      "context" : "Coupled with this observation, clustering with an oracle has generated tremendous interest in the last few years with increasing number of heuristics developed for this purpose [22, 40, 13, 42, 43, 18, 39, 12, 21, 29].",
      "startOffset" : 177,
      "endOffset" : 217
    }, {
      "referenceID" : 37,
      "context" : "Coupled with this observation, clustering with an oracle has generated tremendous interest in the last few years with increasing number of heuristics developed for this purpose [22, 40, 13, 42, 43, 18, 39, 12, 21, 29].",
      "startOffset" : 177,
      "endOffset" : 217
    }, {
      "referenceID" : 11,
      "context" : "Coupled with this observation, clustering with an oracle has generated tremendous interest in the last few years with increasing number of heuristics developed for this purpose [22, 40, 13, 42, 43, 18, 39, 12, 21, 29].",
      "startOffset" : 177,
      "endOffset" : 217
    }, {
      "referenceID" : 20,
      "context" : "Coupled with this observation, clustering with an oracle has generated tremendous interest in the last few years with increasing number of heuristics developed for this purpose [22, 40, 13, 42, 43, 18, 39, 12, 21, 29].",
      "startOffset" : 177,
      "endOffset" : 217
    }, {
      "referenceID" : 28,
      "context" : "Coupled with this observation, clustering with an oracle has generated tremendous interest in the last few years with increasing number of heuristics developed for this purpose [22, 40, 13, 42, 43, 18, 39, 12, 21, 29].",
      "startOffset" : 177,
      "endOffset" : 217
    }, {
      "referenceID" : 15,
      "context" : "On the theoretical side, query complexity or the decision tree complexity is a classical model of computation that has been extensively studied for different problems [16, 4, 8].",
      "startOffset" : 167,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "On the theoretical side, query complexity or the decision tree complexity is a classical model of computation that has been extensively studied for different problems [16, 4, 8].",
      "startOffset" : 167,
      "endOffset" : 177
    }, {
      "referenceID" : 7,
      "context" : "On the theoretical side, query complexity or the decision tree complexity is a classical model of computation that has been extensively studied for different problems [16, 4, 8].",
      "startOffset" : 167,
      "endOffset" : 177
    }, {
      "referenceID" : 38,
      "context" : "For the clustering problem, one can obtain an upper bound of O(nk) on the query complexity easily and it is achievable even when k is unknown [40, 13]: to cluster an element at any stage of the algorithm, ask one query per existing cluster with this element (this is sufficient due to transitivity), and start a new cluster if all queries are negative.",
      "startOffset" : 142,
      "endOffset" : 150
    }, {
      "referenceID" : 12,
      "context" : "For the clustering problem, one can obtain an upper bound of O(nk) on the query complexity easily and it is achievable even when k is unknown [40, 13]: to cluster an element at any stage of the algorithm, ask one query per existing cluster with this element (this is sufficient due to transitivity), and start a new cluster if all queries are negative.",
      "startOffset" : 142,
      "endOffset" : 150
    }, {
      "referenceID" : 16,
      "context" : "Entity resolution (ER, also known as record linkage) is a fundamental problem in data mining and has been studied since 1969 [17].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : "(see the excellent survey by Getoor and Machanavajjhala [20]).",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 29,
      "context" : "For example, DBLP, the popular computer science bibliography dataset is filled with ER errors [30].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 40,
      "context" : "The works of [42, 43, 40] (and many subsequent works) use a computer-generated similarity matrix to come up with a collection of pair-wise questions that are asked interactively to a crowd.",
      "startOffset" : 13,
      "endOffset" : 25
    }, {
      "referenceID" : 41,
      "context" : "The works of [42, 43, 40] (and many subsequent works) use a computer-generated similarity matrix to come up with a collection of pair-wise questions that are asked interactively to a crowd.",
      "startOffset" : 13,
      "endOffset" : 25
    }, {
      "referenceID" : 38,
      "context" : "The works of [42, 43, 40] (and many subsequent works) use a computer-generated similarity matrix to come up with a collection of pair-wise questions that are asked interactively to a crowd.",
      "startOffset" : 13,
      "endOffset" : 25
    }, {
      "referenceID" : 40,
      "context" : "But intriguingly, as shown by extensive experiments on various real datasets, these heuristics use far less queries than nk [42, 43, 40]–barring the Ω(nk) theoretical lower bound.",
      "startOffset" : 124,
      "endOffset" : 136
    }, {
      "referenceID" : 41,
      "context" : "But intriguingly, as shown by extensive experiments on various real datasets, these heuristics use far less queries than nk [42, 43, 40]–barring the Ω(nk) theoretical lower bound.",
      "startOffset" : 124,
      "endOffset" : 136
    }, {
      "referenceID" : 38,
      "context" : "But intriguingly, as shown by extensive experiments on various real datasets, these heuristics use far less queries than nk [42, 43, 40]–barring the Ω(nk) theoretical lower bound.",
      "startOffset" : 124,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : "Indeed the recent works of [18, 33] analyze popular heuristic algorithms of [40, 43] where the probability distributions are obtained from real datasets which show that these heuristics are significantly suboptimal even for very simple distributions.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 32,
      "context" : "Indeed the recent works of [18, 33] analyze popular heuristic algorithms of [40, 43] where the probability distributions are obtained from real datasets which show that these heuristics are significantly suboptimal even for very simple distributions.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 38,
      "context" : "Indeed the recent works of [18, 33] analyze popular heuristic algorithms of [40, 43] where the probability distributions are obtained from real datasets which show that these heuristics are significantly suboptimal even for very simple distributions.",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 41,
      "context" : "Indeed the recent works of [18, 33] analyze popular heuristic algorithms of [40, 43] where the probability distributions are obtained from real datasets which show that these heuristics are significantly suboptimal even for very simple distributions.",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 27,
      "context" : "The model of side information considered in this paper is a direct and significant generalization of the planted partition model, also known as the stochastic block model (SBM) [28, 15, 14, 2, 1, 25, 24, 11, 36].",
      "startOffset" : 177,
      "endOffset" : 211
    }, {
      "referenceID" : 14,
      "context" : "The model of side information considered in this paper is a direct and significant generalization of the planted partition model, also known as the stochastic block model (SBM) [28, 15, 14, 2, 1, 25, 24, 11, 36].",
      "startOffset" : 177,
      "endOffset" : 211
    }, {
      "referenceID" : 13,
      "context" : "The model of side information considered in this paper is a direct and significant generalization of the planted partition model, also known as the stochastic block model (SBM) [28, 15, 14, 2, 1, 25, 24, 11, 36].",
      "startOffset" : 177,
      "endOffset" : 211
    }, {
      "referenceID" : 1,
      "context" : "The model of side information considered in this paper is a direct and significant generalization of the planted partition model, also known as the stochastic block model (SBM) [28, 15, 14, 2, 1, 25, 24, 11, 36].",
      "startOffset" : 177,
      "endOffset" : 211
    }, {
      "referenceID" : 0,
      "context" : "The model of side information considered in this paper is a direct and significant generalization of the planted partition model, also known as the stochastic block model (SBM) [28, 15, 14, 2, 1, 25, 24, 11, 36].",
      "startOffset" : 177,
      "endOffset" : 211
    }, {
      "referenceID" : 24,
      "context" : "The model of side information considered in this paper is a direct and significant generalization of the planted partition model, also known as the stochastic block model (SBM) [28, 15, 14, 2, 1, 25, 24, 11, 36].",
      "startOffset" : 177,
      "endOffset" : 211
    }, {
      "referenceID" : 23,
      "context" : "The model of side information considered in this paper is a direct and significant generalization of the planted partition model, also known as the stochastic block model (SBM) [28, 15, 14, 2, 1, 25, 24, 11, 36].",
      "startOffset" : 177,
      "endOffset" : 211
    }, {
      "referenceID" : 10,
      "context" : "The model of side information considered in this paper is a direct and significant generalization of the planted partition model, also known as the stochastic block model (SBM) [28, 15, 14, 2, 1, 25, 24, 11, 36].",
      "startOffset" : 177,
      "endOffset" : 211
    }, {
      "referenceID" : 34,
      "context" : "The model of side information considered in this paper is a direct and significant generalization of the planted partition model, also known as the stochastic block model (SBM) [28, 15, 14, 2, 1, 25, 24, 11, 36].",
      "startOffset" : 177,
      "endOffset" : 211
    }, {
      "referenceID" : 0,
      "context" : "k = 2 is known as the planted bisection model and is studied extensively [1, 36, 15] or a slowly growing function of n (e.",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 34,
      "context" : "k = 2 is known as the planted bisection model and is studied extensively [1, 36, 15] or a slowly growing function of n (e.",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 14,
      "context" : "k = 2 is known as the planted bisection model and is studied extensively [1, 36, 15] or a slowly growing function of n (e.",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "For SBM, parameter free algorithms are known relatively recently for constant number of linear sized clusters [3, 24].",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 23,
      "context" : "For SBM, parameter free algorithms are known relatively recently for constant number of linear sized clusters [3, 24].",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : ", see [2] and therein for many references).",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "For k = 2 and equal sized clusters, sharp thresholds are derived in [1, 36] for a specific sparse region of p and q 1.",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 34,
      "context" : "For k = 2 and equal sized clusters, sharp thresholds are derived in [1, 36] for a specific sparse region of p and q 1.",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "In a more general setting, the vertices in the ith and the jth communities are connected with probability qij and threshold results for the sparse region has been derived in [2] - our model can be allowed to have this as a special case when we have pmfs fi,js denoting the distributions of the corresponding random variables.",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 8,
      "context" : "Developing lower bounds in the interactive setting appears to be significantly challenging, as algorithms may choose to get any deterministic information adaptively by querying, and standard lower bounding techniques based on Fano-type inequalities [9, 31] do not apply.",
      "startOffset" : 249,
      "endOffset" : 256
    }, {
      "referenceID" : 30,
      "context" : "Developing lower bounds in the interactive setting appears to be significantly challenging, as algorithms may choose to get any deterministic information adaptively by querying, and standard lower bounding techniques based on Fano-type inequalities [9, 31] do not apply.",
      "startOffset" : 249,
      "endOffset" : 256
    }, {
      "referenceID" : 6,
      "context" : "The interactive framework of clustering model has been studied before where the oracle is given the entire clustering and the oracle can answer whether a cluster needs to be split or two clusters must be merged [7, 6].",
      "startOffset" : 211,
      "endOffset" : 217
    }, {
      "referenceID" : 5,
      "context" : "The interactive framework of clustering model has been studied before where the oracle is given the entire clustering and the oracle can answer whether a cluster needs to be split or two clusters must be merged [7, 6].",
      "startOffset" : 211,
      "endOffset" : 217
    }, {
      "referenceID" : 40,
      "context" : "Here we contain our attention to pair-wise queries, as in all practical applications that motivate this work [42, 43, 22, 40].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 41,
      "context" : "Here we contain our attention to pair-wise queries, as in all practical applications that motivate this work [42, 43, 22, 40].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 21,
      "context" : "Here we contain our attention to pair-wise queries, as in all practical applications that motivate this work [42, 43, 22, 40].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 38,
      "context" : "Here we contain our attention to pair-wise queries, as in all practical applications that motivate this work [42, 43, 22, 40].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 37,
      "context" : "Only recently, some heuristic algorithms with k-wise queries for small values of k but k > 2 have been proposed in [39], and a non-interactive algorithm that selects random triangle queries have been analyzed in [41].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 39,
      "context" : "Only recently, some heuristic algorithms with k-wise queries for small values of k but k > 2 have been proposed in [39], and a non-interactive algorithm that selects random triangle queries have been analyzed in [41].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 18,
      "context" : "Also recently, the stochastic block model with active label-queries have been studied in [19].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "Perhaps conceptually closest to us is a recent work by [5] where they consider pair-wise queries for clustering.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 37,
      "context" : "To deal with the possibility that the crowdsourced oracle may give wrong answers, there are simple majority voting mechanisms or more complicated techniques [39, 12, 21, 29, 10, 41] to handle such errors.",
      "startOffset" : 157,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "To deal with the possibility that the crowdsourced oracle may give wrong answers, there are simple majority voting mechanisms or more complicated techniques [39, 12, 21, 29, 10, 41] to handle such errors.",
      "startOffset" : 157,
      "endOffset" : 181
    }, {
      "referenceID" : 20,
      "context" : "To deal with the possibility that the crowdsourced oracle may give wrong answers, there are simple majority voting mechanisms or more complicated techniques [39, 12, 21, 29, 10, 41] to handle such errors.",
      "startOffset" : 157,
      "endOffset" : 181
    }, {
      "referenceID" : 28,
      "context" : "To deal with the possibility that the crowdsourced oracle may give wrong answers, there are simple majority voting mechanisms or more complicated techniques [39, 12, 21, 29, 10, 41] to handle such errors.",
      "startOffset" : 157,
      "endOffset" : 181
    }, {
      "referenceID" : 9,
      "context" : "To deal with the possibility that the crowdsourced oracle may give wrong answers, there are simple majority voting mechanisms or more complicated techniques [39, 12, 21, 29, 10, 41] to handle such errors.",
      "startOffset" : 157,
      "endOffset" : 181
    }, {
      "referenceID" : 39,
      "context" : "To deal with the possibility that the crowdsourced oracle may give wrong answers, there are simple majority voting mechanisms or more complicated techniques [39, 12, 21, 29, 10, 41] to handle such errors.",
      "startOffset" : 157,
      "endOffset" : 181
    }, {
      "referenceID" : 33,
      "context" : "The related problem of clustering with noisy queries is studied by us in a companion work [34].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 31,
      "context" : "Most of the results of the two papers are available online in a more extensive version [32].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "We use the similarity function computation used by [18] to compute f+ and f−.",
      "startOffset" : 51,
      "endOffset" : 55
    } ],
    "year" : 2017,
    "abstractText" : "Suppose, we are given a set of n elements to be clustered into k (unknown) clusters, and an oracle/expert labeler that can interactively answer pair-wise queries of the form, “do two elements u and v belong to the same cluster?”. The goal is to recover the optimum clustering by asking the minimum number of queries. In this paper, we provide a rigorous theoretical study of this basic problem of query complexity of interactive clustering, and give strong information theoretic lower bounds, as well as nearly matching upper bounds. Most clustering problems come with a similarity matrix, which is used by an automated process to cluster similar points together. However, obtaining an ideal similarity function is extremely challenging due to ambiguity in data representation, poor data quality etc., and this is one of the primary reasons that makes clustering hard. To improve accuracy of clustering, a fruitful approach in recent years has been to ask a domain expert or crowd to obtain labeled data interactively. Many heuristics have been proposed, and all of these use a similarity function to come up with a querying strategy. Even so, there is a lack systematic theoretical study. Our main contribution in this paper is to show the dramatic power of side information aka similarity matrix on reducing the query complexity of clustering. A similarity matrix represents noisy pair-wise relationships such as one computed by some function on attributes of the elements. A natural noisy model is where similarity values are drawn independently from some arbitrary probability distribution f+ when the underlying pair of elements belong to the same cluster, and from some f− otherwise. We show that given such a similarity matrix, the query complexity reduces drastically from Θ(nk) (no similarity matrix) to O( k 2 logn H(f+‖f−) ) whereH 2 denotes the squared Hellinger divergence. Moreover, this is also information-theoretic optimal within anO(log n) factor. Our algorithms are all efficient, and parameter free, i.e., they work without any knowledge of k, f+ and f−, and only depend logarithmically with n. Our lower bounds could be of independent interest, and provide a general framework for proving lower bounds for classification problems in the interactive setting. Along the way, our work also reveals intriguing connection to popular community detection models such as the stochastic block model and opens up many avenues for interesting future research.",
    "creator" : null
  }
}