{
  "name" : "6ef80bb237adf4b6f77d0700e1255907.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds",
    "authors" : [ "Yuanyuan Liu", "Fanhua Shang", "James Cheng", "Hong Cheng", "Licheng Jiao" ],
    "emails" : [ "jcheng}@cse.cuhk.edu.hk;", "hcheng@se.cuhk.edu.hk;", "lchjiao@mail.xidian.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ µ/L)k). Moreover, our method also improves\nthe global convergence rate on geodesically general convex problems from O(1/k) to O(1/k2). Finally, we give a specific iterative scheme for matrix Karcher mean problems, and validate our theoretical results with experiments."
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper, we study the following Riemannian optimization problem:\nmin f(x) such that x ∈ X ⊂M, (1)\nwhere (M, %) denotes a Riemannian manifold with the Riemannian metric %, X ⊂M is a nonempty, compact, geodesically convex set, and f :X → R is geodesically convex (G-convex) and geodesically L-smooth (G-L-smooth). Here, G-convex functions may be non-convex in the usual Euclidean space but convex along the manifold, and thus can be solved by a global optimization solver. [5] presented G-convexity and G-convex optimization on geodesic metric spaces, though without any attention to global complexity analysis. As discussed in [11], the topic of \"geometric programming\" may be viewed as a special case of G-convex optimization. [25] developed theoretical tools to recognize and generate G-convex functions as well as cone theoretic fixed point optimization algorithms. However, none of these three works provided a global convergence rate analysis for their algorithms. Very recently, [31] provided the global complexity analysis of first-order algorithms for G-convex optimization, and designed the following Riemannian gradient descent rule:\nxk+1 = Expxk(−η gradf(xk)),\nwhere k is the iterate index, Expxk is an exponential map at xk (see Section 2 for details), η is a step-size or learning rate, and gradf(xk) is the Riemannian gradient of f at xk∈X .\n∗Corresponding author.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nIn this paper, we extend the Nesterov’s accelerated gradient descent method [19] from Euclidean space to nonlinear Riemannian space. Below, we first introduce the Nesterov’s method and its variants for convex optimization on Euclidean space, which can be viewed as a special case of our method, whenM=Rd, and % is the Euclidean inner product. Nowadays many real-world applications involve large data sets. As data sets and problems are getting larger in size, accelerating first-order methods is of both practical and theoretical interests. The earliest first-order method for minimizing a convex function f is perhaps the gradient method. Thirty years ago, Nesterov [19] proposed an accelerated gradient method, which takes the following form: starting with x0 and y0 =x0, and for any k≥1,\nxk = yk−1 − η∇f(yk−1), yk = xk + τk(xk − xk−1),\n(2)\nwhere 0 ≤ τk ≤ 1 is the momentum parameter. For a fixed step-size η = 1/L, where L is the Lipschitz constant of ∇f , this scheme with τk = (k−1)/(k+2) exhibits the optimal convergence rate, f(xk)−f(x?)≤O(L‖x?−x0‖ 2\nk2 ), for general convex (or non-strongly convex) problems [20], where x? is any minimizer of f . In contrast, standard gradient descent methods can only achieve a convergence rate of O(1/k). We can see that this improvement relies on the introduction of the momentum term τk(xk − xk−1) as well as the particularly tuned coefficient (k−1)/(k+2)≈1−3/k. Inspired by the success of the Nesterov’s momentum, there has been much work on the development of first-order accelerated methods, see [2, 8, 21, 26, 27] for example. In addition, for strongly convex problems and setting τk≡(1− √ µ/L)/(1+ √ µ/L), Nesterov’s accelerated gradient method attains\na convergence rate of O((1− √ µ/L)k), while standard gradient descent methods achieve a linear convergence rate of O((1−µ/L)k). It is then natural to ask whether our accelerated method in nonlinear Riemannian space has the same convergence rates as its Euclidean space counterparts (e.g., Nesterov’s accelerated method [20])?"
    }, {
      "heading" : "1.1 Motivation and Challenges",
      "text" : "Zhang and Sra [31] proposed an efficient Riemannian gradient descent (RGD) method, which attains the convergence rates of O((1− µ/L)k) and O(1/k) for geodesically strongly-convex and geodesically convex problems, respectively. Hence, there still remain gaps in convergence rates between RGD and the Nesterov’s accelerated method.\nAs discussed in [31], a long-time question is whether the famous Nesterov’s accelerated gradient descent algorithm has a counterpart in nonlinear Riemannian spaces. Compared with standard gradient descent methods in Euclidean space, Nesterov’s accelerated gradient method involves a linear extrapolation step: yk = xk+τk(xk−xk−1), which can improve its convergence rates for both strongly convex and non-strongly convex problems. It is clear that ϕk(x) := f(yk)+〈∇f(yk), x−yk〉 is a linear function in Euclidean space, while its counterpart in nonlinear space, e.g., ϕk(x) := f(yk) + 〈gradf(yk), Exp−1yk (x)〉yk , is a nonlinear function, where Exp −1 yk\nis the inverse of the exponential map Expyk , and 〈·, ·〉y is the inner product (see Section 2 for details). Therefore, in nonlinear Riemannian spaces, there is no trivial analogy of such a linear extrapolation step. In other words, although Riemannian geometry provides tools that enable generalization of Euclidean algorithms mentioned above [1], we must overcome some fundamental geometric hurdles to analyze the global convergence properties of our accelerated method as in [31]."
    }, {
      "heading" : "1.2 Contributions",
      "text" : "To answer the above-mentioned open problem in [31], in this paper we propose a general accelerated first-order method for nonlinear Riemannian spaces, which is in essence the generalization of the standard Nesterov’s accelerated method. We summarize the key contributions of this paper as follows.\n• We first present a general Nesterov’s accelerated iterative scheme in nonlinear Riemannian spaces, where the linear extrapolation step in (2) is replaced by a nonlinear operator. Furthermore, we derive two equations and obtain two corresponding nonlinear operators for both geodesically strongly-convex and geodesically convex cases, respectively.\n• We provide the global convergence analysis of our accelerated algorithms, which shows that our algorithms attain the convergence rates of O((1− √ µ/L)k) and O(1/k2) for\ngeodesically strongly-convex and geodesically convex objectives, respectively.\n• Finally, we present a specific iterative scheme for matrix Karcher mean problems. Our experimental results verify the effectiveness and efficiency of our accelerated method."
    }, {
      "heading" : "2 Notation and Preliminaries",
      "text" : "We first introduce some key notations and definitions about Riemannian geometry (see [23, 30] for details). A Riemannian manifold (M, %) is a real smooth manifoldM equipped with a Riemannian metric %. Let 〈w1, w2〉x=%x(w1, w2) denote the inner product of w1, w2 ∈ TxM; and the norm of w∈TxM is defined as ‖w‖x= √ %x(w,w), where the metric % induces an inner product structure in each tangent space TxM associated with every x ∈M. A geodesic is a constant speed curve γ : [0, 1]→M that is locally distance minimizing. Let y∈M and w∈TxM, then an exponential map y = Expx(w) :TxM→M maps w to y onM, such that there is a geodesic γ with γ(0) =x, γ(1) = y and γ̇(0) = w. If there is a unique geodesic between any two points in X ⊂ M, the exponential map has inverse Exp−1x :X →TxM, i.e., w = Exp−1x (y), and the geodesic is the unique shortest path with ‖Exp−1x (y)‖x=‖Exp−1y (x)‖y = d(x, y), where d(x, y) is the geodesic distance between x, y∈X . Parallel transport Γyx : TxM→TyM maps a vector w∈TxM to Γyxw∈TyM, and preserves inner products and norm, that is, 〈w1, w2〉x=〈Γyxw1,Γyxw2〉y and ‖w1‖x=‖Γyxw1‖y , where w1, w2∈TxM. For any x, y ∈ X and any geodesic γ with γ(0) = x, γ(1) = y and γ(t) ∈ X for t ∈ [0, 1] such that f(γ(t)) ≤ (1− t)f(x) + tf(y), then f is geodesically convex (G-convex), and an equivalent definition is formulated as follows:\nf(y) ≥ f(x) + 〈gradf(x), Exp−1x (y)〉x,\nwhere gradf(x) is the Riemannian gradient of f at x. A function f : X →R is called geodesically µ-strongly convex (µ-strongly G-convex) if for any x, y∈X , the following inequality holds\nf(y) ≥ f(x) + 〈gradf(x), Exp−1x (y)〉x + µ 2 ‖Exp−1x (y)‖2x.\nA differential function f is geodesically L-smooth (G-L-smooth) if its gradient is L-Lipschitz, i.e.,\nf(y) ≤ f(x) + 〈gradf(x), Exp−1x (y)〉x + L 2 ‖Exp−1x (y)‖2x."
    }, {
      "heading" : "3 An Accelerated Method for Geodesically Convex Optimization",
      "text" : "In this section, we propose a general acceleration method for geodesically convex optimization, which can be viewed as a generalization of the famous Nesterov’s accelerated method from Euclidean space to Riemannian space. Nesterov’s accelerated method involves a linear extrapolation step as in (2), while in nonlinear Riemannian spaces, we do not have a simple way to find an analogy to such a linear extrapolation. Therefore, some standard analysis techniques do not work in nonlinear space. Motivated by this, we derive two equations to bridge the gap for both geodesically stronglyconvex and geodesically convex cases, and then generalized Nesterov’s algorithms are proposed for geodesically convex optimization by solving these two equations.\nWe first propose to replace the classical Nesterov’s scheme in (2) with the following update rules for geodesically convex optimization in Riemannian space:\nxk = Expyk−1(−ηgradf(yk−1)), yk = S(yk−1, xk, xk−1),\n(3)\nwhere yk, xk∈X , S denotes a nonlinear operator, and yk = S(yk−1, xk, xk−1) can be obtained by solving the two proposed equations (see (4) and (5) below, which can be used to deduce the key analysis tools for our convergence analysis) for strongly G-convex and general G-convex cases, respectively. Different from the Riemannian gradient descent rule (e.g., xk+1 =Expxk(−ηgradf(xk))), the Nesterov’s accelerated technique is introduced into our update rule of yk. Compared with the Nesterov’s scheme in (2), the main difference is the update rule of yk. That is, our update rule for yk is an implicit iteration process as shown below, while that of (2) is an explicit iteration one.\nAlgorithm 1 Accelerated method for strongly G-convex optimization Input: µ,L Initialize: x0, y0, η.\n1: for k = 1, 2, . . . ,K do 2: Computing the gradient at yk−1: gk−1 = gradf(yk−1); 3: xk = Expyk−1(−ηgk−1); 4: yk = S(yk−1, xk, xk−1) by solving (4). 5: end for\nOutput: xK"
    }, {
      "heading" : "3.1 Geodesically Strongly Convex Cases",
      "text" : "We first design the following equation with respect to yk∈X for the µ-strongly G-convex case:( 1− √ µ/L ) Γyk−1yk Exp −1 yk (xk)− βΓyk−1yk gradf(yk) = ( 1− √ µ/L )3/2 Exp−1yk−1(xk−1), (4) where β= 4/ √ µL−1/L> 0. Figure 1(a) illustrates the geometric interpretation of the proposed\nequation (4) for the strongly G-convex case, where uk=(1− √ µ/L)Exp−1yk (xk), vk=−βgradf(yk),\nand wk−1 = (1− √ µ/L)3/2Exp−1yk−1(xk−1). The vectors uk, vk ∈ TykM are parallel transported to Tyk−1M, and the sum of their parallel translations is equal to wk−1 ∈ Tyk−1M, which means that the equation (4) holds. We design an accelerated first-order algorithm for solving geodesically strongly-convex problems, as shown in Algorithm 1. In real applications, the proposed equation (4) can be manipulated into simpler forms. For example, we will give a specific equation for the averaging real symmetric positive definite matrices problem below."
    }, {
      "heading" : "3.2 Geodesically Convex Cases",
      "text" : "Let f be G-convex and G-L-smooth, the diameter of X be bounded by D (i.e., maxx,y∈X d(x, y) ≤ D), the variable yk ∈ X can be obtained by solving the following equation:\nΓyk−1yk\n( k\nα−1 Exp−1yk (xk)−Dĝk ) = k−1 α−1 Exp−1yk−1(xk−1)−Dĝk−1 + (k+α−2)η α− 1 gk−1, (5)\nwhere gk−1 = gradf(yk−1), and ĝk = gk/‖gk‖yk , and α ≥ 3 is a given constant. Figure 1(b) illustrates the geometric interpretation of the proposed equation (5) for the G-convex case, where uk= k α−1Exp −1 yk\n(xk)−Dĝk, and vk−1 = (k+α−2)ηα−1 gk−1. We also present an accelerated first-order algorithm for solving geodesically convex problems, as shown in Algorithm 2."
    }, {
      "heading" : "3.3 Key Lemmas",
      "text" : "For the Nesterov’s accelerated scheme in (2) with τk = k−1k+2 (for example, the general convex case) in Euclidean space, the following result in [3, 20] plays a key role in the convergence analysis of Nesterov’s accelerated algorithm.\n2\nk+2 〈∇f(yk), zk−x?〉 −\nη 2 ‖∇f(yk)‖2 =\n2 η(k+2)2 [ ‖zk − x?‖2 − ‖zk+1− x?‖2 ] , (6)\nAlgorithm 2 Accelerated method for general G-convex optimization Input: L,D, α Initialize: x0, y0, η.\n1: for k = 1, 2, . . . ,K do 2: Computing the gradient at yk−1: gk−1 = gradf(yk−1) and ĝk−1 = gk−1/‖gk−1‖yk−1 ; 3: xk = Expyk−1(−ηgk−1); 4: yk = S(yk−1, xk, xk−1) by solving (5). 5: end for\nOutput: xK\nwhere zk=(k+2)yk/2− (k/2)xk. Correspondingly, we can also obtain the following analysis tools for our convergence analysis using the proposed equations (4) and (5). In other words, the following equations (7) and (8) can be viewed as the Riemannian space counterparts of (6). Lemma 1 (Strongly G-convex). If f : X → R is geodesically µ-strongly convex and G-L-smooth, and {yk} satisfies the equation (4), and zk is defined as follows:\nzk = ( 1− √ µ/L ) Exp−1yk (xk) ∈ TykM.\nThen the following results hold: Γyk−1yk (zk − βgradf(yk)) = ( 1− √ µ/L )1/2 zk−1,\n−〈gradf(yk), zk〉yk + β\n2 ‖gradf(yk)‖2yk =\n1\n2β\n( 1− √ µ/L ) ‖zk−1‖2yk−1 − 1\n2β ‖zk‖2yk . (7)\nFor general G-convex objectives, we have the following result. Lemma 2 (General G-convex). If f : X → R is G-convex and G-L-smooth, the diameter of X is bounded by D, and {yk} satisfies the equation (5), and zk is defined as\nzk = k\nα− 1 Exp−1yk (xk)−Dĝk ∈ TykM.\nThen the following results hold:\nΓykyk+1zk+1 = zk + (k + α− 1)η\nα− 1 gradf(yk),\nα−1 k+α−1 〈gradf(yk),−zk〉yk − η 2 ‖gradf(yk)‖2yk = 2(α−1)2 η(k+α−1)2 ( ‖zk‖2yk − ‖zk+1‖ 2 yk+1 ) . (8)\nThe proofs of Lemmas 1 and 2 are provided in the Supplementary Materials."
    }, {
      "heading" : "4 Convergence Analysis",
      "text" : "In this section, we analyze the global convergence properties of the proposed algorithms (i.e., Algorithms 1 and 2) for both geodesically strongly convex and general convex problems. Lemma 3. If f : X → R is G-convex and G-L-smooth for any x ∈ X , and {xk} is the sequence produced by Algorithms 1 and 2 with η ≤ 1/L, then the following result holds:\nf(xk+1) ≤ f(x) + 〈gradf(yk), −Exp−1yk (x)〉yk − η\n2 ‖gradf(yk)‖2yk .\nThe proof of this lemma can be found in the Supplementary Materials. For the geodesically strongly convex case, we have the following result. Theorem 1 (Strongly G-convex). Let x? be the optimal solution of Problem (1), and {xk} be the sequence produced by Algorithm 1. If f : X → R is geodesically µ-strongly convex and G-L-smooth, then the following result holds\nf(xk+1)− f(x?) ≤ ( 1− √ µ/L )k [ f(x0)− f(x?) + 1\n2β\n( 1− √ µ/L ) ‖z0‖2y0 ] ,\nwhere z0 is defined in Lemma 1.\nThe proof of Theorem 1 can be found in the Supplementary Materials. From this theorem, we can see that the proposed algorithm attains a linear convergence rate of O((1− √ µ/L)k) for geodesically strongly convex problems, which is the same as that of its Euclidean space counterparts and significantly faster than that of non-accelerated algorithms such as [31] (i.e., O((1−µ/L)k)), as shown in Table 1. For the geodesically non-strongly convex case, we have the following result. Theorem 2 (General G-convex). Let {xk} be the sequence produced by Algorithm 2. If f :X → R is G-convex and G-L-smooth, and the diameter of X is bounded by D, then\nf(xk+1)− f(x?) ≤ (α− 1)2\n2η(k + α− 2)2 ‖z0‖2y0 ,\nwhere z0 = −Dĝ0, as defined in Lemma 2.\nThe proof of Theorem 2 can be found in the Supplementary Materials. Theorem 2 shows that for general G-convex objectives, our acceleration method improves the theoretical convergence rate from O(1/k) (e.g., RGD [31]) to O(1/k2), which matches the optimal rate for general convex settings in Euclidean space. Please see the detail in Table 1, where the parameter c is defined in [31]."
    }, {
      "heading" : "5 Application for Matrix Karcher Mean Problems",
      "text" : "In this section, we give a specific accelerated scheme for a type of conic geometric optimization problems [25], e.g., the matrix Karcher mean problem. Specifically, the loss function of the Karcher mean problem for a set of N symmetric positive definite (SPD) matrices {Wi}Ni=1 is defined as\nf(X) := 1\n2N N∑ i=1 ‖log(X−1/2WiX−1/2)‖2F , (9)\nwhere X ∈ P := {Z ∈ Rd×d, s.t., Z = ZT 0}. The loss function f is known to be non-convex in Euclidean space but geodesically 2N -strongly convex. The inner product of two tangent vectors at point X on the manifold is given by\n〈ζ, ξ〉X = tr(ζX−1ξX−1), ζ, ξ ∈ TXP, (10)\nwhere tr(·) is the trace of a real square matrix. For any matrices X,Y ∈ P , the Riemannian distance is defined as follows:\nd(X,Y ) = ‖log(X− 12Y X− 12 )‖F ."
    }, {
      "heading" : "5.1 Computation of Yk",
      "text" : "For the accelerated update rules in (3) for Algorithm 1, we need to compute Yk via solving the equation (4). However, for the specific problem in (9) with the inner product in (10), we can derive a simpler form to solve Yk below. We first give the following properties: Property 1. For the loss function f in (9) with the inner product in (10), we have\n1. Exp−1Yk (Xk) = Y 1/2 k log(Y −1/2 k XkY −1/2 k )Y 1/2 k ;\n2. gradf(Yk) = 1N ∑N i=1 Y 1/2 k log(Y 1/2 k W −1 i Y 1/2 k )Y 1/2 k ;\n3. 〈 gradf(Yk), Exp−1Yk (Xk) 〉 Yk = 〈U, V 〉;\n4. ‖gradf(Yk)‖2Yk = ‖U‖ 2 F ,\nwhere U= 1N ∑N i=1log(Y 1/2 k W −1 i Y 1/2 k ) ∈ Rd×d, and V =log(Y −1/2 k XkY −1/2 k ) ∈ Rd×d.\nProof. In this part, we only provide the proof of Result 1 in Property 1, and the proofs of the other results are provided in the Supplementary Materials. The inner product in (10) on the Riemannian manifold leads to the following exponential map:\nExpX(ξX) = X 1 2 exp(X− 1 2 ξXX − 12 )X 1 2 , (11)\nwhere ξX ∈TXP denotes the tangent vector with the geometry, and tangent vectors ξX are expressed as follows (see [17] for details):\nξX = X 1 2 sym(∆)X 1 2 , ∆ ∈ Rd×d,\nwhere sym(·) extracts the symmetric part of its argument, that is, sym(A)=(AT +A)/2. Then we can set Exp−1Yk (Xk) = Y 1/2 k sym(∆Xk)Y 1/2 k ∈ TYkP . By the definition of Exp −1 Yk\n(Xk), we have ExpYk(Exp −1 Yk (Xk)) = Xk, that is,\nExpYk(Y 1/2 k sym(∆Xk)Y 1/2 k ) = Xk. (12)\nUsing (11) and (12), we have\nsym(∆Xk) = log(Y −1/2 k XkY −1/2 k ) ∈ R d×d.\nTherefore, we have\nExp−1Yk (Xk) = Y 1/2 k sym(∆Xk)Y 1/2 k = Y 1/2 k log(Y −1/2 k XkY −1/2 k )Y 1/2 k = −Yk log(X −1 k Yk),\nwhere the last equality holds due to the fact that log(X−1Y X) = X−1 log(Y )X .\nResult 3 in Property 1 shows that the inner product of two tangent vectors at Yk is equal to the Euclidean inner-product of two vectors U, V ∈ Rd×d. Thus, we can reformulate (4) as follows:(\n1− √ µ\nL\n) log(Y\n− 12 k XkY − 12 k )− β\nN N∑ i=1 log(Y 1 2 k W −1 i Y 1 2 k )= ( 1− √ µ L ) 3 2 log(Y − 12 k−1X −1 k−1Y − 12 k−1),\n(13) where β=4/ √ µL−1/L. Then Yk can be obtained by solving (13). From a numerical perspective, log(Y 1 2\nk W −1 i Y\n1 2 k ) can be approximated by log(Y 1 2 k−1W −1 i Y 1 2 k−1), and then Yk is given by\nYk = X 1 2 k exp −1\n[( 1− √ µ\nL\n) 1 2\nlog(Y − 12 k−1Xk−1Y − 12 k−1) + δβ\nN N∑ i=1 log(Y 1 2 k−1W −1 i Y 1 2 k−1)\n] X 1 2\nk ,\n(14) where δ = 1/(1− √ µ/L), and Yk ∈ P ."
    }, {
      "heading" : "6 Experiments",
      "text" : "In this section, we validate the performance of our accelerated method for averaging SPD matrices under the Riemannian metric, e.g., the matrix Karcher mean problem (9), and also compare our method against the state-of-the-art methods: Riemannian gradient descent (RGD) [31] and limitedmemory Riemannian BFGS (LRBFGS) [29]. The matrix Karcher mean problem has been widely applied to many real-world applications such as elasticity [18], radar signal and image processing [6, 15, 22], and medical imaging [9, 7, 13]. In fact, this problem is geodesically strongly convex, but non-convex in Euclidean space.\nOther methods for solving this problem include the relaxed Richardson iteration algorithm [10], the approximated joint diagonalization algorithm [12], and Riemannian stochastic gradient descent (RSGD) [31]. Since all the three methods achieve similar performance to RGD, especially in data science applications where N is large and relatively small optimization error is not required [31], we only report the experimental results of RGD. The step-size η of both RGD and LRBFGS is selected with a line search method as in [29] (see [29] for details), while η of our accelerated method is set to 1/L. For the algorithms, we initialize X using the arithmetic mean of the data set as in [29].\nThe input synthetic data are random SPD matrices of size 100×100 or 200×200 generated by using the technique in [29] or the matrix mean toolbox [10], and all matrices are explicitly normalized so that their norms are all equal to 1. We report the experimental results of RGD, LRBFGS and our accelerated method on the two data sets in Figure 2, where N is set to 100, and the condition number C of each matrix {Wi}Ni=1 is set to 102. Figure 2 shows the evolution of the distance between the exact Karcher mean and current iterate (i.e., dist(X∗, Xk)) of the methods with respect to number of iterations and running time (seconds), where X∗ is the exact Karcher mean. We can observe that our method consistently converges much faster than RGD, which empirically verifies our theoretical result in Theorem 1 that our accelerated method has a much faster convergence rate than RGD. Although LRBFGS outperforms our method in terms of number of iterations, our accelerated method converges much faster than LRBFGS in terms of running time."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper, we proposed a general Nesterov’s accelerated gradient method for nonlinear Riemannian space, which is a generalization of the famous Nesterov’s accelerated method for Euclidean space. We derived two equations and presented two accelerated algorithms for geodesically strongly-convex and general convex optimization problems, respectively. In particular, our theoretical results show that our accelerated method attains the same convergence rates as the standard Nesterov’s accelerated method in Euclidean space for both strongly G-convex and G-convex cases. Finally, we presented a special iteration scheme for solving matrix Karcher mean problems, which in essence is non-convex in Euclidean space, and the numerical results verify the efficiency of our accelerated method.\nWe can extend our accelerated method to the stochastic setting using variance reduction techniques [14, 16, 24, 28], and apply our method to solve more geodesically convex problems in the future, e.g., the general G-convex problem with a non-smooth regularization term as in [4]. In addition, we can replace exponential mapping by computationally cheap retractions together with corresponding theoretical guarantees [31]. An interesting direction of future work is to design accelerated schemes for non-convex optimization in Riemannian space."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is supported in part by Grants (CUHK 14206715 & 14222816) from the Hong Kong RGC, the Major Research Plan of the National Natural Science Foundation of China (Nos. 91438201 and 91438103), and the National Natural Science Foundation of China (No. 61573267)."
    } ],
    "references" : [ {
      "title" : "Optimization algorithms on matrix manifolds",
      "author" : [ "P.-A. Absil", "R. Mahony", "R. Sepulchre" ],
      "venue" : "Princeton University Press, Princeton, N.J.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Katyusha: The first direct acceleration of stochastic gradient methods",
      "author" : [ "Z. Allen-Zhu" ],
      "venue" : "STOC, pages 1200–1205",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "The rate of convergence of Nesterov’s accelerated forwardbackward method is actually faster than 1/k2",
      "author" : [ "H. Attouch", "J. Peypouquet" ],
      "venue" : "SIAM J. Optim., 26:1824–1834",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Inf-convolution and regularization of convex functions on Riemannian manifolds of nonpositive curvature",
      "author" : [ "D. Azagra", "J. Ferrera" ],
      "venue" : "Rev. Mat. Complut.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Convex analysis and optimization in Hadamard spaces",
      "author" : [ "M. Bacak" ],
      "venue" : "Walter de Gruyter GmbH & Co KG",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "New foundation of radar Doppler signal processing based on advanced differential geometry of symmetric spaces: Doppler matrix CFAR radar application",
      "author" : [ "F. Barbaresco" ],
      "venue" : "RADAR",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A rigorous framework for diffusion tensor calculus",
      "author" : [ "P.G. Batchelor", "M. Moakher", "D. Atkinson", "F. Calamante", "A. Connelly" ],
      "venue" : "Magn. Reson. Med., 53:221–225",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM J. Imaging Sci., 2(1):183–202",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Positive definite matrices",
      "author" : [ "R. Bhatia" ],
      "venue" : "Princeton Series in Applied Mathematics. Princeton University Press, Princeton, N.J.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Computing the Karcher mean of symmetric positive definite matrices",
      "author" : [ "D.A. Bini", "B. Iannazzo" ],
      "venue" : "Linear Algebra Appl., 438:1700–1710",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A tutorial on geometric programming",
      "author" : [ "S. Boyd", "S.-J. Kim", "L. Vandenberghe", "A. Hassibi" ],
      "venue" : "Optim. Eng., 8:67–127",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Approximate joint diagonalization and geometric mean of symmetric positive definite matrices",
      "author" : [ "M. Congedo", "B. Afsari", "A. Barachant", "M. Moakher" ],
      "venue" : "PloS one, 10:e0121423",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Riemannian geometry for the statistical analysis of diffusion tensor data",
      "author" : [ "P.T. Fletcher", "S. Joshi" ],
      "venue" : "Signal Process., 87:250–262",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "NIPS, pages 315–323",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Radar detection using Siegel distance between autoregressive processes",
      "author" : [ "J. Lapuyade-Lahorgue", "F. Barbaresco" ],
      "venue" : "application to HF and X-band radar. In RADAR",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Accelerated variance reduced stochastic ADMM",
      "author" : [ "Y. Liu", "F. Shang", "J. Cheng" ],
      "venue" : "AAAI, pages 2287–2293",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Regression on fixed-rank positive semidefinite matrices: A Riemannian approach",
      "author" : [ "G. Meyer", "S. Bonnabel", "R. Sepulchre" ],
      "venue" : "J. Mach. Learn. Res., 12:593–625",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the averaging of symmetric positive-definite tensors",
      "author" : [ "M. Moakher" ],
      "venue" : "J. Elasticity, 82:273–296",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate O(1/k2)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Soviet Mathematics Doklady, 27:372–376",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Introductory Lectures on Convex Optimization: A Basic Course",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Kluwer Academic Publ., Boston",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Gradient methods for minimizing composite functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Math. Program., 140:125– 161",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A Riemannian framework for tensor computing",
      "author" : [ "X. Pennec", "P. Fillard", "N. Ayache" ],
      "venue" : "International Journal of Computer Vision, 66:41–66",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Riemannian Geometry",
      "author" : [ "P. Petersen" ],
      "venue" : "Springer-Verlag, New York",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Larger is better: The effect of learning rates enjoyed by stochastic optimization with progressive variance reduction",
      "author" : [ "F. Shang" ],
      "venue" : "arXiv:1704.04966",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Conic geometric optimization on the manifold of positive definite matrices",
      "author" : [ "S. Sra", "R. Hosseini" ],
      "venue" : "SIAM J. Optim., 25(1):713–739",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights",
      "author" : [ "W. Su", "S. Boyd", "E.J. Candes" ],
      "venue" : "J. Mach. Learn. Res., 17:1–43",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On aacelerated proximal gradient methods for convex-concave optimization",
      "author" : [ "P. Tseng" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2008
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "L. Xiao", "T. Zhang" ],
      "venue" : "SIAM J. Optim., 24(4):2057–2075",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A Riemannian limited-memory BFGS algorithm for computing the matrix geometric mean",
      "author" : [ "X. Yuan", "W. Huang", "P.-A. Absil", "K. Gallivan" ],
      "venue" : "Procedia Computer Science, 80:2147– 2157",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Riemannian SVRG: Fast stochastic optimization on Riemannian manifolds",
      "author" : [ "H. Zhang", "S. Reddi", "S. Sra" ],
      "venue" : "NIPS, pages 4592–4600",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "First-order methods for geodesically convex optimization",
      "author" : [ "H. Zhang", "S. Sra" ],
      "venue" : "COLT, pages 1617–1638",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "[5] presented G-convexity and G-convex optimization on geodesic metric spaces, though without any attention to global complexity analysis.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 10,
      "context" : "As discussed in [11], the topic of \"geometric programming\" may be viewed as a special case of G-convex optimization.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 24,
      "context" : "[25] developed theoretical tools to recognize and generate G-convex functions as well as cone theoretic fixed point optimization algorithms.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "Very recently, [31] provided the global complexity analysis of first-order algorithms for G-convex optimization, and designed the following Riemannian gradient descent rule:",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 18,
      "context" : "In this paper, we extend the Nesterov’s accelerated gradient descent method [19] from Euclidean space to nonlinear Riemannian space.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "Thirty years ago, Nesterov [19] proposed an accelerated gradient method, which takes the following form: starting with x0 and y0 =x0, and for any k≥1, xk = yk−1 − η∇f(yk−1), yk = xk + τk(xk − xk−1), (2)",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : "For a fixed step-size η = 1/L, where L is the Lipschitz constant of ∇f , this scheme with τk = (k−1)/(k+2) exhibits the optimal convergence rate, f(xk)−f(x?)≤O(?0 2 k2 ), for general convex (or non-strongly convex) problems [20], where x? is any minimizer of f .",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 1,
      "context" : "Inspired by the success of the Nesterov’s momentum, there has been much work on the development of first-order accelerated methods, see [2, 8, 21, 26, 27] for example.",
      "startOffset" : 136,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : "Inspired by the success of the Nesterov’s momentum, there has been much work on the development of first-order accelerated methods, see [2, 8, 21, 26, 27] for example.",
      "startOffset" : 136,
      "endOffset" : 154
    }, {
      "referenceID" : 20,
      "context" : "Inspired by the success of the Nesterov’s momentum, there has been much work on the development of first-order accelerated methods, see [2, 8, 21, 26, 27] for example.",
      "startOffset" : 136,
      "endOffset" : 154
    }, {
      "referenceID" : 25,
      "context" : "Inspired by the success of the Nesterov’s momentum, there has been much work on the development of first-order accelerated methods, see [2, 8, 21, 26, 27] for example.",
      "startOffset" : 136,
      "endOffset" : 154
    }, {
      "referenceID" : 26,
      "context" : "Inspired by the success of the Nesterov’s momentum, there has been much work on the development of first-order accelerated methods, see [2, 8, 21, 26, 27] for example.",
      "startOffset" : 136,
      "endOffset" : 154
    }, {
      "referenceID" : 30,
      "context" : "Zhang and Sra [31] proposed an efficient Riemannian gradient descent (RGD) method, which attains the convergence rates of O((1− μ/L)) and O(1/k) for geodesically strongly-convex and geodesically convex problems, respectively.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 30,
      "context" : "As discussed in [31], a long-time question is whether the famous Nesterov’s accelerated gradient descent algorithm has a counterpart in nonlinear Riemannian spaces.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "In other words, although Riemannian geometry provides tools that enable generalization of Euclidean algorithms mentioned above [1], we must overcome some fundamental geometric hurdles to analyze the global convergence properties of our accelerated method as in [31].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 30,
      "context" : "In other words, although Riemannian geometry provides tools that enable generalization of Euclidean algorithms mentioned above [1], we must overcome some fundamental geometric hurdles to analyze the global convergence properties of our accelerated method as in [31].",
      "startOffset" : 261,
      "endOffset" : 265
    }, {
      "referenceID" : 30,
      "context" : "To answer the above-mentioned open problem in [31], in this paper we propose a general accelerated first-order method for nonlinear Riemannian spaces, which is in essence the generalization of the standard Nesterov’s accelerated method.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : "We first introduce some key notations and definitions about Riemannian geometry (see [23, 30] for details).",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 29,
      "context" : "We first introduce some key notations and definitions about Riemannian geometry (see [23, 30] for details).",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "3 Key Lemmas For the Nesterov’s accelerated scheme in (2) with τk = k−1 k+2 (for example, the general convex case) in Euclidean space, the following result in [3, 20] plays a key role in the convergence analysis of Nesterov’s accelerated algorithm.",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 19,
      "context" : "3 Key Lemmas For the Nesterov’s accelerated scheme in (2) with τk = k−1 k+2 (for example, the general convex case) in Euclidean space, the following result in [3, 20] plays a key role in the convergence analysis of Nesterov’s accelerated algorithm.",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 30,
      "context" : "Algorithms RGD [31] RSGD [31] Ours Strongly G-convex and smooth O ( (1−min{ 1 c , μ L }) ) O (1/k) O ( (1− √ μ L ) )",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 30,
      "context" : "Algorithms RGD [31] RSGD [31] Ours Strongly G-convex and smooth O ( (1−min{ 1 c , μ L }) ) O (1/k) O ( (1− √ μ L ) )",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 30,
      "context" : "From this theorem, we can see that the proposed algorithm attains a linear convergence rate of O((1− √ μ/L)) for geodesically strongly convex problems, which is the same as that of its Euclidean space counterparts and significantly faster than that of non-accelerated algorithms such as [31] (i.",
      "startOffset" : 287,
      "endOffset" : 291
    }, {
      "referenceID" : 30,
      "context" : ", RGD [31]) to O(1/k(2)), which matches the optimal rate for general convex settings in Euclidean space.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 30,
      "context" : "Please see the detail in Table 1, where the parameter c is defined in [31].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 24,
      "context" : "In this section, we give a specific accelerated scheme for a type of conic geometric optimization problems [25], e.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : "ExpX(ξX) = X 1 2 exp(X− 1 2 ξXX − 1 2 )X 1 2 , (11) where ξX ∈TXP denotes the tangent vector with the geometry, and tangent vectors ξX are expressed as follows (see [17] for details):",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 30,
      "context" : ", the matrix Karcher mean problem (9), and also compare our method against the state-of-the-art methods: Riemannian gradient descent (RGD) [31] and limitedmemory Riemannian BFGS (LRBFGS) [29].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 28,
      "context" : ", the matrix Karcher mean problem (9), and also compare our method against the state-of-the-art methods: Riemannian gradient descent (RGD) [31] and limitedmemory Riemannian BFGS (LRBFGS) [29].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 17,
      "context" : "The matrix Karcher mean problem has been widely applied to many real-world applications such as elasticity [18], radar signal and image processing [6, 15, 22], and medical imaging [9, 7, 13].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "The matrix Karcher mean problem has been widely applied to many real-world applications such as elasticity [18], radar signal and image processing [6, 15, 22], and medical imaging [9, 7, 13].",
      "startOffset" : 147,
      "endOffset" : 158
    }, {
      "referenceID" : 14,
      "context" : "The matrix Karcher mean problem has been widely applied to many real-world applications such as elasticity [18], radar signal and image processing [6, 15, 22], and medical imaging [9, 7, 13].",
      "startOffset" : 147,
      "endOffset" : 158
    }, {
      "referenceID" : 21,
      "context" : "The matrix Karcher mean problem has been widely applied to many real-world applications such as elasticity [18], radar signal and image processing [6, 15, 22], and medical imaging [9, 7, 13].",
      "startOffset" : 147,
      "endOffset" : 158
    }, {
      "referenceID" : 8,
      "context" : "The matrix Karcher mean problem has been widely applied to many real-world applications such as elasticity [18], radar signal and image processing [6, 15, 22], and medical imaging [9, 7, 13].",
      "startOffset" : 180,
      "endOffset" : 190
    }, {
      "referenceID" : 6,
      "context" : "The matrix Karcher mean problem has been widely applied to many real-world applications such as elasticity [18], radar signal and image processing [6, 15, 22], and medical imaging [9, 7, 13].",
      "startOffset" : 180,
      "endOffset" : 190
    }, {
      "referenceID" : 12,
      "context" : "The matrix Karcher mean problem has been widely applied to many real-world applications such as elasticity [18], radar signal and image processing [6, 15, 22], and medical imaging [9, 7, 13].",
      "startOffset" : 180,
      "endOffset" : 190
    }, {
      "referenceID" : 9,
      "context" : "Other methods for solving this problem include the relaxed Richardson iteration algorithm [10], the approximated joint diagonalization algorithm [12], and Riemannian stochastic gradient descent (RSGD) [31].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "Other methods for solving this problem include the relaxed Richardson iteration algorithm [10], the approximated joint diagonalization algorithm [12], and Riemannian stochastic gradient descent (RSGD) [31].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 30,
      "context" : "Other methods for solving this problem include the relaxed Richardson iteration algorithm [10], the approximated joint diagonalization algorithm [12], and Riemannian stochastic gradient descent (RSGD) [31].",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 30,
      "context" : "Since all the three methods achieve similar performance to RGD, especially in data science applications where N is large and relatively small optimization error is not required [31], we only report the experimental results of RGD.",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 28,
      "context" : "The step-size η of both RGD and LRBFGS is selected with a line search method as in [29] (see [29] for details), while η of our accelerated method is set to 1/L.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 28,
      "context" : "The step-size η of both RGD and LRBFGS is selected with a line search method as in [29] (see [29] for details), while η of our accelerated method is set to 1/L.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 28,
      "context" : "For the algorithms, we initialize X using the arithmetic mean of the data set as in [29].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 28,
      "context" : "The input synthetic data are random SPD matrices of size 100×100 or 200×200 generated by using the technique in [29] or the matrix mean toolbox [10], and all matrices are explicitly normalized so that their norms are all equal to 1.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 9,
      "context" : "The input synthetic data are random SPD matrices of size 100×100 or 200×200 generated by using the technique in [29] or the matrix mean toolbox [10], and all matrices are explicitly normalized so that their norms are all equal to 1.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "We can extend our accelerated method to the stochastic setting using variance reduction techniques [14, 16, 24, 28], and apply our method to solve more geodesically convex problems in the future, e.",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "We can extend our accelerated method to the stochastic setting using variance reduction techniques [14, 16, 24, 28], and apply our method to solve more geodesically convex problems in the future, e.",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "We can extend our accelerated method to the stochastic setting using variance reduction techniques [14, 16, 24, 28], and apply our method to solve more geodesically convex problems in the future, e.",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 27,
      "context" : "We can extend our accelerated method to the stochastic setting using variance reduction techniques [14, 16, 24, 28], and apply our method to solve more geodesically convex problems in the future, e.",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : ", the general G-convex problem with a non-smooth regularization term as in [4].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 30,
      "context" : "In addition, we can replace exponential mapping by computationally cheap retractions together with corresponding theoretical guarantees [31].",
      "startOffset" : 136,
      "endOffset" : 140
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose an accelerated first-order method for geodesically convex optimization, which is the generalization of the standard Nesterov’s accelerated method from Euclidean space to nonlinear Riemannian space. We first derive two equations and obtain two nonlinear operators for geodesically convex optimization instead of the linear extrapolation step in Euclidean space. In particular, we analyze the global convergence properties of our accelerated method for geodesically strongly-convex problems, which show that our method improves the convergence rate fromO((1−μ/L)) toO((1− √ μ/L)). Moreover, our method also improves the global convergence rate on geodesically general convex problems from O(1/k) to O(1/k). Finally, we give a specific iterative scheme for matrix Karcher mean problems, and validate our theoretical results with experiments.",
    "creator" : null
  }
}