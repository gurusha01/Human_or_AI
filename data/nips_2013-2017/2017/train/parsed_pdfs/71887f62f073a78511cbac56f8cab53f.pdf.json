{
  "name" : "71887f62f073a78511cbac56f8cab53f.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization",
    "authors" : [ "Ahmet Alacaoglu", "Quoc Tran-Dinh", "Olivier Fercoq", "Volkan Cevher" ],
    "emails" : [ "volkan.cevher}@epfl.ch", "quoctd@email.unc.edu", "olivier.fercoq@telecom-paristech.fr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We develop randomized coordinate descent methods to solve the following composite convex problem:\nF ? = min x∈Rp {F (x) = f(x) + g(x) + h(Ax)} , (1)\nwhere f : Rp → R, g : Rp → R ∪ {+∞}, and h : Rm → R ∪ {+∞} are proper, closed and convex functions, A ∈ Rm×p is a given matrix. The optimization template (1) covers many important applications including support vector machines, sparse model selection, logistic regression, etc. It is also convenient to formulate generic constrained convex problems by choosing an appropriate h. Within convex optimization, coordinate descent methods have recently become increasingly popular in the literature [1–6]. These methods are particularly well-suited to solve huge-scale problems arising from machine learning applications where matrix-vector operations are prohibitive [1]. To our knowledge, there is no coordinate descent method for the general three-composite form (1) within our structure assumptions studied here that has rigorous convergence guarantees. Our paper specifically fills this gap. For such a theoretical development, coordinate descent algorithms require specific assumptions on the convex optimization problems [1, 4, 6]. As a result, to rigorously handle the three-composite case, we assume that (i) f is smooth, (ii) g is non-smooth but decomposable (each component has an “efficiently computable” proximal operator), and (iii) h is non-smooth. Our approach: In a nutshell, we generalize [4, 7] to the three composite case (1). For this purpose, we combine several classical and contemporary ideas: We exploit the smoothing technique in [8], the efficient implementation technique in [4, 14], the homotopy strategy in [9], and the nonuniform coordinate selection rule in [7] in our algorithm, to achieve the best known complexity estimate for the template.\nSurprisingly, the combination of these ideas is achieved in a very natural and elementary primal-dual gap-based framework. However, the extension is indeed not trivial since it requires to deal with the composition of a non-smooth function h and a linear operator A.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nWhile our work has connections to the methods developed in [7, 10, 11], it is rather distinct. First, we consider a more general problem (1) than the one in [4, 7, 10]. Second, our method relies on Nesterov’s accelerated scheme rather than a primal-dual method as in [11]. Moreover, we obtain the first rigorous convergence rate guarantees as opposed to [11]. In addition, we allow using any sampling distribution for choosing the coordinates.\nOur contributions: We propose a new smooth primal-dual randomized coordinate descent method for solving (1) where f is smooth, g is nonsmooth, separable and has a block-wise proximal operator, and h is a general nonsmooth function. Under such a structure, we show that our algorithm achieves the best known O(n/k) convergence rate, where k is the iteration count and to our knowledge, this is the first time that this convergence rate is proven for a coordinate descent algorithm.\nWe instantiate our algorithm to solve special cases of (1) including the case g = 0 and constrained problems. We analyze the convergence rate guarantees of these variants individually and discuss the choices of sampling distributions.\nExploiting the strategy in [4, 14], our algorithm can be implemented in parallel by breaking up the full vector updates. We also provide a restart strategy to enhance practical performance. Paper organization: We review some preliminary results in Section 2. The main contribution of this paper is in Section 3 with the main algorithm and its convergence guarantee. We also present special cases of the proposed algorithm. Section 4 provides numerical evidence to illustrate the performance of our algorithms in comparison to existing methods. The proofs are deferred to the supplementary document."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Notation: Let [n] := {1, 2, · · · , n} be the set of n positive integer indices. Let us decompose the variable vector x into n-blocks denoted by xi as x = [x1;x2; · · · ;xn] such that each block xi has the size pi ≥ 1 with ∑n i=1 pi = p. We also decompose the identity matrix Ip of Rp into n block as Ip = [U1, U2, · · · , Un], where Ui ∈ Rp×pi has pi unit vectors. In this case, any vector x ∈ Rp can be written as x = ∑n i=1 Uixi, and each block becomes xi = U > i x for i ∈ [n]. We define the partial gradients as ∇if(x) = U>i ∇f(x) for i ∈ [n]. For a convex function f , we use dom (f) to denote its domain, f∗(x) := supu { u>x− f(u) } to denote its Fenchel conjugate, and\nproxf (x) := arg minu { f(u) + (1/2)‖u− x‖2 } to denote its proximal operator. For a convex set X , δX (·) denotes its indicator function. We also need the following weighted norms: ‖xi‖2(i) = 〈Hixi, xi〉, (‖yi‖ ∗ (i)) 2 = 〈H−1i yi, yi〉,\n‖x‖2[α] = ∑n i=1 L α i ‖xi‖2(i), (‖y‖ ∗ [α]) 2 = ∑n i=1 L −α i (‖yi‖∗(i)) 2. (2)\nHere, Hi ∈ Rpi×pi is a symmetric positive definite matrix, and Li ∈ (0,∞) for i ∈ [n] and α > 0. In addition, we use ‖ · ‖ to denote ‖ · ‖2. Formal assumptions on the template: We require the following assumptions to tackle (1): Assumption 1. The functions f , g and h are all proper, closed and convex. Moreover, they satisfy\n(a) The partial derivative ∇if(·) of f is Lipschitz continuous with the Lipschitz constant L̂i ∈ [0,+∞), i.e., ‖∇if(x+ Uidi)−∇if(x)‖∗(i) ≤ L̂i‖di‖(i) for all x ∈ R\np, di ∈ Rpi . (b) The function g is separable, which has the following form g(x) = ∑n i=1 gi(xi). (c) One of the following assumptions for h holds for Subsections 3.3 and 3.4, respectively: i. h is Lipschitz continuous which is equivalent to the boundedness of dom (h∗).\nii. h is the indicator function for an equality constraint, i.e., h(Ax) := δ{c}(Ax).\nNow, we briefly describe the main techniques used in this paper. Acceleration: Acceleration techniques in convex optimization date back to the seminal work of Nesterov in [13], and is one of standard techniques in convex optimization. We exploit such a scheme to achieve the best known O(1/k) rate for the nonsmooth template (1). Nonuniform distribution: We assume that ξ is a random index on [n] associated with a probability distribution q = (q1, · · · , qn)> such that\nP {ξ = i} = qi > 0, i ∈ [n], and n∑ i=1 qi = 1. (3)\nWhen qi = 1n for all i ∈ [n], we obtain the uniform distribution. Let i0, i1, · · · , ik be i.i.d. realizations of the random index ξ after k iteration. We define Fk+1 = σ(i0, i1, · · · , ik) as the σ-field generated by these realizations. Smoothing techniques: We can write the convex function h(u) = supy {〈u, y〉 − h∗(y)} using its Fenchel conjugate h∗. Since h in (1) is convex but possibly nonsmooth, we smooth h as\nhβ(u) := max y∈Rm\n{ 〈u, y〉 − h∗(y)− β2 ‖y − ẏ‖ 2 } , (4)\nwhere ẏ ∈ Rm is given and β > 0 is the smoothness parameter. Moreover, the quadratic function b(y, ẏ) = 12‖y − ẏ‖\n2 is defined based on a given norm in Rm. Let us denote by y∗β(u), the unique solution of this concave maximization problem in (4), i.e.:\ny∗β(u) := arg max y∈Rm { 〈u, y〉 − h∗(y)− β2 ‖y − ẏ‖ 2 } = proxβ−1h∗ ( ẏ + β−1u ) , (5)\nwhere proxh∗ is the proximal operator of h ∗. If we assume that h is Lipschitz continuous, or equivalently that dom (h∗) is bounded, then it holds that\nhβ(u) ≤ h(u) ≤ hβ(u) + βD 2 h∗ 2 , where Dh∗ := maxy∈dom(h∗) ‖y − ẏ‖ < +∞. (6)\nLet us define a new smoothed function ψβ(x) := f(x) + hβ(Ax). Then, ψβ is differentiable, and its block partial gradient\n∇iψβ(x) = ∇if(x) +A>i y∗β(Ax) (7)\nis also Lipschitz continuous with the Lipschitz constant Li(β) := L̂i + ‖Ai‖2 β , where L̂i is given in Assumption 1, and Ai ∈ Rm×pi is the i-th block of A. Homotopy: In smoothing-based methods, the choice of the smoothness parameter is critical. This choice may require the knowledge of the desired accuracy, number of maximum iterations or the diameters of the primal and/or dual domains as in [8]. In order to make this choice flexible and our method applicable to the constrained problems, we employ a homotopy strategy developed in [9] for deterministic algorithms, to gradually update the smoothness parameter while making sure that it converges to 0."
    }, {
      "heading" : "3 Smooth primal-dual randomized coordinate descent",
      "text" : "In this section, we develop a smoothing primal-dual method to solve (1). Or approach is to combine the four key techniques mentioned above: smoothing, acceleration, homotopy, and randomized coordinate descent. Similar to [7] we allow to use arbitrary nonuniform distribution, which may allow to design a good distribution that captures the underlying structure of specific problems."
    }, {
      "heading" : "3.1 The algorithm",
      "text" : "Algorithm 1 below smooths, accelerates, and randomizes the coordinate descent method.\nAlgorithm 1. SMooth, Accelerate, Randomize The Coordinate Descent (SMART-CD)\nInput: Choose β1 > 0 and α ∈ [0, 1] as two input parameters. Choose x0 ∈ Rp. 1 Set B0i := L̂i + ‖Ai‖2 β1 for i ∈ [n]. Compute Sα := ∑n i=1(B 0 i ) α and qi := (B0i ) α Sα for all i ∈ [n].\n2 Set τ0 := min {qi | 1 ≤ i ≤ n} ∈ (0, 1] for i ∈ [n]. Set x̄0 = x̃0 := x0. 3 for k ← 0, 1, · · · , kmax do 4 Update x̂k := (1− τk)x̄k + τkx̃k and compute ûk := Ax̂k. 5 Compute the dual step y∗k := y ∗ βk+1 (ûk) = proxβ−1k+1h∗ ( ẏ + β−1k+1û k ) . 6 Select a block coordinate ik ∈ [n] according to the probability distribution q. 7 Set x̃k+1 := x̃k, and compute the primal ik-block coordinate:\nx̃k+1ik := argmin xik∈R pik\n{ 〈∇ikf(x̂k) +A>iky ∗ k, xik − x̂kik〉+ gik(xik) + τkB k ik\n2τ0 ‖xik − x̃kik‖ 2 (ik)\n} .\n8 Update x̄k+1 := x̂k + τkτ0 (x̃ k+1 − x̃k).\n9 Compute τk+1 ∈ (0, 1) as the unique positive root of τ3 + τ2 + τ2k τ − τ2k = 0. 10 Update βk+2 :=\nβk+1 1+τk+1 and Bk+1i := L̂i + ‖Ai‖2 βk+2\nfor i ∈ [n]. 11 end for\nFrom the update x̄k := x̂k−1 + τk−1τ0 (x̃ k − x̃k−1) and x̂k := (1− τk)x̄k + τkx̃k, it directly follows that x̂k := (1− τk) ( x̂k−1 + τk−1τ0 (x̃ k − x̃k−1) ) + τkx̃ k. Therefore, it is possible to implement the algorithm without forming x̄k."
    }, {
      "heading" : "3.2 Efficient implementation",
      "text" : "While the basic variant in Algorithm 1 requires full vector updates at each iteration, we exploit the idea in [4, 14] and show that we can partially update these vectors in a more efficient manner.\nAlgorithm 2. Efficient SMART-CD\nInput: Choose a parameter β1 > 0 and α ∈ [0, 1] as two input parameters. Choose x0 ∈ Rp. 1 Set B0i := L̂i + ‖Ai‖2 β1 for i ∈ [n]. Compute Sα := ∑n i=1(B 0 i ) α and qi := (B0i ) α Sα for all i ∈ [n].\n2 Set τ0 := min {qi | 1 ≤ i ≤ n} ∈ (0, 1] for i ∈ [n] and c0 = (1− τ0). Set u0 = z̃0 := x0. 3 for k ← 0, 1, · · · , kmax do 4 Compute the dual step y∗βk+1(ckAu k +Az̃k) := proxβ−1k+1h∗ ( ẏ + β−1k+1(ckAu k +Az̃k) ) . 5 Select a block coordinate ik ∈ [n] according to the probability distribution q. 6 Let ∇ki := ∇ikf(ckuk + z̃k) +A>iky ∗ βk+1 (ckAu k +Az̃k). Compute\ntk+1ik := arg min t∈Rpik\n{ 〈∇ki , t〉+ gik(t+ z̃kik) + τkB k ik 2τ0 ‖t‖2(ik) } .\n7 Update z̃k+1ik := z̃ k ik + tk+1ik . 8 Update uk+1ik := u k ik − 1−τk/τ0ck t k+1 ik\n. 9 Compute τk+1 ∈ (0, 1) as the unique positive root of τ3 + τ2 + τ2k τ − τ2k = 0.\n10 Update βk+2 := βk+1 1+τk+1 and Bk+1i := L̂i + ‖Ai‖2 βk+2\nfor i ∈ [n]."
    }, {
      "heading" : "11 end for",
      "text" : "We present the following result which shows the equivalence between Algorithm 1 and Algorithm 2, the proof of which can be found in the supplementary document.\nProposition 3.1. Let ck = ∏k l=0(1− τl), ẑk = ckuk + z̃k and z̄k = ck−1uk + z̃k. Then, x̃k = z̃k, x̂k = ẑk and x̄k = z̄k, for all k ≥ 0, where x̃k, x̂k, and x̄k are defined in Algorithm 1. According to Algorithm 2, we never need to form or update full-dimensional vectors. Only times that we need x̂k are when computing the gradient and the dual variable y∗βk+1 . We present two special cases which are common in machine learning, in which we can compute these steps efficiently.\nRemark 3.2. Under the following assumptions, we can characterize the per-iteration complexity explicitly. Let A,M ∈ Rm×p, and\n(a) f has the form f(x) = ∑m j=1 φj(e > j Mx), where ej is the j th standard unit vector.\n(b) h is separable as in h(Ax) = δ{c}(Ax) or h(Ax) = ‖Ax‖1.\nAssuming that we store and maintain the residuals rku,f = Mu k, rkz̃,f = Mz̃ k, rku,h = Au k, rkz̃,h = Az̃ k, then we have the per-iteration cost as O(max{|{j | Aji 6= 0}|, |{j | Mji 6= 0}|}) arithmetic operations. If h is partially separable as in [3], then the complexity of each iteration will remain moderate."
    }, {
      "heading" : "3.3 Case 1: Convergence analysis of SMART-CD for Lipschitz continuous h",
      "text" : "We provide the following main theorem, which characterizes the convergence rate of Algorithm 1.\nTheorem 3.3. Let x? be an optimal solution of (1) and let β1 > 0 be given. In addition, let τ0 := min {qi | i ∈ [n]} ∈ (0, 1] and β0 := (1 + τ0)β1 be given parameters. For all k ≥ 1, the sequence { x̄k } generated by Algorithm 1 satisfies:\nE [ F (x̄k)− F ? ] ≤ C ∗(x0)\nτ0(k − 1) + 1 + β1(1 + τ0)D\n2 h∗\n2(τ0k + 1) , (8)\nwhere C∗(x0) := (1− τ0)(Fβ0(x0)− F ?) + ∑n i=1 τ0B 0 i 2qi ‖x?i − x0i ‖2(i) and Dh∗ is as defined by (6).\nIn the special case when we use uniform distribution, τ0 = qi = 1/n, the convergence rate reduces to\nE [ F (x̄k)− F ? ] ≤ nC ∗(x0)\nk + n− 1 +\n(n+ 1)β0D 2 h∗\n2k + 2n ,\nwhere C∗(x0) := (1 − 1n )(Fβ0(x 0) − F ?) + ∑n i=1 B0i 2 ‖x ? i − x0i ‖2(i). This estimate shows that the\nconvergence rate of Algorithm 1 is O (n k ) ,\nwhich is the best known so far to the best of our knowledge."
    }, {
      "heading" : "3.4 Case 2: Convergence analysis of SMART-CD for non-smooth constrained optimization",
      "text" : "In this section, we instantiate Algorithm 1 to solve constrained convex optimization problem with possibly non-smooth terms in the objective. Clearly, if we choose h(·) = δ{c}(·) in (1) as the indicator function of the set {c} for a given vector c ∈ Rm, then we obtain a constrained problem:\nF ? := min x∈Rp {F (x) = f(x) + g(x) | Ax = c} , (9)\nwhere f and g are defined as in (1), A ∈ Rm×p, and c ∈ Rm. We can specify Algorithm 1 to solve this constrained problem by modifying the following two steps:\n(a) The update of y∗βk+1(Ax̂ k) at Step 5 is changed to\ny∗βk+1(Ax̂ k) := ẏ + 1βk+1 (Ax̂ k − c), (10)\nwhich requires one matrix-vector multiplication in Ax̂k. (b) The update of τk at Step 9 and βk+1 at Step 10 are changed to\nτk+1 := τk 1+τk and βk+2 := (1− τk+1)βk+1. (11)\nNow, we analyze the convergence of this algorithm by providing the following theorem. Theorem 3.4. Let { x̄k }\nbe the sequence generated by Algorithm 1 for solving (9) using the updates (10) and (11) and let y? be an arbitrary optimal solution of the dual problem of (9). In addition, let τ0 := min {qi | i ∈ [n]} ∈ (0, 1] and β0 := (1 + τ0)β1 be given parameters. Then, we have the following estimates: E [ F (x̄k)− F ? ] ≤ C ∗(x0) τ0(k−1)+1 + β1‖y?−ẏ‖2 2(τ0(k−1)+1) + ‖y ?‖E [ ‖Ax̄k − b‖ ] , E [ ‖Ax̄k − b‖ ] ≤ β1τ0(k−1)+1 [ ‖y? − ẏ‖+ ( ‖y? − ẏ‖2 + 2β−11 C∗(x0) )1/2] , (12)\nwhere C∗(x0) := (1 − τ0)(Fβ0(x0) − F ?) + ∑n i=1 τ0B 0 i 2qi ‖x?i − x0i ‖2(i). We note that the following\nlower bound always holds −‖y?‖E [ ‖Ax̄k − b‖ ] ≤ E [ F (x̄k)− F ? ] ."
    }, {
      "heading" : "3.5 Other special cases",
      "text" : "We consider the following special cases of Algorithm 1: The case h = 0: In this case, we obtain an algorithm similar to the one studied in [7] except that we have non-uniform sampling instead of importance sampling. If the distribution is uniform, then we obtain the method in [4]. The case g = 0: In this case, we have F (x) = f(x) + h(Ax), which can handle the linearly constrained problems with smooth objective function. In this case, we can choose τ0 = 1, and the coordinate proximal gradient step, Step 7 in Algorithm 1, is simplified as\nx̃k+1ik := x̃ k ik − qik τkBkik H−1ik\n( ∇ikf(x̂k) +A>iky ∗ βk+1 (ûk) ) . (13)\nIn addition, we replace Step 8 with\nx̄k+1i = x̂ k i + τk qi (x̃k+1i − x̃ k i ), ∀i ∈ [n]. (14)\nWe then obtain the following results:\nCorollary 3.5. Assume that Assumption 1 holds. Let τ0 = 1, β1 > 0 and Step 7 and 8 of Algorithm 1 be updated by (13) and (14), respectively. If, in addition, h is Lipschitz continuous, then we have\nE [ F (x̄k)− F ? ] ≤ 1 k n∑ i=1 B0i 2q2i ‖x?i − x0i ‖2(i) + β1D 2 h∗ k + 1 , (15)\nwhere Dh∗ is defined by (6).\nIf, instead of Lipschitz continuous h, we have h(·) = δ{c}(·) to solve the constrained problem (9) with g = 0, then we have E [ F (x̄k)− F ? ] ≤ C ∗(x0) k + β1‖y?−ẏ‖2 2k + ‖y ?‖E [ ‖Ax̄k − b‖ ] , E [ ‖Ax̄k − b‖ ] ≤ β1k [ ‖y? − ẏ‖+ ( ‖y? − ẏ‖2 + 2β−11 C∗(x0) )1/2] , (16) where C∗(x0) := n∑ i=1 B0i 2q2i ‖x?i − x0i ‖2(i)."
    }, {
      "heading" : "3.6 Restarting SMART-CD",
      "text" : "It is known that restarting an accelerated method significantly enhances its practical performance when the underlying problem admits a (restricted) strong convexity condition. As a result, we describe below how to restart (i.e., the momentum term) in Efficient SMART-CD. If the restart is injected in the k-th iteration, then we restart the algorithm with the following steps: uk+1 ← 0, rk+1u,f ← 0, rk+1u,h ← 0, ẏ ← y∗βk+1(ckr k u,h + r k z̃,h),\nβk+1 ← β1, τk+1 ← τ0, ck ← 1.\nThe first three steps of the restart procedure is for restarting the primal variable which is classical [15]. Restarting ẏ is also suggested in [9]. The cost of this procedure is essentially equal to the cost of one iteration as described in Remark 3.2, therefore even restarting once every epoch will not cause a significant difference in terms of per-iteration cost."
    }, {
      "heading" : "4 Numerical evidence",
      "text" : "We illustrate the performance of Efficient SMART-CD in brain imaging and support vector machines applications. We also include one representative example of a degenerate linear program to illustrate why the convergence rate guarantees of our algorithm matter. We compare SMART-CD with VuCondat-CD [11], which is a coordinate descent variant of Vu-Condat’s algorithm [16], FISTA [17], ASGARD [9], Chambolle-Pock’s primal-dual algorithm [18], L-BFGS [19] and SDCA [5]."
    }, {
      "heading" : "4.1 A degenerate linear program: Why do convergence rate guarantees matter?",
      "text" : "We consider the following degenerate linear program studied in [9]: min x∈Rp 2xp s.t. ∑p−1 k=1 xk = 1, xp − ∑p−1 k=1 xk = 0, (2 ≤ j ≤ d),\nxp ≥ 0.\n(17)\nHere, the constraint xp − ∑p−1 k=1 xk = 0 is repeated d times. This problem satisfies the linear constraint qualification condition, which guarantees the primal-dual optimality. If we define\nf(x) = 2xp, g(x) = δ{xp≥0}(xp), h(Ax) = δ{c}(Ax),\nwhere\nAx = [ p−1∑ k=1 xk, xp − p−1∑ k=1 xk, . . . , xp − p−1∑ k=1 xk ]> , c = [1, 0, . . . , 0]>,\nwe can fit this problem and its dual form into our template (1).\nFor this experiment, we select the dimensions p = 10 and d = 200. We implement our algorithm and compare it with Vu-Condat-CD. We also combine our method with the restarting strategy proposed above. We use the same mapping to fit the problem into the template of Vu-Condat-CD.\nFigure 1 illustrates the convergence behavior of Vu-Condat-CD and SMART-CD. We compare primal suboptimality and feasibility in the plots. The explicit solution of the problem is used to generate the plot with primal suboptimality. We observe that degeneracy of the problem prevents Vu-Condat-CD from making any progress towards the solution, where SMART-CD preservesO(1/k) rate as predicted by theory. We emphasize that the authors in [11] proved almost sure convergence for Vu-Condat-CD but they did not provide a convergence rate guarantee for this method. Since the problem is certainly non-strongly convex, restarting does not significantly improve performance of SMART-CD.\n4.2 Total Variation and `1-regularized least squares regression with functional MRI data In this experiment, we consider a computational neuroscience application where prediction is done based on a sequence of functional MRI images. Since the images are high dimensional and the number of samples that can be taken is limited, TV-`1 regularization is used to get stable and predictive estimation results [20]. The convex optimization problem we solve is of the form:\nmin x∈Rp\n1 2‖Mx− b‖ 2 + λr‖x‖1 + λ(1− r)‖x‖TV. (18)\nThis problem fits to our template with\nf(x) = 12‖Mx− b‖ 2, g(x) = λr‖x‖1, h(u) = λ(1− r)‖u‖1,\nwhere D is the 3D finite difference operator to define a total variation norm ‖ · ‖TV and u = Dx. We use an fMRI dataset where the primal variable x is 3D image of the brain that contains 33177 voxels. Feature matrix M has 768 rows, each representing the brain activity for the corresponding example [20]. We compare our algorithm with Vu-Condat’s algorithm, FISTA, ASGARD, ChambollePock’s primal-dual algorithm, L-BFGS and Vu-Condat-CD.\nFigure 2 illustrates the convergence behaviour of the algorithms for different values of the regularization parameters. Per-iteration cost of SMART-CD and Vu-Condat-CD is similar, therefore the behavior of these two algorithms are quite similar in this experiment. Since Vu-Condat’s,\nChambolle-Pock’s, FISTA and ASGARD methods work with full dimensional variables, they have slow convergence in time. L-BFGS has a close performance to coordinate descent methods.\nThe simulation in Figure 2 is performed using benchmarking tool of [20]. The algorithms are tuned for the best parameters in practice."
    }, {
      "heading" : "4.3 Linear support vector machines problem with bias",
      "text" : "In this section, we consider an application of our algorithm to support vector machines (SVM) problem for binary classification. Given a training set with m examples {a1, a2, . . . , am} such that ai ∈ Rp and class labels {b1, b2, . . . bm} such that bi ∈ {−1,+1}, we define the soft margin primal support vector machines problem with bias as\nmin w∈Rp m∑ i=1 Ci max ( 0, 1− bi(〈ai, w〉+ w0) ) + λ2 ‖w‖ 2. (19)\nAs it is a common practice, we solve its dual formulation, which is a constrained problem: minx∈Rm { 1 2λ‖MD(b)x‖ 2 − ∑m i=1 xi } s.t. 0 ≤ xi ≤ Ci, i = 1, · · · ,m, b>x = 0,\n(20)\nwhere D(b) represents a diagonal matrix that has the class labels bi in its diagonal and M ∈ Rp×m is formed by the example vectors. If we define\nf(x) = 1\n2λ ‖MD(b)x‖2 − m∑ i=1 xi, gi(xi) = δ{0≤xi≤Ci}, c = 0, A = b >,\nthen, we can fit this problem into our template in (9). We apply the specific version of SMART-CD for constrained setting from Section 3.4 and compare with Vu-Condat-CD and SDCA. Even though SDCA is a state-of-the-art method for SVMs, we are not able to handle the bias term using SDCA. Hence, it only applies to (20) when b>x = 0 constraint is removed. This causes SDCA not to converge to the optimal solution when there is bias term in the problem (19). The following table summarizes the properties of the classification datasets we used.\nData Set Training Size Number of Features Convergence Plot rcv1.binary [21, 22] 20,242 47,236 Figure 3, plot 1 a8a [21, 23] 22,696 123 Figure 3, plot 2 gisette [21, 24] 6,000 5,000 Figure 3, plot 3\nFigure 3 illustrates the performance of the algorithms for solving the dual formulation of SVM in (20). We compute the duality gap for each algorithm and present the results with epochs in the horizontal axis since per-iteration complexity of the algorithms is similar. As expected, SDCA gets stuck at a low accuracy since it ignores one of the constraints in the problem. We demonstrate this fact in the first experiment and then limit the comparison to SMART-CD and Vu-Condat-CD. Equipped with restart strategy, SMART-CD shows the fastest convergence behavior due to the restricted strong convexity of (20).\n100 101 102\nepoch\n10-4\n10-3\n10-2\n10-1\n100\nD ua\nlit y\nga p\nSMART-CD SMART-CD-Restart Vu-Condat-CD SDCA\n100 101 102\nepoch\n10-4\n10-3\n10-2\n10-1\n100\nD ua\nlit y\nga p\nSMART-CD SMART-CD-Restart Vu-Condat-CD\n100 101 102\nepoch\n10-5\n10-4\n10-3\n10-2\n10-1\n100\nD ua\nlit y\nga p\nSMART-CD SMART-CD-Restart Vu-Condat-CD\nFigure 3: The convergence of 4 algorithms on the dual SVM (20) with bias. We only used SDCA in the first dataset since it stagnates at a very low accuracy."
    }, {
      "heading" : "5 Conclusions",
      "text" : "Coordinate descent methods have been increasingly deployed to tackle huge scale machine learning problems in recent years. The most notable works include [1–6]. Our method relates to several works\nin the literature including [1, 4, 7, 9, 10, 12]. The algorithms developed in [2–4] only considered a special case of (1) with h = 0, and cannot be trivially extended to apply to general setting (1). Here, our algorithm can be viewed as an adaptive variant of the method developed in [4] extended to the sum of three functions. The idea of homotopy strategies relate to [9] for first-order primal-dual methods. This paper further extends such an idea to randomized coordinate descent methods for solving (1). We note that a naive application of the method developed in [4] to the smoothed problem with a carefully chosen fixed smoothness parameter would result in the complexityO(n2/k), whereas using our homotopy strategy on the smoothness parameter, we reduced this complexity to O(n/k). With additional strong convexity assumption on problem template (1), it is possible to obtainO(1/k2) rate by using deterministic first-order primal-dual algorithms [9, 18]. It remains as future work to incorporate strong convexity to coordinate descent methods for solving nonsmooth optimization problems with a faster convergence rate."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The work of O. Fercoq was supported by a public grant as part of the Investissement d’avenir project, reference ANR-11-LABX-0056-LMH, LabEx LMH. The work of Q. Tran-Dinh was partly supported by NSF grant, DMS-1619884, USA. The work of A. Alacaoglu and V. Cevher was supported by European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement no 725594 - time-data)."
    } ],
    "references" : [ {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Y. Nesterov" ],
      "venue" : "SIAM Journal on Optimization, vol. 22, no. 2, pp. 341–362, 2012.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
      "author" : [ "P. Richtárik", "M. Takáč" ],
      "venue" : "Mathematical Programming, vol. 144, no. 1-2, pp. 1–38, 2014.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Parallel coordinate descent methods for big data optimization",
      "author" : [ "P. Richtárik", "M. Takáč" ],
      "venue" : "Mathematical Programming, vol. 156, no. 1-2, pp. 433–484, 2016.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Accelerated, parallel, and proximal coordinate descent",
      "author" : [ "O. Fercoq", "P. Richtárik" ],
      "venue" : "SIAM Journal on Optimization, vol. 25, no. 4, pp. 1997–2023, 2015.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research, vol. 14, pp. 567–599, 2013.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Parallel random coordinate descent method for composite minimization: Convergence analysis and error bounds",
      "author" : [ "I. Necoara", "D. Clipici" ],
      "venue" : "SIAM J. on Optimization, vol. 26, no. 1, pp. 197–226, 2016.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Coordinate descent with arbitrary sampling i: Algorithms and complexity",
      "author" : [ "Z. Qu", "P. Richtárik" ],
      "venue" : "Optimization Methods and Software, vol. 31, no. 5, pp. 829–857, 2016.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Math. Prog., vol. 103, no. 1, pp. 127–152, 2005.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A smooth primal-dual optimization framework for nonsmooth composite convex minimization",
      "author" : [ "Q. Tran-Dinh", "O. Fercoq", "V. Cevher" ],
      "venue" : "arXiv preprint arXiv:1507.06243, 2015.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Smooth minimization of nonsmooth functions with parallel coordinate descent methods",
      "author" : [ "O. Fercoq", "P. Richtárik" ],
      "venue" : "arXiv preprint arXiv:1309.5885, 2013.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A coordinate descent primal-dual algorithm with large step size and possibly non separable functions",
      "author" : [ "O. Fercoq", "P. Bianchi" ],
      "venue" : "arXiv preprint arXiv:1508.04625, 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Efficiency of the accelerated coordinate descent method on structured optimization problems",
      "author" : [ "Y. Nesterov", "S.U. Stich" ],
      "venue" : "SIAM J. on Optimization, vol. 27, no. 1, pp. 110–123, 2017.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "A method for unconstrained convex minimization problem with the rate of convergence O(1/k2)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Doklady AN SSSR, vol. 269, translated as Soviet Math. Dokl., pp. 543– 547, 1983.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems",
      "author" : [ "Y.T. Lee", "A. Sidford" ],
      "venue" : "Foundations of Computer Science (FOCS), 2013 IEEE Annual Symp. on, pp. 147–156, IEEE, 2013. 9",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Adaptive restart for accelerated gradient schemes",
      "author" : [ "B. O’Donoghue", "E. Candes" ],
      "venue" : "Foundations of computational mathematics, vol. 15, no. 3, pp. 715–732, 2015.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A splitting algorithm for dual monotone inclusions involving cocoercive operators",
      "author" : [ "B.C. Vũ" ],
      "venue" : "Advances in Computational Mathematics, vol. 38, no. 3, pp. 667–681, 2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM journal on imaging sciences, vol. 2, no. 1, pp. 183–202, 2009.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A first-order primal-dual algorithm for convex problems with applications to imaging",
      "author" : [ "A. Chambolle", "T. Pock" ],
      "venue" : "Journal of mathematical imaging and vision, vol. 40, no. 1, pp. 120– 145, 2011.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A limited memory algorithm for bound constrained optimization",
      "author" : [ "R.H. Byrd", "P. Lu", "J. Nocedal", "C. Zhu" ],
      "venue" : "SIAM Journal on Scientific Computing, vol. 16, no. 5, pp. 1190–1208, 1995.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Benchmarking solvers for tv-`1 least-squares and logistic regression in brain imaging",
      "author" : [ "E.D. Dohmatob", "A. Gramfort", "B. Thirion", "G. Varoquaux" ],
      "venue" : "Pattern Recognition in Neuroimaging, 2014 International Workshop on, pp. 1–4, IEEE, 2014.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Libsvm: a library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM transactions on intelligent systems and technology (TIST), vol. 2, no. 3, p. 27, 2011.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Rcv1: A new benchmark collection for text categorization research",
      "author" : [ "D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li" ],
      "venue" : "Journal of Machine Learning Research, vol. 5, no. Apr, pp. 361–397, 2004.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "UCI machine learning repository",
      "author" : [ "M. Lichman" ],
      "venue" : "2013.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Result analysis of the nips 2003 feature selection challenge",
      "author" : [ "I. Guyon", "S. Gunn", "A. Ben-Hur", "G. Dror" ],
      "venue" : "Advances in neural information processing systems, pp. 545–552, 2005.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "On accelerated proximal gradient methods for convex-concave optimization",
      "author" : [ "P. Tseng" ],
      "venue" : "Submitted to SIAM J. Optim, 2008. 10",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "These methods are particularly well-suited to solve huge-scale problems arising from machine learning applications where matrix-vector operations are prohibitive [1].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "For such a theoretical development, coordinate descent algorithms require specific assumptions on the convex optimization problems [1, 4, 6].",
      "startOffset" : 131,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "For such a theoretical development, coordinate descent algorithms require specific assumptions on the convex optimization problems [1, 4, 6].",
      "startOffset" : 131,
      "endOffset" : 140
    }, {
      "referenceID" : 5,
      "context" : "For such a theoretical development, coordinate descent algorithms require specific assumptions on the convex optimization problems [1, 4, 6].",
      "startOffset" : 131,
      "endOffset" : 140
    }, {
      "referenceID" : 3,
      "context" : "Our approach: In a nutshell, we generalize [4, 7] to the three composite case (1).",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "Our approach: In a nutshell, we generalize [4, 7] to the three composite case (1).",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 7,
      "context" : "For this purpose, we combine several classical and contemporary ideas: We exploit the smoothing technique in [8], the efficient implementation technique in [4, 14], the homotopy strategy in [9], and the nonuniform coordinate selection rule in [7] in our algorithm, to achieve the best known complexity estimate for the template.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "For this purpose, we combine several classical and contemporary ideas: We exploit the smoothing technique in [8], the efficient implementation technique in [4, 14], the homotopy strategy in [9], and the nonuniform coordinate selection rule in [7] in our algorithm, to achieve the best known complexity estimate for the template.",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "For this purpose, we combine several classical and contemporary ideas: We exploit the smoothing technique in [8], the efficient implementation technique in [4, 14], the homotopy strategy in [9], and the nonuniform coordinate selection rule in [7] in our algorithm, to achieve the best known complexity estimate for the template.",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 8,
      "context" : "For this purpose, we combine several classical and contemporary ideas: We exploit the smoothing technique in [8], the efficient implementation technique in [4, 14], the homotopy strategy in [9], and the nonuniform coordinate selection rule in [7] in our algorithm, to achieve the best known complexity estimate for the template.",
      "startOffset" : 190,
      "endOffset" : 193
    }, {
      "referenceID" : 6,
      "context" : "For this purpose, we combine several classical and contemporary ideas: We exploit the smoothing technique in [8], the efficient implementation technique in [4, 14], the homotopy strategy in [9], and the nonuniform coordinate selection rule in [7] in our algorithm, to achieve the best known complexity estimate for the template.",
      "startOffset" : 243,
      "endOffset" : 246
    }, {
      "referenceID" : 6,
      "context" : "While our work has connections to the methods developed in [7, 10, 11], it is rather distinct.",
      "startOffset" : 59,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "While our work has connections to the methods developed in [7, 10, 11], it is rather distinct.",
      "startOffset" : 59,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "While our work has connections to the methods developed in [7, 10, 11], it is rather distinct.",
      "startOffset" : 59,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "First, we consider a more general problem (1) than the one in [4, 7, 10].",
      "startOffset" : 62,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "First, we consider a more general problem (1) than the one in [4, 7, 10].",
      "startOffset" : 62,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "First, we consider a more general problem (1) than the one in [4, 7, 10].",
      "startOffset" : 62,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : "Second, our method relies on Nesterov’s accelerated scheme rather than a primal-dual method as in [11].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "Moreover, we obtain the first rigorous convergence rate guarantees as opposed to [11].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "Exploiting the strategy in [4, 14], our algorithm can be implemented in parallel by breaking up the full vector updates.",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : "Exploiting the strategy in [4, 14], our algorithm can be implemented in parallel by breaking up the full vector updates.",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "Acceleration: Acceleration techniques in convex optimization date back to the seminal work of Nesterov in [13], and is one of standard techniques in convex optimization.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "This choice may require the knowledge of the desired accuracy, number of maximum iterations or the diameters of the primal and/or dual domains as in [8].",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "In order to make this choice flexible and our method applicable to the constrained problems, we employ a homotopy strategy developed in [9] for deterministic algorithms, to gradually update the smoothness parameter while making sure that it converges to 0.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "Similar to [7] we allow to use arbitrary nonuniform distribution, which may allow to design a good distribution that captures the underlying structure of specific problems.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "2 Efficient implementation While the basic variant in Algorithm 1 requires full vector updates at each iteration, we exploit the idea in [4, 14] and show that we can partially update these vectors in a more efficient manner.",
      "startOffset" : 137,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "2 Efficient implementation While the basic variant in Algorithm 1 requires full vector updates at each iteration, we exploit the idea in [4, 14] and show that we can partially update these vectors in a more efficient manner.",
      "startOffset" : 137,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "If h is partially separable as in [3], then the complexity of each iteration will remain moderate.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "5 Other special cases We consider the following special cases of Algorithm 1: The case h = 0: In this case, we obtain an algorithm similar to the one studied in [7] except that we have non-uniform sampling instead of importance sampling.",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 3,
      "context" : "If the distribution is uniform, then we obtain the method in [4].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "The first three steps of the restart procedure is for restarting the primal variable which is classical [15].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "Restarting ẏ is also suggested in [9].",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "We compare SMART-CD with VuCondat-CD [11], which is a coordinate descent variant of Vu-Condat’s algorithm [16], FISTA [17], ASGARD [9], Chambolle-Pock’s primal-dual algorithm [18], L-BFGS [19] and SDCA [5].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 15,
      "context" : "We compare SMART-CD with VuCondat-CD [11], which is a coordinate descent variant of Vu-Condat’s algorithm [16], FISTA [17], ASGARD [9], Chambolle-Pock’s primal-dual algorithm [18], L-BFGS [19] and SDCA [5].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : "We compare SMART-CD with VuCondat-CD [11], which is a coordinate descent variant of Vu-Condat’s algorithm [16], FISTA [17], ASGARD [9], Chambolle-Pock’s primal-dual algorithm [18], L-BFGS [19] and SDCA [5].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "We compare SMART-CD with VuCondat-CD [11], which is a coordinate descent variant of Vu-Condat’s algorithm [16], FISTA [17], ASGARD [9], Chambolle-Pock’s primal-dual algorithm [18], L-BFGS [19] and SDCA [5].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "We compare SMART-CD with VuCondat-CD [11], which is a coordinate descent variant of Vu-Condat’s algorithm [16], FISTA [17], ASGARD [9], Chambolle-Pock’s primal-dual algorithm [18], L-BFGS [19] and SDCA [5].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 18,
      "context" : "We compare SMART-CD with VuCondat-CD [11], which is a coordinate descent variant of Vu-Condat’s algorithm [16], FISTA [17], ASGARD [9], Chambolle-Pock’s primal-dual algorithm [18], L-BFGS [19] and SDCA [5].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 4,
      "context" : "We compare SMART-CD with VuCondat-CD [11], which is a coordinate descent variant of Vu-Condat’s algorithm [16], FISTA [17], ASGARD [9], Chambolle-Pock’s primal-dual algorithm [18], L-BFGS [19] and SDCA [5].",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 8,
      "context" : "1 A degenerate linear program: Why do convergence rate guarantees matter? We consider the following degenerate linear program studied in [9]:  min x∈Rp 2xp s.",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 10,
      "context" : "We emphasize that the authors in [11] proved almost sure convergence for Vu-Condat-CD but they did not provide a convergence rate guarantee for this method.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : "Since the images are high dimensional and the number of samples that can be taken is limited, TV-`1 regularization is used to get stable and predictive estimation results [20].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 19,
      "context" : "Feature matrix M has 768 rows, each representing the brain activity for the corresponding example [20].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "The simulation in Figure 2 is performed using benchmarking tool of [20].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : "binary [21, 22] 20,242 47,236 Figure 3, plot 1 a8a [21, 23] 22,696 123 Figure 3, plot 2 gisette [21, 24] 6,000 5,000 Figure 3, plot 3",
      "startOffset" : 7,
      "endOffset" : 15
    }, {
      "referenceID" : 21,
      "context" : "binary [21, 22] 20,242 47,236 Figure 3, plot 1 a8a [21, 23] 22,696 123 Figure 3, plot 2 gisette [21, 24] 6,000 5,000 Figure 3, plot 3",
      "startOffset" : 7,
      "endOffset" : 15
    }, {
      "referenceID" : 20,
      "context" : "binary [21, 22] 20,242 47,236 Figure 3, plot 1 a8a [21, 23] 22,696 123 Figure 3, plot 2 gisette [21, 24] 6,000 5,000 Figure 3, plot 3",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : "binary [21, 22] 20,242 47,236 Figure 3, plot 1 a8a [21, 23] 22,696 123 Figure 3, plot 2 gisette [21, 24] 6,000 5,000 Figure 3, plot 3",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 20,
      "context" : "binary [21, 22] 20,242 47,236 Figure 3, plot 1 a8a [21, 23] 22,696 123 Figure 3, plot 2 gisette [21, 24] 6,000 5,000 Figure 3, plot 3",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 23,
      "context" : "binary [21, 22] 20,242 47,236 Figure 3, plot 1 a8a [21, 23] 22,696 123 Figure 3, plot 2 gisette [21, 24] 6,000 5,000 Figure 3, plot 3",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "Here, our algorithm can be viewed as an adaptive variant of the method developed in [4] extended to the sum of three functions.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : "The idea of homotopy strategies relate to [9] for first-order primal-dual methods.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "We note that a naive application of the method developed in [4] to the smoothed problem with a carefully chosen fixed smoothness parameter would result in the complexityO(n(2)/k), whereas using our homotopy strategy on the smoothness parameter, we reduced this complexity to O(n/k).",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "With additional strong convexity assumption on problem template (1), it is possible to obtainO(1/k(2)) rate by using deterministic first-order primal-dual algorithms [9, 18].",
      "startOffset" : 166,
      "endOffset" : 173
    }, {
      "referenceID" : 17,
      "context" : "With additional strong convexity assumption on problem template (1), it is possible to obtainO(1/k(2)) rate by using deterministic first-order primal-dual algorithms [9, 18].",
      "startOffset" : 166,
      "endOffset" : 173
    } ],
    "year" : 2017,
    "abstractText" : "We propose a new randomized coordinate descent method for a convex optimization template with broad applications. Our analysis relies on a novel combination of four ideas applied to the primal-dual gap function: smoothing, acceleration, homotopy, and coordinate descent with non-uniform sampling. As a result, our method features the first convergence rate guarantees among the coordinate descent methods, that are the best-known under a variety of common structure assumptions on the template. We provide numerical evidence to support the theoretical results with a comparison to state-of-the-art algorithms.",
    "creator" : null
  }
}