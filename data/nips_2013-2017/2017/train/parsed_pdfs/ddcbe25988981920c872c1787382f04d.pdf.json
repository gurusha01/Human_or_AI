{
  "name" : "ddcbe25988981920c872c1787382f04d.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Approximation Algorithms for String Kernel Based Sequence Classification",
    "authors" : [ "Muhammad Farhan" ],
    "emails" : [ "14030031@lums.edu.pk", "jtariq@emory.edu", "arifz@lums.edu.pk", "mudassir.shabbir@itu.edu.pk", "imdad.khan@lums.edu.pk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Sequence classification is a fundamental task in pattern recognition, machine learning, and data mining with numerous applications in bioinformatics, text mining, and natural language processing. Detecting proteins homology (shared ancestry measured from similarity of their sequences of amino\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nacids) and predicting proteins fold (functional three dimensional structure) are essential tasks in bioinformatics. Sequence classification algorithms have been applied to both of these problems with great success [3, 10, 13, 18, 19, 20, 25]. Music data, a real valued signal when discretized using vector quantization of MFCC features is another flavor of sequential data [26]. Sequence classification has been used for recognizing genres of music sequences with no annotation and identifying artists from albums [12, 13, 14]. Text documents can also be considered as sequences of words from a language lexicon. Categorizing texts into classes based on their topics is another application domain of sequence classification [11, 15].\nWhile general purpose classification methods may be applicable to sequence classification, huge lengths of sequences, large alphabet sizes, and large scale datasets prove to be rather challenging for such techniques. Furthermore, we cannot directly apply classification algorithms devised for vectors in metric spaces because in almost all practical scenarios sequences have varying lengths unless some mapping is done beforehand. In one of the more successful approaches, the variable-length sequences are represented as fixed dimensional feature vectors. A feature vector typically is the spectra (counts) of all k-length substrings (k-mers) present exactly [18] or inexactly (with up to m mismatches) [19] within a sequence. A kernel function is then defined that takes as input a pair of feature vectors and returns a real-valued similarity score between the pair (typically inner-product of the respective spectra’s). The matrix of pairwise similarity scores (the kernel matrix) thus computed is used as input to a standard support vector machine (SVM) [5, 27] classifier resulting in excellent classification performance in many applications [19]. In this setting k (the length of substrings used as bases of feature map) and m (the mismatch parameter) are independent variables directly related to classification accuracy and time complexity of the algorithm. It has been established that using larger values of k and m improve classification performance [11, 13]. On the other hand, the runtime of kernel computation by the efficient trie-based algorithm [19, 24] is O(km+1|Σ|m(|X|+ |Y |)) for two sequences X and Y over alphabet Σ.\nComputation of mismatch kernel between two sequences X and Y reduces to the following two problems. i) Given two k-mers α and β that are at Hamming distance d from each other, determine the size of intersection of m-mismatch neighborhoods of α and β (k-mers that are at distance at most m from both of them). ii) For 0 ≤ d ≤ min{2m, k} determine the number of pairs of k-mers (α, β) ∈ X × Y such that Hamming distance between α and β is d. In the best known algorithm [13] the former problem is addressed by precomputing the intersection size in constant time for m ≤ 2 only. While a sorting and enumeration based technique is proposed for the latter problem that has computational complexity O(2k(|X|+ |Y |), which makes it applicable for moderately large values of k (of course limited to m ≤ 2 only). In this paper, we completely resolve the combinatorial problem (problem i) for all values of m. We prove a closed form expression for the size of intersection of m-mismatch neighborhoods that lets us precompute these values in O(m3) time (independent of |Σ|, k, lengths and number of sequences). For the latter problem we devise an efficient approximation scheme inspired by the theory of locality sensitive hashing to accurately estimate the number of k-mer pairs between the two sequences that are at distance d. Combining the above two we design a polynomial time approximation algorithm for kernel computation. We provide probabilistic guarantees on the quality of our algorithm and analytical bounds on its runtime. Furthermore, we test our algorithm on several real world datasets with large values of k and m to demonstrate that we achieve excellent predictive performance. Note that string kernel based sequence classification was previously not feasible for this range of parameters."
    }, {
      "heading" : "2 Related Work",
      "text" : "In the computational biology community pairwise alignment similarity scores were used traditionally as basis for classification, like the local and global alignment [5, 29]. String kernel based classification was introduced in [30, 9]. Extending this idea, [30] defined the gappy n-gram kernel and used it in conjunction with SVM [27] for text classification. The main drawback of this approach is that runtime for kernel evaluations depends quadratically on lengths of the sequences.\nAn alternative model of string kernels represents sequences as fixed dimensional vectors of counts of occurrences of k-mers in them. These include k-spectrum [18] and substring [28] kernels. This notion is extended to count inexact occurrences of patterns in sequences as in mismatch [19] and profile [10] kernels. In this transformed feature space SVM is used to learn class boundaries. This approach\nyields excellent classification accuracies [13] but computational complexity of kernel evaluation remains a daunting challenge [11].\nThe exponential dimensions (|Σ|k) of the feature space for both the k-spectrum kernel and k,mmismatch kernel make explicit transformation of strings computationally prohibitive. SVM does not require the feature vectors explicitly; it only uses pairwise dot products between them. A trie-based strategy to implicitly compute kernel values for pairs of sequences was proposed in [18] and [19]. A (k,m)-mismatch tree is introduced which is a rooted |Σ|-ary tree of depth k, where each internal node has a child corresponding to each symbol in Σ and every leaf corresponds to a k-mer in Σk. The runtime for computing the k,m mismatch kernel value between two sequences X and Y , under this trie-based framework, is O((|X|+ |Y |)km+1|Σ|m), where |X| and |Y | are lengths of sequences. This makes the algorithm only feasible for small alphabet sizes and very small number of allowed mismatches.\nThe k-mer based kernel framework has been extended in several ways by defining different string kernels such as restricted gappy kernel, substitution kernel, wildcard kernel [20], cluster kernel [32], sparse spatial kernel [12], abstraction-augmented kernel [16], and generalized similarity kernel [14]. For literature on large scale kernel learning and kernel approximation see [34, 1, 7, 22, 23, 33] and references therein."
    }, {
      "heading" : "3 Algorithm for Kernel Computation",
      "text" : "In this section we formulate the problem, describe our algorithm and analyze it’s runtime and quality.\nk-spectrum and k,m-mismatch kernel: Given a sequence X over alphabet Σ, the k,m-mismatch spectrum ofX is a |Σ|k-dimensional vector, Φk,m(X) of number of times each possible k-mer occurs in X with at most m mismatches. Formally,\nΦk,m(X) = (Φk,m(X)[γ])γ∈Σk = (∑ α∈X Im(α, γ) ) γ∈Σk , (1)\nwhere Im(α, γ) = 1, if α belongs to the set of k-mers that differ from γ by at most m mismatches, i.e. the Hamming distance between α and γ, d(α, γ) ≤ m. Note that for m = 0, it is known as k-spectrum of X . The k,m-mismatch kernel value for two sequences X and Y (the mismatch spectrum similarity score) [19] is defined as:\nK(X,Y |k,m) = 〈Φk,m(X),Φk,m(Y )〉 = ∑ γ∈Σk Φk,m(X)[γ]Φk,m(Y )[γ]\n= ∑ γ∈Σk ∑ α∈X Im(α, γ) ∑ β∈Y Im(β, γ) = ∑ α∈X ∑ β∈Y ∑ γ∈Σk Im(α, γ)Im(β, γ). (2)\nFor a k-mer α, let Nk,m(α) = {γ ∈ Σk : d(α, γ) ≤ m} be the m-mutational neighborhood of α. Then for a pair of sequences X and Y , the k,m-mismatch kernel given in eq (2) can be equivalently computed as follows [13]:\nK(X,Y |k,m) = ∑ α∈X ∑ β∈Y ∑ γ∈Σk Im(α, γ)Im(β, γ)\n= ∑ α∈X ∑ β∈Y |Nk,m(α) ∩Nk,m(β)| = ∑ α∈X ∑ β∈Y Im(α, β), (3)\nwhere Im(α, β) = |Nk,m(α) ∩Nk,m(β)| is the size of intersection of m-mutational neighborhoods of α and β. We use the following two facts. Fact 3.1. Im(α, β), the size of the intersection of m-mismatch neighborhoods of α and β, is a function of k, m, |Σ| and d(α, β) and is independent of the actual k-mers α and β or the actual positions where they differ. (See section 3.1) Fact 3.2. If d(α, β) > 2m, then Im(α, β) = 0.\nIn view of the above two facts we can rewrite the kernel value (3) as\nK(X,Y |k,m) = ∑ α∈X ∑ β∈Y Im(α, β) = min{2m,k}∑ i=0 Mi · Ii, (4)\nwhere Ii = Im(α, β) when d(α, β) = i and Mi is the number of pairs of k-mers (α, β) such that d(α, β) = i, where α ∈ X and β ∈ Y . Note that bounds on the last summation follows from Fact 3.2 and the fact that the Hamming distance between two k-mers is at most k. Hence the problem of kernel evaluation is reduced to computing Mi’s and evaluating Ii’s."
    }, {
      "heading" : "3.1 Closed form for Intersection Size",
      "text" : "Let Nk,m(α, β) be the intersection of m-mismatch neighborhoods of α and β i.e.\nNk,m(α, β) = Nk,m(α) ∩Nk,m(β).\nAs defined earlier |Nk,m(α, β)| = Im(α, β). Let Nq(α) = {γ ∈ Σk : d(α, γ) = q} be the set of k-mers that differ with α in exactly q indices. Note that Nq(α) ∩Nr(α) = ∅ for all q 6= r. Using this and defining nqr(α, β) = |Nq(α) ∩Nr(β)|,\nNk,m(α, β) = m⋃ q=0 m⋃ r=0 Nq(α) ∩Nr(β) and Im(α, β) = m∑ q=0 m∑ r=0 nqr(α, β).\nHence we give a formula to compute nij(α, β). Let s = |Σ|. Theorem 3.3. Given two k-mers α and β such that d(α, β) = d, we have that\nnij(α, β) = i+j−d 2∑ t=0 ( 2d− i− j + 2t d− (i− t) )( d i+ j − 2t− d ) (s− 2)i+j−2t−d ( k − d t ) (s− 1)t.\nProof. nij(α, β) can be interpreted as the number of ways to make i changes in α and j changes in β to get the same string. For clarity, we first deal with the case when we have d(α, β) = 0, i.e both strings are identical. We wish to find nij(α, β) = |Ni(α) ∩ Nj(β)|. It is clear that in this case i = j, otherwise making i and j changes to the same string will not result in the same string. Hence nij = ( k i ) (s− 1)i. Second we consider α, β such that d(α, β) = k. Clearly k ≥ i and k ≥ j. Moreover, since both strings do not agree at any index, character at every index has to be changed in at least one of α or β. This gives k ≤ i+ j. Now for a particular index p, α[p] and β[p] can go through any one of the following three changes. Let α[p] = x, β[p] = y. (I) Both α[p] and β[p] may change from x and y respectively to some character z. Let l1 be the count of indices going through this type of change. (II) α[p] changes from x to y, call the count of these l2. (III) β[p] changes from y to x, let this count be l3. It follows that\ni = l1 + l2 , j = l1 + l3, , l1 + l2 + l3 = k.\nThis results in l1 = i + j − k. Since l1 is the count of indices at which characters of both strings change, we have s− 2 character choices for each such index and\n( k i+j−k )\npossible combinations of indices for l1. From the remaining l2 + l3 = 2k − i − j indices, we choose l2 = k − j indices in(\n2k−i−j k−j\n) ways and change the characters at these indices of α to characters of β at respective indices.\nFinally, we are left with only l3 remaining indices and we change them according to the definition of l3. Thus the total number of strings we get after making i changes in α and j changes in β is\n(s− 2)i+j−k (\nk\ni+ j − k )( 2k − i− j k − j ) .\nNow we consider general strings α and β of length k with d(α, β) = d. Without loss of generality assume that they differ in the first d indices. We parameterize the system in terms of the number of changes that occur in the last k − d indices of the strings i.e let t be the number of indices that go through a change in last k − d indices. Number of possible such changes is(\nk − d t\n) (s− 1)t. (5)\nLets call the first d-length substrings of both strings α′ and β′. There are i − t characters to be changed in α′ and j − t in β′. As reasoned above, we have d ≤ (i− t) + (j − t) =⇒ t ≤ i+j−d2 .\nIn this setup we get i− t = l1 + l2, j − t = l1 + l3, l1 + l2 + l3 = d and l1 = (i− t) + (j − t)− d. We immediately get that for a fixed t, the total number of resultant strings after making i− t changes in α′ and j − t changes in β′ is(\n2d− (i− t)− (j − t) d− (i− t)\n)( d\n(i− t) + (j − t)− d\n) (s− 2)(i−t)+(j−t)−d. (6)\nFor a fixed t, every substring counted in (5), every substring counted in (6) gives a required string obtained after i and j changes in α and β respectively. The statement of the theorem follows.\nCorollary 3.4. Runtime of computing Id is O(m3), independent of k and |Σ|. This is so, because if d(α, β) = d, Id = m∑ q=0 m∑ r=0 nqr(α, β) and nqr(α, β) can be computed in O(m)."
    }, {
      "heading" : "3.2 Computing Mi",
      "text" : "Recall that given two sequences X and Y , Mi is the number of pairs of k-mers (α, β) such that d(α, β) = i, where α ∈ X and β ∈ Y . Formally, the problem of computing Mi is as follows: Problem 3.5. Given k, m, and two sets of k-mers SX and SY (set of k-mers extracted from the sequences X and Y respectively) with |SX | = nX and |SY | = nY . Compute\nMi = |{(α, β) ∈ SX × SY : d(α, β) = i}| for 0 ≤ i ≤ min{2m, k}.\nNote that the brute force approach to compute Mi requires O(nX · nY · k) comparisons. Let Qk(j) denote the set of all j-sets of {1, . . . , k} (subsets of indices). For θ ∈ Qk(j) and a k-mer α, let α|θ be the j-mer obtained by selecting the characters at the j indices in θ. Let fθ(X,Y ) be the number of pairs of k-mers in SX × SY as follows;\nfθ(X,Y ) = |{(α, β) ∈ SX × SY : d(α|θ, β|θ) = 0}|. We use the following important observations about fθ. Fact 3.6. For 0 ≤ i ≤ k and θ ∈ Qk(k − i), if d(α|θ, β|θ) = 0, then d(α, β) ≤ i. Fact 3.7. For 0 ≤ i ≤ k and θ ∈ Qk(k − i), fθ(X,Y ) can be computed in O(kn log n) time.\nThis can be done by first lexicographically sorting the k-mers in each of SX and SY by the indices in θ. The pairs in SX × SY that are the same at indices in θ can then be enumerated in one linear scan over the sorted lists. Let n = nX + nY , runtime of this computation is O(k(n + |Σ|)) if we use counting sort (as in [13]) or O(kn log n) for mergesort (since θ has O(k) indices.) Since this procedure is repeated many times, we refer to this as the SORT-ENUMERATE subroutine. We define\nFi(X,Y ) = ∑\nθ∈Qk(k−i)\nfθ(X,Y ). (7)\nLemma 3.8.\nFi(X,Y ) = i∑ j=0 ( k − j k − i ) Mj . (8)\nProof. Let (α, β) be a pair that contributes to Mj , i.e. d(α, β) = j. Then for every θ ∈ Qk(k − i) that has all indices within the k − j positions where α and β agree, the pair (α, β) is counted in fθ(X,Y ). The number of such θ’s are ( k−j k−i ) , henceMj is counted ( k−j k−i )\ntimes in Fi(X,Y ), yielding the required equality. Corollary 3.9. Mi can readily be computed as: Mi = Fi(X,Y )− i−1∑ j=0 ( k−j k−i ) Mj .\nBy definition, Fi(X,Y ) can be computed with ( k k−i ) = ( k i ) fθ computations. Let t = min{2m, k}. K(X,Y |k,m) can be evaluated by (4) after computing Mi (by (8)) and Ii (by Corollary 3.4) for 0 ≤ i ≤ t. The overall complexity of this strategy thus is(\nt∑ i=0 ( k i ) (k − i)(n log n+ n) ) +O(n) = O(k · 2k−1 · (n log n)).\nAlgorithm 1 : Approximate-Kernel(SX ,SY ,k,m, ,δ,B) 1: I,M ′ ← ZEROS(t+ 1) 2: σ ← · √ δ\n3: Populate I using Corollary 3.4 4: for i = 0 to t do 5: µF ← 0 6: iter ← 1 7: varF ←∞ 8: while varF > σ2 ∧ iter < B do 9: θ ← RANDOM( ( k k−i ) )\n10: µF ← µF · (iter − 1) + SORT-ENUMERATE(SX , SY , k, θ)\niter . Application of Fact 3.7\n11: varF ← VARIANCE(µF , varF , iter) . Compute online variance 12: iter ← iter + 1 13: F ′[i]← µF · ( k k−i )"
    }, {
      "heading" : "14: M ′[i]← F ′[i]",
      "text" : "15: for j = 0 to i− 1 do . Application of Corollary 3.9 16: M ′[i]←M ′[i]− ( k−j k−i ) ·M ′[j] 17: K ′ ← SUMPRODUCT(M ′, I) . Applying Equation (4) 18: return K ′\nWe give our algorithm to approximate K(X,Y |k,m), it’s explanation followed by it’s analysis. Algorithm 1 takes , δ ∈ (0, 1), and B ∈ Z+ as input parameters; the first two controls the accuracy of estimate whileB is an upper bound on the sample size. We use (7) to estimate Fi = Fi(X,Y ) with an online sampling algorithm, where we choose θ ∈ Qk(k− i) uniformly at random and compute the online mean and variance of the estimate for Fi. We continue to sample until the variance is below the threshold (σ2 = 2δ) or the sample size reaches the upper bound B. We scale up our estimate by the population size and use it to compute M ′i (estimates of Mi) using Corollary 3.9. These M ′ i ’s together with the precomputed exact values of Ii’s are used to compute our estimate, K ′(X,Y |k,m, σ, δ, B), for the kernel value using (4). First we give an analytical bound on the runtime of Algorithm 1 then we provide guarantees on it’s performance.\nTheorem 3.10. Runtime of Algorithm 1 is bounded above by O(k2n log n).\nProof. Observe that throughout the execution of the algorithm there are at most tB computations of fθ, which by Fact 3.7 needs O(kn log n) time. Since B is an absolute constant and t ≤ k, we get that the total runtime of the algorithm is O(k2n log n). Note that in practice the while loop in line 8 is rarely executed for B iterations; the deviation is within the desired range much earlier.\nLet K ′ = K ′(X,Y |k,m, , δ, B) be our estimate (output of Algorithm 1) for K = K(X,Y |k,m). Theorem 3.11. K ′ is an unbiased estimator of the true kernel value, i.e. E(K ′) = K.\nProof. For this we need the following result, whose proof is deferred.\nLemma 3.12. E(M ′i) = Mi. By Line 17 of Algorithm 1, E(K ′) = E( ∑t i=0 IiM ′i). Using the fact that Ii’s are constants and Lemma 3.12 we get that\nE(K ′) = t∑ i=0 IiE(M ′i) = min{2m,k}∑ i=0 IiMi = K.\nTheorem 3.13. For any 0 < , δ < 1, Algorithm 1 is an ( Imax, δ)−additive approximation algorithm, i.e. Pr(|K −K ′| ≥ Imax) < δ, where Imax = maxi{Ii}.\nNote that these are very loose bounds, in practice we get approximation far better than these bounds. Furthermore, though Imax could be large, but it is only a fraction of one of the terms in summation for the kernel value K(X,Y |k,m).\nProof. Let F ′i be our estimate for Fi (X,Y ) = Fi. We use the following bound on the variance of K ′ that is proved later.\nLemma 3.14. V ar(K ′) ≤ δ( · Imax)2.\nBy Lemma 3.12 we have E(K ′) = K, hence by Lemma 3.14, Pr[|K ′ −K|] ≥ Imax is equivalent to Pr[|K ′ − E(K ′)|] ≥ 1√\nδ\n√ V ar(K ′). By the Chebychev’s inequality, this latter probability is at\nmost δ. Therefore, Algorithm 1 is an ( Imax, δ)−additive approximation algorithm.\nProof. (Proof of Lemma 3.12) We prove it by induction on i. The base case (i = 0) is true as we compute M ′[0] exactly, i.e. M ′[0] = M [0]. Suppose E(M ′j) = Mj for 0 ≤ j ≤ i− 1. Let iter be the number of iterations for i, after execution of Line 10 we get\nF ′[i] = µF\n( k\nk − i\n) = ∑iter r=1 fθr (X,Y )\niter\n( k\nk − i\n) ,\nwhere θr is the random (k − i)-set chosen in the rth iteration of the while loop. Since θr is chosen uniformly at random we get that\nE(F ′[i]) = E(µF )\n( k\nk − i\n) = E(fθr (X,Y )) ( k\nk − i\n) = Fi(X,Y )( k k−i ) ( k k − i ) . (9)\nAfter the loop on Line 15 is executed we get that E(M ′[i]) = Fi(X,Y )− i−1∑ j=0 ( k−j k−i ) E(M ′j). Using E(M ′j) = Mj (inductive hypothesis) in (8) we get that E(M ′ i) = Mi.\nProof. (Proof of Lemma 3.14) After execution of the while loop in Algorithm 1, we have F ′i = i∑\nj=0\n( k−j k−i ) M ′j . We use the following fact that follows from basic calculations.\nFact 3.15. Suppose X0, . . . , Xt are random variables and let S = ∑t i=0 aiXi, where a0, . . . , at are constants. Then\nV ar(S) = t∑ i=0 a2iV ar(Xi) + 2 t∑ i=0 t∑ j=i+1 aiajCov(Xi, Xj).\nUsing fact 3.15 and definitions of Imax and σ we get that\nV ar(K ′) = t∑ i=0 Ii2V ar(M ′i) + 2 t∑ i=0 t∑ j=i+1 IiIjCov(M ′i ,M ′j)\n≤ I2max  t∑ i=0 V ar(M ′i) + 2 t∑ i=0 t∑ j=i+1 Cov(M ′i ,M ′ j)  ≤ I2maxV ar(F ′t ) ≤ I2maxσ2 = δ( ·Imax)2. The last inequality follows from the following relation derived from definition of F ′i and Fact 3.15.\nV ar(F ′t ) = t∑ i=0 ( k − i k − t )2 V ar(M ′i) + 2 t∑ i=0 t∑ j=i+1 ( k − i k − t )( k − j k − t ) Cov(M ′i ,M ′ j). (10)"
    }, {
      "heading" : "4 Evaluation",
      "text" : "We study the performance of our algorithm in terms of runtime, quality of kernel estimates and predictive accuracies on standard benchmark sequences datasets (Table 1) . For the range of parameters feasible for existing solutions, we generated kernel matrices both by algorithm of [13] (exact) and our algorithm (approximate). These experiments are performed on an Intel Xeon machine with (8 Cores, 2.1 GHz and 32 GB RAM) using the same experimental settings as in [13, 15, 17]. Since our algorithm is applicable for significantly wider range of k and m, we also report classification performance with large k and m. For our algorithm we used B ∈ {300, 500} and σ ∈ {0.25, 0.5} with no significant difference in results as implied by the theoretical analysis. In all reported results B = 300 and σ = 0.5. In order to perform comparisons, for a few combinations of parameters we generated exact kernel matrices of each dataset on a much more powerful machine (a cluster of 20 nodes, each having 24 CPU’s with 2.5 GHz speed and 128GB RAM). Sources for datasets and source code are available at 1.\nRunning Times: We report difference in running times for kernels generation in Figure 1. Exact kernels are generated using code provided by authors of [13, 14] for 8 ≤ k ≤ 16 and m = 2 only. We achieve significant speedups for large values of k (for k = 16 we get one order of magnitude gains in computational efficiency on all datasets). The running times for these algorithms are O(2kn) and O(k2n log n) respectively. We can use larger values of k without an exponential penalty, which is visible in the fact that in all graphs, as k increases the growth of running time of the exact algorithm is linear (on the log-scale), while that of our algorithm tends to taper off.\nKernel Error Analysis: We show that despite reduction in runtimes, we get excellent approximation of kernel matrices. In Table 2 we report point-to-point error analysis of the approximate kernel matrices. We compare our estimates with exact kernels for m = 2. For m > 2 we report statistical error analyses. More precisely, we evaluate differences with principal submatrices of the exact kernel matrix. These principal submatrices are selected by randomly sampling 50 sequences and computing their pairwise kernel values. We report errors for four datasets; the fifth one, not included for space reasons, showed no difference in error. From Table 2 it is evident that our empirical performance is significantly more precise than the theoretical bounds proved on errors in our estimates.\n1https://github.com/mufarhan/sequence_class_NIPS_2017\nPrediction Accuracies: We compare the outputs of SVM on the exact and approximate kernels using the publicly available SVM implementation LIBSVM [2]. We computed exact kernel matrices by brute force algorithm for a few combinations of parameters for each dataset on the much more powerful machine. Generating these kernels took days; we only generated to compare classification performance of our algorithm with the exact one. We demonstrate that our predictive accuracies are sufficiently close to that with exact kernels in Table 3 (bio-sequences) and Table 4 (music). The parameters used for reporting classification performance are chosen in order to maintain comparability with previous studies. Similarly all measurements are made as in [13, 14], for instance for music genre classification we report results of 10-fold cross-validation (see Table 1). For our algorithm we used B = 300 and σ = 0.5 and we take an average of performances over three independent runs."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work we devised an efficient algorithm for evaluation of string kernels based on inexact matching of subsequences (k-mers). We derived a closed form expression for the size of intersection of m-mismatch neighborhoods of two k-mers. Another significant contribution of this work is a novel statistical estimate of the number of k-mer pairs at a fixed distance between two sequences. Although large values of the parameters k and m were known to yield better classification results, known algorithms are not feasible even for moderately large values. Using the two above mentioned results our algorithm efficiently approximate kernel matrices with probabilistic bounds on the accuracy. Evaluation on several challenging benchmark datasets for large k and m, show that we achieve state of the art classification performance, with an order of magnitude speedup over existing solutions."
    } ],
    "references" : [ {
      "title" : "Predictive low-rank decomposition for kernel methods",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "A machine learning information retrieval approach to protein fold recognition",
      "author" : [ "J. Cheng", "P. Baldi" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Scop: A structural classification of proteins database",
      "author" : [ "L. Conte", "B. Ailey", "T. Hubbard", "S. Brenner", "A. Murzin", "C. Chothia" ],
      "venue" : "Nucleic Acids Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "An introduction to support vector machines and other kernel-based learning methods",
      "author" : [ "N. Cristianini", "J. Shawe-Taylor" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2000
    }, {
      "title" : "Multi-class protein fold recognition using support vector machines and neural networks",
      "author" : [ "C. Ding", "I. Dubchak" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "On the nyström method for approximating a gram matrix for improved kernel-based learning",
      "author" : [ "P. Drineas", "M.W. Mahoney" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Classifying music audio with timbral and chroma features",
      "author" : [ "D.P. Ellis" ],
      "venue" : "In ISMIR,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Convolution kernels on discrete structures",
      "author" : [ "D. Haussler" ],
      "venue" : "Technical Report UCS-CRL-99-10, University of California at Santa Cruz,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Profile-based string kernels for remote homology detection and motif extraction",
      "author" : [ "R. Kuang", "E. Ie", "K. Wang", "M. Siddiqi", "Y. Freund", "C. Leslie" ],
      "venue" : "Journal of Bioinformatics and Computational Biology,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "Scalable kernel methods and algorithms for general sequence analysis",
      "author" : [ "P. Kuksa" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Fast protein homology and fold detection with sparse spatial sample kernels",
      "author" : [ "P. Kuksa", "P.-H. Huang", "V. Pavlovic" ],
      "venue" : "In 19th International Conference on Pattern Recognition,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Scalable algorithms for string kernels with inexact matching",
      "author" : [ "P. Kuksa", "P.-H. Huang", "V. Pavlovic" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Generalized similarity kernels for efficient sequence classification",
      "author" : [ "P. Kuksa", "I. Khan", "V. Pavlovic" ],
      "venue" : "In SIAM International Conference on Data Mining,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Spatial representation for efficient sequence classification",
      "author" : [ "P. Kuksa", "V. Pavlovic" ],
      "venue" : "In 20th International Conference on Pattern Recognition,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Semi-supervised abstraction-augmented string kernel for multi-level bio-relation extraction",
      "author" : [ "P. Kuksa", "Y. Qi", "B. Bai", "R. Collobert", "J. Weston", "V. Pavlovic", "X. Ning" ],
      "venue" : "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Efficient multivariate sequence classification",
      "author" : [ "P.P. Kuksa" ],
      "venue" : "In CoRR abs/1409.8211,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "The spectrum kernel: A string kernel for svm protein classification",
      "author" : [ "C. Leslie", "E. Eskin", "W. Noble" ],
      "venue" : "In Pacific Symposium on Biocomputing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2002
    }, {
      "title" : "Mismatch string kernels for svm protein classification",
      "author" : [ "C. Leslie", "E. Eskin", "J. Weston", "W. Noble" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2003
    }, {
      "title" : "Fast string kernels using inexact matching for protein sequences",
      "author" : [ "C. Leslie", "R. Kuang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "A comparative study on content-based music genre classification",
      "author" : [ "T. Li", "M. Ogihara", "Q. Li" ],
      "venue" : "In 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2003
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Kernel methods for pattern analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2004
    }, {
      "title" : "Large scale genomic sequence svm classifiers",
      "author" : [ "S. Sonnenburg", "G. Rätsch", "B. Schölkopf" ],
      "venue" : "In 22nd International Conference on Machine Learning,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2005
    }, {
      "title" : "Musical genre classification of audio signals",
      "author" : [ "G. Tzanetakis", "P. Cook" ],
      "venue" : "IEEE Transactions on Speech and Audio Processing,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2002
    }, {
      "title" : "Statistical learning theory, volume 1",
      "author" : [ "V. Vapnik" ],
      "venue" : "Wiley New York,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1998
    }, {
      "title" : "Fast kernels for string and tree matching",
      "author" : [ "S. Vishwanathan", "A. Smola" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2002
    }, {
      "title" : "Dynamic alignment kernels. In Advances in Large Margin Classifiers, pages 39–50",
      "author" : [ "C. Watkins" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1999
    }, {
      "title" : "Semi-supervised protein classification using cluster",
      "author" : [ "J. Weston", "C. Leslie", "E. Ie", "D. Zhou", "A. Elisseeff", "W. Noble" ],
      "venue" : "kernels. Bioinformatics,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2005
    }, {
      "title" : "Semi-supervised protein classification using cluster kernels",
      "author" : [ "J. Weston", "C. Leslie", "D. Zhou", "A. Elisseeff", "W. Noble" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2004
    }, {
      "title" : "Using the nyström method to speed up kernel machines",
      "author" : [ "C.K.I. Williams", "M. Seeger" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2000
    }, {
      "title" : "Nyström method vs random fourier features: A theoretical and empirical comparison",
      "author" : [ "T. Yang", "Y.-F. Li", "M. Mahdavi", "R. Jin", "Z.-H. Zhou" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Sequence classification algorithms have been applied to both of these problems with great success [3, 10, 13, 18, 19, 20, 25].",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "Sequence classification algorithms have been applied to both of these problems with great success [3, 10, 13, 18, 19, 20, 25].",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "Sequence classification algorithms have been applied to both of these problems with great success [3, 10, 13, 18, 19, 20, 25].",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "Sequence classification algorithms have been applied to both of these problems with great success [3, 10, 13, 18, 19, 20, 25].",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 18,
      "context" : "Sequence classification algorithms have been applied to both of these problems with great success [3, 10, 13, 18, 19, 20, 25].",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 19,
      "context" : "Sequence classification algorithms have been applied to both of these problems with great success [3, 10, 13, 18, 19, 20, 25].",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 24,
      "context" : "Sequence classification algorithms have been applied to both of these problems with great success [3, 10, 13, 18, 19, 20, 25].",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 25,
      "context" : "Music data, a real valued signal when discretized using vector quantization of MFCC features is another flavor of sequential data [26].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 11,
      "context" : "Sequence classification has been used for recognizing genres of music sequences with no annotation and identifying artists from albums [12, 13, 14].",
      "startOffset" : 135,
      "endOffset" : 147
    }, {
      "referenceID" : 12,
      "context" : "Sequence classification has been used for recognizing genres of music sequences with no annotation and identifying artists from albums [12, 13, 14].",
      "startOffset" : 135,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "Sequence classification has been used for recognizing genres of music sequences with no annotation and identifying artists from albums [12, 13, 14].",
      "startOffset" : 135,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "Categorizing texts into classes based on their topics is another application domain of sequence classification [11, 15].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 14,
      "context" : "Categorizing texts into classes based on their topics is another application domain of sequence classification [11, 15].",
      "startOffset" : 111,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : "A feature vector typically is the spectra (counts) of all k-length substrings (k-mers) present exactly [18] or inexactly (with up to m mismatches) [19] within a sequence.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "A feature vector typically is the spectra (counts) of all k-length substrings (k-mers) present exactly [18] or inexactly (with up to m mismatches) [19] within a sequence.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 4,
      "context" : "The matrix of pairwise similarity scores (the kernel matrix) thus computed is used as input to a standard support vector machine (SVM) [5, 27] classifier resulting in excellent classification performance in many applications [19].",
      "startOffset" : 135,
      "endOffset" : 142
    }, {
      "referenceID" : 26,
      "context" : "The matrix of pairwise similarity scores (the kernel matrix) thus computed is used as input to a standard support vector machine (SVM) [5, 27] classifier resulting in excellent classification performance in many applications [19].",
      "startOffset" : 135,
      "endOffset" : 142
    }, {
      "referenceID" : 18,
      "context" : "The matrix of pairwise similarity scores (the kernel matrix) thus computed is used as input to a standard support vector machine (SVM) [5, 27] classifier resulting in excellent classification performance in many applications [19].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 10,
      "context" : "It has been established that using larger values of k and m improve classification performance [11, 13].",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "It has been established that using larger values of k and m improve classification performance [11, 13].",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 18,
      "context" : "On the other hand, the runtime of kernel computation by the efficient trie-based algorithm [19, 24] is O(k|Σ|(|X|+ |Y |)) for two sequences X and Y over alphabet Σ.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "On the other hand, the runtime of kernel computation by the efficient trie-based algorithm [19, 24] is O(k|Σ|(|X|+ |Y |)) for two sequences X and Y over alphabet Σ.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "In the best known algorithm [13] the former problem is addressed by precomputing the intersection size in constant time for m ≤ 2 only.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "In the computational biology community pairwise alignment similarity scores were used traditionally as basis for classification, like the local and global alignment [5, 29].",
      "startOffset" : 165,
      "endOffset" : 172
    }, {
      "referenceID" : 28,
      "context" : "String kernel based classification was introduced in [30, 9].",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "String kernel based classification was introduced in [30, 9].",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 28,
      "context" : "Extending this idea, [30] defined the gappy n-gram kernel and used it in conjunction with SVM [27] for text classification.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 26,
      "context" : "Extending this idea, [30] defined the gappy n-gram kernel and used it in conjunction with SVM [27] for text classification.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "These include k-spectrum [18] and substring [28] kernels.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "These include k-spectrum [18] and substring [28] kernels.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "This notion is extended to count inexact occurrences of patterns in sequences as in mismatch [19] and profile [10] kernels.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "This notion is extended to count inexact occurrences of patterns in sequences as in mismatch [19] and profile [10] kernels.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "yields excellent classification accuracies [13] but computational complexity of kernel evaluation remains a daunting challenge [11].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "yields excellent classification accuracies [13] but computational complexity of kernel evaluation remains a daunting challenge [11].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 17,
      "context" : "A trie-based strategy to implicitly compute kernel values for pairs of sequences was proposed in [18] and [19].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "A trie-based strategy to implicitly compute kernel values for pairs of sequences was proposed in [18] and [19].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 19,
      "context" : "The k-mer based kernel framework has been extended in several ways by defining different string kernels such as restricted gappy kernel, substitution kernel, wildcard kernel [20], cluster kernel [32], sparse spatial kernel [12], abstraction-augmented kernel [16], and generalized similarity kernel [14].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 30,
      "context" : "The k-mer based kernel framework has been extended in several ways by defining different string kernels such as restricted gappy kernel, substitution kernel, wildcard kernel [20], cluster kernel [32], sparse spatial kernel [12], abstraction-augmented kernel [16], and generalized similarity kernel [14].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 11,
      "context" : "The k-mer based kernel framework has been extended in several ways by defining different string kernels such as restricted gappy kernel, substitution kernel, wildcard kernel [20], cluster kernel [32], sparse spatial kernel [12], abstraction-augmented kernel [16], and generalized similarity kernel [14].",
      "startOffset" : 223,
      "endOffset" : 227
    }, {
      "referenceID" : 15,
      "context" : "The k-mer based kernel framework has been extended in several ways by defining different string kernels such as restricted gappy kernel, substitution kernel, wildcard kernel [20], cluster kernel [32], sparse spatial kernel [12], abstraction-augmented kernel [16], and generalized similarity kernel [14].",
      "startOffset" : 258,
      "endOffset" : 262
    }, {
      "referenceID" : 13,
      "context" : "The k-mer based kernel framework has been extended in several ways by defining different string kernels such as restricted gappy kernel, substitution kernel, wildcard kernel [20], cluster kernel [32], sparse spatial kernel [12], abstraction-augmented kernel [16], and generalized similarity kernel [14].",
      "startOffset" : 298,
      "endOffset" : 302
    }, {
      "referenceID" : 32,
      "context" : "For literature on large scale kernel learning and kernel approximation see [34, 1, 7, 22, 23, 33] and references therein.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "For literature on large scale kernel learning and kernel approximation see [34, 1, 7, 22, 23, 33] and references therein.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : "For literature on large scale kernel learning and kernel approximation see [34, 1, 7, 22, 23, 33] and references therein.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "For literature on large scale kernel learning and kernel approximation see [34, 1, 7, 22, 23, 33] and references therein.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 22,
      "context" : "For literature on large scale kernel learning and kernel approximation see [34, 1, 7, 22, 23, 33] and references therein.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 31,
      "context" : "For literature on large scale kernel learning and kernel approximation see [34, 1, 7, 22, 23, 33] and references therein.",
      "startOffset" : 75,
      "endOffset" : 97
    }, {
      "referenceID" : 18,
      "context" : "The k,m-mismatch kernel value for two sequences X and Y (the mismatch spectrum similarity score) [19] is defined as: K(X,Y |k,m) = 〈Φk,m(X),Φk,m(Y )〉 = ∑",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "Then for a pair of sequences X and Y , the k,m-mismatch kernel given in eq (2) can be equivalently computed as follows [13]: K(X,Y |k,m) = ∑",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 12,
      "context" : "Let n = nX + nY , runtime of this computation is O(k(n + |Σ|)) if we use counting sort (as in [13]) or O(kn log n) for mergesort (since θ has O(k) indices.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "For the range of parameters feasible for existing solutions, we generated kernel matrices both by algorithm of [13] (exact) and our algorithm (approximate).",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "1 GHz and 32 GB RAM) using the same experimental settings as in [13, 15, 17].",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "1 GHz and 32 GB RAM) using the same experimental settings as in [13, 15, 17].",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "1 GHz and 32 GB RAM) using the same experimental settings as in [13, 15, 17].",
      "startOffset" : 64,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "Evaluation Ding-Dubchak [6] protein fold recognition 27 694 169 10-fold CV SCOP [4, 31] protein homology detection 54 7329 308 54 binary class.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "Evaluation Ding-Dubchak [6] protein fold recognition 27 694 169 10-fold CV SCOP [4, 31] protein homology detection 54 7329 308 54 binary class.",
      "startOffset" : 80,
      "endOffset" : 87
    }, {
      "referenceID" : 29,
      "context" : "Evaluation Ding-Dubchak [6] protein fold recognition 27 694 169 10-fold CV SCOP [4, 31] protein homology detection 54 7329 308 54 binary class.",
      "startOffset" : 80,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : "Music [21, 26] music genre recognition 10 1000 2368 5-fold CV Artist20 [8, 17] artist identification 20 1413 9854 6-fold CV ISMIR [17] music genre recognition 6 729 10137 5-fold CV",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 25,
      "context" : "Music [21, 26] music genre recognition 10 1000 2368 5-fold CV Artist20 [8, 17] artist identification 20 1413 9854 6-fold CV ISMIR [17] music genre recognition 6 729 10137 5-fold CV",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 7,
      "context" : "Music [21, 26] music genre recognition 10 1000 2368 5-fold CV Artist20 [8, 17] artist identification 20 1413 9854 6-fold CV ISMIR [17] music genre recognition 6 729 10137 5-fold CV",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "Music [21, 26] music genre recognition 10 1000 2368 5-fold CV Artist20 [8, 17] artist identification 20 1413 9854 6-fold CV ISMIR [17] music genre recognition 6 729 10137 5-fold CV",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "Music [21, 26] music genre recognition 10 1000 2368 5-fold CV Artist20 [8, 17] artist identification 20 1413 9854 6-fold CV ISMIR [17] music genre recognition 6 729 10137 5-fold CV",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 12,
      "context" : "Exact kernels are generated using code provided by authors of [13, 14] for 8 ≤ k ≤ 16 and m = 2 only.",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "Exact kernels are generated using code provided by authors of [13, 14] for 8 ≤ k ≤ 16 and m = 2 only.",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "Prediction Accuracies: We compare the outputs of SVM on the exact and approximate kernels using the publicly available SVM implementation LIBSVM [2].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 12,
      "context" : "Similarly all measurements are made as in [13, 14], for instance for music genre classification we report results of 10-fold cross-validation (see Table 1).",
      "startOffset" : 42,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "Similarly all measurements are made as in [13, 14], for instance for music genre classification we report results of 10-fold cross-validation (see Table 1).",
      "startOffset" : 42,
      "endOffset" : 50
    } ],
    "year" : 2017,
    "abstractText" : "Sequence classification algorithms, such as SVM, require a definition of distance (similarity) measure between two sequences. A commonly used notion of similarity is the number of matches between k-mers (k-length subsequences) in the two sequences. Extending this definition, by considering two k-mers to match if their distance is at most m, yields better classification performance. This, however, makes the problem computationally much more complex. Known algorithms to compute this similarity have computational complexity that render them applicable only for small values of k and m. In this work, we develop novel techniques to efficiently and accurately estimate the pairwise similarity score, which enables us to use much larger values of k and m, and get higher predictive accuracy. This opens up a broad avenue of applying this classification approach to audio, images, and text sequences. Our algorithm achieves excellent approximation performance with theoretical guarantees. In the process we solve an open combinatorial problem, which was posed as a major hindrance to the scalability of existing solutions. We give analytical bounds on quality and runtime of our algorithm and report its empirical performance on real world biological and music sequences datasets.",
    "creator" : null
  }
}