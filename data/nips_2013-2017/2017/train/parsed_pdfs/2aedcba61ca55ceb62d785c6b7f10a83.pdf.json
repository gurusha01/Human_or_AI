{
  "name" : "2aedcba61ca55ceb62d785c6b7f10a83.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Group Additive Structure Identification for Kernel Nonparametric Regression",
    "authors" : [ "Pan Chao", "Michael Zhu" ],
    "emails" : [ "panchao25@gmail.com", "yuzhu@purdue.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Regression analysis is popularly used to study the relationship between a response variable Y and a vector of predictor variables X . Linear and logistic regression analysis are arguably two most popularly used regression tools in practice, and both postulate explicit parametric models on f(X) = E[Y |X] as a function of X . When no parametric models can be imposed, nonparametric regression analysis can instead be performed. On one hand, nonparametric regression analysis is flexible and not susceptible to model mis-specification, whereas on the other hand, it suffers from a number of well-known drawbacks especially in high dimensional settings. Firstly, the asymptotic error rate of nonparametric regression deteriorates quickly as the dimension of X increases. [16] shows that with some regularity conditions, the optimal asymptotic error rate for estimating a dtimes differentiable function is O ( n−d/(2d+p) ) , where p is the dimensionality of X . Secondly, the resulting fitted nonparametric function is often complicated and difficult to interpret.\nTo overcome the drawbacks of high dimensional nonparametric regression, one popularly used approach is to impose the additive structure [5] on f(X), that is to assume that f(X) = f1(X1) + · · ·+fp(Xp) where f1, . . . , fp are p unspecified univariate functions. Thanks to the additive structure, the nonparametric estimation of f or equivalently the individual fi’s for 1 ≤ i ≤ p becomes efficient and does not suffer from the curse of dimensionality. Furthermore, the interpretability of the resulting model has also been much improved.\nThe key drawback of the additive model is that it does not assume interactions between the predictor variables. To address this limitation, functional ANOVA models were proposed to accommodate higher order interactions, see [4] and [13]. For example, by neglecting interactions of\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\norder higher than 2, the functional ANOVA model can be written as f(X) = ∑p i=1 fi(Xi) +∑\n1≤i,j≤p fij(Xi, Xj), with some marginal constraints. Another higher order interaction model, f(X) = ∑D d=1 ∑ 1≤i1,...,id≤p fj(Xi1 , . . . , Xid), is proposed by [6]. This model considers all interactions of order up to D, which is estimated by Kernel Ridge Regression (KRR) [10] with the elementary symmetric polynomial (ESP) kernel. Both of the two models assume the existence of possible interactions between any two or more predictor variables. This can lead to a serious problem, that is, the number of nonparametric functions that need to be estimated quickly increases as the number of predictor variables increases.\nThere exists another direction to generalize the additive model. When proposing the Optimal Kernel Group Transformation (OKGT) method for nonparametric regression, [11] considers the additive structure of predictor variables in groups instead of individual predictor variables. Let G := {uj}dj=1 be a index partition of the predictor variables, that is, uj∩uk = ∅ if j 6= k and ∪dj=1uj = {1, . . . , p}. Let Xuj = {Xk; k ∈ uj} for j = 1, . . . , d. Then {X1, . . . , Xd} = Xu1 ∪ · · · ∪Xud . For any function f(X), if there exists an index partition G = {u1, . . . ,ud} such that\nf(X) = fu1(Xu1) + . . .+ fud(Xud), (1)\nwhere fu1(Xu1), . . . , fud(Xud) are d unspecified nonparametric functions, then it is said that f(X) admits the group additive structure G. We also refer to (1) as a group additive model for f(X). It is clear that the usual additive model is a special case with G = {(1), . . . , (p)}. Suppose Xj1 and Xj2 are two predictor variables. Intuitively, if Xj1 and Xj2 interact to each other, then they must appear in the same group in an reasonable group additive structure of f(X). On the other hand, if Xj1 and Xj2 belong to two different groups, then they do not interact with each other. Therefore, in terms of accommodating interactions, the group additive model can be considered lying in the middle between the original additive model and the functional ANOVA or higher order interaction models. When the group sizes are small, for example all are less than or equal to 3, the group additive model can maintain the estimation efficiency and interpretability of the original additive model while avoiding the problem of a high order model discussed earlier.\nHowever, in [11], there are two important issues not addressed. First, the group additive structure may not be unique, which will lead to the nonidentifiability problem for the group additive model. (See discussion in Section 2.1). Second, [11] has not proposed a systematic approach to identify the group additive structure. In this paper, we intend to resolve these two issues. To address the first issue, we rigorously define the intrinsic group additive structure for any square integrable function, which in some sense is the minimal group additive structure among all correct group additive structures.\nTo address the second issue, we propose a general approach to simultaneously identifying the intrinsic group additive structure and estimating the nonparametric functions using kernel methods and Reproducing Kernel Hilbert Spaces (RKHSs). For a given group additive structure G = {u1, . . . ,ud}, we first define the corresponding direct sum RKHS asHG = Hu1 ⊕· · ·⊕Hud where Hui is the usual RKHS for the variables in uj only for j = 1, . . . , d. Based on the results on the capacity measure of RKHSs in the literature, we derive a tractable capacity measure of the direct sum RKHSHG which is further used as the complexity measure of G. Then, the identification of the intrinsic group additive structure and the estimation of the nonparametric functions can be performed through the following minimization problem\nf̂ , Ĝ = argmin f∈HG,G\n1\nn n∑ i=1 (yi − f(xi))2 + λ‖f‖2HG + µC(G).\nWe show that when the novel complexity measure of group additive structure C(G) is used, the minimizer Ĝ is consistent for the intrinsic group additive structure. We further develop two algorithms, one uses exhaustive search and the other employs a stepwise approach, for identifying true additive group structures under the small p and large p scenarios. Extensive simulation study and real data applications show that our proposed method can successfully recover the true additive group structures in a variety of model settings.\nThere exists a connection between our proposed group additive model and graphical models ([2], [7]). This is especially true when a sparse block structure is imposed [9]. However, a key difference exists. Let’s consider the following example. Y = sin(X1 +X22 +X3) + cos(X4 +X5 +X 2 6 ) + . A graphical model typically considers the conditional dependence (CD) structure among all of the\nvariables includingX1, . . . , X6 and Y , which is more complex than the group additive (GA) structure {(X1, X2, X3), (X4, X5, X6)}. The CD structure, once known, can be further examined to infer the GA structure. In this paper, we however proposed methods that directly target the GA structure instead of the more complex CD structure.\nThe rest of the paper is organized as follows. In Section 2, we rigorously formulate the problem of Group Additive Structure Identification (GASI) for nonparametric regression and propose the structural penalty method to solve the problem. In Section 3, we prove the selection consistency for the method. We report the experimental results based on simulation studies and real data application in Section 4 and 5. Section 6 concludes this paper with discussion."
    }, {
      "heading" : "2 Method",
      "text" : ""
    }, {
      "heading" : "2.1 Group Additive Structures",
      "text" : "In the Introduction, we discussed that the group additive structure for f(X) may not be unique. Here is an example. Consider the model Y = 2+ 3X1 +1/(1 +X22 +X 2 3 ) + arcsin ((X4 +X5)/2) + , where is the 0 mean error independent of X . According to our definition, this model admits the group additive structure G0 = {(1) , (2, 3) , (4, 5)}. Let G1 = {(1, 2, 3) , (4, 5)} and G2 = {(1, 4, 5) , (2, 3)}. The model can also be said to admit G1 and G2. However, there exists a major difference between G0, G1 and G2. While the groups in G0 cannot be further divided into subgroups, bothG1 andG2 contain groups that can be further split. We define the following partial order between group structures to characterize the difference and their relationship. Definition 1. Let G and G′ be two group additive structures. If for every group u ∈ G there is a group v ∈ G′ such that u ⊆ v, thenG is called a sub group additive structure ofG′. This relation is denoted as G ≤ G′. Equivalently, G′ is a super group additive structure of G, denoted as G′ ≥ G.\nIn the previous example, G0 is a sub group additive structure of both G1 and G2. However, the order between G1 and G2 is not defined. Let X := [0, 1]p be the p-dimensional unit cube for all the predictor variables X with distribution PX . For a group of predictor variables u, we define the space of square integrable functions as L2u(X ) := {g ∈ L2PX (X ) | g(X) = fu(Xu)}, that is L2u contains the functions that only depend on the variables in group u. Then the group additive model f(X) = ∑d j=1 fuj (Xuj ) is a member of the direct sum function space defined as L2G(X ) := ⊕u∈GL2u(X ). Let |u| be the cardinality of the group u. If u is the only group in a group additive structure and |u| = p, then L2u = L2G and f is a fully non-parametric function. The following proposition shows that the order of two different group additive structures is preserved by their corresponding square integrable function spaces. Proposition 1. Let G1 and G2 be two group additive structures. If G1 ≤ G2, then L2G1 ⊆ L 2 G2\n. Furthermore, if X1, . . . , Xp are independent and G1 6= G2, then L2G1 ⊂ L 2 G2\n. Definition 2. Let f(X) be an square integrable function. For a group additive structure G, if there is a function fG ∈ L2G such that fG = f , then G is called an amiable group additive structure for f .\nIn the example discussed in the beginning of the subsection, G0, G1 and G2 are all amiable group structures. So amiable group structures may not be unique. Proposition 2. Suppose G is an amiable group additive structure for f . If there is a second group additive structure G′ such that G ≤ G′, then G′ is also amiable for f .\nWe denote the collection of all amiable group structures for f(X) as Ga, which is partially ordered and complete. Therefore, there exists a minimal group additive structure in Ga, which is the most concise group additive structure for the target function. We state this result as a theorem. Theorem 1. Let Ga be the set of amiable group additive structures for f . There is a unique minimal group additive structure G∗ ∈ Ga such that G∗ ≤ G for all G ∈ Ga, where the order is given by Definition 1. G∗ is called the intrinsic group additive structure for f .\nFor statistical modeling, G∗ achieves the greatest dimension reduction for the relationship between Y and X . It induces the smallest function space which includes the model. In general, the intrinsic group structure can help much mitigate the curse of dimensionality while improving both efficiency and interpretability of high dimensional nonparametric regression."
    }, {
      "heading" : "2.2 Kernel Method with Known Intrinsic Group Additive Structure",
      "text" : "Suppose the intrinsic group additive structure for f(X) = E[Y |X] is known to be G∗ = {uj}dj=1, that is, f(X) = fu1(Xu1) + · · ·+ fud(Xud). Then, we will use the kernel method to estimate the functions fu1 , fu2 , . . ., fud . Let (Kuj ,Huj ) be the kernel and its corresponding RKHS for the j-th group uj . Then using kernel methods is to solve\nf̂λ,G∗ = argmin fG∗∈HG∗\n{ 1\nn n∑ i=1 (yi − fG∗(xi))2 + λ‖fG∗‖2HG∗\n} , (2)\nwhereHG∗ := {f = ∑d j=1 fuj | fuj ∈ Huj}. The subscripts of f̂ are used to explicitly indicate its dependence on the group additive structure G∗ and tuning parameter λ.\nIn general, an RKHS is usually smaller than the L2 space defined on the same input domain. So, it is not always true that f̂λ,G∗ achieves f . However, one can choose to use universal kernels Kuj so that their corresponding RKHSs are dense in the L2 spaces (see [3], [15]). Using universal kernels allows f̂λ,G∗ to not only achieve unbiasedness but also recover the group additive structure of f(X). This is the fundamental reason for the consistency property of our proposed method to identify the intrinsic group additive structure. Two examples of universal kernel are Gaussian and Laplace."
    }, {
      "heading" : "2.3 Identification of Unknown Intrinsic Group Additive Structure",
      "text" : ""
    }, {
      "heading" : "2.3.1 Penalization on Group Additive Structures",
      "text" : "The success of the kernel method hinges on knowing the intrinsic group additive structure G∗. In practice, however, G∗ is seldom known, and it may be of primary interest to identify G∗ while estimating a group additive model. Recall that in Subsection 2.1, we have shown that G∗ exists and is unique. The other group additive structures belong to two categories, amiable and non-amiable.\nLet’s consider an arbitrary non-amiable group additive structure G ∈ G \\ Ga first. If G is used in the place of G∗ in (2), the solution f̂λ,G, as an estimator of f , will have a systematic bias because the L2 distance between any function fG ∈ HG and the true function f will be bounded below. In other words, using a non-amiable structure will result in poor fitting of the model.\nNext we consider an arbitrary amiable group additive structure G ∈ Ga to be used in (2). Recall that because G is amiable, we have fG∗ = fG almost surely (in population) for the true function f(X). The bias of the resulting fitted function f̂λ,G will vanish as the sample size increases. Although their asymptotic rates are in general different, under fixed sample size n, simply using goodness of fit will not be able to distinguish G from G∗. The key difference between G∗ and G is their structural complexities, that is, G∗ is the smallest among all amiable structures (i.e. G∗ ≤ G,∀G ∈ Ga). Suppose a proper complexity measure of a group additive structure G can be defined (to be addressed in the next section) and is denoted as C(G). We can then incorporate C(G) into (2) as an additional penalty term and change the kernel method to the following structure-penalized kernel method.\nf̂λ,µ, Ĝ = argmin fG∈HG,G\n{ 1\nn n∑ i=1 (yi − fG(xi))2 + λ‖fG‖2HG + µC(G) } . (3)\nIt is clear that the only difference between (2) and (3) is the term µC(G). As discussed above, the intrinsic group additive structure G∗ can achieve the goodness of fit represented by the first two terms in (3) and the penalty on the structural complexity represented by the last term. Therefore, by properly choosing the tuning parameters, we expect that Ĝ is consistent in that the probability Ĝ = G∗ increases to one as n increases (see the Theory Section below). In the next section, we derive a tractable complexity measure for a group additive structure."
    }, {
      "heading" : "2.3.2 Complexity Measure of Group additive Structure",
      "text" : "It is tempting to propose an intuitive complexity measure for a group additive structure C(·) such that C(G1) ≤ C(G2) whenever G1 ≤ G2. The intuition however breaks down or at least becomes less clear when the order between G1 and G2 cannot be defined. From Proposition 1, it is known that when G1 < G2, we have L2G1 ⊂ L 2 G2 . It is not difficult to show that it is also true that when\nG1 < G2, thenHK,G1 ⊂ HK,G2 . This observation motivates us to define the structural complexity measure of G through the capacity measure of its corresponding RKHSHG. There exist a number of different capacity measures for RKHSs in the literature, including entropy [18], VC dimensions [17], Rademacher complexity [1], and covering numbers ([14], [18]). After investigating and comparing different measures, we use covering number to design a practically convenient complexity measure for group additive structures.\nIt is known that an RKHS HK can be embedded in the continuous function space C(X ) (see [12], [18]), with the inclusion mapping denoted as IK : HK → C(X ). Let HK,r = {h : ‖h‖Hk ≤ r, and h ∈ HK} be an r-ball in HK and I (HK,r) be the closure of I (HK,r) ⊆ C(X ). One way to measure the capacity of HK is through the covering number of I (HK,r) in C(X ), denoted as N ( , I (HK,r), d∞), which is the smallest cardinalty of a subset S of C(X ) such that I (HK,r) ⊂ ∪s∈S{t ∈ C(X ) : d∞(t, s) ≤ }. Here is any small positive value and d∞ is the sup-norm.\nThe exact formula for N ( , I (HK,r), d∞) is in general not available. Under certain conditions, various upper bounds have been obtained in the literature. One such upper bound is presented below.\nWhen K is a convolution kernel, i.e. K(x, t) = k(x − t), and the Fourier transform of k decays exponentially, then it is given in [18] that\nlnN ( , I(HK,r), d∞ ) ≤ Ck,p ( ln r )p+1 , (4)\nwhere Ck,p is a constant depending on the kernel function k and input dimension p. In particular, when K is a Gaussian kernel, [18] has obtained more elaborate upper bounds.\nThe upper bound in (4) depends on r and through ln(r/ ). When → 0 with r being fixed (e,g. r = 1 when a unit ball is considered), (ln(r/ ))p+1 dominates the upper bound. According to [8], the growth rate of N ( , IK) or its logarithm can be viewed as a capacity measure of RKHS. So we use (ln(r/ ))\np+1 as the capacity measure, which can be reparameterized as αp+1 with α = ln(r/ ). Let C(Hk) denote the capacity measure ofHk, which is defined as C(Hk) = (ln(r/ ))p+1 = α( )p+1. We know is the radius of a covering ball, which is the unit of measurement we use to quantify the capacity. The capacity of two RKHSs with different input dimensions are easier to be differentiated when is small. This gives an interpretation of α.\nWe have defined a capacity measure for a general RKHS. In Problem (3), the model spaceHG is a direct sum of a number of RKHSs. Let G = {u1, . . . ,ud}; let HG,Hu1 , . . . ,Hud be the RKHSs corresponding to G,u1, . . . ,ud, respectively; let IG, Iu1 , . . . , Iud be the inclusion mappings of HG,Hu1 , . . . ,Hud into C(X ). Then, we have the following proposition. Proposition 3. Let G be a group additive structure andHG be the induced direct sum RKHS defined in (3). Then, we have the following inequality relating the covering number ofHG and the covering numbers ofHuj\nlnN ( , IG, d∞) ≤ d∑ j=1 lnN ( |G| , Iuj , d∞ ) , (5)\nwhere |G| denotes the number of groups in G.\nBy applying Proposition 3 and using the parameterized upper bound, we have lnN ( , IG, d∞) = O (∑ u∈G α( ) |u|+1) . The rate can be used as the explicit expression of the complexity measure\nfor group additive structures, that is C(G) = ∑d j=1 α( )\n|uj |+1. Recall that there is another tuning parameter µ which controls the effect of the complexity of group structure on the penalized risk. By combining the common factor 1 in the exponent with µ, we could further simplify the penalty’s expression. Thus, we have the following explicit formulation for GASI\nf̂λ,µ, Ĝ = argmin fG∈HG,G  n∑ i=1 (yi − fG(xi))2 + λ‖fG‖2HG + µ d∑ j=1 α|uj |  . (6)"
    }, {
      "heading" : "2.4 Estimation",
      "text" : "We assume that the value of λ is given. In practice, λ can be tuned separately. If the values of µ and α are also given, Problem (6) can be solved by following a two-step procedure.\nFirst, when the group structure G is known, fu can be estimated by solving the following problem\nR̂λG = min fG∈HG\n{ 1\nn n∑ i=1 (yi − fG(xi))2 + λ ‖fG‖2HG\n} . (7)\nSecond, the optimal group structure is chosen to achieve both small fitting error and complexity, i.e.\nĜ = argmin G∈G R̂λG + µ d∑ j=1 α|uj |  . (8) The two-step procedure above is expected to identify the intrinsic group structure, that is, Ĝ = G∗. Recall a group structure belongs to one of the three categories, intrinsic, amiable, or non-amiable structures. If G is non-amiable, then R̂λG is expected to be large, because G is a wrong structure which will result in a biased estimate. If G is amiable, though R̂λG is expected to be small, the complexity penalty of G is larger than that for G∗. As a consequence, only G∗ can simultaneously achieve a small R̂λG∗ and a relatively small complexity. Therefore, when the sample size is large enough, we expect Ĝ = G∗ with high probability. If the values of µ and α are not given, a separate validation set can be used to select tuning parameters.\nThe two-step estimation is summarized in Algorithm 1. When a model contains a large number of predictor variables, such exhaustive search suffers high computational cost. In order to apply GASI on a large model, we propose a backward stepwise algorithm which is illustrated in Algorithm 2.\nAlgorithm 1: Exhaustive Search w/ Validation 1: Split data into training (T ) and validation (V) sets. 2: for (µ, α) in grid do 3: for G ∈ G do 4: R̂G, f̂G ← solve (7) using G; 5: Calculate the sum in (8), denoted by R̂pen,µ,αG ; 6: end for 7: Ĝµ,α ← argminG∈G R̂ pen,µ,α G ; 8: ŷV ← f̂Ĝµ,α(x V); 9: e2 Ĝµ,α\n← ‖yV − ŷV‖2; 10: end for 11: µ∗, α∗ ← argminµ,α e2Ĝµ,α ; 12: G∗ ← Ĝµ ∗,α∗ ;\nAlgorithm 2: Basic Backward Stepwise\n1: Start with the group structure {(1, . . . , p)}; 2: Solve (6) and obtain its minimum value R̂penG ; 3: for each predictor variable j do 4: G′ ← either split j as a new group or add\nto an existing group; 5: Solve (6) and obtain its minimum value R̂penG′ ; 6: if R̂penG′ < R̂ pen G then 7: Keep G′ as the new group structure; 8: end if 9: end for\n10: return G′;"
    }, {
      "heading" : "3 Theory",
      "text" : "In this section, we prove that the estimated group additive structure Ĝ as a solution to (6) is consistent, that is the probability P (Ĝ = G∗) goes to 1 as the sample size n goes to infinity. The proof and supporting lemmas are included in the supplementary material.\nLet R(fG) = E[(Y − f(X))2] denote the population risk of a function f ∈ HG, and R̂(f) = 1 n ∑n i=1(yi − f(xi))2 be the empirical risk. First, we show that for any amiable structure G ∈ Ga, its minimized empirical risk R̂(f̂G) converges in probability to the optimal population riskR(f∗G∗) achieved by the intrinsic group additive structure. Here f̂G denotes the minimizer of Problem (7) with the given G, and f∗G∗ denotes the minimizer of the population risk when the intrinsic group structure is used. The result is given below as Proposition 4. Proposition 4. Let G∗ be the intrinsic group additive structure, G ∈ Ga a given amiable group structure, andHG∗ andHG the respective direct sum RKHSs. If f̂λG ∈ HG is the optimal solution of\nProblem (7), then for any > 0, we have\nP ( |R̂(f̂G)−R(f∗G∗)| > ) ≤ 12n · exp {∑ u∈G lnN ( 12|G| ,Hu, d∞ ) − 2n 144 } +\n12n · exp {∑ u∈G lnN ( 12|G| ,Hu, d∞ ) − n ( 24 − λn‖f ∗ G∗‖2 12 )2} . (9)\nNote that λn in (9) must be chosen such that /24 − λn‖f∗G∗‖2/12 is positive. For a fixed p, the number of amiable group additive structures is finite. Using a Bonferroni type of technique, we can in fact obtain a uniform upper bound for all of the amiable group additive structures in Ga. Theorem 2. Let Ga be the set of all amiable group structures. For any > 0 and n > 2/ 2, we have\nP ( sup G∈Ga |R̂g(f̂λG)−Rg(f∗G∗)| > ) ≤ 12n|Ga| · [ exp { max G∈Ga lnN ( 12 ,HG, d∞ ) − 2n 144 }\n+ exp { max G∈Ga lnN ( 12 ,HG, d∞ ) − n ( 24 − λn‖f ∗ G∗‖2 12 )2}] (10)\nNext we consider a non-amiable group additive structure G′ ∈ G \\Ga. It turns out that R̂(f̂G) fails to converge toR(f∗G∗), and |R̂(f̂G)−R(f∗G∗)| converges to a positive constant. Furthermore, because the number of non-amiable group additive structures is finite, we can show that |R̂(f̂G)−R(f∗G∗)| is uniformly bounded below from zero with probability going to 1. We state the results below. Theorem 3. (i) For a non-amiable group structure G ∈ G \\ Ga, there exists a constant C > 0 such that |R̂g(f̂λG) − Rg(f∗G∗)| converges to C in probability. (ii) There exits a constant C̃ such that P (|R̂g(f̂λG)−Rg(f∗G∗)| > C̃ for all G ∈ G \\ Ga) goes to 1 as n goes to infinity.\nBy combining Theorem 2 and Theorem 3, we can prove consistency for our GASI method. Theorem 4. Let λn ∗ n → 0. By choosing a proper tuning parameter µ > 0 for the structural penalty , the estimated group structure Ĝ is consistent for the intrinsic group additive structure G∗, that is, P (Ĝ = G∗) goes to one as the sample size n goes to infinity."
    }, {
      "heading" : "4 Simulation",
      "text" : "In this section, we evaluate the performance of GASI using synthetic data. Table 1 lists the five models we are using. Observations of X are simulated independently from N(0, 1) in M1, Unif(−1, 1) in M2 and M3, and Unif(0, 2) in M4 and M5. The noise is i.i.d. N(0, 0.012). The grid values of µ are equally spaced in [1e−10, 1/64] on a log-scale and each α is an integer in [1, 10]. We first show that GASI has the ability to identify the intrinsic group additive structure. The two-step procedure is carried out for each (µ, α) pair multiple times. If there are (µ, α) pairs for each model that the true group structure can be often identified, then GASI has the power to identify true group structures. We also apply Algorithm 1 which uses an additional validation set to select the parameters. We simulate 100 different samples for each model. The frequency of the true group structure being identified is calculated for each (µ, α).\nIn Table 2, we report the maximum frequency and the corresponding (µ, α) for each model. The complete results are included in the supplementary material. It can be seen from the left panel that the intrinsic group additive structures can be successfully identified. When the parameters are tuned, the middle panel shows that the performance of Model 1 deteriorated. This might be caused by the estimation method (KRR to solve Problem (7)) used in the algorithm. It could also be affected by λ.\nWhen the number of predictor variables increases, we use a backward stepwise algorithm. We apply Algorithm 2 on the same models. The results are reported in the right panel in Figure 2. The true group structures could be identified most of time for Model 1, 2, 3, 5. The result of Model 4 is not satisfying. Since stepwise algorithm is greedy, it is possible that the true group structures were never visited. Further research is needed to develop a better algorithms."
    }, {
      "heading" : "5 Real Data",
      "text" : "In this section, we report the results of applying GASI on the Boston Housing data (another real data application is reported in the supplementary material). The data includes 13 predictor variables used to predict the house median value. The sample size is 506. Our goal is to identify a probable group additive structure for the predictor variables. The backward algorithm is used and the tuning parameters µ and α are selected via 10-fold CV. The group structure that achieves the lowest average validation error is {(1, 6) , (2, 11) , (3) , (4, 9) , (5, 8) , (7, 13) , (10, 12)}, which is used for further investigation. Then the nonparametric functions for each group were estimated using the whole data set. Because each group contains no more than two variables, the estimated functions can be visualized. Figure 1 shows the selected results.\nIt is interesting to see some patterns emerging in the plots. For example, the top-left plot shows the function of the average number of rooms per dwelling and per capita crime rate by town. We can see the house value increases with more rooms and decreases as the crime rate increases. However, when the crime rate is low, smaller sized houses (4 or 5 rooms) seem to be preferred. The top-right plot\nshows that there is a changing point in terms of how house value is related to the size of non-retail business in the area. The value initially drops when the percentage of non-retail business is small, then increases at around 8%. The increase in the value might be due to the high demand of housing from the employees of those business."
    }, {
      "heading" : "6 Discussion",
      "text" : "We use group additive model for nonparametric regression and propose a RKHS complexity penalty based approach for identifying the intrinsic group additive structure. There are two main directions for future research. First, our penalty function is based on the covering number of RKHSs. It is of interest to know if there exists other more effective penalty functions. Second, it is of great interest to further improve the proposed method and apply it in general high dimensional nonparametric regression."
    } ],
    "references" : [ {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "The Journal of Machine Learning Research, 3:463–482",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Pattern Recognition and Machine Learning (Information Science and Statistics)",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Springer- Verlag New York, Inc., Secaucus, NJ, USA",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Vector valued reproducing kernel hilbert spaces and universality",
      "author" : [ "C. Carmeli", "E. De Vito", "A. Toigo", "V. Umanitá" ],
      "venue" : "Analysis and Applications, 8(01):19–61",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Smoothing spline ANOVA models",
      "author" : [ "C. Gu" ],
      "venue" : "volume 297. Springer Science & Business Media",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Generalized additive models",
      "author" : [ "T. Hastie", "R. Tibshirani" ],
      "venue" : "Statistical science, pages 297–310",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Additive approximations in high dimensional nonparametric regression via the salsa",
      "author" : [ "K. Kandasamy", "Y. Yu" ],
      "venue" : "Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML’16, pages 69–78. JMLR.org",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : "The MIT Press",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Covering numbers of Gaussian reproducing kernel Hilbert spaces",
      "author" : [ "T. Kühn" ],
      "venue" : "Journal of Complexity, 27(5):489–499",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sparse gaussian graphical models with unknown block structure",
      "author" : [ "B.M. Marlin", "K.P. Murphy" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 705–712, New York, NY, USA",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Machine learning: a probabilistic perspective",
      "author" : [ "K.P. Murphy" ],
      "venue" : "MIT press",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Optimal kernel group transformation for exploratory regression analysis and graphics",
      "author" : [ "C. Pan", "Q. Huang", "M. Zhu" ],
      "venue" : "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 905–914. ACM",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On the mathematical foundations of learning",
      "author" : [ "T. Poggio", "C. Shelton" ],
      "venue" : "American Mathematical Society, 39(1):1–49",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Applied functional data analysis: methods and case studies",
      "author" : [ "J.O. Ramsay", "B.W. Silverman" ],
      "venue" : "volume 77. Citeseer",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Learning with kernels",
      "author" : [ "A.J. Smola", "B. Schölkopf" ],
      "venue" : "Citeseer",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Support vector machines",
      "author" : [ "I. Steinwart", "A. Christmann" ],
      "venue" : "Springer Science & Business Media",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Optimal global rates of convergence for nonparametric regression",
      "author" : [ "C.J. Stone" ],
      "venue" : "The annals of statistics, pages 1040–1053",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "The nature of statistical learning theory",
      "author" : [ "V. Vapnik" ],
      "venue" : "Springer Science & Business Media",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The covering number in learning theory",
      "author" : [ "D.-X. Zhou" ],
      "venue" : "Journal of Complexity, 18(3):739 – 767",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "[16] shows that with some regularity conditions, the optimal asymptotic error rate for estimating a dtimes differentiable function is O ( n−d/(2d+p) ) , where p is the dimensionality of X .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "To overcome the drawbacks of high dimensional nonparametric regression, one popularly used approach is to impose the additive structure [5] on f(X), that is to assume that f(X) = f1(X1) + · · ·+fp(Xp) where f1, .",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "To address this limitation, functional ANOVA models were proposed to accommodate higher order interactions, see [4] and [13].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "To address this limitation, functional ANOVA models were proposed to accommodate higher order interactions, see [4] and [13].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "This model considers all interactions of order up to D, which is estimated by Kernel Ridge Regression (KRR) [10] with the elementary symmetric polynomial (ESP) kernel.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "When proposing the Optimal Kernel Group Transformation (OKGT) method for nonparametric regression, [11] considers the additive structure of predictor variables in groups instead of individual predictor variables.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "However, in [11], there are two important issues not addressed.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "Second, [11] has not proposed a systematic approach to identify the group additive structure.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 1,
      "context" : "There exists a connection between our proposed group additive model and graphical models ([2], [7]).",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "There exists a connection between our proposed group additive model and graphical models ([2], [7]).",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "This is especially true when a sparse block structure is imposed [9].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "However, one can choose to use universal kernels Kuj so that their corresponding RKHSs are dense in the L(2) spaces (see [3], [15]).",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "However, one can choose to use universal kernels Kuj so that their corresponding RKHSs are dense in the L(2) spaces (see [3], [15]).",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 17,
      "context" : "There exist a number of different capacity measures for RKHSs in the literature, including entropy [18], VC dimensions [17], Rademacher complexity [1], and covering numbers ([14], [18]).",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "There exist a number of different capacity measures for RKHSs in the literature, including entropy [18], VC dimensions [17], Rademacher complexity [1], and covering numbers ([14], [18]).",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : "There exist a number of different capacity measures for RKHSs in the literature, including entropy [18], VC dimensions [17], Rademacher complexity [1], and covering numbers ([14], [18]).",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 13,
      "context" : "There exist a number of different capacity measures for RKHSs in the literature, including entropy [18], VC dimensions [17], Rademacher complexity [1], and covering numbers ([14], [18]).",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 17,
      "context" : "There exist a number of different capacity measures for RKHSs in the literature, including entropy [18], VC dimensions [17], Rademacher complexity [1], and covering numbers ([14], [18]).",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 11,
      "context" : "It is known that an RKHS HK can be embedded in the continuous function space C(X ) (see [12], [18]), with the inclusion mapping denoted as IK : HK → C(X ).",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "It is known that an RKHS HK can be embedded in the continuous function space C(X ) (see [12], [18]), with the inclusion mapping denoted as IK : HK → C(X ).",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "K(x, t) = k(x − t), and the Fourier transform of k decays exponentially, then it is given in [18] that",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 17,
      "context" : "In particular, when K is a Gaussian kernel, [18] has obtained more elaborate upper bounds.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "According to [8], the growth rate of N ( , IK) or its logarithm can be viewed as a capacity measure of RKHS.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "The grid values of μ are equally spaced in [1e−10, 1/64] on a log-scale and each α is an integer in [1, 10].",
      "startOffset" : 100,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "The grid values of μ are equally spaced in [1e−10, 1/64] on a log-scale and each α is an integer in [1, 10].",
      "startOffset" : 100,
      "endOffset" : 107
    } ],
    "year" : 2017,
    "abstractText" : "The additive model is one of the most popularly used models for high dimensional nonparametric regression analysis. However, its main drawback is that it neglects possible interactions between predictor variables. In this paper, we reexamine the group additive model proposed in the literature, and rigorously define the intrinsic group additive structure for the relationship between the response variable Y and the predictor vector X , and further develop an effective structure-penalized kernel method for simultaneous identification of the intrinsic group additive structure and nonparametric function estimation. The method utilizes a novel complexity measure we derive for group additive structures. We show that the proposed method is consistent in identifying the intrinsic group additive structure. Simulation study and real data applications demonstrate the effectiveness of the proposed method as a general tool for high dimensional nonparametric regression.",
    "creator" : null
  }
}