{
  "name" : "f0204e1d3ee3e4b05de4e2ddbd39e076.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control",
    "authors" : [ "Fanny Yang", "Aaditya Ramdas" ],
    "emails" : [ "fanny-yang@berkeley.edu", "ramdas@berkeley.edu", "jamieson@cs.washington.edu", "wainwrig@berkeley.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Randomized trials are the default option to determine whether potential improvements of an alternative method (e.g. website design for a tech company, or medication in clinical trials for pharmaceutical companies) are significant compared to a well-established default. In the applied domain, this is often colloquially referred to as A/B testing or A/B/n testing for several alternatives. The standard practice is to divert a small amount of the traffic or patients to the alternative and control. If an alternative appears to be significantly better, it is implemented; otherwise, the default setting is maintained.\nAt first glance, this procedure seems intuitive and simple. However, in cases where the aim is to optimize over one particular metric, one can do better. In particular, this common tool suffers from several downsides. (1) First, one may wish to allocate more traffic to a better treatment if it is clearly better. Yet typical A/B/n testing frameworks split the traffic uniformly over alternatives. Adaptive techniques should help to detect better alternatives faster. (2) Second, companies often desire to continuously monitor an ongoing A/B test as they may adjust their termination criteria as time goes by and possibly stop earlier or later than originally intended. However, this practice may result in many more false alarms if not properly accounted for. This is one of the reasons for the lack of reproducibility of scientific results, an issue recently receiving increased attention from the public media. (3) Third, the lack of sufficient evidence or an insignificant improvement of the metric may make it undesirable from a practical or financial perspective to replace the default. Therefore, when a\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\ncompany runs hundreds to thousands of A/B tests within a year, ideally the number of statistically insignificant changes that it made should be small relative to the total number of changes made. While controlling the false alarm rate of each individual test does not achieve this type of false discovery rate (FDR) control, there are known procedures in the multiple testing literature that are tailored to this problem.\nIn this paper, we provide a novel framework that addresses the above shortcomings of A/B or A/B/n testing. The first concern is tackled by employing recent advances in adaptive sampling like the pureexploration multi-armed bandit (MAB) algorithm. For the second concern, we adopt the notion of any-time p-values for guilt-free continuous monitoring. Finally, we handle the third issue using recent results in online FDR control. Hence the combined framework can be described as doubly-sequential (sequences of MAB tests, each of which is itself sequential). Although each of those problems has been studied in hitherto disparate communities, how to leverage the best of all worlds, if at all possible, has remained an open problem. The main contributions of this paper are in successfully merging these ideas in a meta framework and presenting the conditions under which it can be shown to yield near-optimal sample complexity and FDR control.\nThe remainder of this paper is organized as follows. In Section 2, we lay out the conceptual challenges that we address in the paper, and describe a meta-algorithm that combines adaptive sampling strategies with FDR control procedures. Section 3 is devoted to the description of a concrete procedure, along with some theoretical guarantees on its properties. In Section 4, we discuss some results of our extensive experiments on both simulated and real-world data sets available to us."
    }, {
      "heading" : "2 Formal experimental setup and a meta-algorithm",
      "text" : "In this section provide a high-level overview of our proposed combined framework aimed at addressing the shortcomings mentioned in the introduction. A specific instantiation of this meta-algorithm along with detailed theoretical guarantees are specified in Section 3.\nFor concreteness, we refer to the system designer, whether a tech company or a pharmaceutical company, as a (data) scientist. We assume that the scientist needs to possibly conduct an infinite number of experiments sequentially, indexed by j. Each experiment has one default setting, referred to as the control, and K = K(j) alternative settings, called the treatments or alternatives. The scientist must return one of the K + 1 options that is the “best” according to some predefined metric, before the next experiment is started. Such a setup is a simple mathematical model both for clinical trials run by pharmaceutical labs, and A/B/n testing used at scale by tech companies.\nOne full experiment consists of a sequence of steps. In each step, the scientist assigns a new person to one of the K+ 1 options and observes an outcome. In practice, the role of the scientist could be taken by an adaptive algorithm, which determines the assignment at time step j by careful consideration of all previous outcomes. Borrowing terminology from the multi-armed bandit (MAB) literature, we refer to each of the K + 1 options as an arm, and each assignment to arm i is termed “pulling arm i”. For concreteness, we assign the index 0 to the control arm and note that it is known to the algorithm. Furthermore, we assume that the observable metric from each pull of arm i = 0, 1, . . . ,K corresponds to an independent draw from an unknown probability distribution with expectation µi. In the sequel we use µi? := max\ni=1,...,K µi to denote the mean of the best arm. We refer the reader to\nTable 1 in Appendix A for a glossary of the notation used throughout this paper."
    }, {
      "heading" : "2.1 Some desiderata and difficulties",
      "text" : "Given the setup above, how can we mathematically describe the guarantees that the companies might desire from an improved multiple-A/B/n testing framework? For which parts can we leverage known results and what challenges remain?\nFor the purpose of addressing the first question, let us adopt terminology from the hypothesis testing literature and view each experiment as a test of a null hypothesis. Any claim that an alternative arm is the best is called a discovery, and when such a claim is erroneous, it is called a false discovery. When multiple hypotheses are to be tested, the scientist needs to define the quantity it wants to control. While we may desire that the probability of even a single false discovery is small, this is usually far too stringent for a large and unknown number of tests and results in low power. For this reason, [1] proposed that it may be more useful to control the expected ratio of false discoveries to the total number of discoveries (called the False Discovery Rate, or FDR for short) or the ratio of expected number of false discoveries to the expected number of total discoveries (called the modified FDR\nor mFDR for short). Over the past decades, the FDR and its variants like the mFDR have become standard quantities for multiple testing applications. In the following, if not otherwise specified, we use the term FDR to denote both measures in order to simplify the presentation. In Section 3, we show that both mFDR and FDR can be controlled for different choices of procedures."
    }, {
      "heading" : "2.1.1 Challenges in viewing an MAB instance as a hypothesis test",
      "text" : "In our setup, we want to be able to control the FDR at any time in an online manner. Online FDR procedures were first introduced by Foster and Stine [2], and have since been studied by other authors (e.g., [3, 4]). They are based on comparing a valid p-value P j with carefully-chosen levels αj for each hypothesis test1. We reject the null hypothesis, represented as Rj = 1, when P j ≤ αj and we set Rj = 0 otherwise.\nAs mentioned, we want to use adaptive MAB algorithms to test each hypothesis, since they can find a best arm among K + 1 with near-optimal sample complexity. However the traditional MAB setup does not account for the asymmetry between the arms as is the case in a testing setup, with one being the default (control) and others being alternatives (treatments). This is the standard scenario in A/B/n testing applications, as e.g. a company might prefer wrong claims that the control is the best (false negative), rather than wrong claims that an alternative is the best (false positive), simply because new system-wide adoption of selected alternatives might involve high costs. What would be a suitable null hypothesis in this hybrid setting? For the sake of continuous monitoring, is it possible to define and compute always-valid p-values that are super-uniformly distributed under the null hypothesis when computed at any time t?\nIn addition to asymmetry, the practical scientist might have a different incentive than the ideal outcome for MAB algorithms as he/she might not want to find the best alternative if it is not substantially better than the control. Indeed, if the net gain is small, it might be offset by the cost of implementing the change from the existing default choice. By similar reasoning, we may not require identifying the single best arm if there is a set of arms with similar means all larger than the rest. We propose a sensible null-hypothesis for each experiment which incorporates the approximation and minimum improvement requirement as described above, and provide an always valid p-value which can be easily calculated at each time step in the experiment. We show that a slight modification of the usual LUCB algorithm caters to this specific null-hypothesis while still maintaining near-optimal sample complexity."
    }, {
      "heading" : "2.1.2 Interaction between MAB and FDR",
      "text" : "In order to take advantage of the sample efficiency of best-arm bandit algorithms, it is crucial to set the confidence levels close to what is needed. Given a user-defined level α, at each hypothesis j, online\n1A valid P j must be stochastically dominated by a uniform distribution on [0, 1], which we henceforth refer to as super-uniformly distributed.\nFDR procedures automatically output the significance level αj which are sufficient to guarantee FDR control, based on past decisions. Can we directly set the MAB confidence levels to these output levels αj? If we do, our p-values are not independent across different hypotheses anymore: P j directly depends on the FDR levels αj and each αj in turn depends on past MAB rejections, thus on past MAB p-values (see Figure 1). Does the new interaction compromise FDR guarantees?\nAlthough known procedures as in [2, 4] guarantee FDR control for independent p-values, this does not hold for dependent p-values in general. Hence FDR control guarantees cannot simply be obtained out of the box. A key insight that emerges from our analysis is that an appropriate bandit algorithm actually shapes the p-value distribution under the null in a “good” way that allows us to control FDR."
    }, {
      "heading" : "2.2 A meta-algorithm",
      "text" : "Procedure 1 summarizes our doubly-sequential procedure, with a corresponding flowchart in Figure 1. We will prove theoretical guarantees after instantiating the separate modules. Note that our framework allows the scientist to plug in their favorite best-arm MAB algorithm or online FDR procedure. The choice for each of them determines which guarantees can be proven for the entire setup. Any independent improvement in either of the two parts would immediately lead to an overall performance boost of the overall framework.\nProcedure 1 MAB-FDR Meta algorithm skeleton 1. The scientist sets a desired FDR control rate α. 2. For each j = 1, 2, . . . : • Experiment j receives a designated control arm and some number of alternative arms. • An online-FDR procedure returns an αj that is some function of the past values {P `}j−1`=1 . • An MAB procedure is executed with inputs (a) the control arm and K(j) alternative arms,\n(b) confidence level αj , maintains an always valid p-value for each t and if the procedure self-terminates, returns a recommended arm. • When the MAB procedure is terminated at time t by itself or the user, if the arm with the highest empirical mean is not the control arm and P jt ≤ αj , then we return P j := P jt , and the control arm is rejected in favor of this empirically best arm."
    }, {
      "heading" : "3 A concrete procedure with guarantees",
      "text" : "We now take the high-level road map given in Procedure 1, and show that we can obtain a concrete, practically implementable framework with FDR control and power guarantees. We first discuss the key modeling decisions we have to make in order to seamlessly embed MAB algorithms into an online FDR framework. We then outline a modified version of a commonly used best-arm algorithm, before we finally prove FDR and power guarantees for the concrete combined procedure.\n3.1 Defining null hypotheses and constructing p-values\nOur first task is to define a null hypothesis for each experiment. As mentioned before, the choice of the null is not immediately obvious, since we sample from multiple distributions adaptively instead of independently. In particular, we will generally not have the same number of samples for all arms. Given a default mean µ0 and alternatives means {µi}Ki=1, we propose that the null hypothesis for the j-th experiment should be defined as\nHj0 : µ0 ≥ µi − for all i = 1, . . . ,K, (1) where we usually omit the index j for simplicity. It remains to define an always valid p-value (previously defined by Johari et al. [5]) for each experiment for the purpose of continuous monitoring. It is defined as a stochastic process {Pt}∞t=1 such that for all fixed and random stopping times T , under any distribution P0 over the arm rewards such that the null hypothesis is true, we have\nP0(PT ≤ α) ≤ α. (2) When all arms are drawn independently an equal number of times, by linearity of expectation one can regard the distance of each pair of samples as a random variable drawn i.i.d. from a distribution with mean µ̃ := µ0 − µi. We can then view the problem as testing the standard hypothesis Hj0 : µ̃ > − . However, when the arms are pulled adaptively, a different solution needs to be found—indeed, in this\ncase, the sample means are not unbiased estimators of the true means, since the number of times an arm was pulled now depends on the empirical means of all the arms.\nOur strategy is to construct always valid p-values by using the fact that p-values can be obtained by inverting confidence intervals. To construct always-valid confidence bounds, we resort to the fundamental concept of the law of the iterated logarithm (LIL), for which non-asymptotic versions have been recently derived and used for both bandits and testing problems (see [6], [7]).\nTo elaborate, define the function\nϕn(δ) =\n√ log( 1δ ) + 3 log(log( 1 δ )) + 3 2 log(log(en))\nn . (3)\nIf µ̂i,n is the empirical average of independent samples from a sub-Gaussian distribution, then it is known (see, for instance, [8, Theorem 8]) that for all δ ∈ (0, 1), we have\nmax { P ( ∞⋃ n=1 {µ̂i,n − µi > ϕn(δ ∧ 0.1)} ) , P ( ∞⋃ n=1 {µ̂i,n − µi < −ϕn(δ ∧ 0.1)} )} ≤ δ, (4)\nwhere δ ∧ 0.1 := min{δ, 0.1}. We are now ready to propose single arm p-values of the form\nPi,t : = sup { γ ∈ [0, 1] | µ̂i,ni(t) − ϕni(t)( γ2K ) ≤ µ̂0,n0(t) + ϕn0(t)( γ 2 ) + } (5)\n= sup { γ ∈ [0, 1] | LCBi(t) ≤ UCB0(t) + } Here we set Pi,t = 1 if the supremum is taken over an empty set. Given these single arm p-values, the always-valid p-value for the experiment is defined as\nPt := min s≤t min i=1,...,K Pi,s. (6)\nWe claim that this procedure leads to an always valid p-value (with proof in Appendix C). Proposition 1. The sequence {Pt}∞t=1 defined via equation (6) is an always valid p-value."
    }, {
      "heading" : "3.2 Adaptive sampling for best-arm identification",
      "text" : "In the traditional A/B testing setting described in the introduction, samples are allocated uniformly to the different alternatives. But by allowing adaptivity, decisions can be made with the same statistical significance using far fewer samples. Suppose moreover that there is a unique maximizer i? := arg max\ni=0,1,...,K µi, so that ∆i := µi? − µi > 0 for all i 6= i?. Then for any δ ∈ (0, 1), best-arm MAB algorithms can identify i? with probability at least 1−δ based on at most2 ∑ i 6=i? ∆ −2 i log(1/δ) total samples (see the paper [9] for a brief survey and [10] for an application to clinical trials). In contrast, if samples are allocated uniformly to the alternatives under the same conditions, then the most natural procedures require K max\ni 6=i? ∆−2i log(K/δ) samples before returning i? with probability\nat least 1− δ. However, standard best-arm bandit algorithms do not incorporate asymmetry as induced by nullhypotheses as in definition (1) by default. Furthermore, recall that a practical scientist might desire the ability to incorporate approximation and a minimum improvement requirement. More precisely, it is natural to consider the requirement that the returned arm ib satisfies the bounds µib ≥ µ0 + and µib ≥ µi? − for some > 0. In Algorithm 1 we present a modified MAB algorithm based on the common LUCB algorithm (see [11, 12]) which incorporates the above desiderata. We provide a visualization of how affects the usual stopping condition in Figure 4 in Appendix A.1.\nThe following proposition applies to Algorithm 1 run with a control arm indexed by i = 0 with mean µ0 and alternative arms indexed by i = 1, . . . ,K with means µi, respectively. Let ib denote the random arm returned by the algorithm assuming that it exits, and define the set\nS? := {i? 6= 0 | µi? ≥ max i=1,...,K µi − and µi? > µ0 + }. (7)\n2Here we have ignored some doubly-logarithmic factors.\nAlgorithm 1 Best-arm identification with a control arm for confidence δ and precision ≥ 0 For all t let ni(t) be the number of times arm i has been pulled up to time t. In addition, for each arm i let µ̂i(t) = 1ni(t) ∑ni(t) τ=1 ri(τ), define\nLCBi(t) := µ̂i,ni(t) − ϕni(t)( δ2K ) and UCBi(t) := µ̂i,ni(t) + ϕni(t)( δ2 ). 1. Set t = 1 and sample every arm once. 2. Repeat: Compute ht = arg max\ni=0,1,...,K µ̂i(t), and `t = arg max i=0,1,...,K,i 6=ht UCBi(t)\n(a) If LCB0(t) > UCBi(t)− , for all i 6= 0, then output 0 and terminate. Else if LCBht(t) > UCB`t(t)− and LCBht(t) > UCB0(t) + , then output ht and terminate. (b) If > 0, let ut = arg maxi 6=0 UCBi(t) and pull all distinct arms in {0, ut, ht, `t} once. If = 0, pull arms ht and `t and set t = t+ 1.\nNote that the mean associated with any index i? ∈ S?, assuming that the set is non-empty, is guaranteed to be -superior to the control mean, and at most -inferior to the maximum mean over all arms.\nProposition 2. The algorithm 1 terminates in finite time with probability one. Furthermore, suppose that the samples from each arm are independent and sub-Gaussian with scale 1. Then for any δ ∈ (0, 1) and ≥ 0, Algorithm 1 has the following guarantees:\n(a) Suppose that µ0 > max i=1,...,K µi − . Then with probability at least 1− δ, the algorithm exits with ib = 0 after taking at most O (∑K i=0 ∆̃ −2 i log(K log(∆̃ −2 i )/δ) ) time steps with effective gaps\n∆̃0 = (µ0 + )− max j=1,...,K µj and\n∆̃i = (µ0 + )− µi.\n(b) Otherwise, suppose that the set S? as defined in equation (7) is non-empty. Then with probability at least 1− δ, the algorithm exits with ib ∈ S? after taking at most O (∑K\ni=0 ∆̃ −2 i log(K log(∆̃ −2 i )/δ)\n) time steps with effective gaps\n∆̃0 = min\n{ max\nj=1,...,K µj − (µ0 + ),max{∆0, }\n} and\n∆̃i = max { ∆i,min { max\nj=1,...,K µj − (µ0 + ),\n}} .\nSee Appendix D for the proof of this claim. Part (a) of Proposition 2 guarantees that when no alternative arm is -superior to the control arm (i.e. under the null hypothesis), the algorithm stops and returns the control arm with probability at least 1− δ. Part (b) guarantees that if there is in fact at least one alternative that is -superior to the control arm (i.e. under the alternative), then the algorithm will find at least one of them that is at most -inferior to the best of all possible arms.\nAs our algorithm is a slight modification of the LUCB algorithm, the results of [11, 12] provide insight into the number of samples taken before the algorithm terminates. Indeed, when = 0 and i? = arg maxi=0,1,...,K µi is a unique maximizer, the nearly optimal sample complexity result of [12] implies that the algorithm terminates under settings (a) and (b) after at most maxj 6=i? ∆ −2 j log(K log(∆ −2 j )/δ)+ ∑ i 6=i? ∆ −2 i log(log(∆ −2 i )/δ) samples are taken (ignoring constants), where ∆i = µi? − µi. In our development to follow, we now bring back the index for experiment j, in particular using P j to denote the quantity P jT at any stopping time T . Here the stopping time can either be defined by the scientist, or in an algorithmic manner."
    }, {
      "heading" : "3.3 Best-arm MAB interacting with online FDR",
      "text" : "After having established null hypotheses and p-values in the context of best-arm MAB algorithms, we are now ready to embed them into an online FDR procedure. In the following, we consider p-values for the j-th experiment P j := P jTj which is just the p-value as defined in equation (6) at the stopping time Tj , which depends on αj .\nWe denote the set of true null and false null hypotheses up to experiment J as H0(J) and H1(J) respectively, where we drop the argument whenever it’s clear from the context. The variable Rj = 1P j≤αj indicates whether a the null hypothesis of experiment j has been rejected, where Rj = 1 denotes a claimed discovery that an alternative was better than the control. The false discovery rate (FDR) and modified FDR up to experiment J are then defined as\nFDR(J) := E ∑ j∈H0 Rj∑J i=1Ri ∨ 1\nand mFDR(J) := E ∑ j∈H0 Rj\nE ∑J i=1Ri + 1 . (8)\nHere the expectations are taken with respect to distributions of the arm pulls and the respective sampling algorithm. In general, it is not true that control of one quantity implies control of the other. Nevertheless, in the long run (when the law of large numbers is a good approximation), one does not expect a major difference between the two quantities in practice.\nThe set of true nullsH0 thus includes all experiments where Hj0 is true, and the FDR and mFDR are well-defined for any number of experiments J , since we often desire to control FDR(J) or mFDR(J) for all J ∈ N. In order to measure power, we define the -best-arm discovery rate as\nBDR(J) := E ∑ j∈H1 Rj1µib≥µi?− 1µib≥µ0+\n|H1(J)| (9)\nWe provide a concrete procedure 2 for our doubly sequential framework, where we use a particular online FDR algorithm due to Javanmard and Montanari [4] known as LORD; the reader should note that other online FDR procedure could be used to obtain essentially the same set of guarantees. Given a desired level α, the LORD procedure starts off with an initial “α-wealth” of W (0) < α. Based on a inifinite sequence {γi}∞i=1 that sums to one, and the time of the most recent discovery τj , it uses up a fraction γj−τj of the remaining α-wealth to test. Whenever there is a rejection, we increase the α-wealth by α−W (0). A feasible choice for a stopping time in practice is Tj := min{T (αj), TS}, where TS is a maximal number of samples the scientist wants to pull and T (αj) is the stopping time of the best-arm MAB algorithm run at confidence αj .\nProcedure 2 MAB-LORD: best-arm identification with online FDR control 1. Initialize W (0) < α, set τ0 = 0, and choose a sequence {γi} s.t. ∑∞ i=1 γi = 1\n2. At each step j, compute αj = γj−τjW (τj) and W (j + 1) = W (j)− αj +Rj(α−W (0))\n3. Output αj and run Algorithm 1 using αj-confidence and stop at a stopping time Tj .\n4. Algorithm 1 returns P j and we reject the null hypothesis if P j ≤ αj . 5. Set Rj = 1P j≤αj , τj = τj−1 ∨ jRj , update j = j + 1 and go back to step 2.\nThe following theorem provides guarantees on mFDR and power for the MAB-LORD procedure. Theorem 1 (Online mFDR control for MAB-LORD). (a) Procedure 2 achieves mFDR control at level α for stopping times Tj = min{T (αj), TS}. (b) Furthermore, if we set TS =∞, Procedure 2 satisfies\nBDR(J) ≥ ∑J j=1 1j∈H1(1− αj) |H1(J)| . (10)\nSee Appendix E for the proof of this claim. Note that by the arguments in the proof of Theorem 1, mFDR control itself is actually guaranteed for any generalized α-investing procedure [3] combined with any best-arm MAB algorithm. In fact we could use any adaptive stopping time Tj which depend on the history only via the rejections R1, . . . , Rj−1. Furthermore, using a modified LORD proposed\nby Javanmard and Montanari [13], we can also guarantee FDR control– a result we moved to the Appendix F due to space constraints. It is noteworthy that small values of α do not only guarantee smaller FDR error but also higher BDR. However, there is no free lunch — a smaller α implies a smaller αj at each experiment, resulting in a larger required number of pulls for the the best-arm MAB algorithm."
    }, {
      "heading" : "4 Experimental results",
      "text" : "In the following, we briefly describe some results of our experiments3 on both simulated and realworld data sets, which illustrate that, apart from FDR control, MAB-FDR (used interchangeably with MAB-LORD here) is highly advantageous in terms of sample complexity and power compared to a straightforward embedding of A/B testing in online FDR procedures. Unless otherwise noted, we set = 0 in all of our simulations to focus on the main ideas and keep the discussion concise.\nCompeting procedures There are two natural frameworks to compare against MAB-FDR. The first, called AB-FDR or AB-LORD, swaps the MAB part for an A/B (i.e. A/B/n) test (uniformly sampling all alternatives until termination). The second comparator exchanges the online FDR control for independent testing at α for all hypotheses – we call this MAB-IND. Formally, AB-FDR swaps step 3 in Procedure 2 with “Output αj and uniformly sample each arm until stopping time Tj .” while MAB-IND swaps step 4 in Procedure 2 with “The algorithm returns P j and we reject the null hypothesis if P j ≤ α.”. In order to compare the performances of these procedures, we ran three sets of simulations using Procedure 2 with = 0 and γj = 0.07 log(j∨2) je √ log j as in [4].\nOur experiments are run on artificial data with Gaussian/Bernoulli draws and real-world Bernoulli draws from the New Yorker Cartoon Caption Contest. Recall that the sample complexity of the best-arm MAB algorithm is determined by the gaps ∆j = µi? − µj . One of the main relevant differences to consider between an experiment of artificial or real-world nature is thus the distribution of the means µi for i = 1, . . . ,K. The artificial data simulations are run with a fixed gap ∆ := ∆2 while the means of the other arms are set uniformly in [0, µi? −∆]. For our real-world simulations, we use empirical means computed from the cartoon caption contest (see details in Appendix B.1.1). In addition, the contests actually follow a natural chronological order, which makes this dataset highly relevant to our purposes. In all simulations, 60% of all the hypotheses are true nulls, and their indices are chosen uniformly. Due to space constraints, the experimental results for artificial and real-world Bernoulli draws are deferred to Appendix B.\nPower and sample complexity In this section we include figures on artificial Gaussian trials which confirm that the total number of necessary pulls to determine significance is much smaller for MABFDR than for AB-FDR. In Fig. 2 (a) we fix the number of arms and plot the BDR with = 0 (BDR for short) for both procedures over different choices of truncation times TS . Low BDR indicates that the algorithm often reaches truncation time before it could stop. For Fig. 2 (b) we fix TS and show how the sample complexity varies with the number of arms.\n3The code for reproducing all experiments and plots in this paper is publicly available at https://github.com/fanny-yang/MABFDR\nObserve in Fig. 2 (a) that the power at any given truncation time is much higher for MAB-FDR than AB-FDR. This is because the best-arm MAB is more likely to satisfy the stopping criterion before any given truncation time than the uniform sampling algorithm. Fig. 2(b) qualitatively shows how the total number of necessary arm pulls for AB-FDR increases much faster with the number of arms than for MAB-FDR before it plateaus due to the truncation. Recall that whenever the best-arm MAB stops before the truncation time in each hypothesis, the stopping criterion is met, i.e. the best arm is identified with probability at least 1− αj , so that the power is bound to be close to one whenever Tj = T (αj).\nmFDR control For Fig. 3, we again consider Gaussian draws as in Fig. 2. This time however, for each true null hypothesis we skip the bandit experiment and directly draw P j ∼ [0, 1] to compare with the significance levels αj from our online FDR procedure 2 (see App. B.2 for motivation of this setting). By Theorem 1, mFDR should still be controlled as it only requires the p-values to be super-\nuniform. In Fig. 3(a) we plot the instantaneous false discovery proportion FDP(J) = ∑ j∈H0J Rj∑T\nj=1 Rj\nover the hypothesis index for different runs with the same settings. Apart from initial fluctuations due to the relatively small denominator, observe how the guarantee for the FDR(J) = E FDP(J) with the red line showing its empirical value, transfers to the control of each individual run (blue lines).\nIn Figure 3 (b), we compare the mFDR of MAB-FDR against MAB-IND and a Bonferroni type correction. The latter uses a simple union bound and chooses αj = 6απ2j2 such that ∑∞ j=1 αj ≤ α and thus trivially allows for any time FWER, implying FDR control. As expected, Bonferroni is too conservative and barely makes any rejections whereas the naive MAB-IND approach does not control FDR. LORD avoids both extremes and controls FDR while having reasonable power."
    }, {
      "heading" : "5 Discussion",
      "text" : "The recent focus in popular media about the lack of reproducibility of scientific results erodes the public’s confidence in published scientific research. To maintain credibility of claimed discoveries, simply decreasing the statistical significance levels α of each individual experimental work (e.g., reject at level 0.001 rather than 0.05) would drastically hurt power. A common approach is instead to control the ratio of false discoveries to claimed discoveries at some desired value over many sequential experiments, requiring the statistical significances αj to change from experiment to experiment. Unlike earlier works on online FDR control, our framework synchronously interacts with adaptive sampling methods like MABs to make the overall sampling procedure per experiment much more efficient than uniform sampling. To the best of our knowledge, it is the first work that successfully combines the benefits of adaptive sampling and FDR control. It is worthwhile to note that any improvement, theoretical or practical, to either online FDR algorithms or best-arm identification in MAB, immediately results in a corresponding improvement for our MAB-FDR framework.\nMore general notions of FDR with corresponding online procedures have recently been developed by Ramdas et al [14]. In particular, they incorporate the notion of memory and a priori importance of each hypothesis. This could prove to be a valuable extension for our setting, especially in cases when only the percentage of wrong rejections in the recent past matters. It would be useful to establish FDR control for these generalized notions of FDR as well."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was partially supported by Office of Naval Research MURI grant DOD-002888, Air Force Office of Scientific Research Grant AFOSR-FA9550-14-1-001, and National Science Foundation Grants CIF-31712-23800 and DMS-1309356."
    } ],
    "references" : [ {
      "title" : "Controlling the false discovery rate: a practical and powerful approach to multiple testing",
      "author" : [ "Y. Benjamini", "Y. Hochberg" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological), pp. 289–300, 1995.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "α-investing: a procedure for sequential control of expected false discoveries",
      "author" : [ "D.P. Foster", "R.A. Stine" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 70, no. 2, pp. 429–444, 2008.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Generalized α-investing: definitions, optimality results and application to public databases",
      "author" : [ "E. Aharoni", "S. Rosset" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 76, no. 4, pp. 771–794, 2014.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Online rules for control of false discovery rate and false discovery exceedance",
      "author" : [ "A. Javanmard", "A. Montanari" ],
      "venue" : "The Annals of Statistics, 2017.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Always valid inference: Bringing sequential analysis to A/B testing",
      "author" : [ "R. Johari", "L. Pekelis", "D.J. Walsh" ],
      "venue" : "arXiv preprint arXiv:1512.04922, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "lil’ucb: An optimal exploration algorithm for multi-armed bandits",
      "author" : [ "K.G. Jamieson", "M. Malloy", "R.D. Nowak", "S. Bubeck" ],
      "venue" : "COLT, vol. 35, 2014, pp. 423–439.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sequential nonparametric testing with the law of the iterated logarithm",
      "author" : [ "A. Balsubramani", "A. Ramdas" ],
      "venue" : "Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2016, pp. 42–51.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On the complexity of best arm identification in multi-armed bandit models",
      "author" : [ "E. Kaufmann", "O. Cappé", "A. Garivier" ],
      "venue" : "The Journal of Machine Learning Research, 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting",
      "author" : [ "K. Jamieson", "R. Nowak" ],
      "venue" : "Information Sciences and Systems (CISS), 2014 48th Annual Conference on. IEEE, 2014, pp. 1–6.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Multi-armed bandit models for the optimal design of clinical trials: benefits and challenges",
      "author" : [ "S.S. Villar", "J. Bowden", "J. Wason" ],
      "venue" : "Statistical science: a review journal of the Institute of Mathematical Statistics, vol. 30, no. 2, p. 199, 2015.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Pac subset selection in stochastic multiarmed bandits",
      "author" : [ "S. Kalyanakrishnan", "A. Tewari", "P. Auer", "P. Stone" ],
      "venue" : "Proceedings of the 29th International Conference on Machine Learning (ICML-12), 2012, pp. 655–662.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The simulator: Understanding adaptive sampling in the moderate-confidence regime",
      "author" : [ "M. Simchowitz", "K. Jamieson", "B. Recht" ],
      "venue" : "arXiv preprint arXiv:1702.05186, 2017.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "On online control of false discovery rate",
      "author" : [ "A. Javanmard", "A. Montanari" ],
      "venue" : "arXiv preprint arXiv:1502.06197, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Online control of the false discovery rate with decaying memory",
      "author" : [ "A. Ramdas", "F. Yang", "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) 2017, arXiv preprint arXiv:1710.00499, 2017. 10",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For this reason, [1] proposed that it may be more useful to control the expected ratio of false discoveries to the total number of discoveries (called the False Discovery Rate, or FDR for short) or the ratio of expected number of false discoveries to the expected number of total discoveries (called the modified FDR",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "Online FDR procedures were first introduced by Foster and Stine [2], and have since been studied by other authors (e.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "Although known procedures as in [2, 4] guarantee FDR control for independent p-values, this does not hold for dependent p-values in general.",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "Although known procedures as in [2, 4] guarantee FDR control for independent p-values, this does not hold for dependent p-values in general.",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "[5]) for each experiment for the purpose of continuous monitoring.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "To construct always-valid confidence bounds, we resort to the fundamental concept of the law of the iterated logarithm (LIL), for which non-asymptotic versions have been recently derived and used for both bandits and testing problems (see [6], [7]).",
      "startOffset" : 239,
      "endOffset" : 242
    }, {
      "referenceID" : 6,
      "context" : "To construct always-valid confidence bounds, we resort to the fundamental concept of the law of the iterated logarithm (LIL), for which non-asymptotic versions have been recently derived and used for both bandits and testing problems (see [6], [7]).",
      "startOffset" : 244,
      "endOffset" : 247
    }, {
      "referenceID" : 8,
      "context" : "Then for any δ ∈ (0, 1), best-arm MAB algorithms can identify i? with probability at least 1−δ based on at most2 ∑ i 6=i? ∆ −2 i log(1/δ) total samples (see the paper [9] for a brief survey and [10] for an application to clinical trials).",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : "Then for any δ ∈ (0, 1), best-arm MAB algorithms can identify i? with probability at least 1−δ based on at most2 ∑ i 6=i? ∆ −2 i log(1/δ) total samples (see the paper [9] for a brief survey and [10] for an application to clinical trials).",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 10,
      "context" : "In Algorithm 1 we present a modified MAB algorithm based on the common LUCB algorithm (see [11, 12]) which incorporates the above desiderata.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 11,
      "context" : "In Algorithm 1 we present a modified MAB algorithm based on the common LUCB algorithm (see [11, 12]) which incorporates the above desiderata.",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "As our algorithm is a slight modification of the LUCB algorithm, the results of [11, 12] provide insight into the number of samples taken before the algorithm terminates.",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "As our algorithm is a slight modification of the LUCB algorithm, the results of [11, 12] provide insight into the number of samples taken before the algorithm terminates.",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : ",K μi is a unique maximizer, the nearly optimal sample complexity result of [12] implies that the algorithm terminates under settings (a) and (b) after at most maxj 6=i? ∆ −2 j log(K log(∆ −2 j )/δ)+ ∑",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "We provide a concrete procedure 2 for our doubly sequential framework, where we use a particular online FDR algorithm due to Javanmard and Montanari [4] known as LORD; the reader should note that other online FDR procedure could be used to obtain essentially the same set of guarantees.",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 2,
      "context" : "Note that by the arguments in the proof of Theorem 1, mFDR control itself is actually guaranteed for any generalized α-investing procedure [3] combined with any best-arm MAB algorithm.",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 12,
      "context" : "by Javanmard and Montanari [13], we can also guarantee FDR control– a result we moved to the Appendix F due to space constraints.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 13,
      "context" : "More general notions of FDR with corresponding online procedures have recently been developed by Ramdas et al [14].",
      "startOffset" : 110,
      "endOffset" : 114
    } ],
    "year" : 2017,
    "abstractText" : "We propose an alternative framework to existing setups for controlling false alarms when multiple A/B tests are run over time. This setup arises in many practical applications, e.g. when pharmaceutical companies test new treatment options against control pills for different diseases, or when internet companies test their default webpages versus various alternatives over time. Our framework proposes to replace a sequence of A/B tests by a sequence of best-arm MAB instances, which can be continuously monitored by the data scientist. When interleaving the MAB tests with an online false discovery rate (FDR) algorithm, we can obtain the best of both worlds: low sample complexity and any time online FDR control. Our main contributions are: (i) to propose reasonable definitions of a null hypothesis for MAB instances; (ii) to demonstrate how one can derive an always-valid sequential p-value that allows continuous monitoring of each MAB test; and (iii) to show that using rejection thresholds of online-FDR algorithms as the confidence levels for the MAB algorithms results in both sample-optimality, high power and low FDR at any point in time. We run extensive simulations to verify our claims, and also report results on real data collected from the New Yorker Cartoon Caption contest.",
    "creator" : null
  }
}