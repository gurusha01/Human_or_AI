{
  "name" : "63538fe6ef330c13a05a3ed7e599d5f7.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Greedy Algorithms for Cone Constrained Optimization with Convergence Guarantees",
    "authors" : [ "Francesco Locatello", "Michael Tschannen", "Martin Jaggi" ],
    "emails" : [ "locatelf@ethz.ch", "michaelt@nari.ee.ethz.ch", "raetsch@inf.ethz.ch", "martin.jaggi@epfl.ch" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recent years, greedy optimization algorithms have attracted significant interest in the domains of signal processing and machine learning thanks to their ability to process very large data sets. Arguably two of the most popular representatives are Frank-Wolfe (FW) [12, 21] and Matching Pursuit (MP) algorithms [34], in particular Orthogonal MP (OMP) [9, 49]. While the former targets minimization of a convex function over bounded convex sets, the latter apply to minimization over a linear subspace. In both cases, the domain is commonly parametrized by a set of atoms or dictionary elements, and in each iteration, both algorithms rely on querying a so-called linear minimization oracle (LMO) to find the direction of steepest descent in the set of atoms. The iterate is then updated as a linear or convex combination, respectively, of previous iterates and the newly obtained atom from the LMO. The particular choice of the atom set allows to encode structure such as sparsity and non-negativity (of the atoms) into the solution. This enables control of the trade-off between the amount of structure in the solution and approximation quality via the number of iterations, which was found useful in a large variety of use cases including structured matrix and tensor factorizations [50, 53, 54, 18].\nIn this paper, we target an important “intermediate case” between the two domain parameterizations given by the linear span and the convex hull of an atom set, namely the parameterization of the optimization domain as the conic hull of a possibly infinite atom set. In this case, the solution can be represented as a non-negative linear combination of the atoms, which is desirable in many\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\napplications, e.g., due to the physics underlying the problem at hand, or for the sake of interpretability. Concrete examples include unmixing problems [11, 16, 3], model selection [33], and matrix and tensor factorizations [4, 24]. However, existing convergence analyses do not apply to the currently used greedy algorithms. In particular, all existing MP variants for the conic hull case [5, 38, 52] are not guaranteed to converge and may get stuck far away from the optimum (this can be observed in the experiments in Section 6). From a theoretical perspective, this intermediate case is of paramount interest in the context of MP and FW algorithms. Indeed, the atom set is not guaranteed to contain an atom aligned with a descent direction for all possible suboptimal iterates, as is the case when the optimization domain is the linear span or the convex hull of the atom set [39, 32]. Hence, while conic constraints have been widely studied in the context of a manifold of different applications, none of the existing greedy algorithms enjoys explicit convergence rates.\nWe propose and analyze new MP algorithms tailored for the minimization of smooth convex functions over the conic hull of an atom set. Specifically, our key contributions are:\n• We propose the first (non-orthogonal) MP algorithm for optimization over conic hulls guaranteed to converge, and prove a corresponding sublinear convergence rate with explicit constants. Surprisingly, convergence is achieved without increasing computational complexity compared to ordinary MP.\n• We propose new away-step, pairwise, and fully corrective MP variants, inspired by variants of FW [28] and generalized MP [32], respectively, that allow for different degrees of weight corrections for previously selected atoms. We derive corresponding sublinear and linear (for strongly convex objectives) convergence rates that solely depend on the geometry of the atom set.\n• All our algorithms apply to general smooth convex functions. This is in contrast to all prior work on non-negative MP, which targets quadratic objectives [5, 38, 52]. Furthermore, if the conic hull of the atom set equals its linear span, we recover both algorithms and rates derived in [32] for generalized MP variants.\n• We make no assumptions on the atom set which is simply a subset of a Hilbert space, in particular we do not assume the atom set to be finite.\nBefore presenting our algorithms (Section 3) along with the corresponding convergence guarantees (Section 4), we briefly review generalized MP variants. A detailed discussion of related work can be found in Section 5 followed by illustrative experiments on a least squares problem on synthetic data, and non-negative matrix factorization as well as non-negative garrote logistic regression as applications examples on real data (numerical evaluations of more applications and the dependency between constants in the rate and empirical convergence can be found in the supplementary material).\nNotation. Given a non-empty subset A of some Hilbert space, let conv(A) be the convex hull of A, and let lin(A) denote its linear span. Given a closed set A, we call its diameter diam(A) = maxz1,z2∈A ‖z1 − z2‖ and its radius radius(A) = maxz∈A ‖z‖. ‖x‖A := inf{c > 0: x ∈ c · conv(A)} is the atomic norm of x over a set A (also known as the gauge function of conv(A)). We call a subset A of a Hilbert space symmetric if it is closed under negation."
    }, {
      "heading" : "2 Review of Matching Pursuit Variants",
      "text" : "LetH be a Hilbert space with associated inner product 〈x,y〉, ∀x,y ∈ H. The inner product induces the norm ‖x‖2 := 〈x,x〉, ∀x ∈ H. Let A ⊂ H be a compact set (the “set of atoms” or dictionary) and let f : H→R be convex and L-smooth (L-Lipschitz gradient in the finite dimensional case). If H is an infinite-dimensional Hilbert space, then f is assumed to be Fréchet differentiable. The generalized MP algorithm studied in [32], presented in Algorithm 1, solves the following optimization problem:\nmin x∈lin(A) f(x). (1)\nIn each iteration, MP queries a linear minimization oracle (LMO) solving the following linear problem:\nLMOA(y) := arg min z∈A 〈y, z〉 (2)\nfor a given query y ∈ H. The MP update step minimizes a quadratic upper bound gxt(x) = f(xt) + 〈∇f(xt),x− xt〉+ L2 ‖x− xt‖ 2 of f at xt, where L is an upper bound on the smoothness\nconstant of f with respect to a chosen norm ‖ · ‖. Optimizing this norm problem instead of f directly allows for substantial efficiency gains in the case of complicated f . For symmetric A and for f(x) = 12‖y − x‖\n2, y ∈ H, Algorithm 1 recovers MP (Variant 0) [34] and OMP (Variant 1) [9, 49], see [32] for details.\nAlgorithm 1 Norm-Corrective Generalized Matching Pursuit\n1: init x0 ∈ lin(A), and S := {x0} 2: for t = 0 . . . T 3: Find zt := (Approx-)LMOA(∇f(xt)) 4: S := S ∪ {zt} 5: Let b := xt − 1L∇f(xt) 6: Variant 0:\nUpdate xt+1 := arg min z:=xt+γzt\nγ∈R\n‖z− b‖2\n7: Variant 1: Update xt+1 := arg min\nz∈lin(S) ‖z− b‖2\n8: Optional: Correction of some/all atoms z0...t 9: end for\nApproximate linear oracles. Solving the LMO defined in (2) exactly is often hard in practice, in particular when applied to matrix (or tensor) factorization problems, while approximate versions can be much more efficient. Algorithm 1 allows for an approximate LMO. For given quality parameter δ ∈ (0, 1] and given direction d ∈ H, the approximate LMO for Algorithm 1 returns a vector z̃ ∈ A such that 〈d, z̃〉 ≤ δ〈d, z〉, (3) relative to z = LMOA(d) being an exact solution.\nDiscussion and limitations of MP. The analysis of the convergence of Algorithm 1 in [32] critically relies on the assumption that the origin is in the relative interior of conv(A) with\nrespect to its linear span. This assumption originates from the fact that the convergence of MP- and FW-type algorithms fundamentally depends on an alignment assumption of the search direction returned by the LMO (i.e., zt in Algorithm 1) and the gradient of the objective at the current iteration (see third premise in [39]). Specifically, for Algorithm 1, the LMO is assumed to select a descent direction, i.e., 〈∇f(xt), zt〉 < 0, so that the resulting weight (i.e., γ for Variant 0) is always positive. In this spirit, Algorithm 1 is a natural candidate to minimize f over the conic hull of A. However, if the optimization domain is a cone, the alignment assumption does not hold as there may be non-stationary points x in the conic hull of A for which minz∈A〈∇f(x), z〉 = 0. Algorithm 1 is therefore not guaranteed to converge when applied to conic problems. The same issue arises for essentially all existing non-negative variants of MP, see, e.g., Alg. 2 in [38] and in Alg. 2 in [52]. We now present modifications corroborating this issue along with the resulting MP-type algorithms for conic problems and corresponding convergence guarantees."
    }, {
      "heading" : "3 Greedy Algorithms on Conic Hulls",
      "text" : "The cone cone(A− y) tangent to the convex set conv(A) at a point y is formed by the half-lines emanating from y and intersecting conv(A) in at least one point distinct from y. Without loss of generality we consider 0 ∈ A and assume the set cone(A) (i.e., y = 0) to be closed. If A is finite the cone constraint can be written as cone(A) := {x : x = ∑|A| i=1 αiai s.t. ai ∈ A, αi ≥ 0 ∀i}. We consider conic optimization problems of the form:\nmin x∈cone(A) f(x). (4)\nNote that if the setA is symmetric or if the origin is in the relative interior of conv(A) w.r.t. its linear span then cone(A) = lin(A). We will show later how our results recover known MP rates when the origin is in the relative interior of conv(A). As a first algorithm to solve problems of the form (4), we present the Non-Negative Generalized Matching Pursuit (NNMP) in Algorithm 2 which is an extension of MP to general f and non-negative weights.\nDiscussion: Algorithm 2 differs from Algorithm 1 (Variant 0) in line 4, adding the iterationdependent atom − xt‖xt‖A to the set of possible search directions 1. We use the atomic norm for the\n1This additional direction makes sense only if xt 6= 0. Therefore, we set − xt‖xt‖A = 0 if xt = 0, i.e., no direction is added.\nAlgorithm 2 Non-Negative Matching Pursuit 1: init x0 = 0 ∈ A 2: for t = 0 . . . T 3: Find z̄t := (Approx-)LMOA(∇f(xt)) 4: zt = arg minz∈ { z̄t,\n−xt ‖xt‖A }〈∇f(xt), z〉 5: γ := 〈−∇f(xt),zt〉L‖zt‖2 6: Update xt+1 := xt + γzt 7: end for\nnormalization because it yields the best constant in the convergence rate. In practice, one can replace it with the Euclidean norm, which is often much less expensive to compute. This iteration-dependent additional search direction allows to reduce the weights of the atoms that were previously selected, thus admitting the algorithm to “move back” towards the origin while maintaining the cone constraint. This idea is informally explained here and formally studied in Section 4.1.\nRecall the alignment assumption of the search direction and the gradient of the objective at the current iterate discussed in Section 2 (see also [39]). Algorithm 2 obeys this assumption. The intuition behind this is the following. Whenever xt is not a minimizer of (4) and minz∈A〈∇f(xt), z〉 = 0, the vector − xt‖xt‖A is aligned with ∇f(xt) (i.e., 〈∇f(xt),− xt ‖xt‖A 〉 < 0), preventing the algorithm from stopping at a suboptimal iterate. To make this intuition more formal, let us define the set of feasible descent directions of Algorithm 2 at a point x ∈ cone(A) as:\nTA(x) :=\n{ d ∈ H : ∃z ∈ A ∪ { − x ‖x‖A } s.t. 〈d, z〉 < 0 } . (5)\nIf at some iteration t = 0, 1, . . . the gradient ∇f(xt) is not in TA(xt) Algorithm 2 terminates as minz∈A〈d, z〉 = 0 and 〈d,−xt〉 ≥ 0 (which yields zt = 0). Even though, in general, not every direction in H is a feasible descent direction, ∇f(xt) /∈ TA only occurs if xt is a constrained minimum of Equation 4: Lemma 1. If x̃ ∈ cone(A) and ∇f(x̃) 6∈ TA then x̃ is a solution to minx∈cone(A) f(x). Initializing Algorithm 2 with x0 = 0 guarantees that the iterates xt always remain inside cone(A) even though this is not enforced explicitly (by convexity of f , see proof of Theorem 2 in Appendix D for details).\nLimitations of Algorithm 2: Let us call active the atoms which have nonzero weights in the representation of xt = ∑t−1 i=0 αizi computed by Algorithm 2. Formally, the set of active atoms is defined as S := {zi : αi > 0, i = 0, 1, . . . , t− 1}. The main drawback of Algorithm 2 is that when the direction − xt‖xt‖A is selected, the weight of all active atoms is reduced. This can lead to the algorithm alternately selecting − xt‖xt‖A and an atom from A, thereby slowing down convergence in a similar manner as the zig-zagging phenomenon well-known in the Frank-Wolfe framework [28]. In order to achieve faster convergence we introduce the corrective variants of Algorithm 2."
    }, {
      "heading" : "3.1 Corrective Variants",
      "text" : "To achieve faster (linear) convergence (see Section 4.2) we introduce variants of Algorithm 2, termed Away-steps MP (AMP) and Pairwise MP (PWMP), presented in Algorithm 3. Here, inspired by the away-steps and pairwise variants of FW [12, 28], instead of reducing the weights of the active atoms uniformly as in Algorithm 2, the LMO is queried a second time on the active set S to identify the direction of steepest ascent in S. This allows, at each iteration, to reduce the weight of a previously selected atom (AMP) or swap weight between atoms (PWMP). This selective “reduction” or “swap of weight” helps to avoid the zig-zagging phenomenon which prevent Algorithm 2 from converging linearly.\nAt each iteration, Algorithm 3 updates the weights of zt and vt as αzt = αzt +γ and αvt = αvt −γ, respectively. To ensure that xt+1 ∈ cone(A), γ has to be clipped according to the weight which is currently on vt, i.e., γmax = αvt . If γ = γmax, we set αvt = 0 and remove vt from S as the atom vt is no longer active. If dt ∈ A (i.e., we take a regular MP step and not an away step), the line search is unconstrained (i.e., γmax =∞).\nFor both algorithm variants, the second LMO query increases the computational complexity. Note that an exact search on S is feasible in practice as |S| has at most t elements at iteration t. Taking an additional computational burden allows to update the weights of all active atoms in the spirit of OMP. This approach is implemented in the Fully Corrective MP (FCMP), Algorithm 4.\nAlgorithm 3 Away-steps (AMP) and Pairwise (PWMP) Non-Negative Matching Pursuit\n1: init x0 = 0 ∈ A, and S := {x0} 2: for t = 0 . . . T 3: Find zt := (Approx-)LMOA(∇f(xt)) 4: Find vt := (Approx-)LMOS(−∇f(xt)) 5: S = S ∪ zt 6: AMP: dt=arg mind∈{zt,−vt}〈∇f(xt),d〉 7: PWMP: dt = zt − vt 8: γ := min { 〈−∇f(xt),dt〉\nL‖dt‖2 , γmax } (γmax see text)\n9: Update αzt , αvt and S according to γ (γ see text)\n10: Update xt+1 := xt + γdt 11: end for\nAlgorithm 4 Fully Corrective Non-Negative Matching Pursuit (FCMP)\n1: init x0 = 0 ∈ A,S = {x0} 2: for t = 0 . . . T 3: Find zt := (Approx-)LMOA(∇f(xt)) 4: S := S ∪ {zt} 5: Variant 0:\nxt+1 = arg min x∈cone(S)\n‖x−(xt− 1L∇f(xt))‖ 2\n6: Variant 1: xt+1 = arg minx∈cone(S) f(x) 7: Remove atoms with zero weights from S 8: end for\nAt each iteration, Algorithm 4 maintains the set of active atoms S by adding zt and removing atoms with zero weights after the update. In Variant 0, the algorithm minimizes the quadratic upper bound gxt(x) on f at xt (see Section 2) imitating a gradient descent step with projection onto a “varying” target, i.e., cone(S). In Variant 1, the original objective f is minimized over cone(S) at each iteration, which is in general more efficient than minimizing f over cone(A) using a generic solver for cone constrained problems. For f(x) = 12‖y − x‖\n2, y ∈ H, Variant 1 recovers Algorithm 1 in [52] and the OMP variant in [5] which both only apply to this specific objective f ."
    }, {
      "heading" : "3.2 Computational Complexity",
      "text" : "We briefly discuss the computational complexity of the algorithms we introduced. ForH = Rd, sums and inner products have cost O(d). Let us assume that each call of the LMO has cost C on the set A and O(td) on S. The variants 0 and 1 of FCMP solve a cone problem at each iteration with cost h0 and h1, respectively. In general, h0 can be much smaller than h1. In Table 1\nwe report the cost per iteration for every algorithm along with the asymptotic convergence rates derived in Section 4."
    }, {
      "heading" : "4 Convergence Rates",
      "text" : "In this section, we present convergence guarantees for Algorithms 2, 3, and 4. All proofs are deferred to the Appendix in the supplementary material. We write x? ∈ arg minx∈cone(A) f(x) for an optimal solution. Our rates will depend on the atomic norm of the solution and the iterates of the respective algorithm variant: ρ = max {‖x?‖A, ‖x0‖A . . . , ‖xT ‖A} . (6) If the optimum is not unique, we consider x? to be one of largest atomic norm. A more intuitive and looser notion is to simply upper-bound ρ by the diameter of the level set of the initial iterate x0 measured by the atomic norm. Then, boundedness follows since the presented method is a descent method (due to Lemma 1 and line search on the quadratic upper bound, each iteration strictly\ndecreases the objective and our method stops only at the optimum). This justifies the statement f(xt) ≤ f(x0). Hence, ρ must be bounded for any sequence of iterates produced by the algorithm, and the convergence rates presented in this section are valid as T goes to infinity. A similar notion to measure the convergence of MP was established in [32]. All of our algorithms and rates can be made affine invariant. We defer this discussion to Appendix B."
    }, {
      "heading" : "4.1 Sublinear Convergence",
      "text" : "We now present the convergence results for the non-negative and Fully-Corrective Matching Pursuit algorithms. Sublinear convergence of Algorithm 3 is addressed in Theorem 3. Theorem 2. Let A ⊂ H be a bounded set with 0 ∈ A, ρ := max {‖x?‖A, ‖x0‖A, . . . , ‖xT ‖A, } and f be L-smooth over ρ conv(A ∪−A). Then, Algorithms 2 and 4 converge for t ≥ 0 as\nf(xt)− f(x?) ≤ 4 ( 2 δLρ 2 radius(A)2 + ε0 )\nδt+ 4 ,\nwhere δ ∈ (0, 1] is the relative accuracy parameter of the employed approximate LMO (see Equation (3)).\nRelation to FW rates. By rescaling A by a large enough factor τ > 0, FW with τA as atom set could in principle be used to solve (4). In fact, for large enough τ , only the constraints of (4) become active when minimizing f over conv(τA). The sublinear convergence rate obtained with this approach is up to constants identical to that in Theorem 2 for our MP variants, see [21]. However, as the correct scaling is unknown, one has to either take the risk of choosing τ too small and hence failing to recover an optimal solution of (4), or to rely on too large τ which can result in slow convergence. In contrast, knowledge of ρ is not required to run our MP variants.\nRelation to MP rates. If A is symmetric, we have that lin(A) = cone(A) and it is easy to show that the additional direction − xt‖xt‖ in Algorithm 2 is never selected. Therefore, Algorithm 2 becomes equivalent to Variant 0 of Algorithm 1, while Variant 1 of Algorithm 1 is equivalent to Variant 0 of Algorithm 4. The rate specified in Theorem 2 hence generalizes the sublinear rate in [32, Theorem 2] for symmetric A."
    }, {
      "heading" : "4.2 Linear Convergence",
      "text" : "We start by recalling some of the geometric complexity quantities that were introduced in the context of FW and are adapted here to the optimization problem we aim to solve (minimization over cone(A) instead of conv(A)). Directional Width. The directional width of a set A w.r.t. a direction r ∈ H is defined as:\ndirW (A, r) := max s,v∈A\n〈 r ‖r‖ , s− v 〉 (7)\nPyramidal Directional Width [28]. The Pyramidal Directional Width of a set A with respect to a direction r and a reference point x ∈ conv(A) is defined as:\nPdirW (A, r,x) := min S∈Sx dirW (S ∪ {s(A, r)}, r), (8)\nwhere Sx := {S | S ⊂ A and x is a proper convex combination of all the elements in S} and s(A, r) := maxs∈A〈 r‖r‖ , s〉.\nInspired by the notion of pyramidal width in [28], which is the minimal pyramidal directional width computed over the set of feasible directions, we now define the cone width of a set A where only the generating faces (g-faces) of cone(A) (instead of the faces of conv(A)) are considered. Before doing so we introduce the notions of face, generating face, and feasible direction.\nFace of a convex set. Let us consider a set K with a k−dimensional affine hull along with a point x ∈ K. Then, K is a k−dimensional face of conv(A) if K = conv(A) ∩ {y : 〈r,y − x〉 = 0} for some normal vector r and conv(A) is contained in the half-space determined by r, i.e., 〈r,y − x〉 ≤ 0, ∀ y ∈ conv(A). Intuitively, given a set conv(A) one can think of conv(A) being a dim(conv(A))−dimensional face of itself, an edge on the border of the set a 1-dimensional face and a vertex a 0-dimensional face.\nFace of a cone and g-faces. Similarly, a k−dimensional face of a cone is an open and unbounded set cone(A) ∩ {y : 〈r,y − x〉 = 0} for some normal vector r and cone(A) is contained in the half space determined by r. We can define the generating faces of a cone as:\ng-faces(cone(A)) :={B ∩ conv(A) :B ∈ faces(cone(A))} .\nNote that g-faces(cone(A)) ⊂ faces(conv(A)) and conv(A) ∈ g-faces(cone(A)). Furthermore, for each K ∈ g-faces(cone(A)), cone(K) is a k−dimensional face of cone(A). We now introduce the notion of feasible directions. A direction d is feasible from x ∈ cone(A) if it points inwards cone(A), i.e., if ∃ε > 0 s.t. x + εd ∈ cone(A). Since a face of the cone is itself a cone, if a direction is feasible from x ∈ cone(K) \\ 0, it is feasible from every positive rescaling of x. We therefore can consider only the feasible directions on the generating faces (which are closed and bounded sets). Finally, we define the cone width of A.\nCone Width.\nCWidth(A) := min K∈g-faces(cone(A))\nx∈K r∈cone(K−x)\\{0}\nPdirW (K ∩A, r,x) (9)\nWe are now ready to show the linear convergence of Algorithms 3 and 4.\nTheorem 3. Let A ⊂ H be a bounded set with 0 ∈ A and let the objective function f : H→R be both L-smooth and µ-strongly convex over ρ conv(A ∪−A). Then, the suboptimality of the iterates of Algorithms 3 and 4 decreases geometrically at each step in which γ < αvt (henceforth referred to as “good steps”) as:\nεt+1 ≤ (1− β) εt, (10)\nwhere β := δ2 µCWidth(A) 2\nL diam(A)2 ∈ (0, 1], εt := f(xt)−f(x ?) is the suboptimality at step t and δ ∈ (0, 1]\nis the relative accuracy parameter of the employed approximate LMO (3). For AMP (Algorithm 3), βAMP = β/2. If µ = 0 Algorithm 3 converges with rate O(1/k(t)) where k(t) is the number of\n“good steps” up to iteration t.\nDiscussion. To obtain a linear convergence rate, one needs to upper-bound the number of “bad steps” t−k(t) (i.e., steps with γ ≥ αvt ). We have that k(t) = t for Variant 1 of FCMP (Algorithm 4), k(t) ≥ t/2 for AMP (Algorithm 3) and k(t) ≥ t/(3|A|! + 1) for PWMP (Algorithm 3) and Variant 0 of FCMP (Algorithm 4). This yields a global linear convergence rate of εt ≤ ε0 exp (−βk(t)). The bound for PWMP is very loose and only meaningful for finite sets A. However, it can be observed in the experiments in the supplementary material (Appendix A) that only a very small fraction of iterations result in bad PWMP steps in practice. Further note that Variant 1 of FCMP (Algorithm 4) does not produce bad steps. Also note that the bounds on the number of good steps given above are the same as for the corresponding FW variants and are obtained using the same (purely combinatorial) arguments as in [28].\nRelation to previous MP rates. The linear convergence of the generalized (not non-negative) MP variants studied in [32] crucially depends on the geometry of the set which is characterized by the Minimal Directional Width mDW(A):\nmDW(A) := min d∈lin(A)\nd6=0\nmax z∈A 〈 d ‖d‖ , z〉 . (11)\nThe following Lemma relates the Cone Width with the minimal directional width.\nLemma 4. If the origin is in the relative interior of conv(A) with respect to its linear span, then cone(A) = lin(A) and CWidth(A) = mDW(A).\nNow, if the set A is symmetric or, more generally, if cone(A) spans the linear space lin(A) (which implies that the origin is in the relative interior of conv(A)), there are no bad steps. Hence, by Lemma 4, the linear rate obtained in Theorem 3 for non-negative MP variants generalizes the one presented in [32, Theorem 7] for generalized MP variants.\nRelation to FW rates. Optimization over conic hulls with non-negative MP is more similar to FW than to MP itself in the following sense. For MP, every direction in lin(A) allows for unconstrained steps, from any iterate xt. In contrast, for our non-negative MPs, while some directions allow for unconstrained steps from some iterate xt, others are constrained, thereby leading to the dependence of the linear convergence rate on the cone width, a geometric constant which is very similar in spirit to the Pyramidal Width appearing in the linear convergence bound in [28] for FW. Furthermore, as for Algorithm 3, the linear rate of Away-steps and Pairwise FW holds only for good steps. We finally relate the cone width with the Pyramidal Width [28]. The Pyramidal Width is defined as\nPWidth(A) := min K∈faces(conv(A))\nx∈K r∈cone(K−x)\\{0}\nPdirW (K ∩A, r,x).\nWe have CWidth(A) ≥ PWidth(A) as the minimization in the definition (9) of CWidth(A) is only over the subset g-faces(cone(A)) of faces(conv(A)). As a consequence, the decrease per iteration characterized in Theorem 3 is larger than what one could obtain with FW on the rescaled convex set τA (see Section 4.1 for details about the rescaling). Furthermore, the decrease characterized in [28] scales as 1/τ2 due to the dependence on 1/diam(conv(A))2."
    }, {
      "heading" : "5 Related Work",
      "text" : "The line of recent works by [44, 46, 47, 48, 37, 32] targets the generalization of MP from the least-squares objective to general smooth objectives and derives corresponding convergence rates (see [32] for a more in-depth discussion). However, only little prior work targets MP variants with non-negativity constraint [5, 38, 52]. In particular, the least-squares objective was addressed and no rigorous convergence analysis was carried out. [5, 52] proposed an algorithm equivalent to our Algorithm 4 for the least-squares case. More specifically, [52] then developed an acceleration heuristic, whereas [5] derived a coherence-based recovery guarantee for sparse linear combinations of atoms. Apart from MP-type algorithms, there is a large variety of non-negative least-squares algorithms, e.g., [30], in particular also for matrix and tensor spaces. The gold standard in factorization problems is projected gradient descent with alternating minimization, see [43, 4, 45, 23]. Other related works are [40], which is concerned with the feasibility problem on symmetric cones, and [19], which introduces a norm-regularized variant of problem (4) and solves it using FW on a rescaled convex set. To the best of our knowledge, in the context of MP-type algorithms, we are the first to combine general convex objectives with conic constraints and to derive corresponding convergence guarantees.\nBoosting: In an earlier line of work, a flavor of the generalized MP became popular in the context of boosting, see [35]. The literature on boosting is vast, we refer to [42, 35, 7] for a general overview. Taking the optimization perspective given in [42], boosting is an iterative greedy algorithm minimizing a (strongly) convex objective over the linear span of a possibly infinite set called hypothesis class. The convergence analysis crucially relies on the assumption of the origin being in the relative interior of the hypothesis class, see Theorem 1 in [17]. Indeed, Algorithm 5.2 of [35] might not converge if the [39] alignment assumption is violated. Here, we managed to relax this assumption while preserving essentially the same asymptotic rates in [35, 17]. Our work is therefore also relevant in the context of (non-negative) boosting."
    }, {
      "heading" : "6 Illustrative Experiments",
      "text" : "We illustrate the performance of the presented algorithms on three different exemplary tasks, showing that our algorithms are competitive with established baselines across a wide range of objective functions, domains, and data sets while not being specifically tailored to any of these tasks (see Section 3.2 for a discussion of the computational complexity of the algorithms). Additional experiments targeting KL divergence NMF, non-negative tensor factorization, and hyperspectral image unmixing can be found in the appendix.\nSynthetic data. We consider minimizing the least squares objective on the conic hull of 100 unit-norm vectors sampled at random in the first orthant of R50. We compare the convergence of Algorithms 2, 3, and 4 with the Fast Non-Negative MP (FNNOMP) of [52], and Variant 3 (line-search) of the FW algorithm in [32] on the atom set rescaled by τ = 10‖y‖ (see Section 4.1), observing linear convergence for our corrective variants.\n0 10 20 30 40 50 Iteration\n10-4\n10-2\n100\n102\n104\nSu bo\npt im\nal ity\nSynthetic data PWMP (Alg. 3) NNMP (Alg. 2) FCMP (Alg. 4) FW AMP (Alg. 3) FNNOMP\nFigure 2: Synthetic data experiment.\nFigure 2 shows the suboptimality εt, averaged over 20 realizations ofA and y, as a function of the iteration t. As expected, FCMP achieves fastest convergence followed by PWMP, AMP and NNMP. The FNNOMP gets stuck instead. Indeed, [52] only show that the algorithm terminates and not its convergence.\nNon-negative matrix factorization. The second task consists of decomposing a given matrix into the product of two non-negative matrices as in Equation (1) of [20]. We consider the intersection of the positive semidefinite cone and the positive orthant. We parametrize the set A as the set of matrices obtained as an outer product of vectors\nfrom A1 = {z ∈ Rk : zi ≥ 0 ∀ i} and A2 = {z ∈ Rd : zi ≥ 0 ∀ i}. The LMO is approximated using a truncated power method [55], and we perform atom correction with greedy coordinate descent see, e.g., [29, 18], to obtain a better objective value while maintaining the same (small) number of atoms. We consider three different datasets: The Reuters Corpus2, the CBCL face dataset3 and the KNIX dataset4. The subsample of the Reuters corpus we used is a term frequency matrix of 7,769 documents and 26,001 words. The CBCL face dataset is composed of 2,492 images of 361 pixels each, arranged into a matrix. The KNIX dataset contains 24 MRI slices of a knee, arranged in a matrix of size 262, 144× 24. Pixels are divided by their overall mean intensity. For interpretability reasons, there is interest to decompose MRI data into non-negative factorizations [25]. We compare PWMP and FCMP against the multiplicative (mult) and the alternating (als) algorithm of [4], and the greedy coordinate descent (GCD) of [20]. Since the Reuters corpus is much larger than the CBCL and the KNIX dataset we only used the GCD for which a fast implementation in C is available. We report the objective value for fixed values of the rank in Table 2, showing that FCMP outperform all the baselines across all the datasets. PWMP achieves smallest error on the Reuters corpus.\nNon-negative garrote. We consider the non-negative garrote which is a common approach to model order selection [6]. We evaluate NNMP, PWMP, and FCMP in the experiment described in [33], where the non-negative garrote is used to perform model order selection for logistic regression (i.e., for a non-quadratic objective function). We evaluated training and test accuracy on 100 random splits of the sonar dataset from the UCI machine learning repository. In Table 3 we compare the median classification accuracy of our algorithms with that of the cyclic coordinate descent algorithm (NNG) from [33]."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we considered greedy algorithms for optimization over the convex cone, parametrized as the conic hull of a generic atom set. We presented a novel formulation of NNMP along with a comprehensive convergence analysis. Furthermore, we introduced corrective variants with linear convergence guarantees, and verified this convergence rate in numerical applications. We believe that the generality of our novel analysis will be useful to design new, fast algorithms with convergence guarantees, and to study convergence of existing heuristics, in particular in the context of non-negative matrix and tensor factorization.\n2http://www.nltk.org/book/ch02.html 3http://cbcl.mit.edu/software-datasets/FaceData2.html 4http://www.osirix-viewer.com/resources/dicom-image-library/"
    } ],
    "references" : [ {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "Animashree Anandkumar", "Rong Ge", "Daniel J Hsu", "Sham M Kakade", "Matus Telgarsky" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "The successive projections algorithm for variable selection in spectroscopic multicomponent analysis",
      "author" : [ "Mário César Ugulino Araújo", "Teresa Cristina Bezerra Saldanha", "Roberto Kawakami Harrop Galvao", "Takashi Yoneyama", "Henrique Caldas Chame", "Valeria Visani" ],
      "venue" : "Chemometrics and Intelligent Laboratory Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Mitie: Simultaneous rna-seq-based transcript identification and quantification in multiple samples",
      "author" : [ "Jonas Behr", "André Kahles", "Yi Zhong", "Vipin T Sreedharan", "Philipp Drewe", "Gunnar Rätsch" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Algorithms and applications for approximate nonnegative matrix factorization",
      "author" : [ "Michael W Berry", "Murray Browne", "Amy N Langville", "V Paul Pauca", "Robert J Plemmons" ],
      "venue" : "Computational statistics & data analysis,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "On the uniqueness of nonnegative sparse solutions to underdetermined systems of equations",
      "author" : [ "Alfred M Bruckstein", "Michael Elad", "Michael Zibulevsky" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Boosting, model selection, lasso and nonnegative garrote",
      "author" : [ "P Bühlmann", "B Yu" ],
      "venue" : "Technical Report 127, Seminar für Statistik ETH Zürich,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "Infinite-dimensional optimization and optimal design",
      "author" : [ "Martin Burger" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "Orthogonal least squares methods and their application to non-linear system identification",
      "author" : [ "Sheng Chen", "Stephen A Billings", "Wan Luo" ],
      "venue" : "International Journal of control,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1989
    }, {
      "title" : "Fast local algorithms for large scale nonnegative matrix and tensor factorizations",
      "author" : [ "Andrzej Cichocki", "PHAN Anh-Huy" ],
      "venue" : "IEICE transactions on fundamentals of electronics, communications and computer sciences,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "A method for finding structured sparse solutions to nonnegative least squares problems with applications",
      "author" : [ "Ernie Esser", "Yifei Lou", "Jack Xin" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "An algorithm for quadratic programming",
      "author" : [ "M Frank", "P Wolfe" ],
      "venue" : "Naval research logistics quarterly,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1956
    }, {
      "title" : "Successive nonnegative projection algorithm for robust nonnegative blind source separation",
      "author" : [ "Nicolas Gillis" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Accelerated multiplicative updates and hierarchical als algorithms for nonnegative matrix factorization",
      "author" : [ "Nicolas Gillis", "François Glineur" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Hierarchical clustering of hyperspectral images using rank-two nonnegative matrix factorization",
      "author" : [ "Nicolas Gillis", "Da Kuang", "Haesun Park" ],
      "venue" : "IEEE Transactions on Geoscience and Remote Sensing,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "A fast gradient method for nonnegative sparse regression with self dictionary",
      "author" : [ "Nicolas Gillis", "Robert Luce" ],
      "venue" : "arXiv preprint arXiv:1610.01349,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "Generalized boosting algorithms for convex optimization",
      "author" : [ "Alexander Grubb", "J Andrew Bagnell" ],
      "venue" : "arXiv preprint arXiv:1105.2054,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Efficient sparse low-rank tensor completion using the Frank-Wolfe algorithm",
      "author" : [ "Xiawei Guo", "Quanming Yao", "James T Kwok" ],
      "venue" : "In AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2017
    }, {
      "title" : "Conditional gradient algorithms for norm-regularized smooth convex optimization",
      "author" : [ "Zaid Harchaoui", "Anatoli Juditsky", "Arkadi Nemirovski" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Fast coordinate descent methods with variable selection for non-negative matrix factorization",
      "author" : [ "Cho-Jui Hsieh", "Inderjit S Dhillon" ],
      "venue" : "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization",
      "author" : [ "Martin Jaggi" ],
      "venue" : "In ICML 2013 - Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Non-negative tensor factorization based on alternating large-scale non-negativity-constrained least squares",
      "author" : [ "Hyunsoo Kim", "Haesun Park", "Lars Elden" ],
      "venue" : "In Bioinformatics and Bioengineering,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Algorithms for nonnegative matrix and tensor factorizations: A unified view based on block coordinate descent framework",
      "author" : [ "Jingu Kim", "Yunlong He", "Haesun Park" ],
      "venue" : "Journal of Global Optimization,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Fast nonnegative tensor factorization with an active-set-like method",
      "author" : [ "Jingu Kim", "Haesun Park" ],
      "venue" : "In High-Performance Scientific Computing,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Nonlinear band expansion and 3d nonnegative tensor factorization for blind decomposition of magnetic resonance image of the brain",
      "author" : [ "Ivica Kopriva", "Andrzej Cichocki" ],
      "venue" : "In International Conference on Latent Variable Analysis and Signal Separation,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "An Affine Invariant Linear Convergence Analysis for Frank-Wolfe Algorithms",
      "author" : [ "Simon Lacoste-Julien", "Martin Jaggi" ],
      "venue" : "In NIPS 2013 Workshop on Greedy Algorithms, Frank-Wolfe and Friends,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "On the Global Linear Convergence of Frank-Wolfe Optimization Variants",
      "author" : [ "Simon Lacoste-Julien", "Martin Jaggi" ],
      "venue" : "NIPS",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "A Hybrid Algorithm for Convex Semidefinite Optimization",
      "author" : [ "Sören Laue" ],
      "venue" : "In ICML,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "Solving least squares problems, volume 15",
      "author" : [ "Charles L Lawson", "Richard J Hanson" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1995
    }, {
      "title" : "Algorithms for non-negative matrix factorization",
      "author" : [ "Daniel D Lee", "H Sebastian Seung" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2001
    }, {
      "title" : "A unified optimization view on generalized matching pursuit and frank-wolfe",
      "author" : [ "Francesco Locatello", "Rajiv Khanna", "Michael Tschannen", "Martin Jaggi" ],
      "venue" : "In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2017
    }, {
      "title" : "Logistic regression with the nonnegative garrote",
      "author" : [ "Enes Makalic", "Daniel F Schmidt" ],
      "venue" : "In Australasian Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2011
    }, {
      "title" : "Matching pursuits with time-frequency dictionaries",
      "author" : [ "Stéphane Mallat", "Zhifeng Zhang" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1993
    }, {
      "title" : "An introduction to boosting and leveraging",
      "author" : [ "Ron Meir", "Gunnar Rätsch" ],
      "venue" : "In Advanced lectures on machine learning,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2003
    }, {
      "title" : "Vertex component analysis: A fast algorithm to unmix hyperspectral data",
      "author" : [ "José MP Nascimento", "José MB Dias" ],
      "venue" : "IEEE transactions on Geoscience and Remote Sensing,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2005
    }, {
      "title" : "Greedy strategies for convex optimization",
      "author" : [ "Hao Nguyen", "Guergana Petrova" ],
      "venue" : "Calcolo, pages 1–18,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2014
    }, {
      "title" : "Sparse nonnegative matrix factorization using l0-constraints",
      "author" : [ "Robert Peharz", "Michael Stark", "Franz Pernkopf" ],
      "venue" : "Proceedings of MLSP, pages",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2010
    }, {
      "title" : "Polytope conditioning and linear convergence of the frankwolfe algorithm",
      "author" : [ "Javier Pena", "Daniel Rodriguez" ],
      "venue" : "arXiv preprint arXiv:1512.06142,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2015
    }, {
      "title" : "Solving conic systems via projection and rescaling",
      "author" : [ "Javier Pena", "Negar Soheili" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2016
    }, {
      "title" : "Extrinsic geometry of convex surfaces, volume 35",
      "author" : [ "Aleksei Pogorelov" ],
      "venue" : "American Mathematical Soc.,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 1973
    }, {
      "title" : "On the convergence of leveraging",
      "author" : [ "Gunnar Rätsch", "Sebastian Mika", "Manfred K Warmuth" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2001
    }, {
      "title" : "Multiplicative updates for nonnegative quadratic programming in support vector machines",
      "author" : [ "F Sha", "LK Saul", "Daniel D Lee" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2002
    }, {
      "title" : "Trading Accuracy for Sparsity in Optimization Problems with Sparsity Constraints",
      "author" : [ "Shai Shalev-Shwartz", "Nathan Srebro", "Tong Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2010
    }, {
      "title" : "Non-negative tensor factorization with applications to statistics and computer vision",
      "author" : [ "Amnon Shashua", "Tamir Hazan" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2005
    }, {
      "title" : "Chebushev Greedy Algorithm in convex optimization",
      "author" : [ "Vladimir Temlyakov" ],
      "venue" : null,
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2013
    }, {
      "title" : "Greedy algorithms in convex optimization on Banach spaces",
      "author" : [ "Vladimir Temlyakov" ],
      "venue" : "Asilomar Conference on Signals, Systems and Computers,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2014
    }, {
      "title" : "Greedy approximation in convex optimization",
      "author" : [ "VN Temlyakov" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2015
    }, {
      "title" : "Greed is good: algorithmic results for sparse approximation",
      "author" : [ "Joel A Tropp" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2004
    }, {
      "title" : "Rank-one matrix pursuit for matrix completion",
      "author" : [ "Zheng Wang", "Ming jun Lai", "Zhaosong Lu", "Wei Fan", "Hasan Davulcu", "Jieping Ye" ],
      "venue" : "In ICML,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2014
    }, {
      "title" : "Positive tensor factorization",
      "author" : [ "Max Welling", "Markus Weber" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2001
    }, {
      "title" : "Fast non-negative orthogonal matching pursuit",
      "author" : [ "Mehrdad Yaghoobi", "Di Wu", "Mike E Davies" ],
      "venue" : "IEEE Signal Processing Letters,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2015
    }, {
      "title" : "Higher order Matching Pursuit for Low Rank Tensor Learning",
      "author" : [ "Yuning Yang", "Siamak Mehrkanoon", "Johan A K Suykens" ],
      "venue" : null,
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2015
    }, {
      "title" : "Greedy learning of generalized low-rank models",
      "author" : [ "Quanming Yao", "James T Kwok" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2016
    }, {
      "title" : "Truncated power method for sparse eigenvalue problems",
      "author" : [ "Xiao-Tong Yuan", "Tong Zhang" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Arguably two of the most popular representatives are Frank-Wolfe (FW) [12, 21] and Matching Pursuit (MP) algorithms [34], in particular Orthogonal MP (OMP) [9, 49].",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "Arguably two of the most popular representatives are Frank-Wolfe (FW) [12, 21] and Matching Pursuit (MP) algorithms [34], in particular Orthogonal MP (OMP) [9, 49].",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 31,
      "context" : "Arguably two of the most popular representatives are Frank-Wolfe (FW) [12, 21] and Matching Pursuit (MP) algorithms [34], in particular Orthogonal MP (OMP) [9, 49].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "Arguably two of the most popular representatives are Frank-Wolfe (FW) [12, 21] and Matching Pursuit (MP) algorithms [34], in particular Orthogonal MP (OMP) [9, 49].",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 46,
      "context" : "Arguably two of the most popular representatives are Frank-Wolfe (FW) [12, 21] and Matching Pursuit (MP) algorithms [34], in particular Orthogonal MP (OMP) [9, 49].",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 47,
      "context" : "This enables control of the trade-off between the amount of structure in the solution and approximation quality via the number of iterations, which was found useful in a large variety of use cases including structured matrix and tensor factorizations [50, 53, 54, 18].",
      "startOffset" : 251,
      "endOffset" : 267
    }, {
      "referenceID" : 50,
      "context" : "This enables control of the trade-off between the amount of structure in the solution and approximation quality via the number of iterations, which was found useful in a large variety of use cases including structured matrix and tensor factorizations [50, 53, 54, 18].",
      "startOffset" : 251,
      "endOffset" : 267
    }, {
      "referenceID" : 51,
      "context" : "This enables control of the trade-off between the amount of structure in the solution and approximation quality via the number of iterations, which was found useful in a large variety of use cases including structured matrix and tensor factorizations [50, 53, 54, 18].",
      "startOffset" : 251,
      "endOffset" : 267
    }, {
      "referenceID" : 16,
      "context" : "This enables control of the trade-off between the amount of structure in the solution and approximation quality via the number of iterations, which was found useful in a large variety of use cases including structured matrix and tensor factorizations [50, 53, 54, 18].",
      "startOffset" : 251,
      "endOffset" : 267
    }, {
      "referenceID" : 9,
      "context" : "Concrete examples include unmixing problems [11, 16, 3], model selection [33], and matrix and tensor factorizations [4, 24].",
      "startOffset" : 44,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : "Concrete examples include unmixing problems [11, 16, 3], model selection [33], and matrix and tensor factorizations [4, 24].",
      "startOffset" : 44,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "Concrete examples include unmixing problems [11, 16, 3], model selection [33], and matrix and tensor factorizations [4, 24].",
      "startOffset" : 44,
      "endOffset" : 55
    }, {
      "referenceID" : 30,
      "context" : "Concrete examples include unmixing problems [11, 16, 3], model selection [33], and matrix and tensor factorizations [4, 24].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "Concrete examples include unmixing problems [11, 16, 3], model selection [33], and matrix and tensor factorizations [4, 24].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 22,
      "context" : "Concrete examples include unmixing problems [11, 16, 3], model selection [33], and matrix and tensor factorizations [4, 24].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "In particular, all existing MP variants for the conic hull case [5, 38, 52] are not guaranteed to converge and may get stuck far away from the optimum (this can be observed in the experiments in Section 6).",
      "startOffset" : 64,
      "endOffset" : 75
    }, {
      "referenceID" : 35,
      "context" : "In particular, all existing MP variants for the conic hull case [5, 38, 52] are not guaranteed to converge and may get stuck far away from the optimum (this can be observed in the experiments in Section 6).",
      "startOffset" : 64,
      "endOffset" : 75
    }, {
      "referenceID" : 49,
      "context" : "In particular, all existing MP variants for the conic hull case [5, 38, 52] are not guaranteed to converge and may get stuck far away from the optimum (this can be observed in the experiments in Section 6).",
      "startOffset" : 64,
      "endOffset" : 75
    }, {
      "referenceID" : 36,
      "context" : "Indeed, the atom set is not guaranteed to contain an atom aligned with a descent direction for all possible suboptimal iterates, as is the case when the optimization domain is the linear span or the convex hull of the atom set [39, 32].",
      "startOffset" : 227,
      "endOffset" : 235
    }, {
      "referenceID" : 29,
      "context" : "Indeed, the atom set is not guaranteed to contain an atom aligned with a descent direction for all possible suboptimal iterates, as is the case when the optimization domain is the linear span or the convex hull of the atom set [39, 32].",
      "startOffset" : 227,
      "endOffset" : 235
    }, {
      "referenceID" : 25,
      "context" : "• We propose new away-step, pairwise, and fully corrective MP variants, inspired by variants of FW [28] and generalized MP [32], respectively, that allow for different degrees of weight corrections for previously selected atoms.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 29,
      "context" : "• We propose new away-step, pairwise, and fully corrective MP variants, inspired by variants of FW [28] and generalized MP [32], respectively, that allow for different degrees of weight corrections for previously selected atoms.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "This is in contrast to all prior work on non-negative MP, which targets quadratic objectives [5, 38, 52].",
      "startOffset" : 93,
      "endOffset" : 104
    }, {
      "referenceID" : 35,
      "context" : "This is in contrast to all prior work on non-negative MP, which targets quadratic objectives [5, 38, 52].",
      "startOffset" : 93,
      "endOffset" : 104
    }, {
      "referenceID" : 49,
      "context" : "This is in contrast to all prior work on non-negative MP, which targets quadratic objectives [5, 38, 52].",
      "startOffset" : 93,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "Furthermore, if the conic hull of the atom set equals its linear span, we recover both algorithms and rates derived in [32] for generalized MP variants.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 29,
      "context" : "The generalized MP algorithm studied in [32], presented in Algorithm 1, solves the following optimization problem: min x∈lin(A) f(x).",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 31,
      "context" : "For symmetric A and for f(x) = 12‖y − x‖ (2), y ∈ H, Algorithm 1 recovers MP (Variant 0) [34] and OMP (Variant 1) [9, 49], see [32] for details.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : "For symmetric A and for f(x) = 12‖y − x‖ (2), y ∈ H, Algorithm 1 recovers MP (Variant 0) [34] and OMP (Variant 1) [9, 49], see [32] for details.",
      "startOffset" : 114,
      "endOffset" : 121
    }, {
      "referenceID" : 46,
      "context" : "For symmetric A and for f(x) = 12‖y − x‖ (2), y ∈ H, Algorithm 1 recovers MP (Variant 0) [34] and OMP (Variant 1) [9, 49], see [32] for details.",
      "startOffset" : 114,
      "endOffset" : 121
    }, {
      "referenceID" : 29,
      "context" : "For symmetric A and for f(x) = 12‖y − x‖ (2), y ∈ H, Algorithm 1 recovers MP (Variant 0) [34] and OMP (Variant 1) [9, 49], see [32] for details.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 29,
      "context" : "The analysis of the convergence of Algorithm 1 in [32] critically relies on the assumption that the origin is in the relative interior of conv(A) with respect to its linear span.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 36,
      "context" : ", zt in Algorithm 1) and the gradient of the objective at the current iteration (see third premise in [39]).",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 36,
      "context" : "Recall the alignment assumption of the search direction and the gradient of the objective at the current iterate discussed in Section 2 (see also [39]).",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 25,
      "context" : "This can lead to the algorithm alternately selecting − xt ‖xt‖A and an atom from A, thereby slowing down convergence in a similar manner as the zig-zagging phenomenon well-known in the Frank-Wolfe framework [28].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 10,
      "context" : "Here, inspired by the away-steps and pairwise variants of FW [12, 28], instead of reducing the weights of the active atoms uniformly as in Algorithm 2, the LMO is queried a second time on the active set S to identify the direction of steepest ascent in S.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "Here, inspired by the away-steps and pairwise variants of FW [12, 28], instead of reducing the weights of the active atoms uniformly as in Algorithm 2, the LMO is queried a second time on the active set S to identify the direction of steepest ascent in S.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 49,
      "context" : "For f(x) = 12‖y − x‖ (2), y ∈ H, Variant 1 recovers Algorithm 1 in [52] and the OMP variant in [5] which both only apply to this specific objective f .",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "For f(x) = 12‖y − x‖ (2), y ∈ H, Variant 1 recovers Algorithm 1 in [52] and the OMP variant in [5] which both only apply to this specific objective f .",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 29,
      "context" : "A similar notion to measure the convergence of MP was established in [32].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : "The sublinear convergence rate obtained with this approach is up to constants identical to that in Theorem 2 for our MP variants, see [21].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 25,
      "context" : "Inspired by the notion of pyramidal width in [28], which is the minimal pyramidal directional width computed over the set of feasible directions, we now define the cone width of a set A where only the generating faces (g-faces) of cone(A) (instead of the faces of conv(A)) are considered.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 25,
      "context" : "Also note that the bounds on the number of good steps given above are the same as for the corresponding FW variants and are obtained using the same (purely combinatorial) arguments as in [28].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 29,
      "context" : "The linear convergence of the generalized (not non-negative) MP variants studied in [32] crucially depends on the geometry of the set which is characterized by the Minimal Directional Width mDW(A):",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 25,
      "context" : "In contrast, for our non-negative MPs, while some directions allow for unconstrained steps from some iterate xt, others are constrained, thereby leading to the dependence of the linear convergence rate on the cone width, a geometric constant which is very similar in spirit to the Pyramidal Width appearing in the linear convergence bound in [28] for FW.",
      "startOffset" : 342,
      "endOffset" : 346
    }, {
      "referenceID" : 25,
      "context" : "We finally relate the cone width with the Pyramidal Width [28].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "Furthermore, the decrease characterized in [28] scales as 1/τ(2) due to the dependence on 1/diam(conv(A))(2).",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 41,
      "context" : "The line of recent works by [44, 46, 47, 48, 37, 32] targets the generalization of MP from the least-squares objective to general smooth objectives and derives corresponding convergence rates (see [32] for a more in-depth discussion).",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 43,
      "context" : "The line of recent works by [44, 46, 47, 48, 37, 32] targets the generalization of MP from the least-squares objective to general smooth objectives and derives corresponding convergence rates (see [32] for a more in-depth discussion).",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 44,
      "context" : "The line of recent works by [44, 46, 47, 48, 37, 32] targets the generalization of MP from the least-squares objective to general smooth objectives and derives corresponding convergence rates (see [32] for a more in-depth discussion).",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 45,
      "context" : "The line of recent works by [44, 46, 47, 48, 37, 32] targets the generalization of MP from the least-squares objective to general smooth objectives and derives corresponding convergence rates (see [32] for a more in-depth discussion).",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 34,
      "context" : "The line of recent works by [44, 46, 47, 48, 37, 32] targets the generalization of MP from the least-squares objective to general smooth objectives and derives corresponding convergence rates (see [32] for a more in-depth discussion).",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 29,
      "context" : "The line of recent works by [44, 46, 47, 48, 37, 32] targets the generalization of MP from the least-squares objective to general smooth objectives and derives corresponding convergence rates (see [32] for a more in-depth discussion).",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 29,
      "context" : "The line of recent works by [44, 46, 47, 48, 37, 32] targets the generalization of MP from the least-squares objective to general smooth objectives and derives corresponding convergence rates (see [32] for a more in-depth discussion).",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 4,
      "context" : "However, only little prior work targets MP variants with non-negativity constraint [5, 38, 52].",
      "startOffset" : 83,
      "endOffset" : 94
    }, {
      "referenceID" : 35,
      "context" : "However, only little prior work targets MP variants with non-negativity constraint [5, 38, 52].",
      "startOffset" : 83,
      "endOffset" : 94
    }, {
      "referenceID" : 49,
      "context" : "However, only little prior work targets MP variants with non-negativity constraint [5, 38, 52].",
      "startOffset" : 83,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "[5, 52] proposed an algorithm equivalent to our Algorithm 4 for the least-squares case.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 49,
      "context" : "[5, 52] proposed an algorithm equivalent to our Algorithm 4 for the least-squares case.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 49,
      "context" : "More specifically, [52] then developed an acceleration heuristic, whereas [5] derived a coherence-based recovery guarantee for sparse linear combinations of atoms.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "More specifically, [52] then developed an acceleration heuristic, whereas [5] derived a coherence-based recovery guarantee for sparse linear combinations of atoms.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 27,
      "context" : ", [30], in particular also for matrix and tensor spaces.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 40,
      "context" : "The gold standard in factorization problems is projected gradient descent with alternating minimization, see [43, 4, 45, 23].",
      "startOffset" : 109,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "The gold standard in factorization problems is projected gradient descent with alternating minimization, see [43, 4, 45, 23].",
      "startOffset" : 109,
      "endOffset" : 124
    }, {
      "referenceID" : 42,
      "context" : "The gold standard in factorization problems is projected gradient descent with alternating minimization, see [43, 4, 45, 23].",
      "startOffset" : 109,
      "endOffset" : 124
    }, {
      "referenceID" : 21,
      "context" : "The gold standard in factorization problems is projected gradient descent with alternating minimization, see [43, 4, 45, 23].",
      "startOffset" : 109,
      "endOffset" : 124
    }, {
      "referenceID" : 37,
      "context" : "Other related works are [40], which is concerned with the feasibility problem on symmetric cones, and [19], which introduces a norm-regularized variant of problem (4) and solves it using FW on a rescaled convex set.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "Other related works are [40], which is concerned with the feasibility problem on symmetric cones, and [19], which introduces a norm-regularized variant of problem (4) and solves it using FW on a rescaled convex set.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 32,
      "context" : "Boosting: In an earlier line of work, a flavor of the generalized MP became popular in the context of boosting, see [35].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 39,
      "context" : "The literature on boosting is vast, we refer to [42, 35, 7] for a general overview.",
      "startOffset" : 48,
      "endOffset" : 59
    }, {
      "referenceID" : 32,
      "context" : "The literature on boosting is vast, we refer to [42, 35, 7] for a general overview.",
      "startOffset" : 48,
      "endOffset" : 59
    }, {
      "referenceID" : 39,
      "context" : "Taking the optimization perspective given in [42], boosting is an iterative greedy algorithm minimizing a (strongly) convex objective over the linear span of a possibly infinite set called hypothesis class.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "The convergence analysis crucially relies on the assumption of the origin being in the relative interior of the hypothesis class, see Theorem 1 in [17].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 32,
      "context" : "2 of [35] might not converge if the [39] alignment assumption is violated.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 36,
      "context" : "2 of [35] might not converge if the [39] alignment assumption is violated.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 32,
      "context" : "Here, we managed to relax this assumption while preserving essentially the same asymptotic rates in [35, 17].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "Here, we managed to relax this assumption while preserving essentially the same asymptotic rates in [35, 17].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 49,
      "context" : "We compare the convergence of Algorithms 2, 3, and 4 with the Fast Non-Negative MP (FNNOMP) of [52], and Variant 3 (line-search) of the FW algorithm in [32] on the atom set rescaled by τ = 10‖y‖ (see Section 4.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 29,
      "context" : "We compare the convergence of Algorithms 2, 3, and 4 with the Fast Non-Negative MP (FNNOMP) of [52], and Variant 3 (line-search) of the FW algorithm in [32] on the atom set rescaled by τ = 10‖y‖ (see Section 4.",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 49,
      "context" : "Indeed, [52] only show that the algorithm terminates and not its convergence.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 18,
      "context" : "The second task consists of decomposing a given matrix into the product of two non-negative matrices as in Equation (1) of [20].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 52,
      "context" : "The LMO is approximated using a truncated power method [55], and we perform atom correction with greedy coordinate descent see, e.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 26,
      "context" : ", [29, 18], to obtain a better objective value while maintaining the same (small) number of atoms.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 16,
      "context" : ", [29, 18], to obtain a better objective value while maintaining the same (small) number of atoms.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 23,
      "context" : "For interpretability reasons, there is interest to decompose MRI data into non-negative factorizations [25].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "We compare PWMP and FCMP against the multiplicative (mult) and the alternating (als) algorithm of [4], and the greedy coordinate descent (GCD) of [20].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "We compare PWMP and FCMP against the multiplicative (mult) and the alternating (als) algorithm of [4], and the greedy coordinate descent (GCD) of [20].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 5,
      "context" : "We consider the non-negative garrote which is a common approach to model order selection [6].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 30,
      "context" : "We evaluate NNMP, PWMP, and FCMP in the experiment described in [33], where the non-negative garrote is used to perform model order selection for logistic regression (i.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 30,
      "context" : "In Table 3 we compare the median classification accuracy of our algorithms with that of the cyclic coordinate descent algorithm (NNG) from [33].",
      "startOffset" : 139,
      "endOffset" : 143
    } ],
    "year" : 2017,
    "abstractText" : "Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe (FW) algorithms regained popularity in recent years due to their simplicity, effectiveness and theoretical guarantees. MP and FW address optimization over the linear span and the convex hull of a set of atoms, respectively. In this paper, we consider the intermediate case of optimization over the convex cone, parametrized as the conic hull of a generic atom set, leading to the first principled definitions of non-negative MP algorithms for which we give explicit convergence rates and demonstrate excellent empirical performance. In particular, we derive sublinear (O(1/t)) convergence on general smooth and convex objectives, and linear convergence (O(e−t)) on strongly convex objectives, in both cases for general sets of atoms. Furthermore, we establish a clear correspondence of our algorithms to known algorithms from the MP and FW literature. Our novel algorithms and analyses target general atom sets and general objective functions, and hence are directly applicable to a large variety of learning settings.",
    "creator" : null
  }
}