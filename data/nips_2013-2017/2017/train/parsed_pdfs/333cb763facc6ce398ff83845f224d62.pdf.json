{
  "name" : "333cb763facc6ce398ff83845f224d62.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Spectral Mixture Kernels for Multi-Output Gaussian Processes",
    "authors" : [ "Gabriel Parra", "Felipe Tobar" ],
    "emails" : [ "gparra@dim.uchile.cl", "ftobar@dim.uchile.cl" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The extension of Gaussian processes (GPs [1]) to multiple outputs is referred to as multi-output Gaussian processes (MOGPs). MOGPs model temporal or spatial relationships among infinitelymany random variables, as scalar GPs, but also account for the statistical dependence across different sources of data (or channels). This is crucial in a number of real-world applications such as fault detection, data imputation and denoising. For any two input points x, x′, the covariance function of an m-channel MOGP k(x, x′) is a symmetric positive-definite m×m matrix of scalar covariance functions. The design of this matrix-valued kernel is challenging since we have to deal with the trade off between (i) choosing a broad class of m(m− 1)/2 cross-covariances and m auto-covariances, while at the same time (ii) ensuring positive definiteness of the symmetric matrix containing these m(m+1)/2 covariance functions for any pair of inputs x, x′. In particular, unlike the widely available families of auto-covariance functions (e.g., [2]), cross-covariances are not bound to be positive definite and therefore can be designed freely; the construction of these functions with interpretable functional form is the main focus of this article.\nA classical approach to define cross-covariances for a MOGP is to linearly combine independent latents GPs, this is the case of the Linear Model of Coregionalization (LMC [3]) and the Convolution Model (CONV, [4]). In these cases, the resulting kernel is a function of both the covariance functions of the latent GPs and the parameters of the linear operator considered; this results in symmetric and centred cross-covariances. While these approaches are simple, they lack interpretability of the dependencies learnt and force the auto-covariances to have similar behaviour across different channels. The LMC method has also inspired the Cross-Spectral Mixture (CSM) kernel [5], which uses the\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nSpectral Mixture (SM) kernel in [6] within LMC and model phase differences across channels by manually introducing a shift between the cosine and exponential factors of the SM kernel. Despite exhibiting improved performance wrt previous approaches, the addition of the shift parameter in CSM poses the following question: Can the spectral design of multiouput covariance functions be even more flexible?\nWe take a different approach to extend the spectral mixture concept to multiple outputs: Recall that for stationary scalar-valued GPs, [6] designs the power spectral density (PSD) of the process by a mixture of square exponential functions to then, supported by Bochner’s theorem [7], present the Spectral Mixture kernel via the inverse Fourier transform of the so-constructed PSD. Along the same lines, our main contribution is to propose an expressive family of complex-valued square-exponential cross-spectral densities, and then build on Cramér’s theorem [8, 9], the multivariate extension of Bochner’s, to construct the Multi-Output Spectral Mixture kernel (MOSM). The proposed multivariate covariance function accounts for all the properties of the Cross-Spectral Mixture kernel in [5] plus a delay component across channels and variable parameters for auto-covariances of different channels. Additionally, the proposed MOSM provides clear interpretation of all the parameters in spectral terms. Our experimental contribution includes an illustrative example using a trivariate synthetic signal and validation against all the aforementioned literature using two real-world datasets."
    }, {
      "heading" : "2 Background",
      "text" : "Definition 1. A Gaussian process (GP) over the input set X is a real-valued stochastic process (f(x))x∈X such that for any finite subset of inputs {xi}Ni=1 ⊂ X , the random variables {f(xi)}Ni=1 are jointly Gaussian. Without loss of generality we will choose X = Rn.\nA GP [1] defines a distribution over functions f(x) that is uniquely determined by its mean function m(x) := E(f(x)), typically assumed m(x) = 0, and its covariance function (also known as kernel) k(x, x′) := cov(f(x), f(x′)), x, x′ ∈ X . We now equip the reader with the necessary background to follow our proposal: we first review a spectral-based approach to the design of scalar-valued covariance kernels and then present the definition of a multi-output GP."
    }, {
      "heading" : "2.1 The Spectral Mixture kernel",
      "text" : "To bypass the explicit construction of positive-definite functions within the design of stationary covariance kernels, it is possible to design the power spectral density (PSD) instead [6] and then transform it into a covariance function using the inverse Fourier transform. This is motivated by the fact that the strict positivity requirement of the PSD is much easier to achieve than the positive definiteness requirement of the covariance kernel. The theoretical support of this construction is given by the following theorem:\nTheorem 1. (Bochner’s theorem) An integrable1 function k : Rn → C is the covariance function of a weakly-stationary mean-square-continuous stochastic process f : Rn → C if and only if it admits the following representation\nk(τ) = ∫ Rn eιω >τ S(ω)dω (1)\nwhere S(ω) is a non-negative bounded function on Rn and ι denotes the imaginary unit.\nFor a proof see [9]. The above theorem gives an explicit relationship between the spectral density S and the covariance function k of the stochastic process f . In this sense, [6] proposed to model the spectral density S as a weighted mixture of Q square-exponential functions, with weights wq , centres µq and diagonal covariance matrices Σq , that is,\nS(ω) = Q∑ q=1 wq 1 (2π)n/2|Σq|1/2 exp ( − 12 (ω − µq)>Σ−1q (ω − µq) ) . (2)\nRelying on Theorem 1, the kernel associated to the spectral density S(ω) in eq. (2) is given the spectral mixture kernel defined as follows.\n1A function g(x) is said to be integrable if ∫ Rn |g(x)|dx < +∞\nDefinition 2. A Spectral Mixture (SM) kernel is a positive-definite stationary kernel given by\nk(τ) = Q∑ q=1 wq exp ( −1 2 τ>Σqτ ) cos(µ>q τ) (3)\nwhere µq ∈ Rn, Σq = diag(σ(q)1 , . . . , σ (q) n ) and wq, σq ∈ R+.\nDue to the universal function approximation property of the mixtures of Gaussians (considered here in the frequency domain) and the relationship given by Theorem 1, the SM kernel is able to approximate continuous stationary kernels to an arbitrary precision given enough spectral components as is [10, 11]. This concept points in the direction of sidestepping the kernel selection problem in GPs and it will be extended to cater for multivariate GPs in Section 3."
    }, {
      "heading" : "2.2 Multi-Output Gaussian Processes",
      "text" : "A multivariate extension of GPs can be constructed by considering an ensemble of scalar-valued stochastic processes where any finite collection of values across all such processes are jointly Gaussian. We formalise this definition as follows.\nDefinition 3. An m-channel multi-output Gaussian process f(x) := (f1(x), . . . , fm(x)), x ∈ X , is an m-tuple of stochastic processes fp : X → R ∀p = 1, . . . ,m, such that for any (finite) subset of inputs {xi}Ni=1 ⊂ X , the random variables {fc(i)(xi)}Ni=1 are jointly Gaussian for any choice of indices c(i) ∈ {1, . . . ,m}.\nRecall that the construction of scalar-valued GPs requires choosing a scalar-valued mean function and a scalar-valued covariance function. Conversely, an m-channel MOGP is defined by an Rm-valued mean function, whose ith element denotes the mean function of the ith channel, and an Rm × Rmvalued covariance function, whose (i, j)th element denotes the covariance between the ith and jth channels. The symmetry and positive-definiteness conditions of the MOGP kernel are defined as follows.\nDefinition 4. A two-input matrix-valued function K(x, x′) : X ×X → Rm×m defined element-wise by [K(x, x′)]ij = kij(x, x′) is a multivariate kernel (covariance function) if it is: (i) Symmetric, i.e., K(x, x′) = K(x′, x)>,∀x, x′ ∈ X , and (ii) Positive definite, i.e., ∀N ∈ N, c ∈ RN×m,x ∈ XN such that, [c]pi = cpi, [x]p = xp, we have\nm∑ i,j=1 N∑ p,q=1 cpicqjkij(xp, xq) ≥ 0. (4)\nFurthermore, we say that a multivariate kernel K(x, x′) is stationary if K(x, x′) = K(x − x′) or equivalently kij(x, x′) = kij(x− x′) ∀i, j ∈ {1, . . . ,m}, in this case, we denote τ = x− x′. The design of the MOGP covariance kernel involves jointly choosing functions that model the covariance of each channel (diagonal elements in K) and functions that model the cross-covariance between different channels at different input locations (off-diagonal elements in K). Choosing these m(m+ 1)/2 covariance functions is challenging when we want to be as expressive as possible and include, for instance, delays, phase shifts, negative correlations or to enforce specific spectral content while at the same time maintaining positive definiteness of K. The reader is referred to [12, 13] for a comprehensive review of MOGP models."
    }, {
      "heading" : "3 Designing Multi-Output Gaussian Processes in the Fourier Domain",
      "text" : "We extend the spectral-mixture approach [6] to multi-output Gaussian processes relying on the multivariate version of Theorem 1 first proved by Cramér and thus referred to as Cramér’s Theorem [8, 9] given by\nTheorem 2. (Cramér’s Theorem) A family {kij(τ)}mi,j=1 of integrable functions are the covariance functions of a weakly-stationary multivariate stochastic process if and only if they (i) admit the\nrepresentation\nkij(τ) = ∫ Rn eιω >τ Sij(ω)dω ∀i, j ∈ {1, . . . ,m} (5)\nwhere each Sij is an integrable complex-valued function Sij : Rn → C known as the spectral density associated to the covariance function kij(τ), and (ii) fulfil the positive definiteness condition\nm∑ i,j=1 zizjSij(ω) ≥ 0 ∀{z1, . . . , zm} ⊂ C, ω ∈ Rn (6)\nwhere z denotes the complex conjugate of z ∈ C. Note that eq. (5) states that each covariance function kij is the inverse Fourier transform of a spectral density Sij , therefore, we will say that these functions are Fourier pairs. Accordingly, we refer to the set of arguments of the covariance function τ ∈ Rn as time or space Domain depending of the application considered, and to the set of arguments of the spectral densities ω ∈ Rn as Fourier or spectral domain. Furthermore, a direct consequence of the above theorem is that for any element ω in the Fourier domain, the matrix defined by S(ω) = [Sij(ω)]mi,j=1 ∈ Rm×m is Hermitian, i.e., Sij(ω) = Sji(ω) ∀i, j, ω. Theorem 2 gives the guidelines to construct covariance functions for MOGP by designing their corresponding spectral densities instead, i.e., the design is performed in the Fourier rather than the space domain. The simplicity of design in the Fourier domain stems from the positive-definiteness condition of the spectral densities in eq. (6), which is much easier to achieve than that of the covariance functions in eq. (4). This can be understood through an analogy with the univariate model: in the single-output case the positive-definiteness condition of the kernel only requires positivity of the spectral density, whereas in the multioutput case the positive-definiteness condition of the multivariate kernel only requires that the matrix S(ω), ∀ω ∈ Rn, is positive definite but there are no constraints on each function Sij : ω 7→ Sij(ω)."
    }, {
      "heading" : "3.1 The Multi-Output Spectral Mixture kernel",
      "text" : "We now propose a family of Hermitian positive-definite complex-valued functions {Sij(·)}mi,j=1, thus fulfilling the requirements of Theorem 2, eq. (6), to use them as cross-spectral densities within MOGP. This family of functions is designed with the aim of providing physical parametric interpretation and closed-form covariance functions after applying the inverse Fourier transform.\nRecall that complex-valued positive-definite matrices can be decomposed in the form S(ω) = RH(ω)R(ω), meaning that the (i, j)th entry of S(ω) can be expressed as Sij(ω) = RH:i (ω)R:j(ω); where R(ω) ∈ CQ×m, R:i(ω) is the ith column of R(ω), and (·)H denotes the Hermitian (transpose and conjugate) operator. Note that this factor decomposition fulfils eq. (6) for any choice of R(ω) ∈ CQ×m: m∑\ni,j=1\nziR H :i (ω)R:j(ω)zj = ∣∣∣∣∣ ∣∣∣∣∣ m∑ i=1 ziR:i(ω) ∣∣∣∣∣ ∣∣∣∣∣ 2 = ||R(ω)z||2 ≥ 0 ∀z = [z1, . . . , zm]> ∈ Cm, ω ∈ Rn\n(7) We refer to Q as the rank of the decomposition, since by choosing Q < m the rank of S(ω) = RH(ω)R(ω) can be at most Q. For ease of notation we choose2 Q = 1, where the columns of R(ω) are complex-valued functions {Ri}mi=1, and S(ω) is modeled as a rank-one matrix according to Sij(ω) = Ri(ω)Rj(ω). Since Fourier transforms and multiplications of square exponential (SE) functions are also SE, we model Ri(ω) as a complex-valued SE function so as to ensure closed-form expression of its corresponding covariance kernel, that is,\nRi(ω) = wi exp\n( −1\n4 (ω − µi)>Σ−1i (ω − µi)\n) exp ( −ι(θ>i ω + φi) ) , i = 1, . . . ,m (8)\nwhere wi, φi ∈ R, µi, θi ∈ Rn and Σi = diag([σ2i1, . . . , σ2in]) ∈ Rn×n. With this choice of the functions {Ri}mi=1, the spectral densities {Sij}mi,j=1 are given by\nSij(ω) = wij exp\n( −1\n2 (ω − µij)>Σ−1ij (ω − µij) + ι\n( θ>ijω + φij )) , i, j = 1, . . . ,m (9)\n2The extension to arbitrary Q will be presented at the end of this section.\nmeaning that the cross-spectral density between channels i and j is modeled as a complex-valued SE function with the following parameters:\n• covariance: Σij = 2Σi(Σi + Σj)−1Σj • mean: µij = (Σi + Σj)−1(Σiµj + Σjµi) • magnitude: wij = wiwj exp ( − 14 (µi − µj)>(Σi + Σj)−1(µi − µj)\n) • delay: θij = θi − θj • phase: φij = φi − φj\nwhere the so-constructed magnitudes wij ensure positive definiteness and, in particular, the autospectral densities Sii are real-valued SE functions (since θii = φii = 0) as in the standard (scalarvalued) spectral mixture approach [6].\nThe power spectral density in eq. (9) corresponds to a complex-valued kernel and therefore to a complex-valued GP [14, 15] . In order to restrict this generative model only to real-valued GPs, the proposed power spectral density has to be symmetric with respect to ω [16], we then make Sij(ω) symmetric simply by reassigning Sij(ω) 7→ 12 (Sij(ω) + Sij(−ω)), this is equivalent to choosing Ri(ω) to be a vector of two mirrored complex SE functions.\nThe resulting (symmetric with respect to ω) cross-spectral density between the ith and jth channels Sij(ω) and its corresponding real-valued kernel kij(τ) = F−1{Sij(ω)}(τ) are the following Fourier pairs\nSij(ω) = wij 2\n( e ( −1 2 (ω−µij) >Σ−1ij (ω−µij)+ι ( θ>ijω+φij )) + e ( −1 2 (ω+µij) >Σ−1ij (ω+µij)+ι ( −θ>ijω+φij ))) kij(τ) = αij exp ( −1\n2 (τ + θij)\n>Σij(τ + θij) ) cos ( (τ + θij) >µij + φij )\n(10)\nwhere the magnitude parameter αij = wij(2π) n 2 |Σij |1/2 absorbs the constant resulting from the inverse Fourier transform.\nWe can again confirm that the autocovariances (i = j) are real-valued and contain square-exponential and cosine factors as in the scalar SM approach since αii ≥ 0 and θii = φii = 0. Conversely, the proposed model for the cross-covariance between different channels (i 6= j) allows for (i) both negatively- and positively-correlated signals (αij ∈ R), (ii) delayed channels through the delay parameter θij 6= 0 and (iii) out-of-phase channels where the covariance is not symmetric with respect to the delay for φij 6= 0. Fig. 1 shows cross-spectral densities and their corresponding kernel for a choice of different delay and phase parameters.\nThe kernel in eq. (10) resulted from a low rank choice for the PSD matrix Sij , therefore, increasing the rank in the proposed model for Sij is equivalent to consider several kernel components. Arbitrarily choosing Q of these components yields the expression for the proposed multivariate kernel: Definition 5. The Multi-Output Spectral Mixture kernel (MOSM) has the form:\nkij(τ) = Q∑ q=1 α (q) ij exp ( −1 2 (τ + θ (q) ij ) >Σ (q) ij (τ + θ (q) ij ) ) cos ( (τ + θ (q) ij ) >µ (q) ij + φ (q) ij ) (11)\nwhere α(q)ij = w (q) ij (2π) n 2 |Σ(q)ij |1/2 and the superindex (·)(q) denotes the parameter of the qth component of the spectral mixture.\nThis multivariate covariance function has spectral-mixture positive-definite kernels as autocovariances, while the cross-covariances are spectral mixture functions with different parameters for different output pairs, which can be (i) non-positive-definite, (ii) non-symmetric, and (iii) delayed with respect to one another. Therefore, the MOSM kernel is a multi-output generalisation of the spectral mixture approach [6] where the positive definiteness is guaranteed by the factor decomposition of Sij as shown in eq. (7)."
    }, {
      "heading" : "3.2 Training the model and computing the predictive posterior",
      "text" : "Fitting the model to observed data follows the same rationale of standard GP, that is, maximising log-probability of the data. Recall that the observations in the multioutput case consist of (i) a location x ∈ X , (ii) a channel identifier i ∈ {1, . . . ,m}, and (iii) an observed value y ∈ R; therefore, we denote N observations as the set of 3-tuples D = {(xc, ic, yc)}Nc=1. As all observations are jointly Gaussian, we concatenate the observations into the three vectors x = [x1, . . . , xN ]> ∈ XN , i = [i1, . . . , iN ]\n> ∈ {1, . . . ,m}N , and y = [y1, . . . , yN ]> ∈ RN , to express the negative loglikelihood (NLL) by\n− log p(y|x,Θ) = N 2 log 2π + 1 2 log |Kxi|+ 1 2 y>K−1xi y (12)\nwhere all hyperparameters are denoted by Θ, and Kxi is the covariance matrix of all observed samples, that is, the (r, s)th element [Kxi]rs is the covariance between the process at (location: xr, channel: ir) and the process at (location: xs, channel: is). Recall that, under the proposed MOSM model, this covariance [Kxi]rs is given by eq. (11), that is, kiris(xr − xs) + σ2ir,noiseδiris , where σ2ir,noise is a diagonal term to cater for uncorrelated observation noise. The NLL is then minimised with respect to Θ = {w(q)i , µ (q) i ,Σ (q) i , θ (q) i , φ (q) i , σ 2 i,noise}m,Qi=1,q=1, that is, the original parameters chosen to construct R(ω) in Section 3.1, plus the noise hyperparameters.\nOnce the hyperparameters are optimised, computing the predictive posterior in the proposed MOSM follows the standard GP procedure with the joint covariances given by eq. (11)."
    }, {
      "heading" : "3.3 Related work",
      "text" : "Generalising the scalar spectral mixture kernel to MOGPs can be achieved from the LMC framework as pointed out in [5] (denoted SM-LMC). As this formulation only considers real-valued cross spectral densities, the authors propose a multivariate covariance function by including a complex component to the cross spectral densities to cater for phase differences across channels, which they call the Cross Spectral Mixture kernel (denoted CSM). This multivariate covariance function can be seen as the proposed MOSM model with µi = µj , Σi = Σj , θi = θj ∀i, j ∈ {1, . . . ,m} and φi = µ>i ψi for ψi ∈ Rn. As a consequence, the SM-LMC is a particular case of the proposed MOSM model, where the parameters µi,Σi, θi are restricted to be same for all channels and therefore no phase shifts and no delays are allowed—unlike the MOSM example in Fig. 1. Additionally, Cramér’s theorem has also been used in a similar fashion in [17] but only with real-valued t-Student cross-spectral densities yielding cross-covariances that are either positive-definite or negative-definite."
    }, {
      "heading" : "4 Experiments",
      "text" : "We show two sets of experiments. First, we validated the ability of the proposed MOSM model in the identification of known auto- and cross-covariances of synthetic data. Second, we compared\nMOSM against the spectral-mixture linear model of coregionalization (SM-LMC, [3, 6, 5]), the Gaussian convolution model (CONV, [4]), and the cross-spectral mixture model (CSM, [5]) in the estimation of missing real-world data in two different distributed settings: climate signals and metal concentrations. All models were implemented in Tensorflow [18] using GPflow [19] in order to make use of automatic differentiation to compute the gradients of the NLL. The performance of all the models in the experiments was measured by the mean absolute error given by\nMAE : 1\nN N∑ i=1 |yi − ŷi| (13)\nwhere yi denotes the true value and ŷi the MOGP estimate."
    }, {
      "heading" : "4.1 Synthetic example: Learning derivatives and delayed signals",
      "text" : "All models were implemented to recover the auto- and cross-covariances of a three-output GP with the following components: (i) a reference signal sampled from a GP f(x) ∼ GP(0,KSM ) with spectral mixture covariance kernel KSM and zero mean, (ii) its derivative f ′(x), and (iii) a delayed version fδ(x) = f(x− δ). The motivation for this illustrative example is that the covariances and cross covariances of the aforementioned processes are known explicitly (see [1, Sec. 9.4]) and we can therefore compare our estimates to the true model. The derivative was computed numerically (first order through finite differences) and the training samples were generated as follows: We chose N1 = 500 samples from the reference function in the interval [-20, 20], N2 = 400 samples from the derivative signal in the interval [-20, 0], and N3 = 400 samples from the delayed signal in the interval [-20, 0]. All samples were randomly uniformly chosen in the intervals mentioned and Gaussian noised was added to yield realistic observations. The experiment then consisted in the reconstruction the reference signal in the interval [-20, 20], and the imputation of the derivative and delayed signals over the interval [0, 20].\nFig. 2 shows the ground truth and MOSM estimates for all three synthetic signals and the covariances (normalised), and Table 1 reports the MAE for all models over ten realisations of the experiment. Notice that the proposed model successfully learnt all cross-covariances cov(f(·), f ′(x)) and cov(f(x), f(x − δ)), and autocovariances without prior information about the delayed or the derivative relationship between the two channels. Furthermore, MOSM was the only model that successfully extrapolated the derivate signal and the delayed signal simultaneously, this is due the fact that the cross-covariances needed for this setting are not linear combinations of univariate kernels, hence models based on latent processes fail in this synthetic example.\nCONV 0.211 ± 0.085 0.759 ± 0.075 0.524 ± 0.097 SM-LMC 0.166 ± 0.009 0.747 ± 0.101 0.398 ± 0.042 CSM 0.148 ± 0.010 0.262 ± 0.032 0.368 ± 0.089 MOSM 0.127 ± 0.011 0.223 ± 0.015 0.146 ± 0.017"
    }, {
      "heading" : "4.2 Climate data",
      "text" : "The first real-world dataset contained measurements3 from a sensor network of four climate stations in the south on England: Cambermet, Chimet, Sotonmet and Bramblemet. We considered the normalised air temperature signal from 12 March, 2017 to 16 March, 2017, in 5-minute intervals (5692 samples), from where we randomly chose N = 1000 samples for training. Following [4], we simulated a sensor failure by removing the second half of the measurements for one sensor and leaving the remaining three sensors operating correctly; we reproduced the same setup across all four sensors thus producing four experiments. All models considered had five latent signals/spectral components.\nFor all four models considered, Fig. 3 shows the estimates of missing data for the Cambermet-failure case. Table 2 shows the mean absolute error for all models and failure cases over the missing data region. Observe how all models were able to capture the behaviour of the signal in the missing range, this is because the considered climate signals are very similar to one another. This shows that the MOSM can also collapse to models that share parameters across pairs of outputs when required.\nCONV 0.098 ± 0.008 0.192 ± 0.015 0.211 ± 0.038 0.163 ± 0.009 SM-LMC 0.084 ± 0.004 0.176 ± 0.003 0.273 ± 0.001 0.134 ± 0.002 CSM 0.094 ± 0.003 0.129 ± 0.004 0.195 ± 0.011 0.130 ± 0.004 MOSM 0.097 ± 0.006 0.137 ± 0.007 0.162 ± 0.011 0.129 ± 0.003\nThese results do not show a significant difference between the proposed model and the latent processes based models. In order to test for statistical significance, the Kolmogorov-Smirnov test [20, Ch. 7] was used with a significance level α = 0.05, concluding that for the Sotonmet sensor we can assure that the MOSM model yields the best results. Conversely, for the Cambermet, Chimet and Bramblemet sensors, MOSM and CSM provided similar results, though we cannot confirm their difference is statistically significant. However, given the high correlation of these signals and the\n3The data can be obtained from www.cambermet.co.uk. and the sites therein.\nsimilarity between the MOSM model and the CSM model, the close performance of these two models on this dataset is to be expected."
    }, {
      "heading" : "4.3 Heavy metal concentration",
      "text" : "The Jura dataset [3] contains, in addition to other geological data, the concentration of seven heavy metals in a region of 14.5 km2 of the Swiss Jura, and it is divided into a training set (259 locations) and a validation set (100 locations). We followed [3, 4], where the motivation was to aid the prediction of a variable that is expensive to measure by using abundant measurements of correlated variables which are less expensive to acquire. Specifically, we estimated Cadmium and Copper at the validation locations using measurements of related variables at the training and test locations: Nickel and Zinc for Cadmium; and Lead, Nickel and Zinc for Copper. The MAE—see eq. (13)—is shown in Table 3, where the results for the CONV model were obtained from [4] and all models considered five latent signals/spectral components, except for the independent Gaussian process (denoted IGP).\nObserve how the proposed MOSM model outperforms all other models over the Cadmium data, which is statistical significant with a significance level α = 0.05. Conversely, we cannot guarantee a statistically-significant difference between the CSM model and the MOSM in the Copper case. In both cases, testing for statistical significance against the CONV model was not possible since those results were obtained from [4]. On the other hand, the higher variability and non-Gaussianity of the Copper data may be the reason of why the simplest MOGP model (SM-LMC) achieves the best results."
    }, {
      "heading" : "5 Discussion",
      "text" : "We have proposed the multioutput spectral mixture (MOSM) kernel to model rich relationships across multiple outputs within Gaussian processes regression models. This has been achieved by constructing a positive-definite matrix of complex-valued spectral densities, and then transforming them via the inverse Fourier transform according to Cramér’s Theorem. The resulting kernel provides a clear interpretation from a spectral viewpoint, where each of its parameters can be identified with frequency, magnitude, phase and delay for a pair of channels. Furthermore, a key feature that is unique to the proposed kernel is the ability joint model delays and phase differences, this is possible due to the complex-valued model for the cross-spectral density considered and validated experimentally using a synthetic example—see Fig. 2. The MOSM kernel has also been compared against existing MOGP models on two real-world datasets, where the proposed model performed competitively in terms of the mean absolute error. Further research should point towards a sparse implementation of the proposed MOGP which can build on [4, 21] to design inducing variables that exploit the spectral content of the processes as in [22, 23]."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Cristóbal Silva (Universidad de Chile) for useful recommendations about GPU implementation, Rasmus Bonnevie from the GPflow team for his assistance on the experimental MOGP module within GPflow, and the anonymous reviewers. This work was financially supported by Conicyt Basal-CMM."
    } ],
    "references" : [ {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Automatic model construction with Gaussian processes",
      "author" : [ "D. Duvenaud" ],
      "venue" : "Ph.D. dissertation, University of Cambridge, 2014.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Geostatistics for natural resources evaluation",
      "author" : [ "P. Goovaerts" ],
      "venue" : "Oxford University Press on Demand,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1997
    }, {
      "title" : "Sparse convolved Gaussian processes for multi-output regression",
      "author" : [ "M.A. Álvarez", "N.D. Lawrence" ],
      "venue" : "Advances in Neural Information Processing Systems 21, 2008, pp. 57–64.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "GP kernels for cross-spectrum analysis",
      "author" : [ "K.R. Ulrich", "D.E. Carlson", "K. Dzirasa", "L. Carin" ],
      "venue" : "Advances in Neural Information Processing Systems 28, 2015, pp. 1999–2007.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Gaussian process kernels for pattern discovery and extrapolation",
      "author" : [ "A.G. Wilson", "R.P. Adams" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML-13), 2013, pp. 1067–1075.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Lectures on Fourier Integrals, ser. Annals of mathematics studies",
      "author" : [ "S. Bochner", "M. Tenenbaum", "H. Pollard" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1959
    }, {
      "title" : "On the theory of stationary random processes",
      "author" : [ "H. Cramér" ],
      "venue" : "Annals of Mathematics, pp. 215–230, 1940.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1940
    }, {
      "title" : "Correlation Theory of Stationary and Related Random Functions, ser",
      "author" : [ "A. Yaglom" ],
      "venue" : "Correlation Theory of Stationary and Related Random Functions. Springer,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1987
    }, {
      "title" : "Learning stationary time series using Gaussian processes with nonparametric kernels",
      "author" : [ "F. Tobar", "T.D. Bui", "R.E. Turner" ],
      "venue" : "Advances in Neural Information Processing Systems 28. Curran Associates, Inc., 2015, pp. 3501–3509.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Modelling time series via automatic learning of basis functions",
      "author" : [ "F. Tobar", "R.E. Turner" ],
      "venue" : "Proc. of IEEE SAM, 2016, pp. 2209–2213.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Kernels for vector-valued functions: A review",
      "author" : [ "M.A. Álvarez", "L. Rosasco", "N.D. Lawrence" ],
      "venue" : "Found. Trends Mach. Learn., vol. 4, no. 3, pp. 195–266, Mar. 2012.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Cross-covariance functions for multivariate geostatistics",
      "author" : [ "M.G. Genton", "W. Kleiber" ],
      "venue" : "Institute of Mathematical Statistics, vol. 30, no. 2, 2015.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Modelling of complex signals using Gaussian processes",
      "author" : [ "F. Tobar", "R.E. Turner" ],
      "venue" : "Proc. of IEEE ICASSP, 2015, pp. 2209–2213.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Gaussian processes regressors for complex proper signals in digital communications",
      "author" : [ "R. Boloix-Tortosa", "F.J. Payán-Somet", "J.J. Murillo-Fuentes" ],
      "venue" : "Proc. of IEEE SAM, 2014, pp. 137–140.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Modern spectral estimation : Theory and application",
      "author" : [ "S.M. Kay" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1988
    }, {
      "title" : "Matérn cross-covariance functions for multivariate random fields",
      "author" : [ "T. Gneiting", "W. Kleiber", "M. Schlather" ],
      "venue" : "Journal of the American Statistical Association, vol. 105, no. 491, pp. 1167–1177, 2010.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "M. Abadi" ],
      "venue" : "2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "GPflow: A Gaussian process library using TensorFlow",
      "author" : [ "A.G. d. G. Matthews", "M. van der Wilk", "T. Nickson", "K. Fujii", "A. Boukouvalas", "P. León-Villagrá", "Z. Ghahramani", "J. Hensman" ],
      "venue" : "2016.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Efficient multioutput Gaussian processes through variational inducing kernels.",
      "author" : [ "M.A. Álvarez", "D. Luengo", "M.K. Titsias", "N.D. Lawrence" ],
      "venue" : "in AISTATS, vol",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Variational Fourier features for Gaussian processes",
      "author" : [ "J. Hensman", "N. Durrande", "A. Solin" ],
      "venue" : "arXiv preprint arXiv:1611.06740, 2016.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Design of covariance functions using inter-domain inducing variables",
      "author" : [ "F. Tobar", "T.D. Bui", "R.E. Turner" ],
      "venue" : "NIPS 2015 - Time Series Workshop, December 2015. 10",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The extension of Gaussian processes (GPs [1]) to multiple outputs is referred to as multi-output Gaussian processes (MOGPs).",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : ", [2]), cross-covariances are not bound to be positive definite and therefore can be designed freely; the construction of these functions with interpretable functional form is the main focus of this article.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 2,
      "context" : "A classical approach to define cross-covariances for a MOGP is to linearly combine independent latents GPs, this is the case of the Linear Model of Coregionalization (LMC [3]) and the Convolution Model (CONV, [4]).",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "A classical approach to define cross-covariances for a MOGP is to linearly combine independent latents GPs, this is the case of the Linear Model of Coregionalization (LMC [3]) and the Convolution Model (CONV, [4]).",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 4,
      "context" : "The LMC method has also inspired the Cross-Spectral Mixture (CSM) kernel [5], which uses the",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "Spectral Mixture (SM) kernel in [6] within LMC and model phase differences across channels by manually introducing a shift between the cosine and exponential factors of the SM kernel.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "Despite exhibiting improved performance wrt previous approaches, the addition of the shift parameter in CSM poses the following question: Can the spectral design of multiouput covariance functions be even more flexible? We take a different approach to extend the spectral mixture concept to multiple outputs: Recall that for stationary scalar-valued GPs, [6] designs the power spectral density (PSD) of the process by a mixture of square exponential functions to then, supported by Bochner’s theorem [7], present the Spectral Mixture kernel via the inverse Fourier transform of the so-constructed PSD.",
      "startOffset" : 355,
      "endOffset" : 358
    }, {
      "referenceID" : 6,
      "context" : "Despite exhibiting improved performance wrt previous approaches, the addition of the shift parameter in CSM poses the following question: Can the spectral design of multiouput covariance functions be even more flexible? We take a different approach to extend the spectral mixture concept to multiple outputs: Recall that for stationary scalar-valued GPs, [6] designs the power spectral density (PSD) of the process by a mixture of square exponential functions to then, supported by Bochner’s theorem [7], present the Spectral Mixture kernel via the inverse Fourier transform of the so-constructed PSD.",
      "startOffset" : 500,
      "endOffset" : 503
    }, {
      "referenceID" : 7,
      "context" : "Along the same lines, our main contribution is to propose an expressive family of complex-valued square-exponential cross-spectral densities, and then build on Cramér’s theorem [8, 9], the multivariate extension of Bochner’s, to construct the Multi-Output Spectral Mixture kernel (MOSM).",
      "startOffset" : 177,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "Along the same lines, our main contribution is to propose an expressive family of complex-valued square-exponential cross-spectral densities, and then build on Cramér’s theorem [8, 9], the multivariate extension of Bochner’s, to construct the Multi-Output Spectral Mixture kernel (MOSM).",
      "startOffset" : 177,
      "endOffset" : 183
    }, {
      "referenceID" : 4,
      "context" : "The proposed multivariate covariance function accounts for all the properties of the Cross-Spectral Mixture kernel in [5] plus a delay component across channels and variable parameters for auto-covariances of different channels.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "A GP [1] defines a distribution over functions f(x) that is uniquely determined by its mean function m(x) := E(f(x)), typically assumed m(x) = 0, and its covariance function (also known as kernel) k(x, x′) := cov(f(x), f(x′)), x, x′ ∈ X .",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "To bypass the explicit construction of positive-definite functions within the design of stationary covariance kernels, it is possible to design the power spectral density (PSD) instead [6] and then transform it into a covariance function using the inverse Fourier transform.",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : "In this sense, [6] proposed to model the spectral density S as a weighted mixture of Q square-exponential functions, with weights wq , centres μq and diagonal covariance matrices Σq , that is,",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "Due to the universal function approximation property of the mixtures of Gaussians (considered here in the frequency domain) and the relationship given by Theorem 1, the SM kernel is able to approximate continuous stationary kernels to an arbitrary precision given enough spectral components as is [10, 11].",
      "startOffset" : 297,
      "endOffset" : 305
    }, {
      "referenceID" : 10,
      "context" : "Due to the universal function approximation property of the mixtures of Gaussians (considered here in the frequency domain) and the relationship given by Theorem 1, the SM kernel is able to approximate continuous stationary kernels to an arbitrary precision given enough spectral components as is [10, 11].",
      "startOffset" : 297,
      "endOffset" : 305
    }, {
      "referenceID" : 11,
      "context" : "The reader is referred to [12, 13] for a comprehensive review of MOGP models.",
      "startOffset" : 26,
      "endOffset" : 34
    }, {
      "referenceID" : 12,
      "context" : "The reader is referred to [12, 13] for a comprehensive review of MOGP models.",
      "startOffset" : 26,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "We extend the spectral-mixture approach [6] to multi-output Gaussian processes relying on the multivariate version of Theorem 1 first proved by Cramér and thus referred to as Cramér’s Theorem [8, 9] given by Theorem 2.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "We extend the spectral-mixture approach [6] to multi-output Gaussian processes relying on the multivariate version of Theorem 1 first proved by Cramér and thus referred to as Cramér’s Theorem [8, 9] given by Theorem 2.",
      "startOffset" : 192,
      "endOffset" : 198
    }, {
      "referenceID" : 8,
      "context" : "We extend the spectral-mixture approach [6] to multi-output Gaussian processes relying on the multivariate version of Theorem 1 first proved by Cramér and thus referred to as Cramér’s Theorem [8, 9] given by Theorem 2.",
      "startOffset" : 192,
      "endOffset" : 198
    }, {
      "referenceID" : 5,
      "context" : "meaning that the cross-spectral density between channels i and j is modeled as a complex-valued SE function with the following parameters: • covariance: Σij = 2Σi(Σi + Σj)Σj • mean: μij = (Σi + Σj)(Σiμj + Σjμi) • magnitude: wij = wiwj exp ( − 14 (μi − μj)(Σi + Σj)(μi − μj) ) • delay: θij = θi − θj • phase: φij = φi − φj where the so-constructed magnitudes wij ensure positive definiteness and, in particular, the autospectral densities Sii are real-valued SE functions (since θii = φii = 0) as in the standard (scalarvalued) spectral mixture approach [6].",
      "startOffset" : 553,
      "endOffset" : 556
    }, {
      "referenceID" : 13,
      "context" : "(9) corresponds to a complex-valued kernel and therefore to a complex-valued GP [14, 15] .",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "(9) corresponds to a complex-valued kernel and therefore to a complex-valued GP [14, 15] .",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : "In order to restrict this generative model only to real-valued GPs, the proposed power spectral density has to be symmetric with respect to ω [16], we then make Sij(ω) symmetric simply by reassigning Sij(ω) 7→ 12 (Sij(ω) + Sij(−ω)), this is equivalent to choosing Ri(ω) to be a vector of two mirrored complex SE functions.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "Therefore, the MOSM kernel is a multi-output generalisation of the spectral mixture approach [6] where the positive definiteness is guaranteed by the factor decomposition of Sij as shown in eq.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 4,
      "context" : "Generalising the scalar spectral mixture kernel to MOGPs can be achieved from the LMC framework as pointed out in [5] (denoted SM-LMC).",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : "Additionally, Cramér’s theorem has also been used in a similar fashion in [17] but only with real-valued t-Student cross-spectral densities yielding cross-covariances that are either positive-definite or negative-definite.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "MOSM against the spectral-mixture linear model of coregionalization (SM-LMC, [3, 6, 5]), the Gaussian convolution model (CONV, [4]), and the cross-spectral mixture model (CSM, [5]) in the estimation of missing real-world data in two different distributed settings: climate signals and metal concentrations.",
      "startOffset" : 77,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "MOSM against the spectral-mixture linear model of coregionalization (SM-LMC, [3, 6, 5]), the Gaussian convolution model (CONV, [4]), and the cross-spectral mixture model (CSM, [5]) in the estimation of missing real-world data in two different distributed settings: climate signals and metal concentrations.",
      "startOffset" : 77,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "MOSM against the spectral-mixture linear model of coregionalization (SM-LMC, [3, 6, 5]), the Gaussian convolution model (CONV, [4]), and the cross-spectral mixture model (CSM, [5]) in the estimation of missing real-world data in two different distributed settings: climate signals and metal concentrations.",
      "startOffset" : 77,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "MOSM against the spectral-mixture linear model of coregionalization (SM-LMC, [3, 6, 5]), the Gaussian convolution model (CONV, [4]), and the cross-spectral mixture model (CSM, [5]) in the estimation of missing real-world data in two different distributed settings: climate signals and metal concentrations.",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "MOSM against the spectral-mixture linear model of coregionalization (SM-LMC, [3, 6, 5]), the Gaussian convolution model (CONV, [4]), and the cross-spectral mixture model (CSM, [5]) in the estimation of missing real-world data in two different distributed settings: climate signals and metal concentrations.",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : "All models were implemented in Tensorflow [18] using GPflow [19] in order to make use of automatic differentiation to compute the gradients of the NLL.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : "All models were implemented in Tensorflow [18] using GPflow [19] in order to make use of automatic differentiation to compute the gradients of the NLL.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "Following [4], we simulated a sensor failure by removing the second half of the measurements for one sensor and leaving the remaining three sensors operating correctly; we reproduced the same setup across all four sensors thus producing four experiments.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 2,
      "context" : "3 Heavy metal concentration The Jura dataset [3] contains, in addition to other geological data, the concentration of seven heavy metals in a region of 14.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "We followed [3, 4], where the motivation was to aid the prediction of a variable that is expensive to measure by using abundant measurements of correlated variables which are less expensive to acquire.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "We followed [3, 4], where the motivation was to aid the prediction of a variable that is expensive to measure by using abundant measurements of correlated variables which are less expensive to acquire.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "(13)—is shown in Table 3, where the results for the CONV model were obtained from [4] and all models considered five latent signals/spectral components, except for the independent Gaussian process (denoted IGP).",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "In both cases, testing for statistical significance against the CONV model was not possible since those results were obtained from [4].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "Further research should point towards a sparse implementation of the proposed MOGP which can build on [4, 21] to design inducing variables that exploit the spectral content of the processes as in [22, 23].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 19,
      "context" : "Further research should point towards a sparse implementation of the proposed MOGP which can build on [4, 21] to design inducing variables that exploit the spectral content of the processes as in [22, 23].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 20,
      "context" : "Further research should point towards a sparse implementation of the proposed MOGP which can build on [4, 21] to design inducing variables that exploit the spectral content of the processes as in [22, 23].",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 21,
      "context" : "Further research should point towards a sparse implementation of the proposed MOGP which can build on [4, 21] to design inducing variables that exploit the spectral content of the processes as in [22, 23].",
      "startOffset" : 196,
      "endOffset" : 204
    } ],
    "year" : 2017,
    "abstractText" : "Early approaches to multiple-output Gaussian processes (MOGPs) relied on linear combinations of independent, latent, single-output Gaussian processes (GPs). This resulted in cross-covariance functions with limited parametric interpretation, thus conflicting with the ability of single-output GPs to understand lengthscales, frequencies and magnitudes to name a few. On the contrary, current approaches to MOGP are able to better interpret the relationship between different channels by directly modelling the cross-covariances as a spectral mixture kernel with a phase shift. We extend this rationale and propose a parametric family of complex-valued cross-spectral densities and then build on Cramér’s Theorem (the multivariate version of Bochner’s Theorem) to provide a principled approach to design multivariate covariance functions. The so-constructed kernels are able to model delays among channels in addition to phase differences and are thus more expressive than previous methods, while also providing full parametric interpretation of the relationship across channels. The proposed method is first validated on synthetic data and then compared to existing MOGP methods on two real-world examples.",
    "creator" : null
  }
}