{
  "name" : "2b45e8d6abf59038a975faeeb6dc0782.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Population Matching Discrepancy and Applications in Deep Learning",
    "authors" : [ "Jianfei Chen", "Chongxuan Li", "Yizhong Ru", "Jun Zhu" ],
    "emails" : [ "chenjian14@mails.tsinghua.edu.cn,", "licx14@mails.tsinghua.edu.cn,", "ruyz13@mails.tsinghua.edu.cn,", "dcszj@tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recent advances on image classification [26], speech recognition [19] and machine translation [9] suggest that properly building large models with a deep hierarchy can be effective to solve realistic learning problems. Many deep learning tasks, such as generative modeling [16, 3], domain adaptation [5, 47], model criticism [32] and metric learning [14], require estimating the statistical divergence of two probability distributions. A challenge is that in many tasks, only the samples instead of the closed-form distributions are available. Such distributions include implicit probability distributions and intractable marginal distributions. Without making explicit assumption on the parametric form, these distributions are richer and hence can lead to better estimates [35]. In these cases, the estimation of the statistical divergence based on samples is important. Furthermore, as the distance can be used as a training objective, it need to be differentiable with respect to the parameters of the distributions to enable efficient gradient-based training.\nOne popular sample-based statistical divergence is the maximum mean discrepancy (MMD) [17], which compares the kernel mean embedding of two distributions in RKHS. MMD has a closed-form estimate of the statistical distance in quadratic time, and there are theoretical results on bounding the approximation error. Due to its simplicity and theoretical guarantees, MMD have been widely adopted in many tasks such as belief propagation [44], domain adaptation [47] and generative modeling [31]. However, MMD has several drawbacks. For instance, it has a kernel bandwidth parameter that needs tuning [18], and the kernel can saturate so that the gradient vanishes [3] in a deep generative model. Furthermore, in order to have a reliable estimate of the distance, the mini-batch size must be large, e.g., 1000, which slows down the training by stochastic gradient descent [31].\n∗Corresponding author.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nIn this paper, we consider a sample-based estimation of the Wasserstein metric [49], which we refer to as population matching discrepancy (PMD). PMD is the cost of the minimum weight matching of the two sample populations from the distributions, and we show that it is a strongly consistent estimator of the first Wasserstein metric. We propose an algorithm to use PMD as a training objective to learn the parameters of the distribution, and reveal that PMD has some advantages over MMD: PMD has no bandwidth hyper-parameter, has stronger gradient, and can use normal mini-batch size, such as 100, during the learning. We compare PMD with MMD on two deep learning tasks, domain adaptation and generative modeling. PMD outperforms MMD in terms of both the performance and the speed of convergence."
    }, {
      "heading" : "2 Population Matching Discrepancy",
      "text" : "In this section, we give the definition of the population matching discrepancy (PMD) and propose an algorithm to learn with PMD."
    }, {
      "heading" : "2.1 Population Matching Discrepancy",
      "text" : "Consider the general case where we have two distributions pX(x) and pY (y), whose PDFs are unknown, but we are allowed to draw samples from them. Let X = {xi} N i=1 and Y = {yj} N j=1 denote the N i.i.d. samples from each distribution respectively. We define the N -PMD of the two distributions as\nDN (X,Y) = min M\n1\nN N∑ i=1 d(xi, yMi), (1)\nwhere d(·, ·) is any distance in the sample space (e.g., Euclidean distance) and M is a permutation to derive a matching between the two sets of samples. The optimal M corresponds to the bipartite minimum weight matching [27], where each element of the cost matrix is dij = d(xi, yj) with i, j ∈ [N ], where [N ] = {1, · · · , N}. Intuitively, PMD is the average distance of the matched pairs of samples, therefore it is non-negative and symmetric. Furthermore, as we shall see in Sec. 3.1, PMD is a strongly consistent estimator of the first Wasserstein metric [49] between pX and pY , which is a valid statistical distance, i.e., D∞(X,Y) = 0 iff the two distributions pX and pY are identical."
    }, {
      "heading" : "2.2 Parameter Learning",
      "text" : "While the N -PMD in Eq. (1) itself can serve as a measure of the closeness of two distributions, we are more interested in learning the parameter of the distributions using PMD as an objective. For instance, in generative modeling [31], we have a parameterized generator distribution pX(x; θX) and a data distribution pY (y), and we wish to minimize the distance of these two distributions. We\nassume the samples are obtained by applying some parameterized transformations to a known and fixed noise distribution, i.e.,\nǫi ∼ qX(ǫ), xi;θX = T X θX (ǫi); and ξj ∼ qY (ξ), yj;θY = T Y θY (ξj).\nFor flexibility, the transformations can be implemented by deep neural networks. Without loss of generality, we assume both pX and pY are parameterized distributions by θX and θY , respectively. If pX is a fixed distribution, we can take qX = pX and T\nX θX\nto be a fixed identity mapping. Our goal for parameter learning is to minimize the expected N -PMD over different populations\nmin θX ,θY Eǫ,ξDN (XθX ,YθY ), (2)\nwhere ǫ = {ǫi} N i=1, ξ = {ξj} N j=1, XθX = {xi;θX} N i=1 and YθY = {yj;θY } N j=1, and the expectation is for preventing over-fitting the parameter with respect to particular populations. The parameters can be optimized by stochastic gradient descent (SGD) [7]. At each iteration, we draw ǫ and ξ, and compute an unbiased stochastic gradient\n∇θDN (XθX ,YθY ) = ∇θ min M\n1\nN N∑ i=1 d(xi;θX , yMi;θY ) = ∇θ 1 N N∑ i=1 d(xi;θX , yM∗i ;θY ), (3)\nwhere M∗ = argmin M ∑N i=1 d(xi;θX , yMi;θY ) is the minimum weight matching for XθX and YθY . The second equality in Eq. (3) holds because the discrete matching M ∗ should not change for infinitesimal change of θ, as long as the transformations TX , TY , and the distance d(·, ·) are continuous. In other words, the gradient does not propagate through the matching.\nFurthermore, assuming that the matching M∗ does not change much within a small number of gradient updates, we can have an even cheaper stochastic gradient by subsampling the populations\n∇θDN (XθX ,YθY ) ≈ ∇θ 1\n|B| |B|∑ i=1 d(xBi;θX , yM∗Bi ;θY ), (4)\nwhere a mini-batch of |B|, e.g., 100, samples is used to approximate the whole N -sample population. To clarify, our population size N is known as the mini-batch size in some maximum mean discrepancy (MMD) literature [31], and is around 1000. Fig. 1 is the pseudocode of parameter learning for PMD along with a graphical illustration. In the outer loop, we generate populations and compute the matching; and in the inner loop, we perform several SGD updates of the parameter θ, assuming the matching M does not change much. In the graphical illustration, the distribution pY is fixed, and we want to optimize the parameters of pX to minimize their PMD."
    }, {
      "heading" : "2.3 Solving the Matching Problem",
      "text" : "The minimum weight matching can be solved exactly in O(N3) by the Hungarian algorithm [27]. When the problem is simple enough, so that small N , e.g., hundreds, is sufficient for reliable distance estimation, O(N3) time complexity is acceptable comparing with the O(N × BackProp) time complexity of computing the gradient with respect to the transformations TXθX and T Y θY\n. When N is larger, e.g., a few thousands, the Hungarian algorithm takes seconds to run. We resort to Drake and Hougardy’s approximated matching algorithm [11] in O(N2) time. The running time and model quality of PMD using both matching algorithms are reported in Sec. 5.3. In practice, we find PMD with both the exact and approximate matching algorithms works well. This is not surprising because training each sample towards its approximate matching sample is still reasonable. Finally, while we only implement the serial CPU version of the matching algorithms, both algorithm can be parallelized on GPU to further improve the running speed [10, 34]."
    }, {
      "heading" : "3 Theoretical Analysis and Connections to Other Discrepancies",
      "text" : "In this section, we establish the connection between PMD with the Wasserstein metric and the maximum mean discrepancy (MMD). We show that PMD is a strongly consistent estimator of the Wasserstein metric, and compare its advantages and disadvantages with MMD."
    }, {
      "heading" : "3.1 Relationship with the Wasserstein Metric",
      "text" : "The Wasserstein metric [49] was initially studied in the optimal transport theory, and has been adopted in computer vision [40], information retrival [50] and differential privacy [30]. The first Wasserstein metric of two distributions pX(x) and pY (y) is defined as\ninf γ(x,y)\n∫ d(x, y)γ(x, y)dxdy\ns.t. ∫ γ(x, y)dx = pY (y), ∀y; ∫ γ(x, y)dy = pX(x), ∀x; γ(x, y) ≥ 0, ∀x, y. (5)\nIntuitively, the Wasserstein metric is the optimal cost to move some mass distributed as pX to pY , where the transference plan γ(x, y) is the amount of mass to move from x to y. Problem (5) is not tractable because the PDFs of pX and pY are unknown. We approximate them with empirical distributions p̂X(x) = 1 N ∑N i=1 δxi(x) and p̂Y (y) = 1 N ∑N j=1 δyj (y), where δx(·) is the Dirac delta function\nat x. To satisfy the constraints, γ should have the form γ(x, y) = ∑N\ni=1 ∑N j=1 γijδxi,yj (x, y), where\nγij ≥ 0. Letting pX = p̂X and pY = p̂Y , we can simplify problem (5) as follows\nmin γ N∑ i=1 N∑ j=1 d(xi, yj)γij s.t. N∑ j=1 γij = 1 N , i ∈ [N ]; N∑ i=1 γij = 1 N , j ∈ [N ]; γij ≥ 0. (6)\nThe linear program (6) is equivalent to the minimum weight matching problem [27], i.e., there exists a permutation M1, . . . ,MN , such that γ(xi, yMi) = 1 N\nis an optimal solution (see Proposition 5.4 in [6]). Plugging such γ back to problem (6), we obtain Eq. (1), the original definition of PMD.\nFurthermore, we can show that the solution of problem (6), i.e., the N -PMD, is a strongly consistent estimator of the first Wasserstein metric in problem (5).\nDefinition 1 (Weak Convergence of Measure [48]). A sequence of probability distributions pN , N = 1, 2, ... converges weakly to the probability distribution p, denoted as pn ⇒ p, if limN→∞ EpN [f ] = Ep[f ] for all bounded continuous functions f .\nProposition 3.1 (Varadarajan Theorem [48]). Let x1, ..., xN , ... be independent, identically distributed real random variables with the density function p(x), let pN (x) = 1 N ∑N i=1 δxN (x) where δxN (·) is the Dirac delta function. Then pN ⇒ p almost surely.\nProposition 3.2 (Stability of Optimal Transport [49]). Let X and Y be Polish spaces and let d : X × Y → R be a continuous function s.t. inf d > −∞. Let {pXN}N∈N and {p Y N}N∈N be sequences of probability distributions on X and Y respectively. Assume that pXN ⇒ pX (resp. pYN ⇒ pY ). For each N , let γN be an optimal transference plan between p X N and p Y N . If\nlim infN∈N ∫ d(x, y)γN (x, y)dxdy < +∞, then γN ⇒ γ, where γ is an optimal transference plan between pX and pY .\nProposition 3.2 is a special case of Theorem 5.20 in [49] with fixed function d. The following theorem is the main result of this section.\nTheorem 3.3 (Strong Consistency of PMD). Let x1, ..., xN , ... and y1, ..., yN , ... be independent, identically distributed real random variables from pX and pY , respectively. We construct a sequence of PMD problems (6) between pXN (x) = 1 N ∑N i=1 δxN (x) and p Y N (y) = 1 N ∑N i=1 δyN (y). Let γN be the optimal transference plan of the N -th PMD problem. Then the sequence γN ⇒ γ almost surely, where γ is the optimal transference plan between pX and pY . Moreover, limN→∞ ∫ d(x, y)γN (x, y)dxdy = ∫ d(x, y)γ(x, y)dxdy almost surely.\nThe proof is straightforward by applying Proposition 3.1 and 3.2. We also perform an empirical study of the approximation error with respect to the population size in Fig. 2(a).\nWhile the Wasserstein metric has been widely adopted in various machine learning and data mining tasks [40, 50, 30], it is usually used to measure the similarity between two discrete distributions, e.g., histograms. In contrast, PMD is a stochastic approximation of the Wasserstein metric between two continuous distributions. There is also work on estimating the Wasserstein metric of continuous distributions based on samples [45]. Unlike PMD, which is approximating the primal problem, they approximate the dual. Their approximation is not differentiable with respect to the distribution\n1 2 4 8 16 32 64 128 256 Population size N\n10 1\n100\nRe la\ntiv e\nap pr\nox im\nat io\nn er\nro r\n(a) Relative approximation error w.r.t the population size\n0 1 2 3 4 5 Normalized magnitude of gradient\n0\n50\n100\n150\n200\n250\n300\nFr eq\nue nc\ny\nPMD MMD\n(b) Distribution of normalized gradients\nFigure 2: Some empirical analysis results. The detailed experiment setting is described in Sec. 5.4.\nparameters because the parameters appear in the constraint instead of the objective. Recently, Wasserstein GAN (WGAN) [3] proposes approximating the dual Wasserstein metric by using a neural network “critic” in place of a 1-Lipschitz function. While WGAN has shown excellent performance on generative modeling, it can only compute a relative value of the Wasserstein metric upon to an unknown scale factor depending on the Lipschitz constant of the critic neural network. PMD also differs from WGAN by not requiring a separate critic network with additional parameters. Instead, PMD is parameter free and can be computed in polynomial time."
    }, {
      "heading" : "3.2 Relationship with MMD",
      "text" : "Maximum mean discrepancy (MMD) [17] is a popular method for estimating the distance between two distributions by samples, defined as follows\nDMMD(X,Y) = 1\nN2 N∑ i=1 N∑ j=1 k(xi, xj)− 2 NM N∑ i=1 M∑ j=1 k(xi, yj) + 1 M2 M∑ i=1 M∑ j=1 k(yi, yj),\nwhere k(·, ·) is a kernel, e.g., k(x, y) = exp(−‖x− y‖ 2 /2σ2) is the RBF kernel with bandwidth σ. Both MMD and the Wasserstein metric are integral probability metrics [17], with different function classes. MMD has a closed-form objective, and can be evaluated in O(NMD) if x and y are Ddimensional vectors. In contrast, PMD needs to solve a matching problem, and the time complexity is O(N2D) for computing the distance matrix, O(N3) for exact Hungarian matching, and O(N2) for approximated matching. However, as we argued in Sec. 2.3, the time complexity for computing matching is still acceptable comparing with the cost of training neural networks.\nComparing with MMD, PMD has a number of advantages:\nFewer hyper-parameter PMD do not have the kernel bandwidth σ, which needs tuning.\nStronger gradient Using the RBF kernel, the gradient of MMD w.r.t a particular sample xi is ∇xiDMMD(X,Y) = 1\nN2 ∑ j k(xi, xj) xj−xi σ2 − 2 NM ∑ j k(xi, yj) yj−xi σ2 . When minimizing MMD,\nthe first term is a repulsive term between the samples from pX , and the second term is an attractive term between the samples from pX and pY . The L2 norm of the term between two samples x and y is k(x, y) ‖x−y‖ 2\nσ2 , which is small if ‖x− y‖2 is either too small or too large. As a result, if\na sample xi is an outlier, i.e., it is not close to any samples from pY , all the k(xi, yj) terms are small and xi will not receive strong gradients. On the other hand, if all the samples xi, i ∈ [N ] are close to each other, xj − xi is small, so that repulsive term of the gradient is weak. Both cases slow down the training. In contrast, if d(x, y) = |x − y| is the L1 distance, the gradient of PMD ∇xiDN (X,Y) = 1 N\nsgn(xi − yMi), where sgn(·) is the sign function, is always strong regardless of the closeness between xi and yMi . We compare the distribution of the relative magnitude of the gradient of the parameters contributed by each sample in Fig. 2(b). The PMD gradients have similar magnitude for each sample, while there are many samples have small gradients for MMD.\nSmaller mini-batch size As we see in Sec 2.2, the SGD mini-batch size for PMD can be smaller than the population size; while the mini-batch size for MMD must be equal with the population size. This is because PMD only considers the distance between a sample and its matched sample, while\nMMD considers the distance between all pairs of samples. As the result of smaller mini-batch size, PMD can converge faster than MMD when used as a training objective."
    }, {
      "heading" : "4 Applications",
      "text" : ""
    }, {
      "heading" : "4.1 Domain Adaptation",
      "text" : "Now we consider a scenario where the labeled data is scarce in some domain of interest (target domain) but that is abundant in some related domain (source domain). Assuming that the data distribution pS(X, y) for the source domain and that of the target domain, i.e. pT (X, y) are similar but not the same, unsupervised domain adaptation aims to train a model for the target domain, given some labeled data {(XSi , y S i )} NS i=1 from the source domain and some unlabeled data {X T j } NT j=1 from the target domain. According to the domain adaptation theory [5], the generalization error on the target domain depends on the generalization error on the source domain as well as the difference between the two domains. Therefore, one possible solution for domain adaptation is to learn a feature extractor φ(X) shared by both domains, which defines feature distributions pφS and p φ T for both domains, and minimize some distance between the feature distributions [47] as a regularization. Since the data distribution is inaccessible, we replace all distributions with their empirical distributions p̂S , p̂T , p̂ φ S and p̂ φ T , and the training objective is\nEX,y∼p̂SL(y, h(φ(X))) + λD(p̂ φ S , p̂ φ T ),\nwhereL(·, ·) is a loss function, h(·) is a classifier, λ is a hyper-parameter, and D(p̂φS , p̂ φ T ) is the domain adaptation regularization. While the Wasserstein metric itself of two empirical distribution is tractable, it can be too expensive to compute due to the large size of the dataset. Therefore, we still approximate the distance with (expected) PMD, i.e., D(p̂φS , p̂ φ T ) ≈ EXS∼p̂S ,XT∼p̂TDPMD(φ(X S), φ(XT ))."
    }, {
      "heading" : "4.2 Deep Generative Modeling",
      "text" : "Deep generative models (DGMs) aim at capturing the complex structures of the data by combining hierarchical architectures and probabilistic modelling. They have been proven effective on image generation [38] and semi-supervised learning [23] recently. There are many different DGMs, including tractable auto-regressive models [37], latent variable models [24, 39], and implicit probabilistic models [16, 31]. We focus on learning implicit probabilistic models, which define probability distributions on sample space flexibly without a closed-form. However, as described in Sec. 2.2, we can draw samples X = TXθX (ǫ) efficiently from the models by transforming a random noise ǫ ∼ q(ǫ), where q is a simple distribution (e.g. uniform), to X through a parameterized model (e.g. neural network). The parameters in the models are trained to minimize some distance between the model distribution pX(X) and the empirical data distribution p̂Y (Y ). The distance can be defined based on an parameterized adversary, i.e., another neural network [16, 3], or directly with the samples [31]. We choose the distance to be the first Wasserstein metric, and employ its finite-sample estimator (i.e., the N -PMD defined in Eq. (2)) as training objective directly. Training this model with MMD is known as generative moment matching networks [31, 12]."
    }, {
      "heading" : "5 Experiments",
      "text" : "We now study the empirical performance of PMD and compare it with MMD. In the experiments, PMD always use the L1 distance, and MMD always use the RBF kernel. Our experiment is conducted on a machine with Nvidia Titan X (Pascal) GPU and Intel E5-2683v3 CPU. We implement the models in TensorFlow [1]. The matching algorithms are implemented in C++ with a single thread, and we write a CUDA kernel for computing the all-pair L1 distance within a population. The CUDA program is compiled with nvcc 8.0 and the C++ program is compiled with g++ 4.8.4, while -O3 flag is used for both programs. We use the approximate matching for the generative modeling experiment and exact Hungarian matching for all the other experiments."
    }, {
      "heading" : "5.1 Domain Adaptation",
      "text" : "We compare the performance of PMD and MMD on the standard Office [41] object recognition benchmark for domain adaptation. The dataset contains three domains: amazon, dslr and webcam, and\nthere are 31 classes. Following [52], we use the 4096-dimensional VGG-16 [43] feature pretrained on ImageNet as the input. The classifier is a fully-connected neural network with a single hidden layer of 256 ReLU [15] units, trained with AdaDelta [51]. The domain regularization term is put on the hidden layer. We apply batch normalization [21] on the hidden layer, and the activations from the source and the target domain are normalized separately. Following [8], we validate the domain regularization strength λ and the MMD kernel bandwidth σ on a random 100-sample labeled dataset on the target domain, but the model is trained without any labeled data from the target domain. The experiment is then repeated for 10 times on the hyper-parameters with the best validation error. Since we perform such validation for both PMD and MMD, the comparison between them is fair. The result is reported in Table 1, and PMD outperforms MMD on the a→ w and a→ d tasks by a large margin, and is comparable with MMD on the other 4 tasks.\nThen, we compare the convergence speed of PMD and MMD on the d → a task. We choose this task because PMD and MMD have similar performance on it. The result is shown in Fig. 3(a), where PMD converges faster than MMD. We also show the parameter sensitivity of MMD and PMD as Fig. 3(b) and Fig. 3(c), respectively. The performance of MMD is sensitive to both the regularization parameter λ and the kernel bandwidth σ, so we need to tune both parameters. In contrast, PMD only has one parameter to tune."
    }, {
      "heading" : "5.2 Generative Modeling",
      "text" : "We compare PMD with MMD for image generation on the MNIST [28], SVHN [36] and LFW [20] dataset. For SVHN, we train the models on the 73257-image training set. The LFW dataset is converted to 32 × 32 gray-scale images [2], and there are 13233 images for training. The noise ǫ follows a uniform distribution [−1, 1]40. We implemented three architectures, including a fullyconnected (fc) network as the transformation TXθX , a deconvolutional (conv) network, and a fullyconnected network for generating the auto-encoder codes (ae) [31], where the auto-encoder is a convolutional one pre-trained on the dataset. For MMD, we use a mixture of kernels of different bandwidths for the fc and conv architecture, and the bandwidth is fixed at 1 for the ae architecture, following the settings in the generative moment matching networks (GMMN) paper. We set the population size N = 2000 for both PMD and MMD, and the mini-batch size |B| = 100 for PMD. We use the AdaM optimizer [22] with batch normalization [21], and train the model for 100 epoches for PMD, and 500 epoches for MMD. The generated images on the SVHN and LFW dataset are\n4000 2000 1000 500 250 100 Mini-batch size |B|\n52\n54\n56\n58\n60\nFi na\nl P M\nD\nN=500 N=1000 N=2000 N=4000 Exact N=500\n(a) PMD sensitivity w.r.t. N and |B|\n8000 6000 4000 2000 1000 500 250 100 Population size N\n0.0120\n0.0121\n0.0122\n0.0123\n0.0124\n0.0125\n0.0126\nFi na\nl M M\nD\n(b) sensitivity of MMD w.r.t. N\n500 1000 2000 4000 Population size N\n100\n101\n102\nTi m\ne (s\nec on\nds )\nExact Randomized SGD\n(c) split of the time per epoch\nFigure 5: Convergence and timing results. The “Exact N = 500” curve in (a) uses the Hungarian algorithm, and the rest uses the approximated matching algorithm.\npresented in Fig. 4, and the images on the MNIST dataset can be found in the supplementary material. We observe that the images generated by PMD are less noisy than that generated by MMD. While MMD only performs well on the autoencoder code space (ae), PMD generates acceptable images on pixel space. We also noticed the generated images of PMD on the SVHN and LFW datasets are blurry. One reason for this is the pixel-level L1 distance is not good for natural images. Therefore, learning the generative model on the code space helps. To verify that PMD does not trivially reproduce the training dataset, we perform a circular interpolation in the representation space q(ǫ) between 5 random points, the result is available in the supplementary material."
    }, {
      "heading" : "5.3 Convergence Speed and Time Consumption",
      "text" : "We study the impact of the population size N , the mini-batch size |B| and the choice of matching algorithm to PMD. Fig. 5(a) shows the final PMD evaluated on N = 2000 samples on the MNIST dataset, using the fc architecture, after 100 epoches. The results show that the solution is insensitive to neither the population size N nor the choice of the matching algorithm, which implies that we can use the cheap approximated matching and relatively small population size for speed. On the other hand, decreasing the mini-batch size |B| improves the final PMD significantly, supporting our claim in Sec. 3.2 that the ability of using small |B| is indeed an advantage for PMD. Unlike PMD, there is a trade-off for selecting the population size N for MMD, as shown in Fig. 5(b). If N is too large, the SGD optimization converges slowly; if N is too small, the MMD estimation is unreliable. Fig. 5(c) shows the total time spent on exact matching, approximated matching and SGD respectively for each epoch. The cost of approximated matching is comparable with the cost of SGD. Again, we emphasize while we only have single thread implementations for the matching algorithms, both the exact [10] and approximated matching [34] can be significantly accelerated with GPU."
    }, {
      "heading" : "5.4 Empirical Studies",
      "text" : "We examine the approximation error of PMD on a toy dataset. We compute the distances between two 5-dimensional standard isotropic Gaussian distributions. One distribution is centered at the origin and the other is at (10, 0, 0, 0, 0). The first Wasserstein metric between these two distributions is 10. We vary the population size N and compute the relative approximation error = |DN (X,Y)− 10|/10 for 100 different populations (X,Y) for each N . The result is shown in Fig. 2(a). We perform a\nlinear regression between logN and the logarithm of expected approximation error, and find that the error is roughly proportional to N−0.23.\nWe also validate the claim in Sec. 3.2 on the stronger gradients of PMD than that of MMD. We calculate the magnitude (in L2 norm) of the gradient of the parameters contributed by each sample. The gradients are computed on the converged model, and the model is the same as Sec. 5.3. Because the scale of the gradients depend on the scale of the loss function, we normalize the magnitudes by dividing them with the average magnitude of the gradients. We then show the distribution of normalized magnitudes of gradients in Fig. 2(b). The PMD gradients contributed by each sample are close with each other, while there are many samples contributing small gradients for MMD, which may slow down the fitting of these samples."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We present population matching discrepancy (PMD) for estimating the distance between two probability distributions by samples. PMD is the minimum weight matching between two random populations from the distributions, and we show that PMD is a strongly consistent estimator of the first Wasserstein metric. We also propose a stochastic gradient descent algorithm to learn parameters of the distributions using PMD. Comparing with the popular maximum mean discrepancy (MMD), PMD has no kernel bandwidth hyper-parameter, stronger gradient and smaller mini-batch size for gradient-based optimization. We apply PMD to domain adaptation and generative modeling tasks. Empirical results show that PMD outperforms MMD in terms of performance and convergence speed in both tasks. In the future, we plan to derive finite-sample error bounds for PMD, study its testing power, and accelerate the computation of minimum weight matching with GPU."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported by the National NSF of China (Nos. 61620106010, 61621136008, 61332007), the MIIT Grant of Int. Man. Comp. Stan (No. 2016ZXFB00001), the Youth Top-notch Talent Support Program, Tsinghua Tiangong Institute for Intelligent Computing and the NVIDIA NVAIL Program."
    } ],
    "references" : [ {
      "title" : "Tensorflow: Large-scale machine learning on heterogeneous distributed systems",
      "author" : [ "Martín Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin" ],
      "venue" : "arXiv preprint arXiv:1603.04467,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "The cramer distance as a solution to biased wasserstein gradients",
      "author" : [ "Marc G Bellemare", "Ivo Danihelka", "Will Dabney", "Shakir Mohamed", "Balaji Lakshminarayanan", "Stephan Hoyer", "Rémi Munos" ],
      "venue" : "arXiv preprint arXiv:1705.10743,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2017
    }, {
      "title" : "A theory of learning from different domains",
      "author" : [ "Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Network optimization: continuous and discrete models",
      "author" : [ "Dimitri P Bertsekas" ],
      "venue" : "Citeseer,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Large-scale machine learning with stochastic gradient descent",
      "author" : [ "Léon Bottou" ],
      "venue" : "In Proceedings of COMP- STAT’2010,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Domain separation networks",
      "author" : [ "Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Gpu-accelerated hungarian algorithms for the linear assignment problem",
      "author" : [ "Ketan Date", "Rakesh Nagi" ],
      "venue" : "Parallel Computing,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Improved linear time approximation algorithms for weighted matchings. Approximation, Randomization, and Combinatorial Optimization",
      "author" : [ "Doratha Drake", "Stefan Hougardy" ],
      "venue" : "Algorithms and Techniques,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "Training generative neural networks via maximum mean discrepancy optimization",
      "author" : [ "Gintare Karolina Dziugaite", "Daniel M Roy", "Zoubin Ghahramani" ],
      "venue" : "In UAI,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Domain-adversarial training of neural networks",
      "author" : [ "Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "François Laviolette", "Mario Marchand", "Victor Lempitsky" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Daml: Domain adaptation metric learning",
      "author" : [ "Bo Geng", "Dacheng Tao", "Chao Xu" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Schölkopf", "Alexander Smola" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Optimal kernel choice for large-scale two-sample tests",
      "author" : [ "Arthur Gretton", "Dino Sejdinovic", "Heiko Strathmann", "Sivaraman Balakrishnan", "Massimiliano Pontil", "Kenji Fukumizu", "Bharath K Sriperumbudur" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Labeled faces in the wild: A database for studying face recognition in unconstrained environments",
      "author" : [ "Gary B. Huang", "Manu Ramesh", "Tamara Berg", "Erik Learned-Miller" ],
      "venue" : "Technical Report 07-49,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In ICML,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Semi-supervised learning with deep generative models",
      "author" : [ "Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Auto-encoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "The hungarian method for the assignment problem",
      "author" : [ "Harold W Kuhn" ],
      "venue" : "Naval research logistics quarterly,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1955
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1998
    }, {
      "title" : "Mmd gan: Towards deeper understanding of moment matching network",
      "author" : [ "Chun-Liang Li", "Wei-Cheng Chang", "Yu Cheng", "Yiming Yang", "Barnabás Póczos" ],
      "venue" : "arXiv preprint arXiv:1705.08584,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2017
    }, {
      "title" : "t-closeness: Privacy beyond k-anonymity and l-diversity",
      "author" : [ "Ninghui Li", "Tiancheng Li", "Suresh Venkatasubramanian" ],
      "venue" : "In Data Engineering,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2007
    }, {
      "title" : "Generative moment matching networks",
      "author" : [ "Yujia Li", "Kevin Swersky", "Richard S Zemel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "Statistical model criticism using kernel two sample tests",
      "author" : [ "James R Lloyd", "Zoubin Ghahramani" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2015
    }, {
      "title" : "Deep transfer learning with joint adaptation networks",
      "author" : [ "Mingsheng Long", "Jianmin Wang", "Michael I Jordan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2017
    }, {
      "title" : "A parallel approximation algorithm for the weighted maximum matching problem",
      "author" : [ "Fredrik Manne", "Rob Bisseling" ],
      "venue" : "Parallel Processing and Applied Mathematics,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2008
    }, {
      "title" : "Learning in implicit generative models",
      "author" : [ "Shakir Mohamed", "Balaji Lakshminarayanan" ],
      "venue" : "arXiv preprint arXiv:1610.03483,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2016
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng" ],
      "venue" : "In NIPS workshop on deep learning and unsupervised feature learning,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2011
    }, {
      "title" : "Pixel recurrent neural networks",
      "author" : [ "Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu" ],
      "venue" : "In ICML,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "author" : [ "Alec Radford", "Luke Metz", "Soumith Chintala" ],
      "venue" : "arXiv preprint arXiv:1511.06434,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2015
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra" ],
      "venue" : "In ICML,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2014
    }, {
      "title" : "The earth mover’s distance as a metric for image retrieval",
      "author" : [ "Yossi Rubner", "Carlo Tomasi", "Leonidas J Guibas" ],
      "venue" : "International journal of computer vision,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2000
    }, {
      "title" : "Adapting visual category models to new domains",
      "author" : [ "Kate Saenko", "Brian Kulis", "Mario Fritz", "Trevor Darrell" ],
      "venue" : "Computer Vision–ECCV",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2010
    }, {
      "title" : "Improved techniques for training gans",
      "author" : [ "Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2016
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2015
    }, {
      "title" : "Kernel belief propagation",
      "author" : [ "Le Song", "Arthur Gretton", "Danny Bickson", "Yucheng Low", "Carlos Guestrin" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2011
    }, {
      "title" : "Non-parametric estimation of integral probability metrics",
      "author" : [ "Bharath K Sriperumbudur", "Kenji Fukumizu", "Arthur Gretton", "Bernhard Schölkopf", "Gert RG Lanckriet" ],
      "venue" : "In Information Theory Proceedings (ISIT),",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2010
    }, {
      "title" : "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "Tijmen Tieleman", "Geoffrey Hinton" ],
      "venue" : "COURSERA: Neural networks for machine learning,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2012
    }, {
      "title" : "Deep domain confusion: Maximizing for domain invariance",
      "author" : [ "Eric Tzeng", "Judy Hoffman", "Ning Zhang", "Kate Saenko", "Trevor Darrell" ],
      "venue" : "arXiv preprint arXiv:1412.3474,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2014
    }, {
      "title" : "Weak convergence of measures on separable metric spaces",
      "author" : [ "VS Varadarajan" ],
      "venue" : "Sankhyā: The Indian Journal of Statistics (1933-1960),",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 1958
    }, {
      "title" : "Optimal transport: old and new, volume 338",
      "author" : [ "Cédric Villani" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2008
    }, {
      "title" : "A novel document similarity measure based on earth mover’s distance",
      "author" : [ "Xiaojun Wan" ],
      "venue" : "Information Sciences,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2007
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler" ],
      "venue" : "arXiv preprint arXiv:1212.5701,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2012
    }, {
      "title" : "Central moment discrepancy (cmd) for domain-invariant representation learning",
      "author" : [ "Werner Zellinger", "Thomas Grubinger", "Edwin Lughofer", "Thomas Natschläger", "Susanne Saminger-Platz" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Recent advances on image classification [26], speech recognition [19] and machine translation [9] suggest that properly building large models with a deep hierarchy can be effective to solve realistic learning problems.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "Recent advances on image classification [26], speech recognition [19] and machine translation [9] suggest that properly building large models with a deep hierarchy can be effective to solve realistic learning problems.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 6,
      "context" : "Recent advances on image classification [26], speech recognition [19] and machine translation [9] suggest that properly building large models with a deep hierarchy can be effective to solve realistic learning problems.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "Many deep learning tasks, such as generative modeling [16, 3], domain adaptation [5, 47], model criticism [32] and metric learning [14], require estimating the statistical divergence of two probability distributions.",
      "startOffset" : 54,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "Many deep learning tasks, such as generative modeling [16, 3], domain adaptation [5, 47], model criticism [32] and metric learning [14], require estimating the statistical divergence of two probability distributions.",
      "startOffset" : 81,
      "endOffset" : 88
    }, {
      "referenceID" : 44,
      "context" : "Many deep learning tasks, such as generative modeling [16, 3], domain adaptation [5, 47], model criticism [32] and metric learning [14], require estimating the statistical divergence of two probability distributions.",
      "startOffset" : 81,
      "endOffset" : 88
    }, {
      "referenceID" : 29,
      "context" : "Many deep learning tasks, such as generative modeling [16, 3], domain adaptation [5, 47], model criticism [32] and metric learning [14], require estimating the statistical divergence of two probability distributions.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "Many deep learning tasks, such as generative modeling [16, 3], domain adaptation [5, 47], model criticism [32] and metric learning [14], require estimating the statistical divergence of two probability distributions.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 32,
      "context" : "Without making explicit assumption on the parametric form, these distributions are richer and hence can lead to better estimates [35].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "One popular sample-based statistical divergence is the maximum mean discrepancy (MMD) [17], which compares the kernel mean embedding of two distributions in RKHS.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 41,
      "context" : "Due to its simplicity and theoretical guarantees, MMD have been widely adopted in many tasks such as belief propagation [44], domain adaptation [47] and generative modeling [31].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 44,
      "context" : "Due to its simplicity and theoretical guarantees, MMD have been widely adopted in many tasks such as belief propagation [44], domain adaptation [47] and generative modeling [31].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 28,
      "context" : "Due to its simplicity and theoretical guarantees, MMD have been widely adopted in many tasks such as belief propagation [44], domain adaptation [47] and generative modeling [31].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 15,
      "context" : "For instance, it has a kernel bandwidth parameter that needs tuning [18], and the kernel can saturate so that the gradient vanishes [3] in a deep generative model.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 28,
      "context" : ", 1000, which slows down the training by stochastic gradient descent [31].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 46,
      "context" : "In this paper, we consider a sample-based estimation of the Wasserstein metric [49], which we refer to as population matching discrepancy (PMD).",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 24,
      "context" : "The optimal M corresponds to the bipartite minimum weight matching [27], where each element of the cost matrix is dij = d(xi, yj) with i, j ∈ [N ], where [N ] = {1, · · · , N}.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 46,
      "context" : "1, PMD is a strongly consistent estimator of the first Wasserstein metric [49] between pX and pY , which is a valid statistical distance, i.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 28,
      "context" : "For instance, in generative modeling [31], we have a parameterized generator distribution pX(x; θX) and a data distribution pY (y), and we wish to minimize the distance of these two distributions.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 4,
      "context" : "The parameters can be optimized by stochastic gradient descent (SGD) [7].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 28,
      "context" : "To clarify, our population size N is known as the mini-batch size in some maximum mean discrepancy (MMD) literature [31], and is around 1000.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 24,
      "context" : "3 Solving the Matching Problem The minimum weight matching can be solved exactly in O(N(3)) by the Hungarian algorithm [27].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "We resort to Drake and Hougardy’s approximated matching algorithm [11] in O(N(2)) time.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "Finally, while we only implement the serial CPU version of the matching algorithms, both algorithm can be parallelized on GPU to further improve the running speed [10, 34].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 31,
      "context" : "Finally, while we only implement the serial CPU version of the matching algorithms, both algorithm can be parallelized on GPU to further improve the running speed [10, 34].",
      "startOffset" : 163,
      "endOffset" : 171
    }, {
      "referenceID" : 46,
      "context" : "The Wasserstein metric [49] was initially studied in the optimal transport theory, and has been adopted in computer vision [40], information retrival [50] and differential privacy [30].",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 37,
      "context" : "The Wasserstein metric [49] was initially studied in the optimal transport theory, and has been adopted in computer vision [40], information retrival [50] and differential privacy [30].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 47,
      "context" : "The Wasserstein metric [49] was initially studied in the optimal transport theory, and has been adopted in computer vision [40], information retrival [50] and differential privacy [30].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 27,
      "context" : "The Wasserstein metric [49] was initially studied in the optimal transport theory, and has been adopted in computer vision [40], information retrival [50] and differential privacy [30].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 24,
      "context" : "The linear program (6) is equivalent to the minimum weight matching problem [27], i.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 45,
      "context" : "Definition 1 (Weak Convergence of Measure [48]).",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 46,
      "context" : "2 (Stability of Optimal Transport [49]).",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 37,
      "context" : "While the Wasserstein metric has been widely adopted in various machine learning and data mining tasks [40, 50, 30], it is usually used to measure the similarity between two discrete distributions, e.",
      "startOffset" : 103,
      "endOffset" : 115
    }, {
      "referenceID" : 47,
      "context" : "While the Wasserstein metric has been widely adopted in various machine learning and data mining tasks [40, 50, 30], it is usually used to measure the similarity between two discrete distributions, e.",
      "startOffset" : 103,
      "endOffset" : 115
    }, {
      "referenceID" : 27,
      "context" : "While the Wasserstein metric has been widely adopted in various machine learning and data mining tasks [40, 50, 30], it is usually used to measure the similarity between two discrete distributions, e.",
      "startOffset" : 103,
      "endOffset" : 115
    }, {
      "referenceID" : 42,
      "context" : "There is also work on estimating the Wasserstein metric of continuous distributions based on samples [45].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "Maximum mean discrepancy (MMD) [17] is a popular method for estimating the distance between two distributions by samples, defined as follows",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 14,
      "context" : "Both MMD and the Wasserstein metric are integral probability metrics [17], with different function classes.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "According to the domain adaptation theory [5], the generalization error on the target domain depends on the generalization error on the source domain as well as the difference between the two domains.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 44,
      "context" : "Therefore, one possible solution for domain adaptation is to learn a feature extractor φ(X) shared by both domains, which defines feature distributions pφS and p φ T for both domains, and minimize some distance between the feature distributions [47] as a regularization.",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 35,
      "context" : "They have been proven effective on image generation [38] and semi-supervised learning [23] recently.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 20,
      "context" : "They have been proven effective on image generation [38] and semi-supervised learning [23] recently.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 34,
      "context" : "There are many different DGMs, including tractable auto-regressive models [37], latent variable models [24, 39], and implicit probabilistic models [16, 31].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 21,
      "context" : "There are many different DGMs, including tractable auto-regressive models [37], latent variable models [24, 39], and implicit probabilistic models [16, 31].",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 36,
      "context" : "There are many different DGMs, including tractable auto-regressive models [37], latent variable models [24, 39], and implicit probabilistic models [16, 31].",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "There are many different DGMs, including tractable auto-regressive models [37], latent variable models [24, 39], and implicit probabilistic models [16, 31].",
      "startOffset" : 147,
      "endOffset" : 155
    }, {
      "referenceID" : 28,
      "context" : "There are many different DGMs, including tractable auto-regressive models [37], latent variable models [24, 39], and implicit probabilistic models [16, 31].",
      "startOffset" : 147,
      "endOffset" : 155
    }, {
      "referenceID" : 13,
      "context" : ", another neural network [16, 3], or directly with the samples [31].",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 28,
      "context" : ", another neural network [16, 3], or directly with the samples [31].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 28,
      "context" : "Training this model with MMD is known as generative moment matching networks [31, 12].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "Training this model with MMD is known as generative moment matching networks [31, 12].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "We implement the models in TensorFlow [1].",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 38,
      "context" : "We compare the performance of PMD and MMD on the standard Office [41] object recognition benchmark for domain adaptation.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 49,
      "context" : "Following [52], we use the 4096-dimensional VGG-16 [43] feature pretrained on ImageNet as the input.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 40,
      "context" : "Following [52], we use the 4096-dimensional VGG-16 [43] feature pretrained on ImageNet as the input.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "The classifier is a fully-connected neural network with a single hidden layer of 256 ReLU [15] units, trained with AdaDelta [51].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 48,
      "context" : "The classifier is a fully-connected neural network with a single hidden layer of 256 ReLU [15] units, trained with AdaDelta [51].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 18,
      "context" : "We apply batch normalization [21] on the hidden layer, and the activations from the source and the target domain are normalized separately.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "Following [8], we validate the domain regularization strength λ and the MMD kernel bandwidth σ on a random 100-sample labeled dataset on the target domain, but the model is trained without any labeled data from the target domain.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 25,
      "context" : "We compare PMD with MMD for image generation on the MNIST [28], SVHN [36] and LFW [20] dataset.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 33,
      "context" : "We compare PMD with MMD for image generation on the MNIST [28], SVHN [36] and LFW [20] dataset.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "We compare PMD with MMD for image generation on the MNIST [28], SVHN [36] and LFW [20] dataset.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 28,
      "context" : "We implemented three architectures, including a fullyconnected (fc) network as the transformation T θX , a deconvolutional (conv) network, and a fullyconnected network for generating the auto-encoder codes (ae) [31], where the auto-encoder is a convolutional one pre-trained on the dataset.",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 19,
      "context" : "We use the AdaM optimizer [22] with batch normalization [21], and train the model for 100 epoches for PMD, and 500 epoches for MMD.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "We use the AdaM optimizer [22] with batch normalization [21], and train the model for 100 epoches for PMD, and 500 epoches for MMD.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "Again, we emphasize while we only have single thread implementations for the matching algorithms, both the exact [10] and approximated matching [34] can be significantly accelerated with GPU.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 31,
      "context" : "Again, we emphasize while we only have single thread implementations for the matching algorithms, both the exact [10] and approximated matching [34] can be significantly accelerated with GPU.",
      "startOffset" : 144,
      "endOffset" : 148
    } ],
    "year" : 2017,
    "abstractText" : "A differentiable estimation of the distance between two distributions based on samples is important for many deep learning tasks. One such estimation is maximum mean discrepancy (MMD). However, MMD suffers from its sensitive kernel bandwidth hyper-parameter, weak gradients, and large mini-batch size when used as a training objective. In this paper, we propose population matching discrepancy (PMD) for estimating the distribution distance based on samples, as well as an algorithm to learn the parameters of the distributions using PMD as an objective. PMD is defined as the minimum weight matching of sample populations from each distribution, and we prove that PMD is a strongly consistent estimator of the first Wasserstein metric. We apply PMD to two deep learning tasks, domain adaptation and generative modeling. Empirical results demonstrate that PMD overcomes the aforementioned drawbacks of MMD, and outperforms MMD on both tasks in terms of the performance as well as the convergence speed.",
    "creator" : null
  }
}