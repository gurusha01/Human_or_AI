{
  "name" : "bdc4626aa1d1df8e14d80d345b2a442d.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Conservative Contextual Linear Bandits",
    "authors" : [ "Abbas Kazerouni", "Mohammad Ghavamzadeh", "Benjamin Van Roy" ],
    "emails" : [ "abbask@stanford.edu", "ghavamza@google.com", "abbasiya@adobe.com", "bvr@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Many problems in science and engineering can be formulated as decision-making problems under uncertainty. Although many learning algorithms have been developed to find a good policy/strategy for these problems, most of them do not provide any guarantee for the performance of their resulting policy during the initial exploratory phase. This is a major obstacle in using learning algorithms in many different fields, such as online marketing, health sciences, finance, and robotics. Therefore, developing learning algorithms with safety guarantees can immensely increase the applicability of learning in solving decision problems. A policy generated by a learning algorithm is considered to be safe, if it is guaranteed to perform at least as well as a baseline. The baseline can be either a baseline value or the performance of a baseline strategy. It is important to note that since the policy is learned from data, it is a random variable, and thus, the safety guarantees are in high probability.\nSafety can be studied in both offline and online scenarios. In the offline case, the algorithm learns the policy from a batch of data, usually generated by the current strategy or recent strategies of the company, and the question is whether the learned policy will perform as well as the current strategy or no worse than a baseline value, when it is deployed. This scenario has been recently studied heavily in both model-based (e.g., Petrik et al. [2016]) and model-free (e.g., Bottou et al. 2013; Thomas et al. 2015a,b; Swaminathan and Joachims 2015a,b) settings. In the model-based approach, we first use the batch of data and build a simulator that mimics the behavior of the dynamical system under study (hospital’s ER, financial market, robot), and then use this simulator to generate data and learn the policy. The main challenge here is to have guarantees on the performance of the learned policy, given the error in the simulator. This line of research is closely related to the area of robust learning and control. In the model-free approach, we learn the policy directly from the batch of data, without building a simulator. This line of research is related to off-policy evaluation and control. While the\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nmodel-free approach is more suitable for problems in which we have access to a large batch of data, such as in online marketing, the model-based approach works better in problems in which data is harder to collect, but instead, we have good knowledge about the underlying dynamical system that allows us to build an accurate simulator.\nIn the online scenario, the algorithm learns a policy while interacting with the real system. Although (reasonable) online algorithms will eventually learn a good or an optimal policy, there is no guarantee for their performance along the way (the performance of their intermediate policies), especially at the very beginning, when they perform a large amount of exploration. Thus, in order to guarantee safety in online algorithms, it is important to control their exploration and make it more conservative. Consider a manager that allows our learning algorithm runs together with her company’s current strategy (baseline policy), as long as it is safe, i.e., the loss incurred by letting a portion of the traffic handled by our algorithm (instead of by the baseline policy) does not exceed a certain threshold. Although we are confident that our algorithm will eventually perform at least as well as the baseline strategy, it should be able to remain alive (not terminated by the manager) long enough for this to happen. Therefore, we should make it more conservative (less exploratory) in a way not to violate the manager’s safety constraint. This setting has been studied in the multi-armed bandit (MAB) [Wu et al., 2016]. Wu et al. [2016] considered the baseline policy as a fixed arm in MAB, formulated safety using a constraint defined based on the performance of the baseline policy (mean of the baseline arm), and modified the UCB algorithm [Auer et al., 2002] to satisfy this constraint.\nIn this paper, we study the notion of safety in contextual linear bandits, a setting that has application in many different fields including personalized recommendation. We first formulate safety in this setting, as a constraint that must hold uniformly in time, in Section 2. Our goal is to design learning algorithms that minimize regret under the constraint that at any given time, their expected sum of rewards should be above a fixed percentage of the expected sum of rewards of the baseline policy. This fixed percentage depends on the amount of risk that the manager is willing to take. In Section 3, we propose an algorithm, called conservative linear UCB (CLUCB), that satisfies the safety constraint. At each round, CLUCB plays the action suggested by the standard linear UCB (LUCB) algorithm (e.g., Dani et al. 2008; Rusmevichientong and Tsitsiklis 2010; Abbasi-Yadkori et al. 2011; Chu et al. 2011; Russo and Van Roy 2014), only if it satisfies the safety constraint for the worst choice of the parameter in the confidence set, and plays the action suggested by the baseline policy, otherwise. We prove an upper-bound for the regret of CLUCB, which can be decomposed into two terms. The first term is an upper-bound on the regret of LUCB that grows at the rate √ T log(T ). The second term is constant (does not grow with the horizon T ) and accounts for the loss of being conservative in order to satisfy the safety constraint. This improves over the regret bound derived in Wu et al. [2016] for the MAB setting, where the regret of being conservative grows with time. In Section 4, we show how CLUCB can be extended to the case that the reward of the baseline policy is unknown without a change in its rate of regret. Finally in Section 5, we report experimental results that show CLUCB behaves as expected in practice and validate our theoretical analysis."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "In this section, we first review the standard linear bandit setting and then introduce the conservative linear bandit formulation considered in this paper."
    }, {
      "heading" : "2.1 Linear Bandit",
      "text" : "In the linear bandit setting, at any time t, the agent is given a set of (possibly) infinitely many actions/options At, where each action a ∈ At is associated with a feature vector φta ∈ Rd. At each round t, the agent selects an action at ∈ At and observes a random reward yt generated as\nyt = 〈θ∗, φtat〉+ ηt, (1)\nwhere θ∗ ∈ Rd is the unknown reward parameter, 〈θ∗, φtat〉 = r t at is the expected reward of action at at time t, i.e., rtat = E[yt], and ηt is a random noise such that\nAssumption 1 Each element ηt of the noise sequence {ηt}∞t=1 is conditionally σ-sub-Gaussian, i.e., E[eζηt | a1:t, η1:t−1] ≤ exp(ζ2σ2/2), ∀ζ ∈ R.\nThe sub-Gaussian assumption implies that E[ηt | a1:t, η1:t−1] = 0 and Var[ηt | a1:t, η1:t−1] ≤ σ2.\nNote that the above formulation contains time-varying action sets and time-dependent feature vectors for each action, and thus, includes the linear contextual bandit setting. In linear contextual bandit, if we denote by xt, the state of the system at time t, the time-dependent feature vector φta for action a will be equal to φ(xt, a), the feature vector of state-action pair (xt, a).\nWe also make the following standard assumption on the unknown parameter θ∗ and feature vectors:\nAssumption 2 There exist constants B,D ≥ 0 such that ‖θ∗‖2 ≤ B, ‖φta‖2 ≤ D, and 〈θ∗, φta〉 ∈ [0, 1], for all t and all a ∈ At.\nWe define B = { θ ∈ Rd : ‖θ‖2 ≤ B } and F = { φ ∈ Rd : ‖φ‖2 ≤ D, 〈θ∗, φ〉 ∈ [0, 1] } to be the parameter space and feature space, respectively.\nObviously, if the agent knows θ∗, she will choose the optimal action a∗t = arg maxa∈At〈θ∗, φta〉 at each round t. Since θ∗ is unknown, the agent’s goal is to maximize her cumulative expected rewards after T rounds, i.e., ∑T t=1〈θ∗, φtat〉, or equivalently, to minimize its (pseudo)-regret, i.e.,\nRT = T∑ t=1 〈θ∗, φta∗t 〉 − T∑ t=1 〈θ∗, φtat〉, (2)\nwhich is the difference between the cumulative expected rewards of the optimal and agent’s strategies."
    }, {
      "heading" : "2.2 Conservative Linear Bandit",
      "text" : "The conservative linear bandit setting is exactly the same as the linear bandit, except that there exists a baseline policy πb (e.g., the company’s current strategy) that at each round t, selects action bt ∈ At and incurs the expected reward rtbt = 〈θ\n∗, φtbt〉. We assume that the expected rewards of the actions taken by the baseline policy, rtbt , are known (see Remark 1). We relax this assumption in Section 4 and extend our proposed algorithm to the case that the reward function of the baseline policy is not known in advance. Another difference between the conservative and standard linear bandit settings is the performance constraint, which is defined as follows:\nDefinition 1 (Performance Constraint) At each round t, the difference between the performances of the baseline and the agent’s policies should remain below a pre-defined fraction α ∈ (0, 1) of the baseline performance. This constraint may be written formally as\n∀t ∈ {1, . . . , T}, t∑ i=1 ribi− t∑ i=1 riai ≤ α t∑ i=1 ribi or equivalently as t∑ i=1 riai ≥ (1−α) t∑ i=1 ribi . (3) The parameter α controls the level of conservatism of the agent. Small values show that only small losses are tolerated and the agent should be overly conservative, whereas large values indicate that the manager is willing to take risk and the agent can be more explorative. Here, given the value of α, the agent should select her actions in a way to both minimize her regret (2) and to satisfy the performance constraint (3). In the next section, we propose a linear bandit algorithm to achieve this goal with high probability.\nRemark 1. Since the baseline policy is often our company’s strategy, it is reasonable to assume that a large amount of data generated by this policy is available, and thus, we have an accurate estimate of its reward function. If in addition to this accurate estimate, we have access to the actual data, we can use them in our algorithms. The reason we do not use the data generated by the actions suggested by the baseline policy in constructing the confidence sets of our algorithm in Section 3 is mainly to keep the analysis simple. However, when dealing with the more general case of unknown baseline reward in Section 4, we construct the confidence sets using all available data, including those generated by the baseline policy. It is important to note that having a good estimate of the baseline reward function does not necessarily mean that we know the unknown parameter θ∗. This is because the data used for this estimate has been generated by the baseline policy, and thus, may only provide a good estimate of θ∗ in a limited subspace."
    }, {
      "heading" : "3 A Conservative Linear Bandit Algorithm",
      "text" : "In this section, we propose a linear bandit algorithm, called conservative linear upper confidence bound (CLUCB), whose pseudocode is shown in Algorithm 1. CLUCB is based on the optimism in the face of uncertainty principle, and given the value of α, minimizes the regret (2) and satisfies the performance constraint (3) with high probability. At each round t, CLUCB uses the previous\nAlgorithm 1 CLUCB Input: α,B,F Initialize: S0 = ∅, z0 = 0 ∈ Rd, and C1 = B for t = 1, 2, 3, · · · do\nFind (a′t, θ̃t) ∈ arg max(a,θ)∈At×Ct 〈θ, φta〉 Compute Lt = minθ∈Ct 〈θ, zt−1 + φta′t〉 if Lt + ∑ i∈Sct−1 ribi ≥ (1− α) ∑t i=1 r i bi\nthen Play at = a′t and observe reward yt defined by (1) Set zt = zt−1 + φtat , St = St−1 ∪ t, S c t = S c t−1\nGiven at and yt, construct the confidence set Ct+1 according to (5) else\nPlay at = bt and observe reward yt defined by (1) Set zt = zt−1, St = St−1, Sct = S c t−1 ∪ t, Ct+1 = Ct\nend if end for\nobservations and builds a confidence set Ct that with high probability contains the unknown parameter θ∗. It then selects the optimistic action a′t ∈ arg maxa∈At maxθ∈Ct〈θ, φta〉, which has the best performance among all the actions available in At, within the confidence set Ct. In order to make sure that the constraint (3) is satisfied, the algorithm plays the optimistic action a′t, only if it satisfies the constraint for the worst choice of the parameter θ ∈ Ct. To make this more precise, let St−1 be the set of rounds i < t at which CLUCB has played the optimistic action, i.e., ai = a′i. Similarly, Sct−1 = {1, 2, · · · , t − 1} − St−1 is the set of rounds j < t at which CLUCB has followed the baseline policy, i.e., aj = bj .\nIn order to guarantee that it does not violate constraint (3), at each round t, CLUCB plays the optimistic action, i.e., at = a′t, only if\nmin θ∈Ct [ ∑ i∈Sct−1 ribi + 〈 θ, zt−1︷ ︸︸ ︷∑ i∈St−1 φiai 〉 + 〈θ, φta′t〉 ] ≥ (1− α) t∑ i=1 ribi ,\nand plays the conservative action, i.e., at = bt, otherwise. In the following, we describe how CLUCB constructs and updates its confidence sets Ct."
    }, {
      "heading" : "3.1 Construction of Confidence Sets",
      "text" : "CLUCB starts by the most general confidence set C1 = B and updates its confidence set only when it plays an optimistic action. This is mainly to simplify the analysis and is based on the idea that since the reward function of the baseline policy is known ahead of time, playing a baseline action does not provide any new information about the unknown parameter θ∗. However, this can be easily changed to update the confidence set after each action. In fact, this is what we do in the algorithm proposed in Section 4. We follow the approach of Abbasi-Yadkori et al. [2011] to build confidence sets for θ∗. Let St = {i1, . . . , imt} be the set of rounds up to and including round t at which CLUCB has played the optimistic action. Note that we have defined mt = |St|. For a fixed value of λ > 0, let\nθ̂t = (ΦtΦ ᵀ t + λI) −1 ΦtYt, (4)\nbe the regularized least square estimate of θ at round t, where Φt = [φi1ai1 , . . . , φ imt aimt\n] and Yt = [yi1 , . . . , yimt ]\n>. For a fixed confidence parameter δ ∈ (0, 1), we construct the confidence set for the next round t+ 1 as Ct+1 = { θ ∈ Rd : ‖θ − θ̂t‖Vt ≤ βt+1 } , (5)\nwhere βt+1 = σ √ d log ( 1+(mt+1)D2/λ\nδ\n) + √ λB, Vt = λI + ΦtΦ>t , and the weighted norm is defined\nas ‖x‖V = √ x>V x for any x ∈ Rd and any positive definite V ∈ Rd×d. Note that similar to the linear UCB algorithm (LUCB) in Abbasi-Yadkori et al. [2011], the sub-Gaussian parameter σ and the regularization parameter λ that appear in the definitions of βt+1 and Vt should also be given to the CLUCB algorithm as input. The following proposition (Theorem 2 in Abbasi-Yadkori et al. 2011) shows that the confidence sets constructed by (5) contain the true parameter θ∗ with high probability.\nProposition 1 For the confidence set Ct defined by (5), we have P [ θ∗ ∈ Ct, ∀t ∈ N ] ≥ 1− δ.\nAs mentioned before, CLUCB ensures that performance constraint (3) holds for all θ ∈ Ct at all rounds t. As a result, if all the confidence sets hold (i.e., contain the true parameter θ∗), CLUCB is guaranteed to satisfy performance constraint (3). Proposition 1 indicates that this happens with probability at least 1− δ. It is worth noting that satisfying constraint (3) implies that CLUCB is at least as good as the baseline policy at all rounds. In this vein, Proposition 1 guarantees that, with probability at least 1− δ, CLUCB performs no worse than the baseline policy at all rounds."
    }, {
      "heading" : "3.2 Regret Analysis of CLUCB",
      "text" : "In this section, we prove a regret bound for the proposed CLUCB algorithm. Let ∆tbt = r t a∗t − rtbt be the baseline gap at round t, i.e., the difference between the expected rewards of the optimal and baseline actions at round t. This quantity shows how sub-optimal the action suggested by the baseline policy is at round t. We make the following assumption on the performance of the baseline policy πb.\nAssumption 3 There exist 0 ≤ ∆l ≤ ∆h and 0 < rl such that, at each round t,\n∆l ≤ ∆tbt ≤ ∆h and rl ≤ r t bt . (6)\nAn obvious candidate for both ∆h and rh is 1, as all the mean rewards are confined in [0, 1]. The reward lower-bound rl ensures that the baseline policy maintains a minimum level of performance at each round. Finally, ∆l = 0 is a reasonable candidate for the lower-bound of the baseline gap.\nThe following proposition shows that the regret of CLUCB can be decomposed into the regret of a linear UCB (LUCB) algorithm (e.g., Abbasi-Yadkori et al. 2011) and a regret caused by being conservative in order to satisfy the performance constraint (3).\nProposition 2 The regret of CLUCB can be decomposed into two terms as follows:\nRT (CLUCB) ≤ RST (LUCB) + nT∆h, (7)\nwhere RST (LUCB) is the cumulative (pseudo)-regret of LUCB at rounds t ∈ ST and nT = |ScT | = T −mT is the number of rounds (in T rounds) at which CLUCB has played a conservative action.\nProof: From the definition of regret (2), we have\nRT (CLUCB) = T∑ t=1 rta∗t − T∑ t=1 rtat = ∑ t∈ST (rta∗t −r t at)+ ∑ t∈Sc\nT\n∆tbt︷ ︸︸ ︷ (rta∗t − r t bt) ≤ ∑ t∈ST (rta∗t −r t at)+nT∆h. (8)\nThe result follows from the fact that for t ∈ ST , CLUCB plays the exact same actions as LUCB, and thus, the first term in (8) represents LUCB’s regret for these rounds.\nThe regret bound of LUCB for the confidence set (5) can be derived from the results of AbbasiYadkori et al. [2011]. Let E be the event that θ∗ ∈ Ct, ∀t ∈ N, which according to Proposition 1 holds w.p. at least 1− δ. The following proposition provides a bound on RST (LUCB). Since this proposition is a direct application of Thm. 3 in Abbasi-Yadkori et al. [2011], we omit its proof here.\nProposition 3 On event E = {θ∗ ∈ Ct, ∀t ∈ N}, for any T ∈ N, we have\nRST (LUCB) ≤ 4 √ mT d log ( λ+ mTD\nd\n) × [ B √ λ+ σ √ 2 log( 1\nδ ) + d log\n( 1 + mTD\nλd )] = O ( d log ( D\nλδ T\n)√ T ) . (9)\nNow in order to bound the regret of CLUCB, we only need to find an upper-bound on nT , i.e., the number of times that CLUCB deviates from LUCB and selects the action suggested by the baseline policy. We prove an upper-bound on nT in Theorem 4, which is the main technical result of this section. Due to space constraint, we only provide a proof sketch for Theorem 4 in the paper and report its detailed proof in Appendix A. The proof requires several technical lemmas that have been proved in Appendix C.\nTheorem 4 Let λ ≥ max(1, D2). Then, on event E , for any horizon T ∈ N, we have\nnT ≤ 1 + 114d2 (B √ λ+ σ)2\nαrl(∆l + αrl)\n[ log ( 62d(B √ λ+ σ)√\nδ(∆l + αrl)\n)]2 .\nProof Sketch: Let τ = max { 1 ≤ t ≤ T | at 6= a′t }\nbe the last round that CLUCB takes an action suggested by the baseline policy. We first show that at round τ , the following holds:\nα τ∑ t=1 rtbt ≤ −(mτ−1 + 1)∆l + 2βτ ∥∥φτa′τ ∥∥V−1τ + 2 ∑\nt∈Sτ−1\nβt ∥∥φtat∥∥V−1t + 2βτ ∥∥∥∥∥∥φτa′τ + ∑\nt∈Sτ−1\nφtat ∥∥∥∥∥∥ V−1τ .\nNext, using Lemmas 7 and 8 (reported in Appendix C), and the Cauchy-Schwartz inequality, we deduce that\nα τ∑ t=1 rtbt ≤ −(mτ−1 + 1)∆l + 8d(B √ λ+ σ) log ( 2(mτ−1 + 1) δ )√ (mτ−1 + 1).\nSince rtbt ≥ rl for all t, and τ = nτ−1 +mτ−1 + 1, it follows that\nαrlnτ−1 ≤ −(mτ−1 + 1)(∆l + αrl) + 8d(B √ λ+ σ) log\n( 2(mτ−1 + 1)\nδ\n)√ (mτ−1 + 1). (10)\nNote that nτ−1 and mτ−1 appear on the LHS and RHS of (10), respectively. The key point is that the RHS is positive only for a finite number of integers mτ−1, and thus, it has a finite upper bound. Using Lemma 9 (reported and proved in Appendix C), we prove that\nαrlnτ−1 ≤ 114d2 (B √ λ+ σ)2\n∆l + αrl ×\n[ log ( 62d(B √ λ+ σ)√\nδ(∆l + αrl)\n)]2 .\nFinally, the fact that nT = nτ = nτ−1 + 1 completes the proof.\nWe now have all the necessary ingredients to derive a regret bound on the performance of the CLUCB algorithm. We report the regret bound of CLUCB in Theorem 5, whose proof is a direct consequence of the results of Propositions 2 and 3, and Theorem 4.\nTheorem 5 Let λ ≥ max(1, D2). With probability at least 1− δ, the CLUCB algorithm satisfies the performance constraint (3) for all t ∈ N, and has the regret bound\nRT (CLUCB) = O ( d log (DT λδ )√ T + K∆h αrl ) , (11)\nwhere K is a constant that only depends on the parameters of the problem as\nK = 1 + 114d2 (B √ λ+ σ)2\n∆l + αrl\n[ log ( 62d(B √ λ+ σ)√\nδ(∆l + αrl)\n)]2 .\nRemark 2. The first term in the regret bound (11) is the regret of LUCB, which grows at the rate√ T log(T ). The second term accounts for the loss incurred by being conservative in order to satisfy the performance constraint (3). Our results indicate that this loss does not grow with time (since CLUCB acts conservatively only in a finite number of rounds). This is a clear improvement over the regret bound reported in Wu et al. [2016] for the MAB setting, in which the regret of being conservative grows with time. Furthermore, the regret bound of Theorem 5 clearly indicates that CLUCB’s regret is larger for smaller values of α. This perfectly matches the intuition that the agent must be more conservative, and thus, suffers higher regret for smaller values of α. Theorem 5 also indicates that CLUCB’s regret is smaller for smaller values of ∆h, because when the baseline policy πb is close to optimal, the algorithm does not lose much by being conservative.\nAlgorithm 2 CLUCB2 Input: α, rl,B,F Initialize: n← 0, z ← 0, w ← 0, v ← 0 and C1 ← B for t = 1, 2, 3, · · · do\nLet bt be the action suggested by πb at round t Find (a′t, θ̃) = arg max(a,θ)∈At×Ct 〈θ, φta〉 Find Rt = maxθ∈Ct〈θ, v+φtbt〉 & Lt = minθ∈Ct〈θ, z+φ t a′t 〉+αmax { minθ∈Ct〈θ, w〉, nrl } if Lt ≥ (1− α)Rt then\nPlay at = a′t and observe yt defined by (1) Set z ← z + φta′t and v ← v + φ t bt else Play at = bt and observe yt defined by (1) Set w = w + φtbt and n← n+ 1 end if Given at and yt, construct the confidence set Ct+1 according to (15)\nend for"
    }, {
      "heading" : "4 Unknown Baseline Reward",
      "text" : "In this section, we consider the case where the expected rewards of the actions taken by the baseline policy, rtbt , are unknown at the beginning. We show how the CLUCB algorithm presented in Section 3 should be changed to handle this case, and present a new algorithm, called CLUCB2. We prove a regret bound for CLUCB2, which is at the same rate as that for CLUCB. This shows that the lack of knowledge about the reward function of the baseline policy does not hurt our algorithm in terms of the rate of the regret. The pseudocode of CLUCB2 is shown in Algorithm 2. The main difference with CLUCB is in the condition that should be checked at each round t to see whether we should play the optimistic action a′t or the conservative action bt. This condition should be selected in a way that CLUCB2 satisfies constraint (3). We may rewrite (3) as∑\ni∈St−1\nriai + r t a′t\n+ α ∑\ni∈Sct−1\nribi ≥ (1− α) ( rtbt + ∑ i∈St−1 ribi ) . (12)\nIf we lower-bound the LHS and upper-bound the RHS of (12), we obtain\nmin θ∈Ct 〈θ, ∑ i∈St−1 φiai + φ t a′t 〉+ αmin θ∈Ct 〈θ, ∑ i∈Sct−1 φibi〉 ≥ (1− α) maxθ∈Ct 〈θ, ∑ i∈St−1 φibi + φ t bt〉. (13)\nSince each confidence set Ct is built in a way to contain the true parameter θ∗ with high probability, it is easy to see that (12) is satisfied whenever (13) is true.\nCLUCB2 uses both optimistic and conservative actions, and their corresponding rewards in building its confidence sets. Specifically for any t, we let Φt = [φ1a1 , φ 2 a2 , · · · , φ t at ], Yt = [y1, y2, · · · , yt]\nᵀ, Vt = λI + Φ ᵀ tΦt, and define the least-square estimate after round t as\nθ̂t = (ΦtΦ ᵀ t + λI) −1 ΦtYt. (14)\nGiven Vt and θ̂t, the confidence set for round t+ 1 is constructed as Ct+1 = { θ ∈ Ct : ‖θ − θ̂t‖Vt ≤ βt+1 } , (15)\nwhere C1 = B and βt = σ √ d log ( 1+tD2/λ\nδ\n) + B √ λ. Similar to Proposition 1, we can easily\nprove that the confidence sets built by (15) contain the true parameter θ∗ with high probability, i.e., P [ θ∗ ∈ Ct, ∀t ∈ N ] ≥ 1− δ.\nRemark 3. Note that unlike the CLUCB algorithm, here we build nested confidence sets, i.e., · · · ⊆ Ct+1 ⊆ Ct ⊆ Ct−1 ⊆ · · · , which is necessary for the proof of the algorithm. This can potentially increase the computational complexity of CLUCB2, but from a practical point of view, the confidence\nsets become nested automatically after sufficient data has been observed. Therefore, the nested constraint in building the confidence sets can be relaxed after sufficiently large number of rounds.\nThe following theorem guarantees that CLUCB2 satisfies the safety constraint (3) with high probability, while its regret has the same rate as that of CLUCB and is worse than that of LUCB only up to an additive constant.\nTheorem 6 Let λ ≥ max(1, D2) and δ ≤ 2/e. Then, with probability at least 1 − δ, CLUCB2 algorithm satisfies the performance constraint (3) for all t ∈ N, and has the regret bound\nRT (CLUCB2) = O ( d log ( DT\nλδ\n)√ T +\nK∆h α2r2l\n) , (16)\nwhere K is a constant that depends only on the parameters of the problem as\nK = 256d2(B √ λ+ σ)2 [ log ( 10d(B √ λ+ σ)\nαrl(δ)1/4\n)]2 + 1.\nWe report the proof of Theorem 6 in Appendix B. The proof follows the same steps as that of Theorem 5, with additional non-trivial technicalities that have been highlighted there."
    }, {
      "heading" : "5 Simulation Results",
      "text" : "In this section, we provide simulation results to illustrate the performance of the proposed CLUCB algorithm. We considered a time independent action set of 100 arms each having a time independent feature vector living in R4 space. These feature vectors and the parameter θ∗ are randomly drawn fromN ( 0, I4 ) such that the mean reward associated to each arm is positive. The observation noise at each time step is also generated independently from N (0, 1), and the mean reward of the baseline policy at any time is taken to be the reward associated to the 10’th best action. We have taken λ = 1, δ = 0.001 and the results are averaged over 1,000 realizations.\nIn Figure 1, we plot per-step regret (i.e., Rtt ) of LUCB and CLUCB for different values of α over a horizon T = 40, 000. Figure 1 shows that per-step regret of CLUCB remains constant at the beginning (the conservative phase). This is because during this phase, CLUCB follows the baseline policy to make sure that the performance constraint (3) is satisfied. As expected, the length of the conservative phase decreases as α is increased, since the performance constraint is relaxed for larger values of α, and hence, CLUCB starts playing optimistic actions more quickly. After this initial conservative phase, CLUCB has learned enough about the optimal action and its performance starts converging to that of LUCB. On the other hand, Figure 1 shows that per-step regret of CLUCB at the first few periods remains much lower than that of LUCB. This is because LUCB plays agnostic to the safety constraint, and thus, may select very poor actions in its initial exploration phase. In regard to this, Figure 2(a) plots the percentage of the rounds, in the first 1, 000 rounds, at which the safety constraint (3) is violated by LUCB and CLUCB for different values of α. According to this figure,\nCLUCB satisfies the performance constraint for all values of α, while LUCB fails in a significant number of rounds, specially for small values of α (i.e., tight constraint).\nTo better illustrate the effect of the performance constraint (3) on the regret of the algorithms, Figure 2(b) plots the per-step regret achieved by CLUCB at round t = 40, 000 for different values of α, as well as that for LUCB. As expected from our analysis and is shown in Figure 1, the performance of CLUCB converges to that of LUCB after an initial conservative phase. Figure 2(b) confirms that the convergence happens more quickly for larger values of α, where the constraint is more relaxed."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we studied the concept of safety in contextual linear bandits to address the challenges that arise in implementing such algorithms in practical situations such as personalized recommendation systems. Most of the existing linear bandit algorithms, such as LUCB [Abbasi-Yadkori et al., 2011], suffer from a large regret at their initial exploratory rounds. This unsafe behavior is not acceptable in many practical situations, where having a reasonable performance at any time is necessary for a learning algorithm to be considered reliable and to remain in production.\nTo guarantee safe learning, we formulated a conservative linear bandit problem, where the performance of the learning algorithm (measured in terms of its cumulative rewards) at any time is constrained to be at least as good as a fraction of the performance of a baseline policy. We proposed a conservative version of LUCB algorithm, called CLUCB, to solve this constrained problem, and showed that it satisfies the safety constraint with high probability, while achieving a regret bound equivalent to that of LUCB up to an additive time-independent constant. We designed two versions of CLUCB that can be used depending on whether the reward function of the baseline policy is known or unknown, and showed that in each case, CLUCB acts conservatively (i.e., plays the action suggested by the baseline policy) only at a finite number of rounds, which depends on how suboptimal the baseline policy is. We reported simulation results that support our analysis and show the performance of the proposed CLUCB algorithm."
    } ],
    "references" : [ {
      "title" : "Improved algorithms for linear stochastic bandits",
      "author" : [ "Y. Abbasi-Yadkori", "D. Pál", "C. Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Abbasi.Yadkori et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Abbasi.Yadkori et al\\.",
      "year" : 2011
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning Journal,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Counterfactual reasoning and learning systems: The example of computational advertising",
      "author" : [ "L. Bottou", "J. Peters", "J. Quinonero-Candela", "D. Charles", "D. Chickering", "E. Portugaly", "D. Ray", "P. Simard", "E. Snelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bottou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bottou et al\\.",
      "year" : 2013
    }, {
      "title" : "Contextual bandits with linear payoff functions",
      "author" : [ "W. Chu", "L. Li", "L. Reyzin", "R. Schapire" ],
      "venue" : "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Chu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2011
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "V. Dani", "T. Hayes", "S. Kakade" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "Safe policy improvement by minimizing robust baseline regret",
      "author" : [ "M. Petrik", "M. Ghavamzadeh", "Y. Chow" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Petrik et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Petrik et al\\.",
      "year" : 2016
    }, {
      "title" : "Linearly parameterized bandits",
      "author" : [ "P. Rusmevichientong", "J. Tsitsiklis" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Rusmevichientong and Tsitsiklis.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rusmevichientong and Tsitsiklis.",
      "year" : 2010
    }, {
      "title" : "Learning to optimize via posterior sampling",
      "author" : [ "D. Russo", "B. Van Roy" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Russo and Roy.,? \\Q2014\\E",
      "shortCiteRegEx" : "Russo and Roy.",
      "year" : 2014
    }, {
      "title" : "Batch learning from logged bandit feedback through counterfactual risk minimization",
      "author" : [ "A. Swaminathan", "T. Joachims" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Swaminathan and Joachims.,? \\Q2015\\E",
      "shortCiteRegEx" : "Swaminathan and Joachims.",
      "year" : 2015
    }, {
      "title" : "Counterfactual risk minimization: Learning from logged bandit feedback",
      "author" : [ "A. Swaminathan", "T. Joachims" ],
      "venue" : "In Proceedings of The 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Swaminathan and Joachims.,? \\Q2015\\E",
      "shortCiteRegEx" : "Swaminathan and Joachims.",
      "year" : 2015
    }, {
      "title" : "High confidence off-policy evaluation",
      "author" : [ "P. Thomas", "G. Theocharous", "M. Ghavamzadeh" ],
      "venue" : "In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence,",
      "citeRegEx" : "Thomas et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Thomas et al\\.",
      "year" : 2015
    }, {
      "title" : "High confidence policy improvement",
      "author" : [ "P. Thomas", "G. Theocharous", "M. Ghavamzadeh" ],
      "venue" : "In Proceedings of the Thirty-Second International Conference on Machine Learning,",
      "citeRegEx" : "Thomas et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Thomas et al\\.",
      "year" : 2015
    }, {
      "title" : "Conservative bandits",
      "author" : [ "Y. Wu", "R. Shariff", "T. Lattimore", "C. Szepesvári" ],
      "venue" : "In Proceedings of The 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Wu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "This setting has been studied in the multi-armed bandit (MAB) [Wu et al., 2016].",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "[2016] considered the baseline policy as a fixed arm in MAB, formulated safety using a constraint defined based on the performance of the baseline policy (mean of the baseline arm), and modified the UCB algorithm [Auer et al., 2002] to satisfy this constraint.",
      "startOffset" : 213,
      "endOffset" : 232
    }, {
      "referenceID" : 0,
      "context" : "At each round, CLUCB plays the action suggested by the standard linear UCB (LUCB) algorithm (e.g., Dani et al. 2008; Rusmevichientong and Tsitsiklis 2010; Abbasi-Yadkori et al. 2011; Chu et al. 2011; Russo and Van Roy 2014), only if it satisfies the safety constraint for the worst choice of the parameter in the confidence set, and plays the action suggested by the baseline policy, otherwise.",
      "startOffset" : 92,
      "endOffset" : 223
    }, {
      "referenceID" : 3,
      "context" : "At each round, CLUCB plays the action suggested by the standard linear UCB (LUCB) algorithm (e.g., Dani et al. 2008; Rusmevichientong and Tsitsiklis 2010; Abbasi-Yadkori et al. 2011; Chu et al. 2011; Russo and Van Roy 2014), only if it satisfies the safety constraint for the worst choice of the parameter in the confidence set, and plays the action suggested by the baseline policy, otherwise.",
      "startOffset" : 92,
      "endOffset" : 223
    }, {
      "referenceID" : 7,
      "context" : "This setting has been studied in the multi-armed bandit (MAB) [Wu et al., 2016]. Wu et al. [2016] considered the baseline policy as a fixed arm in MAB, formulated safety using a constraint defined based on the performance of the baseline policy (mean of the baseline arm), and modified the UCB algorithm [Auer et al.",
      "startOffset" : 63,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "2008; Rusmevichientong and Tsitsiklis 2010; Abbasi-Yadkori et al. 2011; Chu et al. 2011; Russo and Van Roy 2014), only if it satisfies the safety constraint for the worst choice of the parameter in the confidence set, and plays the action suggested by the baseline policy, otherwise. We prove an upper-bound for the regret of CLUCB, which can be decomposed into two terms. The first term is an upper-bound on the regret of LUCB that grows at the rate √ T log(T ). The second term is constant (does not grow with the horizon T ) and accounts for the loss of being conservative in order to satisfy the safety constraint. This improves over the regret bound derived in Wu et al. [2016] for the MAB setting, where the regret of being conservative grows with time.",
      "startOffset" : 44,
      "endOffset" : 683
    }, {
      "referenceID" : 0,
      "context" : "We follow the approach of Abbasi-Yadkori et al. [2011] to build confidence sets for θ∗.",
      "startOffset" : 26,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Note that similar to the linear UCB algorithm (LUCB) in Abbasi-Yadkori et al. [2011], the sub-Gaussian parameter σ and the regularization parameter λ that appear in the definitions of βt+1 and Vt should also be given to the CLUCB algorithm as input.",
      "startOffset" : 56,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "This is a clear improvement over the regret bound reported in Wu et al. [2016] for the MAB setting, in which the regret of being conservative grows with time.",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "Most of the existing linear bandit algorithms, such as LUCB [Abbasi-Yadkori et al., 2011], suffer from a large regret at their initial exploratory rounds.",
      "startOffset" : 60,
      "endOffset" : 89
    } ],
    "year" : 2017,
    "abstractText" : "Safety is a desirable property that can immensely increase the applicability of learning algorithms in real-world decision-making problems. It is much easier for a company to deploy an algorithm that is safe, i.e., guaranteed to perform at least as well as a baseline. In this paper, we study the issue of safety in contextual linear bandits that have application in many different fields including personalized recommendation. We formulate a notion of safety for this class of algorithms. We develop a safe contextual linear bandit algorithm, called conservative linear UCB (CLUCB), that simultaneously minimizes its regret and satisfies the safety constraint, i.e., maintains its performance above a fixed percentage of the performance of a baseline strategy, uniformly over time. We prove an upper-bound on the regret of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound for the regret of the standard linear UCB algorithm that grows with the time horizon and 2) a constant term that accounts for the loss of being conservative in order to satisfy the safety constraint. We empirically show that our algorithm is safe and validate our theoretical analysis.",
    "creator" : null
  }
}