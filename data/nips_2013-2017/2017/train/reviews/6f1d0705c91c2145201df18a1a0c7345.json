{"title": "Hierarchical Implicit Models and Likelihood-Free Variational Inference", "abstract": "Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for models which encompass our understanding of the physical word. Despite this fundamental nature, the use of implicit models remains limited due to challenge in positing complex latent structure in them, and the ability to inference in such models with large data sets. In this paper, we first introduce the hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for symbol generation.", "id": "6f1d0705c91c2145201df18a1a0c7345", "authors": ["Dustin Tran", "Rajesh Ranganath", "David Blei"], "conference": "NIPS2017", "accepted": true, "reviews": [{"comments": "The paper presents an implicit variational inference method for likelihood-free inference. This approach builds on previous work and particularly on Hierarchical Variational Inference and Implicit Variational Bayes. \n\nThe key trick used in the paper is the subtraction of the log empirical distribution log q(xn) and the transformation of the ELBO in the form given by eq. 4, which suggests the use of log density ratio estimation as a tool for likelihood-free variational inference. The rest methodological details of the papers are based on standard tools, such as log density \nratio estimation, reparametrization and hierarchical variational distributions. \n\nWhile I found the trick to deal with likelihood intractability very interesting, it requires log density ratio estimation in high-dimensional spaces (in the joint space of data x_n and latent variable z_n). This is very challenging since log density ratio estimation in high dimensions is an extremely difficult problem and there is no clear evidence that the authors provide a stable algorithm to deal that. For instance, the fact that the authors have not applied their method to a standard GAN (for generating high dimensional data such as images) but instead they have constructed this rather weird Bayesian GAN for classification (see page 7) indicates that the current algorithm is very unstable. In fact it is hard to see how to stabilize the proposed \nalgorithm since initially the \u201cvariational joint\u201d will be very different from the \"real joint\" and it is precisely this situation that makes log density ratio estimation completely unreliable, leading to very biased gradients in the early crucial iterations of the optimization.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper defines a class of probability models -- hierarchical\nimplicit models -- consisting of observations with associated 'local'\nlatent variables that are conditionally independent given a set of\n'global' latent variables, and in which the observation likelihood is\nnot assumed to be tractable. It describes an approach for KL-based\nvariational inference in such 'likelihood-free' models, using a\nGAN-style discriminator to estimate the log ratio between a\n'variational joint' q(x, z), constructed using the empirical\ndistribution on observations, and the true model joint density. This\napproach has the side benefit of supporting implicit variational\nmodels ('variational programs'). Proof-of-concept applications are\ndemonstrated to ecological simulation, a Bayesian GAN, and sequence\nmodeling with a stochastic RNN.\n\nThe exposition is very clear, well cited, and the technical machinery\nis carefully explained. Although the the application of density ratio\nestimation to variational inference seems to be an idea 'in the air'\nand building blocks of this paper have appeared elsewhere (for example\nthe Adversarial VB paper), I found this synthesis to be cleaner,\neasier to follow, and more general (supporting implicit models) than\nany of the similar papers I've read so far.\n\nThe definition of hierarchical implicit models is a useful point in\ntheoretical space, and serves to introduce the setup for inference\nin section 3. However the factorization (1), which assumes iid\nobservations, is quite restrictive -- I don't believe it technically\neven includes the Lotka-Volterra or stochastic RNN models explored in\nthe paper itself! (since both have temporal dependence). It seems\nworth acknowledging that the inference approach in this paper is more\ngeneral, and perhaps discussing how it could be adapted to problems\nand models with more structured (time series, text, graph)\nobservations and/or latents.\n\nExperiments are probably the weakest point of this paper. The 'Bayesian GAN' \nis a toy and the classification setup is artificial; supervised learning is\nnot why people care about GANs. The symbol generation RNN is not\nevaluated against any other methods and it's not clear it works\nparticularly well. The Lotka-Volterra simulation is the most\ncompelling; although the model has few parameters and no latent\nvariables, it nicely motivates the notion of implicit models and shows\nclear improvement on the (ABC) state of the art.\n\nOverall there are no groundbreaking results, and much of this\nmachinery could be quite tricky to get working in practice (as with\nvanilla GANS). I wish the experiments were more compelling. \nBut the approach seems general and powerful, with the\npotential to open up entire new classes of models to effective\nBayesian inference, and the formulation in this paper will likely be\nuseful to many reasearchers as they begin to flesh it out. For that\nreason I think this paper is a valuable contribution.\n\nMisc comments and questions:\n\nLotka-Volterra model: I'm not sure the given eqns (ln 103) are\ncorrect. Shouldn't the Beta_3 be added, not subtracted, to model the\npredator birth rate? As written, dx_2/dt is always negative in\nexpectation which seems wrong. Also Beta_2 is serving double duty as\nthe predator *and* prey death rate, is this intentional? Most sources\n(including the cited Papamakarios and Murray paper) seem to use four\nindependent coefficients.\n\nline 118: \"We described two classes of implicit models\" but I only see\none? (HIMs)\nline 146: \"log empirical log q(x_n)\" is redundant\n\nSuppose we have an implicit model, but want to use an explicit\nvariational approximation (for example the mean-field Gaussian in the\nLotka-Volterra experiment). Is there any natural way to exploit the\nexplicit variational density for faster inference?\n\nSubtracting the constant log q(x) from the ELBO means the ratio\nobjective (4) no longer yields a lower bound to the true model\nevidence; this should probably be noted somewhere. Is there an\nprincipled interpretation of the quantity (4)? It is a lower bound on\nlog p(x)/q(x), which (waving hands) looks like an estimate of the\nnegative KL divergence between the model and empirical distribution --\nmaybe this is useful for model criticism?", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Thank you for an interesting read.\n\nThis paper proposed a hierarchical probabilistic model using implicit distributions. To perform posterior inference the authors also proposed a variational method based on GAN-related density ratio estimation techniques. The proposed method is evaluated with a number of different tasks including ABC, supervised learning and generative modeling.\n\nI like the idea in general but I think there are a few points that need to be made clearer.\n\n1. How is your method related to AVB [36] and ALI [13]? I can see these connections, but not all the readers you're targeting could see it easily.\n\n2. In the AVB paper they mentioned a crucial trick (adaptive contrast) to improve the density ratio estimations in high dimensions. You only did a toy case (2D linear regression) to demonstrate the stability of your method, and your findings are essentially the same as in the toy example in the AVB paper (naive density ratio estimation works well in low dimensional case). It would be better if you could provide an analysis in high dimensional case, e.g. your BNN example.\n\n3. Hinge loss: why the optimal r is the log ratio?\n\n4. Generating discrete data: yes using r(x, w) instead of r(x) could provide gradients, however this means you need to input w to the discriminator network as well. Usually you need quite a deep (and wide) neural network to generate realistic data so I presume w could be of very high dimensions. How scalable is your approach here?\n\n5. I'm a bit worried about no quantitative results for the sequence generation part. I think it's not a good practice for just including generative samples and letting the readers judge the fidelity.\n\nIn summary I think this paper is borderline. I would be happy to see clarifications if the authors think I've missed some important points.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
