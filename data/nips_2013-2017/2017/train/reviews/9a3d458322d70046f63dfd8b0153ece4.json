{"title": "Revisit Fuzzy Neural Network: Demystifying Batch Normalization and ReLU with Generalized Hamming Network", "abstract": "We revisit fuzzy neural network with a cornerstone notion of generalized hamming distance, which provides a novel and theoretically justified framework to re-interpret many useful neural network techniques in terms of fuzzy logic. In particular, we conjecture and empirically illustrate that, the celebrated batch normalization (BN) technique actually adapts the \u201cnormalized\u201d bias such that it approximates the rightful bias induced by the generalized hamming distance. Once the due bias is enforced analytically, neither the optimization of bias terms nor the sophisticated batch normalization is needed. Also in the light of generalized hamming distance, the popular rectified linear units (ReLU) can be treated as setting a minimal hamming distance threshold between network inputs and weights. This thresholding scheme, on the one hand, can be improved by introducing double-thresholding on both positive and negative extremes of neuron outputs. On the other hand, ReLUs turn out to be non-essential and can be removed from networks trained for simple tasks like MNIST classification. The proposed generalized hamming network (GHN) as such not only lends itself to rigorous analysis and interpretation within the fuzzy logic theory but also demonstrates fast learning speed, well-controlled behaviour and state-of-the-art performances on a variety of learning tasks.", "id": "9a3d458322d70046f63dfd8b0153ece4", "authors": ["Lixin Fan"], "conference": "NIPS2017", "accepted": true, "reviews": [{"comments": "The authors use a notion of generalized hamming distance, to shed light on the success of Batch normalization and ReLU units. \n\nAfter reading the paper, I am still very confused about its contribution. The authors claim that generalized hamming distance offers a better view of batch normalization and relus, and explain that in two paragraphs in pages 4,5. The explanation for batch normalization is essentially contained in the following phrase:\n\n\u201cIt turns out BN is indeed attempting to compensate for deficiencies in neuron outputs with respect to GHD. This surprising observation indeed adheres to our conjecture that an optimized neuron should faithfully measure the GHD between inputs and weights.\u201d\n\nI do not understand how this is explaining the effects or performance of batch normalization.\n\nThe authors then propose a generalized hamming network, and suggest that \"it demystified and confirmed effectiveness of practical\ntechniques such as batch normalization and ReLU\".\n\nOverall, this is a poorly written paper, with no major technical contribution, or novelty, and does not seem to provide any theoretical insights on the effectiveness of BN or ReLUs. Going beyond the unclear novelty and technical contribution, the paper is riddled with typos, grammar and syntax mistakes (below is a list from just the abstract and intro). \n\nThis is a clear rejection.\n\nTypos and grammar/syntax mistakes:\n\n\u2014\u2014 abstract \u2014\u2014 \ngeneralized hamming network (GNN)\n-> generalized hamming network (GHN)\n\n GHN not only lends itself to rigiour analysis\n-> GHN not only lends itself to rigorous analysis\n\n\u201cbut also demonstrates superior performances\u201d\n-> but also demonstrates superior performance\n\u2014\u2014 \n\n\u2014\u2014 intro \u2014\u2014 \n\u201ccomputational neutral networks\u201d\n-> computational neural networks\n\n\u201chas given birth\u201d\n-> have given birth\n\n\u201cto rectifying misunderstanding of neural computing\u201d\n-> not sure what the authors are trying to say\n\n\nOnce the appropriate rectification is applied ,\n-> Once the appropriate rectification is applied,\n\nthe ill effects of internal covariate shift is automatically eradicated\n-> the ill effects of internal covariate shift are automatically eradicated\n\nThe resulted learning process\n-> The resulting learning process\n\nlends itself to rigiour analysis\n-> lends itself to rigorous analysis\n\nthe flexaible knowledge\n-> the flexible knowledge\n\nare equivalent and convertible with other\n-> are equivalent and convertible with others, or other architectures?\n\nsuccessful applications of FNN\n-> successful applications of FNNs", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This is a beautiful paper that interprets batch normalization and relu in terms of generalized hamming distance network and develops variations that improves. This connection is surprising (esp fig 2) and very intereting.\n\nThe correspondence between ghd and bn is interesting. However, it is also a bit non-obvious why this is true. It seems to me that the paper claims that in practice the estimated bias is equal to sum_w and sum_x in (3) and the whole wx+b equates the ghd. However, is it on avearge across all nodes in one layer? Also how does it vary across layers? Does the ghd mainly serve to ensure that there is no information loss going from x to w.? It is a little hard to imagine why we want layers of ghd stacked together. Any explanation in this direction could be helpful.\n\nA minor thing: typo in the crucial box in line 80\n\nOverall a great paper.\n\n----------------------\nAfter author rebuttal:\n\nMany thanks for the rebuttal from the authors. My scores remain the same. Thanks for the beautiful paper!", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper explores generalized hamming distance in the context of fuzzy neural networks. The paper shows that the neural computation at each neuron can be viewed as calculating the generalized hamming distance between the corresponding weight vector and input vector. This view point is useful as it automatically gives candidate bias parameter at the neuron and learning of additional parameters (using batch normalization) is not required during the training. Furthermore, this view also allows the authors to understand the role of rectified linear units and propose a double-thresholding scheme to make the training process fast and stable. Based on these two observations, the paper proposes generalized hamming networks (GHN) where each neuron exactly computes the generalized hamming distance. These networks also utilize non-linear activations. \n\nThe paper then presents simulation results showcasing the performance of the proposed GHN for image classification and auto-encoder design. The simulation results demonstrate that GHN have fast learning process and achieve very good performances on the underlying tasks. Interestingly, the simulation results show that the GHN do not necessarily require max-pooling. \n\nThe paper is well written and explains the main ideas in a clear manner. The simulation results are also convincing. However, it appears that the plots are not complete, e.g., Fig. 4.2 does not show a curve for learning rate = 0.0001. Even though I would have preferred a more comprehensive (both theoretical and practical) evaluations, I over all find the idea of GHN interesting.\n\nMinor comments:\n\n1) In abstract, '(GNN)' should be '(GHN)'.\n2) In line 55 of page 2, 'felxaible' should be 'flexible'.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
