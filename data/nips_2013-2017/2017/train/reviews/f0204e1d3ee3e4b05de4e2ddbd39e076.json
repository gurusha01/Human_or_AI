{"title": "A framework for Multi-A(rmed)/B(andit) Testing with Online FDR Control", "abstract": "We propose an alternative framework to existing setups for controlling false alarms when multiple A/B tests are run over time. This setup arises in many practical applications, e.g. when pharmaceutical companies test new treatment options against control pills for different diseases, or when internet companies test their default webpages versus various alternatives over time. Our framework proposes to replace a sequence of A/B tests by a sequence of best-arm MAB instances, which can be continuously monitored by the data scientist. When interleaving the MAB tests with an online false discovery rate (FDR) algorithm, we can obtain the best of both worlds: low sample complexity and any time online FDR control. Our main contributions are: (i) to propose reasonable definitions of a null hypothesis for MAB instances; (ii) to demonstrate how one can derive an always-valid sequential p-value that allows continuous monitoring of each MAB test; and (iii) to show that using rejection thresholds of online-FDR algorithms as the confidence levels for the MAB algorithms results in both sample-optimality, high power and low FDR at any point in time. We run extensive simulations to verify our claims, and also report results on real data collected from the New Yorker Cartoon Caption contest.", "id": "f0204e1d3ee3e4b05de4e2ddbd39e076", "authors": ["Fanny Yang", "Aaditya Ramdas", "Kevin G. Jamieson", "Martin J. Wainwright"], "conference": "NIPS2017", "accepted": true, "reviews": [{"comments": "The paper looks at continuous improvement using a sequence of A/B tests, and proposes instead to implement adaptive testing such as \n multi-armed bandit problem while controlling the false discovery rate. This is an important problem discussed in statistical literature, but still unsolved.\n\n The approach proposed in this paper seems to apparently solve the issues. This is a very interesting paper, that despite minor concerns listed below, could lead to a potentially new avenue of research.\n \n Lines 24-37: There are well-known issues, and it would be desirable to add citations. Although authors clearly focus on CS/ML literature, there is also a relevant body of literature in biometrics, see e.g. survey by Villar, Bowden and Wason (Statistical Science 2015), the references therein and the more recent papers citing this survey.\n Line 37: \"testing multiple literature\" -> \"multiple testing literature\" \n Line 38-47: A similar concept exists in biometrics, called \"platform trials\" - please describe how your concept differs\n Line 112: \"and and\" -> \"and\"\n Line 115: please provide reference for and description of LUCB\n Line 153: \"samplesas\" -> \"samples as\"\n Line 260: \"are ran\" -> ?\n Line 273: It is not clear what \"truncation time\" is and why it is introduced - it seems to have a huge effect on the results in Figure 2\n Line 288-291: While this motivation is interesting, it seems to be mentioned at an inappropriate place in the paper - why not to do it in the introduction, alongside website management and clinical trials?", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "If no proper correction is applied, the repetition of individually well-grounded high-confidence tests is known to lead irremediably to absurd \"statistically certified\" discoveries.\nFalse discovery rate control (FDR) is an alternative of family-wise test corrections like Bonferroni's which are known to be too conservative.\n\nThis paper propose a general framework for repeated A/B/n tests which integrates several concrete ideas:\n- introduce a \"best-arm identification with control\" variant of the (epsilon,delta)-PAC best-arm identification problem that we could also name \"better-arm identification\";\n- replace inefficient static A/B/n testing procedures by adaptive PAC \"better-arm\" algorithms;\n- propose a variant of the LUCB algorithm for this purpose;\n- integrate anytime p-values calculus for continuous FDR control.\n\nThe proposed \"better-arm identification\" algorithm is analyzed in section 3.2.\nThe proposed meta-procedures are analyzed and shown to guarantee this FRD control.\nSome experiments are provided at the end of the paper.\n\nAt first sight I was a bit surprised by this strongly application-oriented cross-discipline paper, but I really liked the fresh ideas and the concrete perspectives they give for MABs both on the applicative and theoretic grounds.\n\nTypo:\nl37 \"testing multiple\" -> \"multiple testing \"\nl153 \"samplesas\" -> \"samples as\"", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper studies sequential hypothesis testing problems, where the decision maker processes hypotheses sequentially (as opposed to processing them jointly). The objective is to control false discovery rate (FDR) or modified FDR (mFDR), and also keep the best arm discovery rate (BDR) high. \n\nThe proposed algorithm is based on a combination of pure-exploration multi-armed bandit (MAB) methods and results from online FDR control. Using an appropriate pure exploration method ensures that BDR is high, while the online FDR method ensure low FDR. The main contribution of the paper is an appropriate combination of these two components that guarantees simultaneous control of FDR and BDR. \n\nOverall, the paper is well-written and the ideas are very clear and easy to follow. The technical contribution is interesting and relevant to NIPS community.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
