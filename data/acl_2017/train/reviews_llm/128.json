{
    "reviews": [
        {
            "comments": "**Strengths:**\nThe paper presents a novel approach named Knowledge-Guided Structural Attention Networks (K-SAN) that integrates external linguistic and semantic knowledge into an end-to-end trainable neural network structure for natural language understanding (NLU) tasks. The main strengths of the paper are as follows:\n- **Novelty and originality:** The proposed K-SAN model is innovative in its use of general knowledge as guidance in an end-to-end learning process with an attention mechanism. The authors state that this is the first approach of its kind.\n- **Technical soundness:** The methodology appears to be solid, with the model leveraging prior knowledge to guide attention and improve the understanding of sentence structures. The use of different knowledge encoding techniques (NN, RNN, CNN) and the integration of this knowledge into a recurrent neural network tagger demonstrate a robust approach to the NLU problem.\n- **Clarity and presentation:** The paper is well-structured and presents a clear explanation of the K-SAN model, including its components and how it differs from previous models. The use of figures to illustrate the architecture and knowledge-guided substructures aids in the comprehension of the model.\n- **Reproducibility and impact:** By providing sufficient details about the model architecture, experiments, and datasets used (the ATIS corpus), the paper sets a foundation for reproducibility. The performance gains reported over baseline models suggest that K-SAN could have a significant impact on the field of NLU and potentially other areas in NLP.\n\n**Weaknesses:**\n- **Limited evaluation scope:** The evaluation focuses on the ATIS corpus, which, while standard, is somewhat limited in scope. It would be beneficial to see the model's performance on a variety of datasets and in different NLU contexts to fully understand its generalizability.\n- **Potential overfitting concerns:** While the paper claims better generalization, the risk of overfitting to the peculiarities of the ATIS dataset or to specific knowledge structures is not thoroughly discussed. It would be helpful to include an analysis of the model's robustness against overfitting, especially when training with smaller datasets.\n- **Comparison with state-of-the-art:** Though the model outperforms the stated baselines, it is not clear how it compares to the absolute state-of-the-art in NLU, considering models that might not use knowledge-guided structures. A broader comparison would strengthen the case for K-SAN's effectiveness.\n\n**General discussion:**\nThe paper makes a compelling case for the use of knowledge-guided attention in improving NLU systems. However, the authors could expand on the potential limitations of their approach and discuss how the model performs with noisy or incorrect external knowledge. Furthermore, exploring the model's adaptability to different languages or domains could provide insights into its versatility. Future work might also focus on reducing the computational complexity and exploring the interpretability of the attention mechanism within the model.\n\n**Overall Evaluation:**\nThe paper introduces a significant contribution to the field of NLU with the K-SAN model, demonstrating improved performance on the ATIS benchmark. The innovative use of external knowledge to guide attention in neural networks is both novel and technically sound. While there are areas for expansion in future work, the current results are promising and indicate that K-SAN is a valuable step forward in developing more sophisticated and accurate NLU systems."
        }
    ]
}