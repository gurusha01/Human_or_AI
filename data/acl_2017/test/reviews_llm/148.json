{
    "reviews": [
        {
            "comments": "\n\nSummary:\nThe paper investigates the relationship between the difficulty of answering questions and the readability of text in reading comprehension (RC) tasks. It introduces two classes of evaluation metrics: prerequisite skills and readability, based on psychological research on human text comprehension. The authors annotate six existing RC datasets with these metrics to analyze the quality of datasets and the performance of systems.\n\nStrengths:\n1. The paper addresses an important gap in the evaluation of RC datasets by considering both the difficulty of questions and the readability of texts.\n2. It provides a comprehensive set of metrics for prerequisite skills and readability, grounded in psychological research, which could lead to more nuanced evaluations of RC systems.\n3. The annotation of six RC datasets with these metrics is a significant contribution, offering insights into the characteristics of each dataset and guiding future dataset construction and system development.\n4. The paper makes the annotation results publicly available, which is beneficial for the research community.\n\nWeaknesses:\n1. The paper's methodology does not evaluate the competence of selecting sentences necessary to answer questions, which is a critical aspect of RC.\n2. There is a lack of empirical validation of the proposed metrics against actual system performance, which would strengthen the argument for their utility.\n3. The readability metrics are based on features proposed for human readability and may not correlate directly with the difficulty for RC systems, which the authors acknowledge but do not address in depth.\n\nQuestions and Suggestions:\n1. Could the authors provide empirical evidence showing that the proposed metrics correlate with the performance of state-of-the-art RC systems?\n2. It would be beneficial to extend the methodology to evaluate the competence of selecting relevant sentences, as this is a crucial part of RC tasks.\n3. The authors might consider exploring how the readability metrics could be adapted or extended to better reflect the challenges faced by RC systems.\n\nNovelty and Originality:\nThe paper's approach to evaluating RC datasets using two classes of metrics based on human text comprehension is novel and provides a fresh perspective on the evaluation of RC tasks.\n\nTechnical Soundness:\nThe methodology appears sound, but the lack of empirical validation with RC systems is a limitation. The annotation process is well-defined, and the high inter-annotator agreement suggests reliability.\n\nClarity and Presentation:\nThe paper is well-structured and clearly written. The figures and tables effectively support the analysis and findings.\n\nReproducibility:\nThe authors have made their annotation results publicly available, which is commendable. However, more details on the annotation process and the selection of annotators could improve reproducibility.\n\nImpact:\nThe study has the potential to impact the development of RC systems by providing a more nuanced understanding of dataset difficulty. It could also influence the creation of future RC datasets that better challenge and evaluate RC systems.\n\nOverall, the paper makes a valuable contribution to the field of NLP by proposing a novel approach to evaluating RC datasets. However, further work is needed to validate the proposed metrics and extend the methodology to cover additional aspects of RC tasks.\n\n"
        }
    ]
}