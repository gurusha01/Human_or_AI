{
    "reviews": [
        {
            "comments": "\n\n**Summary:**\nThe paper investigates what neural machine translation (NMT) models learn about word structure, focusing on morphology and part-of-speech (POS) tagging. The authors train NMT systems on parallel corpora and use the trained models to extract features for words, which are then used to train classifiers for POS and morphological tagging. They explore the effects of word vs. character representations, encoder depth, and target language on the learned representations.\n\n**Strengths:**\n1. The paper addresses an important and under-explored question in the understanding of NMT systems, specifically how they capture word structure and morphology.\n2. The methodology is sound, using a data-driven approach to quantitatively measure the quality of word representations learned by NMT systems.\n3. The experiments are thorough, covering several languages with varying morphological richness and different NMT configurations.\n4. The findings are insightful, revealing that character-based representations outperform word-based ones for morphology, especially for infrequent words, and that lower layers of the encoder capture word structure better than higher layers.\n\n**Weaknesses:**\n1. The paper could benefit from a broader comparison with other types of sub-word representations, such as byte-pair encoding.\n2. While the paper focuses on morphology and POS tagging, it would be interesting to see if the findings extend to more semantically-oriented tasks.\n3. The impact of the target language on learned representations is intriguing, but the paper could delve deeper into the reasons behind these observations.\n\n**Questions and Suggestions:**\n1. Could the authors extend their analysis to include other sub-word encoding methods and compare their effectiveness in learning morphology?\n2. It would be beneficial to see an extension of this work to semantic tasks to understand if the insights gained from morphology and POS tagging generalize to other aspects of language understanding.\n3. The paper could explore in more detail why translating into morphologically-poor languages leads to better source-side representations, perhaps by examining the interplay between the complexity of the target language and the quality of the source language representations.\n\n**Evaluation of Specific Aspects:**\n1. **Novelty and Originality:** The paper presents novel insights into the learning dynamics of NMT systems with respect to word structure and morphology.\n2. **Technical Soundness:** The methodology is technically sound, with a clear experimental setup and a logical approach to evaluating the learned representations.\n3. **Clarity and Presentation:** The paper is well-structured and clearly written, with informative figures and tables that aid in understanding the results.\n4. **Reproducibility:** The paper provides sufficient detail for reproducibility, though access to the datasets and code would further enhance this aspect.\n5. **Impact:** The findings have the potential to influence the design of future NMT systems, particularly in improving the handling of morphology, which is crucial for translating between morphologically-diverse languages.\n\n**Overall, this paper makes a significant contribution to the understanding of NMT systems, particularly in how they learn and represent morphological information. The insights provided could be valuable for the development of more linguistically-informed NMT architectures.**\n\n"
        }
    ]
}